{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeN8KXdA7my6",
        "outputId": "1b100f2b-6d90-47dd-f50d-4078a83e15e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MaskDINO'...\n",
            "remote: Enumerating objects: 395, done.\u001b[K\n",
            "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 395 (delta 121), reused 101 (delta 101), pack-reused 219 (from 1)\u001b[K\n",
            "Receiving objects: 100% (395/395), 2.29 MiB | 5.00 MiB/s, done.\n",
            "Resolving deltas: 100% (189/189), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/IDEA-Research/MaskDINO.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MaskDINO\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "waFuIZQP7rVL",
        "outputId": "4a64db20-3b90-474b-c10d-2cf7d9729f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (3.0.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2.1.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.0.18)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.14.0)\n",
            "Collecting submitit (from -r requirements.txt (line 6))\n",
            "  Downloading submitit-1.5.3-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.11/dist-packages (from scipy->-r requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm->-r requirements.txt (line 4)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm->-r requirements.txt (line 4)) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm->-r requirements.txt (line 4)) (0.33.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->-r requirements.txt (line 4)) (0.5.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from submitit->-r requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.11/dist-packages (from submitit->-r requirements.txt (line 6)) (4.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 7)) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 7)) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2025.6.11)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 7)) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (1.1.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm->-r requirements.txt (line 4)) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm->-r requirements.txt (line 4)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm->-r requirements.txt (line 4)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm->-r requirements.txt (line 4)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm->-r requirements.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm->-r requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (2025.7.14)\n",
            "Downloading submitit-1.5.3-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: submitit, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 submitit-1.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/make.sh\n",
        "#!/usr/bin/env bash\n",
        "# ------------------------------------------------------------------------------------------------\n",
        "# Deformable DETR\n",
        "# Copyright (c) 2020 SenseTime. All Rights Reserved.\n",
        "# Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n",
        "# ------------------------------------------------------------------------------------------------\n",
        "# Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n",
        "# ------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# Modified by Bowen Cheng from https://github.com/fundamentalvision/Deformable-DETR\n",
        "\n",
        "# TORCH_CUDA_ARCH_LIST='8.0' FORCE_CUDA=1 python setup.py build install\n",
        "\n",
        "python setup.py build install\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L10RtYhW7rXP",
        "outputId": "ba2fd131-643d-4c17-fd03-76c967311ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/make.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### install requirements\n"
      ],
      "metadata": {
        "id": "rv5LSWRZV2sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5cK6fBHV7vC",
        "outputId": "bb38ae74-1630-4adc-f244-331f6d39f0a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121 --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LT6lkXuwWHxo",
        "outputId": "95b9a420-7350-48e9-f14d-91628fb502f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.1.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.1.0%2Bcu121-cp311-cp311-linux_x86_64.whl (2200.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m207.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.16.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.16.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m264.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.1.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.1.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m289.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.3.0)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m284.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (2025.7.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Installing collected packages: triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed torch-2.1.0+cu121 torchaudio-2.1.0+cu121 torchvision-0.16.0+cu121 triton-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MaskDINO/maskdino/modeling/pixel_decoder/ops\n",
        "!sh make.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKOnSmcJ7rZL",
        "outputId": "9836c216-6135-47ca-f8c3-fc576f23ba90",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/ops\n",
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/setup.py\", line 15, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1382, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "running build\n",
            "running build_py\n",
            "creating build/lib.linux-x86_64-cpython-311/modules\n",
            "copying modules/__init__.py -> build/lib.linux-x86_64-cpython-311/modules\n",
            "copying modules/ms_deform_attn.py -> build/lib.linux-x86_64-cpython-311/modules\n",
            "creating build/lib.linux-x86_64-cpython-311/functions\n",
            "copying functions/ms_deform_attn_func.py -> build/lib.linux-x86_64-cpython-311/functions\n",
            "copying functions/__init__.py -> build/lib.linux-x86_64-cpython-311/functions\n",
            "running build_ext\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:502: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "building 'MultiScaleDeformableAttention' extension\n",
            "creating build/temp.linux-x86_64-cpython-311/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cpu\n",
            "creating build/temp.linux-x86_64-cpython-311/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda\n",
            "x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DWITH_CUDA -I/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cpu/ms_deform_attn_cpu.cpp -o build/temp.linux-x86_64-cpython-311/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cpu/ms_deform_attn_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu -o build/temp.linux-x86_64-cpython-311/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(266)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_im2col_cuda(cudaStream_t, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 69 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(767)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 139 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(877)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 139 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(336)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 139 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(441)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 139 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(549)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 139 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(654)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 139 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor ms_deform_attn_cuda_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:39:61:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   39 |     AT_ASSERTM(value.type().is_cuda(), \"value must\u001b[01;35m\u001b[K be a CUDA t\u001b[m\u001b[Kensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:40:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   40 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"s\u001b[01;35m\u001b[Kpatial_shapes must be\u001b[m\u001b[K a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:41:73:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   41 |     AT_ASSERTM(level_start_index.type().is_cuda(),\u001b[01;35m\u001b[K \"level_start_index must\u001b[m\u001b[K be a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:42:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   42 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sam\u001b[01;35m\u001b[Kpling_loc must be a\u001b[m\u001b[K CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:43:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   43 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn\u001b[01;35m\u001b[K_weight must be a \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_\u001b[01;35m\u001b[KTYPES(value.ty\u001b[m\u001b[Kpe(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K         \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/Dispatch.h:109:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:1064:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:1150:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:1193:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:1226:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:1309:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:1467:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:2348:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:2434:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:2477:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:2509:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:2591:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:2748:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> ms_deform_attn_cuda_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:105:61:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  105 |     AT_ASSERTM(value.type().is_cuda(), \"value must\u001b[01;35m\u001b[K be a CUDA t\u001b[m\u001b[Kensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:106:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  106 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"s\u001b[01;35m\u001b[Kpatial_shapes must be\u001b[m\u001b[K a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:107:73:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  107 |     AT_ASSERTM(level_start_index.type().is_cuda(),\u001b[01;35m\u001b[K \"level_start_index must\u001b[m\u001b[K be a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:108:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  108 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sam\u001b[01;35m\u001b[Kpling_loc must be a\u001b[m\u001b[K CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:109:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  109 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn\u001b[01;35m\u001b[K_weight must be a \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:110:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  110 |     AT_ASSERTM(grad_output.type().is_cuda(), \"grad\u001b[01;35m\u001b[K_output must be a \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_\u001b[01;35m\u001b[KTYPES(value.ty\u001b[m\u001b[Kpe(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:164:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K         \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/Dispatch.h:109:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1074:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1100:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1186:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1229:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1262:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1506:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1590:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1678:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:2620:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:2645:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:2731:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:2774:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:2806:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:2888:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:3048:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:3131:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:3218:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DWITH_CUDA -I/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.cpp -o build/temp.linux-x86_64-cpython-311/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "In file included from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.cpp:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/ms_deform_attn.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor ms_deform_attn_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/ms_deform_attn.h:34:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   34 |     if (\u001b[01;35m\u001b[Kvalue.type()\u001b[m\u001b[K.is_cuda())\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/variable.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/extension.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cpu/ms_deform_attn_cpu.h:17\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/ms_deform_attn.h:18\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.cpp:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.cpp:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/ms_deform_attn.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> ms_deform_attn_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/ms_deform_attn.h:56:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   56 |     if (\u001b[01;35m\u001b[Kvalue.type()\u001b[m\u001b[K.is_cuda())\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/variable.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/torch/extension.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cpu/ms_deform_attn_cpu.h:17\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/ms_deform_attn.h:18\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.cpp:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions build/temp.linux-x86_64-cpython-311/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cpu/ms_deform_attn_cpu.o build/temp.linux-x86_64-cpython-311/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.o build/temp.linux-x86_64-cpython-311/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.o -L/usr/local/lib/python3.11/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/MultiScaleDeformableAttention.cpython-311-x86_64-linux-gnu.so\n",
            "running install\n",
            "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating MultiScaleDeformableAttention.egg-info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-cpython-311/MultiScaleDeformableAttention.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-311/modules/__init__.py -> build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-311/modules/ms_deform_attn.py -> build/bdist.linux-x86_64/egg/modules\n",
            "creating build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-311/functions/ms_deform_attn_func.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-311/functions/__init__.py -> build/bdist.linux-x86_64/egg/functions\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/__init__.py to __init__.cpython-311.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/ms_deform_attn.py to ms_deform_attn.cpython-311.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/ms_deform_attn_func.py to ms_deform_attn_func.cpython-311.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/__init__.py to __init__.cpython-311.pyc\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-311-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-311.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-311: module references __file__\n",
            "creating dist\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.11/dist-packages/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg to /usr/local/lib/python3.11/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CMD2s8GRX2oz",
        "outputId": "cb70bc14-2563-4306-d9e6-3011e05ddb82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "452a38227a5f40299212e09b4b6d8484"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/detectron2.git\n",
        "%cd /content/detectron2\n",
        "!pip install -e .\n",
        "\n",
        "!pip install git+https://github.com/cocodataset/panopticapi.git\n",
        "!pip install git+https://github.com/mcordts/cityscapesScripts.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ghnmUb2ePYdE",
        "outputId": "6c2ca625-7567-49d3-a4de-ec3f071ddb13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'detectron2'...\n",
            "remote: Enumerating objects: 15900, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 15900 (delta 55), reused 21 (delta 21), pack-reused 15802 (from 4)\u001b[K\n",
            "Receiving objects: 100% (15900/15900), 6.45 MiB | 10.36 MiB/s, done.\n",
            "Resolving deltas: 100% (11564/11564), done.\n",
            "/content/detectron2\n",
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mObtaining file:///content/detectron2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.0.10)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.18.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (8.2.1)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (4.3.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.73.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.8.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=2c1b6f5eb2967bd0750f55238f7f44c80c7a0b1e6c79ff39ad4254d23a648122\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "Successfully built fvcore\n",
            "Installing collected packages: yacs, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "  Running setup.py develop for detectron2\n",
            "Successfully installed black-25.1.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-0.12.1 portalocker-3.2.0 yacs-0.1.8\n",
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/cocodataset/panopticapi.git\n",
            "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-req-build-b6ehz87u\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/panopticapi.git /tmp/pip-req-build-b6ehz87u\n",
            "  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from panopticapi==0.1) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from panopticapi==0.1) (11.3.0)\n",
            "Building wheels for collected packages: panopticapi\n",
            "  Building wheel for panopticapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for panopticapi: filename=panopticapi-0.1-py3-none-any.whl size=8259 sha256=8b1d54a32b801bb38fb465a0fad236a4b09e372d7c1a9af45d8ea9736f76c355\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-97pnf3fo/wheels/1e/dc/23/d70628297e507c01e9be79a815856549c351a79f86a1af064d\n",
            "Successfully built panopticapi\n",
            "Installing collected packages: panopticapi\n",
            "Successfully installed panopticapi-0.1\n",
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/mcordts/cityscapesScripts.git\n",
            "  Cloning https://github.com/mcordts/cityscapesScripts.git to /tmp/pip-req-build-d96f3lda\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mcordts/cityscapesScripts.git /tmp/pip-req-build-d96f3lda\n",
            "  Resolved https://github.com/mcordts/cityscapesScripts.git to commit 5d3ad48726f28d07b5771509be113dd1bed9cb08\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from cityscapesScripts==2.2.4) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from cityscapesScripts==2.2.4) (3.10.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from cityscapesScripts==2.2.4) (11.3.0)\n",
            "Collecting appdirs (from cityscapesScripts==2.2.4)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pyquaternion (from cityscapesScripts==2.2.4)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting coloredlogs (from cityscapesScripts==2.2.4)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from cityscapesScripts==2.2.4) (4.67.1)\n",
            "Collecting typing (from cityscapesScripts==2.2.4)\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from cityscapesScripts==2.2.4) (2.32.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->cityscapesScripts==2.2.4)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cityscapesScripts==2.2.4) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cityscapesScripts==2.2.4) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cityscapesScripts==2.2.4) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cityscapesScripts==2.2.4) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cityscapesScripts==2.2.4) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cityscapesScripts==2.2.4) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cityscapesScripts==2.2.4) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->cityscapesScripts==2.2.4) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->cityscapesScripts==2.2.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->cityscapesScripts==2.2.4) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->cityscapesScripts==2.2.4) (2025.7.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->cityscapesScripts==2.2.4) (1.17.0)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: cityscapesScripts, typing\n",
            "  Building wheel for cityscapesScripts (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cityscapesScripts: filename=cityscapesScripts-2.2.4-py3-none-any.whl size=473635 sha256=f91f94eb67169f0ce83584c3b88b2075982344cfcf3c2e88550c94c0075aa816\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qqkmi8iw/wheels/49/be/03/2e8c5ed96636480849595a5a12144c460a94404154ba78757e\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26304 sha256=06b20302fc54d7009b28291d58d8826a782ae8b036fac5c6e05e9e19cd0002b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/67/2f/53e3ef32ec48d11d7d60245255e2d71e908201d20c880c08ee\n",
            "Successfully built cityscapesScripts typing\n",
            "Installing collected packages: appdirs, typing, pyquaternion, humanfriendly, coloredlogs, cityscapesScripts\n",
            "Successfully installed appdirs-1.4.4 cityscapesScripts-2.2.4 coloredlogs-15.0.1 humanfriendly-10.0 pyquaternion-0.9.9 typing-3.7.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              },
              "id": "7cca8a6cdcc54878bd04c0884e83fe34"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show detectron2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5LVkBMSa9Jq",
        "outputId": "cc62f523-24e2-4b80-8f62-fe756d7d7a7d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mName: detectron2\n",
            "Version: 0.6\n",
            "Summary: Detectron2 is FAIR's next-generation research platform for object detection and segmentation.\n",
            "Home-page: https://github.com/facebookresearch/detectron2\n",
            "Author: FAIR\n",
            "Author-email: \n",
            "License: \n",
            "Location: /content/detectron2\n",
            "Editable project location: /content/detectron2\n",
            "Requires: black, cloudpickle, fvcore, hydra-core, iopath, matplotlib, omegaconf, packaging, Pillow, pycocotools, tabulate, tensorboard, termcolor, tqdm, yacs\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download weight, pretrained and dataset"
      ],
      "metadata": {
        "id": "4f2eD0pPHnjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MaskDINO\n",
        "!mkdir weights\n",
        "!mkdir pretrained\n",
        "%cd pretrained\n",
        "!wget https://download.pytorch.org/models/resnet50-19c8e357.pth\n",
        "!wget https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth\n",
        "%cd ../weights\n",
        "!wget https://github.com/IDEA-Research/OpenSeeD/releases/download/ade20k_swinl/openseed_ade20k_swinl.pt\n",
        "!wget https://github.com/IDEA-Research/detrex-storage/releases/download/maskdino-v0.1.0/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEikCBEG7rbL",
        "outputId": "f9223db4-96b7-4d13-efc9-aa57c497f70e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO\n",
            "/content/MaskDINO/pretrained\n",
            "--2025-07-27 14:05:35--  https://download.pytorch.org/models/resnet50-19c8e357.pth\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 18.65.25.45, 18.65.25.85, 18.65.25.126, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|18.65.25.45|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102502400 (98M) [application/octet-stream]\n",
            "Saving to: ‘resnet50-19c8e357.pth’\n",
            "\n",
            "resnet50-19c8e357.p 100%[===================>]  97.75M   345MB/s    in 0.3s    \n",
            "\n",
            "2025-07-27 14:05:36 (345 MB/s) - ‘resnet50-19c8e357.pth’ saved [102502400/102502400]\n",
            "\n",
            "--2025-07-27 14:05:36--  https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/357198522/6c765b00-9bd4-11eb-8f17-e28fbfaad866?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-27T15%3A05%3A58Z&rscd=attachment%3B+filename%3Dswin_large_patch4_window12_384_22k.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-27T14%3A05%3A36Z&ske=2025-07-27T15%3A05%3A58Z&sks=b&skv=2018-11-09&sig=GLzmfFwtWLVD0daYNNCZRJzAsrP%2BMlDYntw5oWNjJAg%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MzYyNTQzNiwibmJmIjoxNzUzNjI1MTM2LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.inGHIwojWYx-QJw7qggXLWoEyWi15vL7fOpzplYPp6U&response-content-disposition=attachment%3B%20filename%3Dswin_large_patch4_window12_384_22k.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-07-27 14:05:36--  https://release-assets.githubusercontent.com/github-production-release-asset/357198522/6c765b00-9bd4-11eb-8f17-e28fbfaad866?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-27T15%3A05%3A58Z&rscd=attachment%3B+filename%3Dswin_large_patch4_window12_384_22k.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-27T14%3A05%3A36Z&ske=2025-07-27T15%3A05%3A58Z&sks=b&skv=2018-11-09&sig=GLzmfFwtWLVD0daYNNCZRJzAsrP%2BMlDYntw5oWNjJAg%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MzYyNTQzNiwibmJmIjoxNzUzNjI1MTM2LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.inGHIwojWYx-QJw7qggXLWoEyWi15vL7fOpzplYPp6U&response-content-disposition=attachment%3B%20filename%3Dswin_large_patch4_window12_384_22k.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 928819451 (886M) [application/octet-stream]\n",
            "Saving to: ‘swin_large_patch4_window12_384_22k.pth’\n",
            "\n",
            "swin_large_patch4_w 100%[===================>] 885.79M   113MB/s    in 12s     \n",
            "\n",
            "2025-07-27 14:05:48 (74.8 MB/s) - ‘swin_large_patch4_window12_384_22k.pth’ saved [928819451/928819451]\n",
            "\n",
            "/content/MaskDINO/weights\n",
            "--2025-07-27 14:05:49--  https://github.com/IDEA-Research/OpenSeeD/releases/download/ade20k_swinl/openseed_ade20k_swinl.pt\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/613983414/e7a2167f-3c3e-4d76-bcc1-414a49dcad90?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-27T14%3A59%3A39Z&rscd=attachment%3B+filename%3Dopenseed_ade20k_swinl.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-27T13%3A58%3A47Z&ske=2025-07-27T14%3A59%3A39Z&sks=b&skv=2018-11-09&sig=WQZv8J3PgF1MPvgBEr8Y7yY%2FrS%2BUsnb2DJQ30Yd9yEM%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MzYyNTQ0OSwibmJmIjoxNzUzNjI1MTQ5LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.TNa2yKZ7FZAwOwbZaZoSdci3HD-YeSjBeKl0GSBv6eM&response-content-disposition=attachment%3B%20filename%3Dopenseed_ade20k_swinl.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-07-27 14:05:49--  https://release-assets.githubusercontent.com/github-production-release-asset/613983414/e7a2167f-3c3e-4d76-bcc1-414a49dcad90?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-27T14%3A59%3A39Z&rscd=attachment%3B+filename%3Dopenseed_ade20k_swinl.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-27T13%3A58%3A47Z&ske=2025-07-27T14%3A59%3A39Z&sks=b&skv=2018-11-09&sig=WQZv8J3PgF1MPvgBEr8Y7yY%2FrS%2BUsnb2DJQ30Yd9yEM%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MzYyNTQ0OSwibmJmIjoxNzUzNjI1MTQ5LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.TNa2yKZ7FZAwOwbZaZoSdci3HD-YeSjBeKl0GSBv6eM&response-content-disposition=attachment%3B%20filename%3Dopenseed_ade20k_swinl.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1150154384 (1.1G) [application/octet-stream]\n",
            "Saving to: ‘openseed_ade20k_swinl.pt’\n",
            "\n",
            "openseed_ade20k_swi 100%[===================>]   1.07G  90.6MB/s    in 12s     \n",
            "\n",
            "2025-07-27 14:06:01 (92.2 MB/s) - ‘openseed_ade20k_swinl.pt’ saved [1150154384/1150154384]\n",
            "\n",
            "--2025-07-27 14:06:01--  https://github.com/IDEA-Research/detrex-storage/releases/download/maskdino-v0.1.0/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/538847073/f81d6540-c21c-41bf-b9cf-361620b22bb8?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-27T15%3A00%3A42Z&rscd=attachment%3B+filename%3Dmaskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-27T13%3A59%3A49Z&ske=2025-07-27T15%3A00%3A42Z&sks=b&skv=2018-11-09&sig=94dxxhwN7Lgseg9GAVvi%2FycE2Cffr26fl1KHoIRAUdM%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MzYyNTQ2MiwibmJmIjoxNzUzNjI1MTYyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.jTDoLqO_BRR9yeFKeaT0UWPiI0MLM4fViMK9inmtGyw&response-content-disposition=attachment%3B%20filename%3Dmaskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-07-27 14:06:02--  https://release-assets.githubusercontent.com/github-production-release-asset/538847073/f81d6540-c21c-41bf-b9cf-361620b22bb8?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-27T15%3A00%3A42Z&rscd=attachment%3B+filename%3Dmaskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-27T13%3A59%3A49Z&ske=2025-07-27T15%3A00%3A42Z&sks=b&skv=2018-11-09&sig=94dxxhwN7Lgseg9GAVvi%2FycE2Cffr26fl1KHoIRAUdM%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MzYyNTQ2MiwibmJmIjoxNzUzNjI1MTYyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.jTDoLqO_BRR9yeFKeaT0UWPiI0MLM4fViMK9inmtGyw&response-content-disposition=attachment%3B%20filename%3Dmaskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526702767 (502M) [application/octet-stream]\n",
            "Saving to: ‘maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth’\n",
            "\n",
            "maskdino_r50_50ep_1 100%[===================>] 502.30M   127MB/s    in 4.0s    \n",
            "\n",
            "2025-07-27 14:06:06 (126 MB/s) - ‘maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth’ saved [526702767/526702767]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"awsaf49/ade20k-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5qk0qjO7rdW",
        "outputId": "a93edd96-f27e-4d9d-a72f-40a1cf58da5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/awsaf49/ade20k-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.10G/1.10G [00:12<00:00, 92.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/awsaf49/ade20k-dataset/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /root/.cache/kagglehub/datasets/awsaf49/ade20k-dataset/versions/2/ADEChallengeData2016  /content/MaskDINO/datasets"
      ],
      "metadata": {
        "id": "emVhxv-_TgTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MaskDINO\n",
        "!python datasets/prepare_ade20k_sem_seg.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZJYnK0J7rfi",
        "outputId": "83174078-5107-4134-b47b-d86e4016d79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO\n",
            "100% 20210/20210 [01:43<00:00, 194.74it/s]\n",
            "100% 2000/2000 [00:10<00:00, 195.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change config"
      ],
      "metadata": {
        "id": "xwJUf607IIfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml\n",
        "_BASE_: Base-ADE20K-SemanticSegmentation.yaml\n",
        "MODEL:\n",
        "  META_ARCHITECTURE: \"MaskDINO\"\n",
        "  BACKBONE:\n",
        "    NAME: \"D2SwinTransformer\"\n",
        "  SWIN:\n",
        "    EMBED_DIM: 192\n",
        "    DEPTHS: [ 2, 2, 18, 2 ]\n",
        "    NUM_HEADS: [ 6, 12, 24, 48 ]\n",
        "    WINDOW_SIZE: 12\n",
        "    APE: False\n",
        "    DROP_PATH_RATE: 0.3\n",
        "    PATCH_NORM: True\n",
        "    PRETRAIN_IMG_SIZE: 384\n",
        "  WEIGHTS: \"pretrained/swin_large_patch4_window12_384_22k.pkl\"\n",
        "  PIXEL_MEAN: [ 123.675, 116.280, 103.530 ]\n",
        "  PIXEL_STD: [ 58.395, 57.120, 57.375 ]\n",
        "  SEM_SEG_HEAD:\n",
        "    NAME: \"MaskDINOHead\"\n",
        "    IGNORE_VALUE: 255\n",
        "    NUM_CLASSES: 150\n",
        "    LOSS_WEIGHT: 1.0\n",
        "    CONVS_DIM: 256\n",
        "    MASK_DIM: 256\n",
        "    NORM: \"GN\"\n",
        "    # pixel decoder\n",
        "    PIXEL_DECODER_NAME: \"MaskDINOEncoder\"\n",
        "    DIM_FEEDFORWARD: 1024\n",
        "    NUM_FEATURE_LEVELS: 4\n",
        "    TOTAL_NUM_FEATURE_LEVELS: 5\n",
        "    IN_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
        "    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
        "    COMMON_STRIDE: 4\n",
        "    TRANSFORMER_ENC_LAYERS: 6\n",
        "  MaskDINO:\n",
        "    TRANSFORMER_DECODER_NAME: \"MaskDINODecoder\"\n",
        "    DEEP_SUPERVISION: True\n",
        "    NO_OBJECT_WEIGHT: 0.1\n",
        "    CLASS_WEIGHT: 4.0\n",
        "    MASK_WEIGHT: 5.0\n",
        "    DICE_WEIGHT: 5.0\n",
        "    HIDDEN_DIM: 256\n",
        "    NUM_OBJECT_QUERIES: 100\n",
        "    NHEADS: 8\n",
        "    DROPOUT: 0.0\n",
        "    DIM_FEEDFORWARD: 2048\n",
        "    ENC_LAYERS: 0\n",
        "    PRE_NORM: False\n",
        "    ENFORCE_INPUT_PROJ: False\n",
        "    SIZE_DIVISIBILITY: 32\n",
        "    DEC_LAYERS: 9 # 9 decoder layers, add one for the loss on learnable query\n",
        "    TRAIN_NUM_POINTS: 12544\n",
        "    OVERSAMPLE_RATIO: 3.0\n",
        "    IMPORTANCE_SAMPLE_RATIO: 0.75\n",
        "    TWO_STAGE: False\n",
        "    DN: \"seg\"\n",
        "    DN_NUM: 100\n",
        "    INITIALIZE_BOX_TYPE: \"no\"\n",
        "    SEMANTIC_CE_LOSS: True\n",
        "    TEST:\n",
        "      SEMANTIC_ON: True\n",
        "      INSTANCE_ON: False\n",
        "      PANOPTIC_ON: False\n",
        "      OVERLAP_THRESHOLD: 0.8\n",
        "      OBJECT_MASK_THRESHOLD: 0.8\n",
        "SOLVER:\n",
        "  AMP:\n",
        "    ENABLED: False\n",
        "  BACKBONE_MULTIPLIER: 0.1\n",
        "  BASE_LR: 0.0001\n",
        "  BASE_LR_END: 0.0\n",
        "  BIAS_LR_FACTOR: 1.0\n",
        "  CHECKPOINT_PERIOD: 5000\n",
        "\n",
        "  IMS_PER_BATCH: 8\n",
        "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
        "  MAX_ITER: 320000\n",
        "\n",
        "  STEPS: (270000,300000)\n",
        "  WARMUP_FACTOR: 1.0\n",
        "  WARMUP_ITERS: 10\n",
        "  WARMUP_METHOD: linear"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6fSwqfaBNGH",
        "outputId": "7efc3b56-ac55-476d-de11-4d8c7e279bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert model"
      ],
      "metadata": {
        "id": "scz_YOMx2ywG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MaskDINO/\n",
        "!python tools/convert-pretrained-swin-model-to-d2.py \\\n",
        " /content/MaskDINO/pretrained/swin_large_patch4_window12_384_22k.pth \\\n",
        " /content/MaskDINO/pretrained/swin_large_patch4_window12_384_22k.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WReuESQTOf-c",
        "outputId": "2512066c-3271-4b97-fc0a-ecbcaa1edc3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MaskDINO/\n",
        "!python tools/convert-torchvision-to-d2.py \\\n",
        " /content/MaskDINO/pretrained/resnet50-19c8e357.pth \\\n",
        " /content/MaskDINO/pretrained/resnet50-19c8e357.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yWD76PZD2x-9",
        "outputId": "9dd3c729-9d14-481b-acf3-e119fe8bc3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO\n",
            "conv1.weight -> stem.conv1.weight\n",
            "bn1.running_mean -> stem.conv1.norm.running_mean\n",
            "bn1.running_var -> stem.conv1.norm.running_var\n",
            "bn1.weight -> stem.conv1.norm.weight\n",
            "bn1.bias -> stem.conv1.norm.bias\n",
            "layer1.0.conv1.weight -> res2.0.conv1.weight\n",
            "layer1.0.bn1.running_mean -> res2.0.conv1.norm.running_mean\n",
            "layer1.0.bn1.running_var -> res2.0.conv1.norm.running_var\n",
            "layer1.0.bn1.weight -> res2.0.conv1.norm.weight\n",
            "layer1.0.bn1.bias -> res2.0.conv1.norm.bias\n",
            "layer1.0.conv2.weight -> res2.0.conv2.weight\n",
            "layer1.0.bn2.running_mean -> res2.0.conv2.norm.running_mean\n",
            "layer1.0.bn2.running_var -> res2.0.conv2.norm.running_var\n",
            "layer1.0.bn2.weight -> res2.0.conv2.norm.weight\n",
            "layer1.0.bn2.bias -> res2.0.conv2.norm.bias\n",
            "layer1.0.conv3.weight -> res2.0.conv3.weight\n",
            "layer1.0.bn3.running_mean -> res2.0.conv3.norm.running_mean\n",
            "layer1.0.bn3.running_var -> res2.0.conv3.norm.running_var\n",
            "layer1.0.bn3.weight -> res2.0.conv3.norm.weight\n",
            "layer1.0.bn3.bias -> res2.0.conv3.norm.bias\n",
            "layer1.0.downsample.0.weight -> res2.0.shortcut.weight\n",
            "layer1.0.downsample.1.running_mean -> res2.0.shortcut.norm.running_mean\n",
            "layer1.0.downsample.1.running_var -> res2.0.shortcut.norm.running_var\n",
            "layer1.0.downsample.1.weight -> res2.0.shortcut.norm.weight\n",
            "layer1.0.downsample.1.bias -> res2.0.shortcut.norm.bias\n",
            "layer1.1.conv1.weight -> res2.1.conv1.weight\n",
            "layer1.1.bn1.running_mean -> res2.1.conv1.norm.running_mean\n",
            "layer1.1.bn1.running_var -> res2.1.conv1.norm.running_var\n",
            "layer1.1.bn1.weight -> res2.1.conv1.norm.weight\n",
            "layer1.1.bn1.bias -> res2.1.conv1.norm.bias\n",
            "layer1.1.conv2.weight -> res2.1.conv2.weight\n",
            "layer1.1.bn2.running_mean -> res2.1.conv2.norm.running_mean\n",
            "layer1.1.bn2.running_var -> res2.1.conv2.norm.running_var\n",
            "layer1.1.bn2.weight -> res2.1.conv2.norm.weight\n",
            "layer1.1.bn2.bias -> res2.1.conv2.norm.bias\n",
            "layer1.1.conv3.weight -> res2.1.conv3.weight\n",
            "layer1.1.bn3.running_mean -> res2.1.conv3.norm.running_mean\n",
            "layer1.1.bn3.running_var -> res2.1.conv3.norm.running_var\n",
            "layer1.1.bn3.weight -> res2.1.conv3.norm.weight\n",
            "layer1.1.bn3.bias -> res2.1.conv3.norm.bias\n",
            "layer1.2.conv1.weight -> res2.2.conv1.weight\n",
            "layer1.2.bn1.running_mean -> res2.2.conv1.norm.running_mean\n",
            "layer1.2.bn1.running_var -> res2.2.conv1.norm.running_var\n",
            "layer1.2.bn1.weight -> res2.2.conv1.norm.weight\n",
            "layer1.2.bn1.bias -> res2.2.conv1.norm.bias\n",
            "layer1.2.conv2.weight -> res2.2.conv2.weight\n",
            "layer1.2.bn2.running_mean -> res2.2.conv2.norm.running_mean\n",
            "layer1.2.bn2.running_var -> res2.2.conv2.norm.running_var\n",
            "layer1.2.bn2.weight -> res2.2.conv2.norm.weight\n",
            "layer1.2.bn2.bias -> res2.2.conv2.norm.bias\n",
            "layer1.2.conv3.weight -> res2.2.conv3.weight\n",
            "layer1.2.bn3.running_mean -> res2.2.conv3.norm.running_mean\n",
            "layer1.2.bn3.running_var -> res2.2.conv3.norm.running_var\n",
            "layer1.2.bn3.weight -> res2.2.conv3.norm.weight\n",
            "layer1.2.bn3.bias -> res2.2.conv3.norm.bias\n",
            "layer2.0.conv1.weight -> res3.0.conv1.weight\n",
            "layer2.0.bn1.running_mean -> res3.0.conv1.norm.running_mean\n",
            "layer2.0.bn1.running_var -> res3.0.conv1.norm.running_var\n",
            "layer2.0.bn1.weight -> res3.0.conv1.norm.weight\n",
            "layer2.0.bn1.bias -> res3.0.conv1.norm.bias\n",
            "layer2.0.conv2.weight -> res3.0.conv2.weight\n",
            "layer2.0.bn2.running_mean -> res3.0.conv2.norm.running_mean\n",
            "layer2.0.bn2.running_var -> res3.0.conv2.norm.running_var\n",
            "layer2.0.bn2.weight -> res3.0.conv2.norm.weight\n",
            "layer2.0.bn2.bias -> res3.0.conv2.norm.bias\n",
            "layer2.0.conv3.weight -> res3.0.conv3.weight\n",
            "layer2.0.bn3.running_mean -> res3.0.conv3.norm.running_mean\n",
            "layer2.0.bn3.running_var -> res3.0.conv3.norm.running_var\n",
            "layer2.0.bn3.weight -> res3.0.conv3.norm.weight\n",
            "layer2.0.bn3.bias -> res3.0.conv3.norm.bias\n",
            "layer2.0.downsample.0.weight -> res3.0.shortcut.weight\n",
            "layer2.0.downsample.1.running_mean -> res3.0.shortcut.norm.running_mean\n",
            "layer2.0.downsample.1.running_var -> res3.0.shortcut.norm.running_var\n",
            "layer2.0.downsample.1.weight -> res3.0.shortcut.norm.weight\n",
            "layer2.0.downsample.1.bias -> res3.0.shortcut.norm.bias\n",
            "layer2.1.conv1.weight -> res3.1.conv1.weight\n",
            "layer2.1.bn1.running_mean -> res3.1.conv1.norm.running_mean\n",
            "layer2.1.bn1.running_var -> res3.1.conv1.norm.running_var\n",
            "layer2.1.bn1.weight -> res3.1.conv1.norm.weight\n",
            "layer2.1.bn1.bias -> res3.1.conv1.norm.bias\n",
            "layer2.1.conv2.weight -> res3.1.conv2.weight\n",
            "layer2.1.bn2.running_mean -> res3.1.conv2.norm.running_mean\n",
            "layer2.1.bn2.running_var -> res3.1.conv2.norm.running_var\n",
            "layer2.1.bn2.weight -> res3.1.conv2.norm.weight\n",
            "layer2.1.bn2.bias -> res3.1.conv2.norm.bias\n",
            "layer2.1.conv3.weight -> res3.1.conv3.weight\n",
            "layer2.1.bn3.running_mean -> res3.1.conv3.norm.running_mean\n",
            "layer2.1.bn3.running_var -> res3.1.conv3.norm.running_var\n",
            "layer2.1.bn3.weight -> res3.1.conv3.norm.weight\n",
            "layer2.1.bn3.bias -> res3.1.conv3.norm.bias\n",
            "layer2.2.conv1.weight -> res3.2.conv1.weight\n",
            "layer2.2.bn1.running_mean -> res3.2.conv1.norm.running_mean\n",
            "layer2.2.bn1.running_var -> res3.2.conv1.norm.running_var\n",
            "layer2.2.bn1.weight -> res3.2.conv1.norm.weight\n",
            "layer2.2.bn1.bias -> res3.2.conv1.norm.bias\n",
            "layer2.2.conv2.weight -> res3.2.conv2.weight\n",
            "layer2.2.bn2.running_mean -> res3.2.conv2.norm.running_mean\n",
            "layer2.2.bn2.running_var -> res3.2.conv2.norm.running_var\n",
            "layer2.2.bn2.weight -> res3.2.conv2.norm.weight\n",
            "layer2.2.bn2.bias -> res3.2.conv2.norm.bias\n",
            "layer2.2.conv3.weight -> res3.2.conv3.weight\n",
            "layer2.2.bn3.running_mean -> res3.2.conv3.norm.running_mean\n",
            "layer2.2.bn3.running_var -> res3.2.conv3.norm.running_var\n",
            "layer2.2.bn3.weight -> res3.2.conv3.norm.weight\n",
            "layer2.2.bn3.bias -> res3.2.conv3.norm.bias\n",
            "layer2.3.conv1.weight -> res3.3.conv1.weight\n",
            "layer2.3.bn1.running_mean -> res3.3.conv1.norm.running_mean\n",
            "layer2.3.bn1.running_var -> res3.3.conv1.norm.running_var\n",
            "layer2.3.bn1.weight -> res3.3.conv1.norm.weight\n",
            "layer2.3.bn1.bias -> res3.3.conv1.norm.bias\n",
            "layer2.3.conv2.weight -> res3.3.conv2.weight\n",
            "layer2.3.bn2.running_mean -> res3.3.conv2.norm.running_mean\n",
            "layer2.3.bn2.running_var -> res3.3.conv2.norm.running_var\n",
            "layer2.3.bn2.weight -> res3.3.conv2.norm.weight\n",
            "layer2.3.bn2.bias -> res3.3.conv2.norm.bias\n",
            "layer2.3.conv3.weight -> res3.3.conv3.weight\n",
            "layer2.3.bn3.running_mean -> res3.3.conv3.norm.running_mean\n",
            "layer2.3.bn3.running_var -> res3.3.conv3.norm.running_var\n",
            "layer2.3.bn3.weight -> res3.3.conv3.norm.weight\n",
            "layer2.3.bn3.bias -> res3.3.conv3.norm.bias\n",
            "layer3.0.conv1.weight -> res4.0.conv1.weight\n",
            "layer3.0.bn1.running_mean -> res4.0.conv1.norm.running_mean\n",
            "layer3.0.bn1.running_var -> res4.0.conv1.norm.running_var\n",
            "layer3.0.bn1.weight -> res4.0.conv1.norm.weight\n",
            "layer3.0.bn1.bias -> res4.0.conv1.norm.bias\n",
            "layer3.0.conv2.weight -> res4.0.conv2.weight\n",
            "layer3.0.bn2.running_mean -> res4.0.conv2.norm.running_mean\n",
            "layer3.0.bn2.running_var -> res4.0.conv2.norm.running_var\n",
            "layer3.0.bn2.weight -> res4.0.conv2.norm.weight\n",
            "layer3.0.bn2.bias -> res4.0.conv2.norm.bias\n",
            "layer3.0.conv3.weight -> res4.0.conv3.weight\n",
            "layer3.0.bn3.running_mean -> res4.0.conv3.norm.running_mean\n",
            "layer3.0.bn3.running_var -> res4.0.conv3.norm.running_var\n",
            "layer3.0.bn3.weight -> res4.0.conv3.norm.weight\n",
            "layer3.0.bn3.bias -> res4.0.conv3.norm.bias\n",
            "layer3.0.downsample.0.weight -> res4.0.shortcut.weight\n",
            "layer3.0.downsample.1.running_mean -> res4.0.shortcut.norm.running_mean\n",
            "layer3.0.downsample.1.running_var -> res4.0.shortcut.norm.running_var\n",
            "layer3.0.downsample.1.weight -> res4.0.shortcut.norm.weight\n",
            "layer3.0.downsample.1.bias -> res4.0.shortcut.norm.bias\n",
            "layer3.1.conv1.weight -> res4.1.conv1.weight\n",
            "layer3.1.bn1.running_mean -> res4.1.conv1.norm.running_mean\n",
            "layer3.1.bn1.running_var -> res4.1.conv1.norm.running_var\n",
            "layer3.1.bn1.weight -> res4.1.conv1.norm.weight\n",
            "layer3.1.bn1.bias -> res4.1.conv1.norm.bias\n",
            "layer3.1.conv2.weight -> res4.1.conv2.weight\n",
            "layer3.1.bn2.running_mean -> res4.1.conv2.norm.running_mean\n",
            "layer3.1.bn2.running_var -> res4.1.conv2.norm.running_var\n",
            "layer3.1.bn2.weight -> res4.1.conv2.norm.weight\n",
            "layer3.1.bn2.bias -> res4.1.conv2.norm.bias\n",
            "layer3.1.conv3.weight -> res4.1.conv3.weight\n",
            "layer3.1.bn3.running_mean -> res4.1.conv3.norm.running_mean\n",
            "layer3.1.bn3.running_var -> res4.1.conv3.norm.running_var\n",
            "layer3.1.bn3.weight -> res4.1.conv3.norm.weight\n",
            "layer3.1.bn3.bias -> res4.1.conv3.norm.bias\n",
            "layer3.2.conv1.weight -> res4.2.conv1.weight\n",
            "layer3.2.bn1.running_mean -> res4.2.conv1.norm.running_mean\n",
            "layer3.2.bn1.running_var -> res4.2.conv1.norm.running_var\n",
            "layer3.2.bn1.weight -> res4.2.conv1.norm.weight\n",
            "layer3.2.bn1.bias -> res4.2.conv1.norm.bias\n",
            "layer3.2.conv2.weight -> res4.2.conv2.weight\n",
            "layer3.2.bn2.running_mean -> res4.2.conv2.norm.running_mean\n",
            "layer3.2.bn2.running_var -> res4.2.conv2.norm.running_var\n",
            "layer3.2.bn2.weight -> res4.2.conv2.norm.weight\n",
            "layer3.2.bn2.bias -> res4.2.conv2.norm.bias\n",
            "layer3.2.conv3.weight -> res4.2.conv3.weight\n",
            "layer3.2.bn3.running_mean -> res4.2.conv3.norm.running_mean\n",
            "layer3.2.bn3.running_var -> res4.2.conv3.norm.running_var\n",
            "layer3.2.bn3.weight -> res4.2.conv3.norm.weight\n",
            "layer3.2.bn3.bias -> res4.2.conv3.norm.bias\n",
            "layer3.3.conv1.weight -> res4.3.conv1.weight\n",
            "layer3.3.bn1.running_mean -> res4.3.conv1.norm.running_mean\n",
            "layer3.3.bn1.running_var -> res4.3.conv1.norm.running_var\n",
            "layer3.3.bn1.weight -> res4.3.conv1.norm.weight\n",
            "layer3.3.bn1.bias -> res4.3.conv1.norm.bias\n",
            "layer3.3.conv2.weight -> res4.3.conv2.weight\n",
            "layer3.3.bn2.running_mean -> res4.3.conv2.norm.running_mean\n",
            "layer3.3.bn2.running_var -> res4.3.conv2.norm.running_var\n",
            "layer3.3.bn2.weight -> res4.3.conv2.norm.weight\n",
            "layer3.3.bn2.bias -> res4.3.conv2.norm.bias\n",
            "layer3.3.conv3.weight -> res4.3.conv3.weight\n",
            "layer3.3.bn3.running_mean -> res4.3.conv3.norm.running_mean\n",
            "layer3.3.bn3.running_var -> res4.3.conv3.norm.running_var\n",
            "layer3.3.bn3.weight -> res4.3.conv3.norm.weight\n",
            "layer3.3.bn3.bias -> res4.3.conv3.norm.bias\n",
            "layer3.4.conv1.weight -> res4.4.conv1.weight\n",
            "layer3.4.bn1.running_mean -> res4.4.conv1.norm.running_mean\n",
            "layer3.4.bn1.running_var -> res4.4.conv1.norm.running_var\n",
            "layer3.4.bn1.weight -> res4.4.conv1.norm.weight\n",
            "layer3.4.bn1.bias -> res4.4.conv1.norm.bias\n",
            "layer3.4.conv2.weight -> res4.4.conv2.weight\n",
            "layer3.4.bn2.running_mean -> res4.4.conv2.norm.running_mean\n",
            "layer3.4.bn2.running_var -> res4.4.conv2.norm.running_var\n",
            "layer3.4.bn2.weight -> res4.4.conv2.norm.weight\n",
            "layer3.4.bn2.bias -> res4.4.conv2.norm.bias\n",
            "layer3.4.conv3.weight -> res4.4.conv3.weight\n",
            "layer3.4.bn3.running_mean -> res4.4.conv3.norm.running_mean\n",
            "layer3.4.bn3.running_var -> res4.4.conv3.norm.running_var\n",
            "layer3.4.bn3.weight -> res4.4.conv3.norm.weight\n",
            "layer3.4.bn3.bias -> res4.4.conv3.norm.bias\n",
            "layer3.5.conv1.weight -> res4.5.conv1.weight\n",
            "layer3.5.bn1.running_mean -> res4.5.conv1.norm.running_mean\n",
            "layer3.5.bn1.running_var -> res4.5.conv1.norm.running_var\n",
            "layer3.5.bn1.weight -> res4.5.conv1.norm.weight\n",
            "layer3.5.bn1.bias -> res4.5.conv1.norm.bias\n",
            "layer3.5.conv2.weight -> res4.5.conv2.weight\n",
            "layer3.5.bn2.running_mean -> res4.5.conv2.norm.running_mean\n",
            "layer3.5.bn2.running_var -> res4.5.conv2.norm.running_var\n",
            "layer3.5.bn2.weight -> res4.5.conv2.norm.weight\n",
            "layer3.5.bn2.bias -> res4.5.conv2.norm.bias\n",
            "layer3.5.conv3.weight -> res4.5.conv3.weight\n",
            "layer3.5.bn3.running_mean -> res4.5.conv3.norm.running_mean\n",
            "layer3.5.bn3.running_var -> res4.5.conv3.norm.running_var\n",
            "layer3.5.bn3.weight -> res4.5.conv3.norm.weight\n",
            "layer3.5.bn3.bias -> res4.5.conv3.norm.bias\n",
            "layer4.0.conv1.weight -> res5.0.conv1.weight\n",
            "layer4.0.bn1.running_mean -> res5.0.conv1.norm.running_mean\n",
            "layer4.0.bn1.running_var -> res5.0.conv1.norm.running_var\n",
            "layer4.0.bn1.weight -> res5.0.conv1.norm.weight\n",
            "layer4.0.bn1.bias -> res5.0.conv1.norm.bias\n",
            "layer4.0.conv2.weight -> res5.0.conv2.weight\n",
            "layer4.0.bn2.running_mean -> res5.0.conv2.norm.running_mean\n",
            "layer4.0.bn2.running_var -> res5.0.conv2.norm.running_var\n",
            "layer4.0.bn2.weight -> res5.0.conv2.norm.weight\n",
            "layer4.0.bn2.bias -> res5.0.conv2.norm.bias\n",
            "layer4.0.conv3.weight -> res5.0.conv3.weight\n",
            "layer4.0.bn3.running_mean -> res5.0.conv3.norm.running_mean\n",
            "layer4.0.bn3.running_var -> res5.0.conv3.norm.running_var\n",
            "layer4.0.bn3.weight -> res5.0.conv3.norm.weight\n",
            "layer4.0.bn3.bias -> res5.0.conv3.norm.bias\n",
            "layer4.0.downsample.0.weight -> res5.0.shortcut.weight\n",
            "layer4.0.downsample.1.running_mean -> res5.0.shortcut.norm.running_mean\n",
            "layer4.0.downsample.1.running_var -> res5.0.shortcut.norm.running_var\n",
            "layer4.0.downsample.1.weight -> res5.0.shortcut.norm.weight\n",
            "layer4.0.downsample.1.bias -> res5.0.shortcut.norm.bias\n",
            "layer4.1.conv1.weight -> res5.1.conv1.weight\n",
            "layer4.1.bn1.running_mean -> res5.1.conv1.norm.running_mean\n",
            "layer4.1.bn1.running_var -> res5.1.conv1.norm.running_var\n",
            "layer4.1.bn1.weight -> res5.1.conv1.norm.weight\n",
            "layer4.1.bn1.bias -> res5.1.conv1.norm.bias\n",
            "layer4.1.conv2.weight -> res5.1.conv2.weight\n",
            "layer4.1.bn2.running_mean -> res5.1.conv2.norm.running_mean\n",
            "layer4.1.bn2.running_var -> res5.1.conv2.norm.running_var\n",
            "layer4.1.bn2.weight -> res5.1.conv2.norm.weight\n",
            "layer4.1.bn2.bias -> res5.1.conv2.norm.bias\n",
            "layer4.1.conv3.weight -> res5.1.conv3.weight\n",
            "layer4.1.bn3.running_mean -> res5.1.conv3.norm.running_mean\n",
            "layer4.1.bn3.running_var -> res5.1.conv3.norm.running_var\n",
            "layer4.1.bn3.weight -> res5.1.conv3.norm.weight\n",
            "layer4.1.bn3.bias -> res5.1.conv3.norm.bias\n",
            "layer4.2.conv1.weight -> res5.2.conv1.weight\n",
            "layer4.2.bn1.running_mean -> res5.2.conv1.norm.running_mean\n",
            "layer4.2.bn1.running_var -> res5.2.conv1.norm.running_var\n",
            "layer4.2.bn1.weight -> res5.2.conv1.norm.weight\n",
            "layer4.2.bn1.bias -> res5.2.conv1.norm.bias\n",
            "layer4.2.conv2.weight -> res5.2.conv2.weight\n",
            "layer4.2.bn2.running_mean -> res5.2.conv2.norm.running_mean\n",
            "layer4.2.bn2.running_var -> res5.2.conv2.norm.running_var\n",
            "layer4.2.bn2.weight -> res5.2.conv2.norm.weight\n",
            "layer4.2.bn2.bias -> res5.2.conv2.norm.bias\n",
            "layer4.2.conv3.weight -> res5.2.conv3.weight\n",
            "layer4.2.bn3.running_mean -> res5.2.conv3.norm.running_mean\n",
            "layer4.2.bn3.running_var -> res5.2.conv3.norm.running_var\n",
            "layer4.2.bn3.weight -> res5.2.conv3.norm.weight\n",
            "layer4.2.bn3.bias -> res5.2.conv3.norm.bias\n",
            "fc.weight -> stem.fc.weight\n",
            "fc.bias -> stem.fc.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### start training"
      ],
      "metadata": {
        "id": "zBLzDDmG21QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MaskDINO\n",
        "!python train_net.py \\\n",
        "    --eval-only \\\n",
        "    --num-gpus 1 \\\n",
        "    --config-file /content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml \\\n",
        "    MODEL.WEIGHTS /content/MaskDINO/weights/openseed_ade20k_swinl.pt \\\n",
        "    MODEL.DEVICE cuda\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sxUCsTV97ri8",
        "outputId": "c28e03ab-4524-4475-b44e-f3d101f97767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Command Line Args: Namespace(config_file='/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:7597', opts=['MODEL.WEIGHTS', '/content/MaskDINO/weights/openseed_ade20k_swinl.pt', 'MODEL.DEVICE', 'cuda'], EVAL_FLAG=1)\n",
            "pwd: /content/MaskDINO\n",
            "Loading config /content/MaskDINO/configs/ade20k/semantic-segmentation/Base-ADE20K-SemanticSegmentation.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "\u001b[32m[07/27 03:29:35 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[07/27 03:29:36 detectron2]: \u001b[0mEnvironment info:\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "sys.platform                     linux\n",
            "Python                           3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "numpy                            1.26.4\n",
            "detectron2                       0.6 @/content/detectron2/detectron2\n",
            "Compiler                         GCC 11.4\n",
            "CUDA compiler                    CUDA 12.5\n",
            "detectron2 arch flags            7.5\n",
            "DETECTRON2_ENV_MODULE            <not set>\n",
            "PyTorch                          2.1.0+cu121 @/usr/local/lib/python3.11/dist-packages/torch\n",
            "PyTorch debug build              False\n",
            "torch._C._GLIBCXX_USE_CXX11_ABI  False\n",
            "GPU available                    Yes\n",
            "GPU 0                            Tesla T4 (arch=7.5)\n",
            "Driver version                   550.54.15\n",
            "CUDA_HOME                        /usr/local/cuda\n",
            "Pillow                           11.3.0\n",
            "torchvision                      0.16.0+cu121 @/usr/local/lib/python3.11/dist-packages/torchvision\n",
            "torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0\n",
            "fvcore                           0.1.5.post20221221\n",
            "iopath                           0.1.9\n",
            "cv2                              4.12.0\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX512\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "\u001b[32m[07/27 03:29:36 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:7597', opts=['MODEL.WEIGHTS', '/content/MaskDINO/weights/openseed_ade20k_swinl.pt', 'MODEL.DEVICE', 'cuda'], EVAL_FLAG=1)\n",
            "\u001b[32m[07/27 03:29:36 detectron2]: \u001b[0mContents of args.config_file=/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml:\n",
            "\u001b[38;5;204m_BASE_\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mBase-ADE20K-SemanticSegmentation.yaml\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINO\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mD2SwinTransformer\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m192\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m18\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m6\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m12\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m24\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m48\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mpretrained/swin_large_patch4_window12_384_22k.pkl\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m123.675\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m116.280\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m103.530\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m58.395\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m57.120\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m57.375\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOHead\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mGN\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;245m# pixel decoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOEncoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres2\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres2\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINODecoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;245m# 9 decoder layers, add one for the loss on learnable query\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mseg\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR_END\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m320000\u001b[39m\n",
            "\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m(270000,300000)\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\n",
            "\u001b[32m[07/27 03:29:36 detectron2]: \u001b[0mRunning with full config:\n",
            "\u001b[38;5;204mCUDNN_BENCHMARK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;204mDATALOADER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mASPECT_RATIO_GROUPING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFILTER_EMPTY_ANNOTATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_WORKERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_SQRT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSAMPLER_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrainingSampler\u001b[39m\n",
            "\u001b[38;5;204mDATASETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141made20k_sem_seg_val\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141made20k_sem_seg_train\u001b[39m\n",
            "\u001b[38;5;204mDefault_loading\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;204mFLOAT32_PRECISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;204mGLOBAL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mHACK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;204mINPUT\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCOLOR_AUG_SSD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCROP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSINGLE_CATEGORY_MAX_AREA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mabsolute\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDATASET_MAPPER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mmask_former_semantic\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRGB\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMAGE_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_FORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mpolygon\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m307\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m358\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m409\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m460\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m563\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m614\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m665\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m716\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m819\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m870\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m921\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m972\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN_SAMPLING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mchoice\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRANDOM_FLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhorizontal\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mANCHOR_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mANGLES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-90\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m90\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPECT_RATIOS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mDefaultAnchorGenerator\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOFFSET\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m128\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFREEZE_AT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mD2SwinTransformer\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDEVICE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcuda\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFUSE_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msum\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLOAD_PROPOSALS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINO\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_BOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_CLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_DICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_GIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_MASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mseg\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NOISE_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEVAL_FLAG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mGIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIAL_PRED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLEARN_TGT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPANO_BOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRED_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TEMPERATURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.06\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TRANSFORM_EVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mTEST_FOUCUS_ON_BOX\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINODecoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPANOPTIC_FPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMBINE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCES_CONFIDENCE_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSTUFF_AREA_LIMIT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4096\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINSTANCE_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m123.675\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m116.28\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m103.53\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m58.395\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.375\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRPN\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESNETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_MODULATED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_NUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_ON_PER_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFrozenBN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES2_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES4_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_MULTI_GRID\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mbasic\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTRIDE_IN_1X1\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWIDTH_PER_GROUP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRETINANET\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_ALPHA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_GAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONVS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRIOR_PROB\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_LOSS_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOPK_CANDIDATES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_CASCADE_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id001\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOUS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id001\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_BBOX_REG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFC_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_FREQ_WEIGHT_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_NUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FC\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_ON_PRED_BOXES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_FED_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_SIGMOID_CE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRes5ROIHeads\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROPOSAL_APPEND_GT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_KEYPOINT_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_KEYPOINTS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mKRCNNConvDeconvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m17\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_MASK_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_MASK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskRCNNConvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOUNDARY_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHEAD_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mStandardRPNHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DILATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFEATURE_ORDER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhigh2low\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhard_pixel_mining\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mGN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOEncoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_DEPTHWISE_SEPARABLE_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mATTN_DROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m192\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMLP_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m24\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQKV_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQK_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_CHECKPOINT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m/content/MaskDINO/weights/openseed_ade20k_swinl.pt\u001b[39m\n",
            "\u001b[38;5;204mOUTPUT_DIR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m./output\u001b[39m\n",
            "\u001b[38;5;204mSEED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR_END\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCLIP_GRADIENTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfull_model\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mGAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m320000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMOMENTUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNESTEROV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_DECAYS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mOPTIMIZER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mADAMW\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_CONSTANT_ENDING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREFERENCE_WORLD_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESCALE_INTERVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m270000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m300000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_EMBED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAUG\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMAX_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3584\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m896\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDETECTIONS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEXPECTED_RESULTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_OKS_SIGMAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECISE_BN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m200\u001b[39m\n",
            "\u001b[38;5;204mVERSION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;204mVIS_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\n",
            "\u001b[32m[07/27 03:29:36 detectron2]: \u001b[0mFull config saved to ./output/config.yaml\n",
            "\u001b[32m[07/27 03:29:36 d2.utils.env]: \u001b[0mUsing a generated random seed 36707520\n",
            "Command cfg: CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_SQRT: True\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('ade20k_sem_seg_val',)\n",
            "  TRAIN: ('ade20k_sem_seg_train',)\n",
            "Default_loading: True\n",
            "FLOAT32_PRECISION: \n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  COLOR_AUG_SSD: True\n",
            "  CROP:\n",
            "    ENABLED: True\n",
            "    SINGLE_CATEGORY_MAX_AREA: 1.0\n",
            "    SIZE: [512, 512]\n",
            "    TYPE: absolute\n",
            "  DATASET_MAPPER_NAME: mask_former_semantic\n",
            "  FORMAT: RGB\n",
            "  IMAGE_SIZE: 1024\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SCALE: 2.0\n",
            "  MAX_SIZE_TEST: 2048\n",
            "  MAX_SIZE_TRAIN: 2048\n",
            "  MIN_SCALE: 0.1\n",
            "  MIN_SIZE_TEST: 512\n",
            "  MIN_SIZE_TRAIN: (256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768, 819, 870, 921, 972, 1024)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "  SIZE_DIVISIBILITY: 512\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: DefaultAnchorGenerator\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[32, 64, 128, 256, 512]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 0\n",
            "    NAME: D2SwinTransformer\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: []\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: MaskDINO\n",
            "  MaskDINO:\n",
            "    BOX_LOSS: True\n",
            "    BOX_WEIGHT: 5.0\n",
            "    CLASS_WEIGHT: 4.0\n",
            "    COST_BOX_WEIGHT: 5.0\n",
            "    COST_CLASS_WEIGHT: 4.0\n",
            "    COST_DICE_WEIGHT: 5.0\n",
            "    COST_GIOU_WEIGHT: 2.0\n",
            "    COST_MASK_WEIGHT: 5.0\n",
            "    DEC_LAYERS: 9\n",
            "    DEEP_SUPERVISION: True\n",
            "    DICE_WEIGHT: 5.0\n",
            "    DIM_FEEDFORWARD: 2048\n",
            "    DN: seg\n",
            "    DN_NOISE_SCALE: 0.4\n",
            "    DN_NUM: 100\n",
            "    DROPOUT: 0.0\n",
            "    ENC_LAYERS: 0\n",
            "    ENFORCE_INPUT_PROJ: False\n",
            "    EVAL_FLAG: 1\n",
            "    GIOU_WEIGHT: 2.0\n",
            "    HIDDEN_DIM: 256\n",
            "    IMPORTANCE_SAMPLE_RATIO: 0.75\n",
            "    INITIALIZE_BOX_TYPE: no\n",
            "    INITIAL_PRED: True\n",
            "    LEARN_TGT: False\n",
            "    MASK_WEIGHT: 5.0\n",
            "    NHEADS: 8\n",
            "    NO_OBJECT_WEIGHT: 0.1\n",
            "    NUM_OBJECT_QUERIES: 100\n",
            "    OVERSAMPLE_RATIO: 3.0\n",
            "    PANO_BOX_LOSS: False\n",
            "    PRED_CONV: False\n",
            "    PRE_NORM: False\n",
            "    SEMANTIC_CE_LOSS: True\n",
            "    SIZE_DIVISIBILITY: 32\n",
            "    TEST:\n",
            "      INSTANCE_ON: False\n",
            "      OBJECT_MASK_THRESHOLD: 0.8\n",
            "      OVERLAP_THRESHOLD: 0.8\n",
            "      PANOPTIC_ON: False\n",
            "      PANO_TEMPERATURE: 0.06\n",
            "      PANO_TRANSFORM_EVAL: True\n",
            "      SEMANTIC_ON: True\n",
            "      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: False\n",
            "      TEST_FOUCUS_ON_BOX: False\n",
            "    TRAIN_NUM_POINTS: 12544\n",
            "    TRANSFORMER_DECODER_NAME: MaskDINODecoder\n",
            "    TWO_STAGE: False\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [123.675, 116.28, 103.53]\n",
            "  PIXEL_STD: [58.395, 57.12, 57.375]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES4_DILATION: 1\n",
            "    RES5_DILATION: 1\n",
            "    RES5_MULTI_GRID: [1, 1, 1]\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STEM_TYPE: basic\n",
            "    STRIDE_IN_1X1: False\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 80\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    FED_LOSS_FREQ_WEIGHT_POWER: 0.5\n",
            "    FED_LOSS_NUM_CLASSES: 50\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "    USE_FED_LOSS: False\n",
            "    USE_SIGMOID_CE: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    CONV_DIMS: [-1]\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    ASPP_CHANNELS: 256\n",
            "    ASPP_DILATIONS: [6, 12, 18]\n",
            "    ASPP_DROPOUT: 0.1\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 256\n",
            "    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8\n",
            "    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4\n",
            "    DIM_FEEDFORWARD: 1024\n",
            "    FEATURE_ORDER: high2low\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    LOSS_TYPE: hard_pixel_mining\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MASK_DIM: 256\n",
            "    NAME: MaskDINOHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 150\n",
            "    NUM_FEATURE_LEVELS: 4\n",
            "    PIXEL_DECODER_NAME: MaskDINOEncoder\n",
            "    PROJECT_CHANNELS: [48]\n",
            "    PROJECT_FEATURES: ['res2']\n",
            "    TOTAL_NUM_FEATURE_LEVELS: 5\n",
            "    TRANSFORMER_ENC_LAYERS: 6\n",
            "    USE_DEPTHWISE_SEPARABLE_CONV: False\n",
            "  SWIN:\n",
            "    APE: False\n",
            "    ATTN_DROP_RATE: 0.0\n",
            "    DEPTHS: [2, 2, 18, 2]\n",
            "    DROP_PATH_RATE: 0.3\n",
            "    DROP_RATE: 0.0\n",
            "    EMBED_DIM: 192\n",
            "    MLP_RATIO: 4.0\n",
            "    NUM_HEADS: [6, 12, 24, 48]\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    PATCH_NORM: True\n",
            "    PATCH_SIZE: 4\n",
            "    PRETRAIN_IMG_SIZE: 384\n",
            "    QKV_BIAS: True\n",
            "    QK_SCALE: None\n",
            "    USE_CHECKPOINT: False\n",
            "    WINDOW_SIZE: 12\n",
            "  WEIGHTS: /content/MaskDINO/weights/openseed_ade20k_swinl.pt\n",
            "OUTPUT_DIR: ./output\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: False\n",
            "  BACKBONE_MULTIPLIER: 0.1\n",
            "  BASE_LR: 0.0001\n",
            "  BASE_LR_END: 0.0\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 5000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: full_model\n",
            "    CLIP_VALUE: 0.01\n",
            "    ENABLED: True\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 8\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 320000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  NUM_DECAYS: 3\n",
            "  OPTIMIZER: ADAMW\n",
            "  POLY_LR_CONSTANT_ENDING: 0.0\n",
            "  POLY_LR_POWER: 0.9\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  RESCALE_INTERVAL: False\n",
            "  STEPS: (270000, 300000)\n",
            "  WARMUP_FACTOR: 1.0\n",
            "  WARMUP_ITERS: 10\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.05\n",
            "  WEIGHT_DECAY_BIAS: None\n",
            "  WEIGHT_DECAY_EMBED: 0.0\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 3584\n",
            "    MIN_SIZES: (256, 384, 512, 640, 768, 896)\n",
            "  DETECTIONS_PER_IMAGE: 100\n",
            "  EVAL_PERIOD: 5000\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VIS_PERIOD: 0\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "criterion.weight_dict  {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0}\n",
            "\u001b[32m[07/27 03:29:40 d2.engine.defaults]: \u001b[0mModel:\n",
            "MaskDINO(\n",
            "  (backbone): D2SwinTransformer(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
            "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.013)\n",
            "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchMerging(\n",
            "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.026)\n",
            "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.039)\n",
            "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchMerging(\n",
            "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
            "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.052)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.065)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.078)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.091)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.104)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.117)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.130)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.143)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.157)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.170)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.183)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.196)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (12): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.209)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (13): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.222)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (14): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.235)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (15): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.248)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (16): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.261)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (17): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.274)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchMerging(\n",
            "          (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
            "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (3): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.287)\n",
            "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.300)\n",
            "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (sem_seg_head): MaskDINOHead(\n",
            "    (pixel_decoder): MaskDINOEncoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (3): Sequential(\n",
            "          (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (4): Sequential(\n",
            "          (0): Conv2d(1536, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MaskDINODecoder(\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 4)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-4): 5 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=151, bias=True)\n",
            "      (label_enc): Embedding(150, 256)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (decoder): TransformerDecoder(\n",
            "        (layers): ModuleList(\n",
            "          (0-8): 9 x DeformableTransformerDecoderLayer(\n",
            "            (cross_attn): MSDeformAttn(\n",
            "              (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
            "              (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
            "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "            )\n",
            "            (dropout2): Dropout(p=0.0, inplace=False)\n",
            "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "            (dropout4): Dropout(p=0.0, inplace=False)\n",
            "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ref_point_head): MLP(\n",
            "          (layers): ModuleList(\n",
            "            (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (bbox_embed): ModuleList(\n",
            "          (0-8): 9 x MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (_bbox_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (bbox_embed): ModuleList(\n",
            "        (0-8): 9 x MLP(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 4.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks', 'boxes']\n",
            "      weight_dict: {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0}\n",
            "      num_classes: 150\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")\n",
            "\u001b[32m[07/27 03:29:40 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /content/MaskDINO/weights/openseed_ade20k_swinl.pt ...\n",
            "\u001b[32m[07/27 03:29:40 fvcore.common.checkpoint]: \u001b[0m[Checkpointer] Loading from /content/MaskDINO/weights/openseed_ade20k_swinl.pt ...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.input_proj.0.0.weight' to the model due to incompatible shapes: (256, 192, 1, 1) in the checkpoint but (256, 1536, 1, 1) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.input_proj.1.0.weight' to the model due to incompatible shapes: (256, 384, 1, 1) in the checkpoint but (256, 768, 1, 1) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.input_proj.2.0.weight' to the model due to incompatible shapes: (256, 768, 1, 1) in the checkpoint but (256, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.input_proj.3.0.weight' to the model due to incompatible shapes: (256, 1536, 1, 1) in the checkpoint but (256, 192, 1, 1) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.weight' to the model due to incompatible shapes: (2048, 256) in the checkpoint but (1024, 256) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.bias' to the model due to incompatible shapes: (2048,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.weight' to the model due to incompatible shapes: (256, 2048) in the checkpoint but (256, 1024) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.weight' to the model due to incompatible shapes: (2048, 256) in the checkpoint but (1024, 256) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.bias' to the model due to incompatible shapes: (2048,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.weight' to the model due to incompatible shapes: (256, 2048) in the checkpoint but (256, 1024) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.weight' to the model due to incompatible shapes: (2048, 256) in the checkpoint but (1024, 256) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.bias' to the model due to incompatible shapes: (2048,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.weight' to the model due to incompatible shapes: (256, 2048) in the checkpoint but (256, 1024) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.weight' to the model due to incompatible shapes: (2048, 256) in the checkpoint but (1024, 256) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.bias' to the model due to incompatible shapes: (2048,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.weight' to the model due to incompatible shapes: (256, 2048) in the checkpoint but (256, 1024) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.weight' to the model due to incompatible shapes: (2048, 256) in the checkpoint but (1024, 256) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.bias' to the model due to incompatible shapes: (2048,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.weight' to the model due to incompatible shapes: (256, 2048) in the checkpoint but (256, 1024) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.weight' to the model due to incompatible shapes: (2048, 256) in the checkpoint but (1024, 256) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.bias' to the model due to incompatible shapes: (2048,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.weight' to the model due to incompatible shapes: (256, 2048) in the checkpoint but (256, 1024) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (151,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSome model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mcriterion.empty_weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.0.0.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.1.0.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.2.0.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.3.0.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.class_embed.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.label_enc.weight\u001b[0m\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mThe checkpoint state_dict contains keys that are not used by the model:\n",
            "  \u001b[35msem_seg_head.predictor.lang_mapper\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.enc_output.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.enc_output_norm.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.{lang_proj, logit_scale}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.positional_embedding\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.token_embedding.weight\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.{bias, weight}\u001b[0m\n",
            "\u001b[32m[07/27 03:29:41 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /content/MaskDINO/weights/openseed_ade20k_swinl.pt ...\n",
            "\u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0m[Checkpointer] Loading from /content/MaskDINO/weights/openseed_ade20k_swinl.pt ...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.input_proj.0.0.weight' to the model due to incompatible shapes: (256, 192, 1, 1) in the checkpoint but (256, 1536, 1, 1) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.input_proj.1.0.weight' to the model due to incompatible shapes: (256, 384, 1, 1) in the checkpoint but (256, 768, 1, 1) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.input_proj.2.0.weight' to the model due to incompatible shapes: (256, 768, 1, 1) in the checkpoint but (256, 384, 1, 1) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.input_proj.3.0.weight' to the model due to incompatible shapes: (256, 1536, 1, 1) in the checkpoint but (256, 192, 1, 1) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.weight' to the model due to incompatible shapes: (2048, 256) in the checkpoint but (1024, 256) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.bias' to the model due to incompatible shapes: (2048,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.weight' to the model due to incompatible shapes: (256, 2048) in the checkpoint but (256, 1024) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.weight' to the model due to incompatible shapes: (2048, 256) in the checkpoint but (1024, 256) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.bias' to the model due to incompatible shapes: (2048,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.weight' to the model due to incompatible shapes: (256, 2048) in the checkpoint but (256, 1024) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.weight' to the model due to incompatible shapes: (2048, 256) in the checkpoint but (1024, 256) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.bias' to the model due to incompatible shapes: (2048,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.weight' to the model due to incompatible shapes: (256, 2048) in the checkpoint but (256, 1024) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.weight' to the model due to incompatible shapes: (2048, 256) in the checkpoint but (1024, 256) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.bias' to the model due to incompatible shapes: (2048,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.weight' to the model due to incompatible shapes: (256, 2048) in the checkpoint but (256, 1024) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.weight' to the model due to incompatible shapes: (2048, 256) in the checkpoint but (1024, 256) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.bias' to the model due to incompatible shapes: (2048,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.weight' to the model due to incompatible shapes: (256, 2048) in the checkpoint but (256, 1024) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.weight' to the model due to incompatible shapes: (2048, 256) in the checkpoint but (1024, 256) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.bias' to the model due to incompatible shapes: (2048,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.weight' to the model due to incompatible shapes: (256, 2048) in the checkpoint but (256, 1024) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (151,) in the model! You might want to double check if this is expected.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mSome model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mcriterion.empty_weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.0.0.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.1.0.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.2.0.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.3.0.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.class_embed.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.label_enc.weight\u001b[0m\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 03:29:41 fvcore.common.checkpoint]: \u001b[0mThe checkpoint state_dict contains keys that are not used by the model:\n",
            "  \u001b[35msem_seg_head.predictor.lang_mapper\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.enc_output.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.enc_output_norm.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.{lang_proj, logit_scale}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.positional_embedding\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.token_embedding.weight\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.{bias, weight}\u001b[0m\n",
            "  \u001b[35msem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.{bias, weight}\u001b[0m\n",
            "\u001b[32m[07/27 03:29:41 d2.data.datasets.coco]: \u001b[0mLoaded 2000 images with semantic segmentation from datasets/ADEChallengeData2016/images/validation\n",
            "\u001b[32m[07/27 03:29:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=2048, sample_style='choice')]\n",
            "\u001b[32m[07/27 03:29:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[07/27 03:29:41 d2.data.common]: \u001b[0mSerializing 2000 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[07/27 03:29:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.39 MiB\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\u001b[32m[07/27 03:29:41 d2.data.datasets.coco]: \u001b[0mLoaded 2000 images with semantic segmentation from datasets/ADEChallengeData2016/images/validation\n",
            "\u001b[32m[07/27 03:29:41 d2.evaluation.evaluator]: \u001b[0mStart inference on 2000 batches\n",
            "\u001b[32m[07/27 03:29:47 d2.evaluation.evaluator]: \u001b[0mInference done 11/2000. Dataloading: 0.0018 s/iter. Inference: 0.3719 s/iter. Eval: 0.0145 s/iter. Total: 0.3881 s/iter. ETA=0:12:51\n",
            "\u001b[32m[07/27 03:29:52 d2.evaluation.evaluator]: \u001b[0mInference done 25/2000. Dataloading: 0.0038 s/iter. Inference: 0.3574 s/iter. Eval: 0.0128 s/iter. Total: 0.3741 s/iter. ETA=0:12:18\n",
            "\u001b[32m[07/27 03:29:57 d2.evaluation.evaluator]: \u001b[0mInference done 39/2000. Dataloading: 0.0031 s/iter. Inference: 0.3592 s/iter. Eval: 0.0131 s/iter. Total: 0.3755 s/iter. ETA=0:12:16\n",
            "\u001b[32m[07/27 03:30:02 d2.evaluation.evaluator]: \u001b[0mInference done 52/2000. Dataloading: 0.0031 s/iter. Inference: 0.3642 s/iter. Eval: 0.0140 s/iter. Total: 0.3813 s/iter. ETA=0:12:22\n",
            "\u001b[32m[07/27 03:30:07 d2.evaluation.evaluator]: \u001b[0mInference done 65/2000. Dataloading: 0.0035 s/iter. Inference: 0.3657 s/iter. Eval: 0.0140 s/iter. Total: 0.3833 s/iter. ETA=0:12:21\n",
            "\u001b[32m[07/27 03:30:13 d2.evaluation.evaluator]: \u001b[0mInference done 79/2000. Dataloading: 0.0032 s/iter. Inference: 0.3660 s/iter. Eval: 0.0137 s/iter. Total: 0.3830 s/iter. ETA=0:12:15\n",
            "\u001b[32m[07/27 03:30:18 d2.evaluation.evaluator]: \u001b[0mInference done 93/2000. Dataloading: 0.0036 s/iter. Inference: 0.3655 s/iter. Eval: 0.0134 s/iter. Total: 0.3825 s/iter. ETA=0:12:09\n",
            "\u001b[32m[07/27 03:30:23 d2.evaluation.evaluator]: \u001b[0mInference done 106/2000. Dataloading: 0.0034 s/iter. Inference: 0.3669 s/iter. Eval: 0.0135 s/iter. Total: 0.3839 s/iter. ETA=0:12:07\n",
            "\u001b[32m[07/27 03:30:28 d2.evaluation.evaluator]: \u001b[0mInference done 120/2000. Dataloading: 0.0035 s/iter. Inference: 0.3650 s/iter. Eval: 0.0135 s/iter. Total: 0.3820 s/iter. ETA=0:11:58\n",
            "\u001b[32m[07/27 03:30:33 d2.evaluation.evaluator]: \u001b[0mInference done 133/2000. Dataloading: 0.0034 s/iter. Inference: 0.3662 s/iter. Eval: 0.0140 s/iter. Total: 0.3837 s/iter. ETA=0:11:56\n",
            "\u001b[32m[07/27 03:30:39 d2.evaluation.evaluator]: \u001b[0mInference done 146/2000. Dataloading: 0.0033 s/iter. Inference: 0.3677 s/iter. Eval: 0.0141 s/iter. Total: 0.3852 s/iter. ETA=0:11:54\n",
            "\u001b[32m[07/27 03:30:44 d2.evaluation.evaluator]: \u001b[0mInference done 158/2000. Dataloading: 0.0035 s/iter. Inference: 0.3702 s/iter. Eval: 0.0146 s/iter. Total: 0.3884 s/iter. ETA=0:11:55\n",
            "\u001b[32m[07/27 03:30:49 d2.evaluation.evaluator]: \u001b[0mInference done 171/2000. Dataloading: 0.0035 s/iter. Inference: 0.3705 s/iter. Eval: 0.0146 s/iter. Total: 0.3886 s/iter. ETA=0:11:50\n",
            "\u001b[32m[07/27 03:30:54 d2.evaluation.evaluator]: \u001b[0mInference done 184/2000. Dataloading: 0.0036 s/iter. Inference: 0.3711 s/iter. Eval: 0.0148 s/iter. Total: 0.3896 s/iter. ETA=0:11:47\n",
            "\u001b[32m[07/27 03:30:59 d2.evaluation.evaluator]: \u001b[0mInference done 197/2000. Dataloading: 0.0035 s/iter. Inference: 0.3718 s/iter. Eval: 0.0144 s/iter. Total: 0.3898 s/iter. ETA=0:11:42\n",
            "\u001b[32m[07/27 03:31:04 d2.evaluation.evaluator]: \u001b[0mInference done 212/2000. Dataloading: 0.0035 s/iter. Inference: 0.3689 s/iter. Eval: 0.0138 s/iter. Total: 0.3862 s/iter. ETA=0:11:30\n",
            "\u001b[32m[07/27 03:31:09 d2.evaluation.evaluator]: \u001b[0mInference done 225/2000. Dataloading: 0.0035 s/iter. Inference: 0.3691 s/iter. Eval: 0.0137 s/iter. Total: 0.3863 s/iter. ETA=0:11:25\n",
            "\u001b[32m[07/27 03:31:14 d2.evaluation.evaluator]: \u001b[0mInference done 237/2000. Dataloading: 0.0034 s/iter. Inference: 0.3715 s/iter. Eval: 0.0136 s/iter. Total: 0.3886 s/iter. ETA=0:11:25\n",
            "\u001b[32m[07/27 03:31:20 d2.evaluation.evaluator]: \u001b[0mInference done 250/2000. Dataloading: 0.0035 s/iter. Inference: 0.3719 s/iter. Eval: 0.0137 s/iter. Total: 0.3891 s/iter. ETA=0:11:20\n",
            "\u001b[32m[07/27 03:31:25 d2.evaluation.evaluator]: \u001b[0mInference done 264/2000. Dataloading: 0.0034 s/iter. Inference: 0.3707 s/iter. Eval: 0.0135 s/iter. Total: 0.3877 s/iter. ETA=0:11:12\n",
            "\u001b[32m[07/27 03:31:30 d2.evaluation.evaluator]: \u001b[0mInference done 277/2000. Dataloading: 0.0034 s/iter. Inference: 0.3705 s/iter. Eval: 0.0137 s/iter. Total: 0.3876 s/iter. ETA=0:11:07\n",
            "\u001b[32m[07/27 03:31:35 d2.evaluation.evaluator]: \u001b[0mInference done 290/2000. Dataloading: 0.0034 s/iter. Inference: 0.3709 s/iter. Eval: 0.0137 s/iter. Total: 0.3881 s/iter. ETA=0:11:03\n",
            "\u001b[32m[07/27 03:31:40 d2.evaluation.evaluator]: \u001b[0mInference done 303/2000. Dataloading: 0.0033 s/iter. Inference: 0.3719 s/iter. Eval: 0.0138 s/iter. Total: 0.3891 s/iter. ETA=0:11:00\n",
            "\u001b[32m[07/27 03:31:45 d2.evaluation.evaluator]: \u001b[0mInference done 316/2000. Dataloading: 0.0034 s/iter. Inference: 0.3722 s/iter. Eval: 0.0140 s/iter. Total: 0.3896 s/iter. ETA=0:10:56\n",
            "\u001b[32m[07/27 03:31:51 d2.evaluation.evaluator]: \u001b[0mInference done 329/2000. Dataloading: 0.0034 s/iter. Inference: 0.3726 s/iter. Eval: 0.0140 s/iter. Total: 0.3900 s/iter. ETA=0:10:51\n",
            "\u001b[32m[07/27 03:31:56 d2.evaluation.evaluator]: \u001b[0mInference done 341/2000. Dataloading: 0.0034 s/iter. Inference: 0.3738 s/iter. Eval: 0.0140 s/iter. Total: 0.3912 s/iter. ETA=0:10:49\n",
            "\u001b[32m[07/27 03:32:01 d2.evaluation.evaluator]: \u001b[0mInference done 354/2000. Dataloading: 0.0033 s/iter. Inference: 0.3737 s/iter. Eval: 0.0140 s/iter. Total: 0.3910 s/iter. ETA=0:10:43\n",
            "\u001b[32m[07/27 03:32:06 d2.evaluation.evaluator]: \u001b[0mInference done 368/2000. Dataloading: 0.0033 s/iter. Inference: 0.3731 s/iter. Eval: 0.0137 s/iter. Total: 0.3901 s/iter. ETA=0:10:36\n",
            "\u001b[32m[07/27 03:32:11 d2.evaluation.evaluator]: \u001b[0mInference done 380/2000. Dataloading: 0.0033 s/iter. Inference: 0.3744 s/iter. Eval: 0.0138 s/iter. Total: 0.3916 s/iter. ETA=0:10:34\n",
            "\u001b[32m[07/27 03:32:16 d2.evaluation.evaluator]: \u001b[0mInference done 392/2000. Dataloading: 0.0033 s/iter. Inference: 0.3760 s/iter. Eval: 0.0138 s/iter. Total: 0.3933 s/iter. ETA=0:10:32\n",
            "\u001b[32m[07/27 03:32:22 d2.evaluation.evaluator]: \u001b[0mInference done 404/2000. Dataloading: 0.0034 s/iter. Inference: 0.3773 s/iter. Eval: 0.0138 s/iter. Total: 0.3944 s/iter. ETA=0:10:29\n",
            "\u001b[32m[07/27 03:32:27 d2.evaluation.evaluator]: \u001b[0mInference done 420/2000. Dataloading: 0.0033 s/iter. Inference: 0.3753 s/iter. Eval: 0.0134 s/iter. Total: 0.3921 s/iter. ETA=0:10:19\n",
            "\u001b[32m[07/27 03:32:32 d2.evaluation.evaluator]: \u001b[0mInference done 432/2000. Dataloading: 0.0033 s/iter. Inference: 0.3764 s/iter. Eval: 0.0135 s/iter. Total: 0.3934 s/iter. ETA=0:10:16\n",
            "\u001b[32m[07/27 03:32:37 d2.evaluation.evaluator]: \u001b[0mInference done 444/2000. Dataloading: 0.0033 s/iter. Inference: 0.3778 s/iter. Eval: 0.0134 s/iter. Total: 0.3947 s/iter. ETA=0:10:14\n",
            "\u001b[32m[07/27 03:32:43 d2.evaluation.evaluator]: \u001b[0mInference done 456/2000. Dataloading: 0.0033 s/iter. Inference: 0.3786 s/iter. Eval: 0.0134 s/iter. Total: 0.3954 s/iter. ETA=0:10:10\n",
            "\u001b[32m[07/27 03:32:48 d2.evaluation.evaluator]: \u001b[0mInference done 468/2000. Dataloading: 0.0033 s/iter. Inference: 0.3797 s/iter. Eval: 0.0135 s/iter. Total: 0.3965 s/iter. ETA=0:10:07\n",
            "\u001b[32m[07/27 03:32:53 d2.evaluation.evaluator]: \u001b[0mInference done 480/2000. Dataloading: 0.0033 s/iter. Inference: 0.3806 s/iter. Eval: 0.0136 s/iter. Total: 0.3975 s/iter. ETA=0:10:04\n",
            "\u001b[32m[07/27 03:32:58 d2.evaluation.evaluator]: \u001b[0mInference done 493/2000. Dataloading: 0.0033 s/iter. Inference: 0.3812 s/iter. Eval: 0.0135 s/iter. Total: 0.3980 s/iter. ETA=0:09:59\n",
            "\u001b[32m[07/27 03:33:04 d2.evaluation.evaluator]: \u001b[0mInference done 505/2000. Dataloading: 0.0033 s/iter. Inference: 0.3817 s/iter. Eval: 0.0135 s/iter. Total: 0.3986 s/iter. ETA=0:09:55\n",
            "\u001b[32m[07/27 03:33:09 d2.evaluation.evaluator]: \u001b[0mInference done 517/2000. Dataloading: 0.0033 s/iter. Inference: 0.3826 s/iter. Eval: 0.0137 s/iter. Total: 0.3997 s/iter. ETA=0:09:52\n",
            "\u001b[32m[07/27 03:33:14 d2.evaluation.evaluator]: \u001b[0mInference done 529/2000. Dataloading: 0.0033 s/iter. Inference: 0.3832 s/iter. Eval: 0.0138 s/iter. Total: 0.4003 s/iter. ETA=0:09:48\n",
            "\u001b[32m[07/27 03:33:19 d2.evaluation.evaluator]: \u001b[0mInference done 541/2000. Dataloading: 0.0033 s/iter. Inference: 0.3838 s/iter. Eval: 0.0138 s/iter. Total: 0.4009 s/iter. ETA=0:09:44\n",
            "\u001b[32m[07/27 03:33:24 d2.evaluation.evaluator]: \u001b[0mInference done 553/2000. Dataloading: 0.0033 s/iter. Inference: 0.3844 s/iter. Eval: 0.0138 s/iter. Total: 0.4017 s/iter. ETA=0:09:41\n",
            "\u001b[32m[07/27 03:33:30 d2.evaluation.evaluator]: \u001b[0mInference done 567/2000. Dataloading: 0.0033 s/iter. Inference: 0.3840 s/iter. Eval: 0.0138 s/iter. Total: 0.4011 s/iter. ETA=0:09:34\n",
            "\u001b[32m[07/27 03:33:35 d2.evaluation.evaluator]: \u001b[0mInference done 579/2000. Dataloading: 0.0033 s/iter. Inference: 0.3843 s/iter. Eval: 0.0140 s/iter. Total: 0.4017 s/iter. ETA=0:09:30\n",
            "\u001b[32m[07/27 03:33:40 d2.evaluation.evaluator]: \u001b[0mInference done 591/2000. Dataloading: 0.0033 s/iter. Inference: 0.3848 s/iter. Eval: 0.0140 s/iter. Total: 0.4021 s/iter. ETA=0:09:26\n",
            "\u001b[32m[07/27 03:33:45 d2.evaluation.evaluator]: \u001b[0mInference done 603/2000. Dataloading: 0.0033 s/iter. Inference: 0.3854 s/iter. Eval: 0.0141 s/iter. Total: 0.4028 s/iter. ETA=0:09:22\n",
            "\u001b[32m[07/27 03:33:50 d2.evaluation.evaluator]: \u001b[0mInference done 616/2000. Dataloading: 0.0033 s/iter. Inference: 0.3855 s/iter. Eval: 0.0140 s/iter. Total: 0.4028 s/iter. ETA=0:09:17\n",
            "\u001b[32m[07/27 03:33:55 d2.evaluation.evaluator]: \u001b[0mInference done 628/2000. Dataloading: 0.0033 s/iter. Inference: 0.3859 s/iter. Eval: 0.0139 s/iter. Total: 0.4031 s/iter. ETA=0:09:13\n",
            "\u001b[32m[07/27 03:34:01 d2.evaluation.evaluator]: \u001b[0mInference done 640/2000. Dataloading: 0.0034 s/iter. Inference: 0.3864 s/iter. Eval: 0.0139 s/iter. Total: 0.4037 s/iter. ETA=0:09:09\n",
            "\u001b[32m[07/27 03:34:06 d2.evaluation.evaluator]: \u001b[0mInference done 652/2000. Dataloading: 0.0033 s/iter. Inference: 0.3871 s/iter. Eval: 0.0138 s/iter. Total: 0.4042 s/iter. ETA=0:09:04\n",
            "\u001b[32m[07/27 03:34:11 d2.evaluation.evaluator]: \u001b[0mInference done 664/2000. Dataloading: 0.0034 s/iter. Inference: 0.3877 s/iter. Eval: 0.0137 s/iter. Total: 0.4049 s/iter. ETA=0:09:00\n",
            "\u001b[32m[07/27 03:34:16 d2.evaluation.evaluator]: \u001b[0mInference done 676/2000. Dataloading: 0.0033 s/iter. Inference: 0.3885 s/iter. Eval: 0.0137 s/iter. Total: 0.4056 s/iter. ETA=0:08:57\n",
            "\u001b[32m[07/27 03:34:22 d2.evaluation.evaluator]: \u001b[0mInference done 688/2000. Dataloading: 0.0033 s/iter. Inference: 0.3890 s/iter. Eval: 0.0137 s/iter. Total: 0.4061 s/iter. ETA=0:08:52\n",
            "\u001b[32m[07/27 03:34:27 d2.evaluation.evaluator]: \u001b[0mInference done 700/2000. Dataloading: 0.0034 s/iter. Inference: 0.3896 s/iter. Eval: 0.0137 s/iter. Total: 0.4067 s/iter. ETA=0:08:48\n",
            "\u001b[32m[07/27 03:34:32 d2.evaluation.evaluator]: \u001b[0mInference done 712/2000. Dataloading: 0.0033 s/iter. Inference: 0.3898 s/iter. Eval: 0.0136 s/iter. Total: 0.4069 s/iter. ETA=0:08:44\n",
            "\u001b[32m[07/27 03:34:37 d2.evaluation.evaluator]: \u001b[0mInference done 724/2000. Dataloading: 0.0034 s/iter. Inference: 0.3904 s/iter. Eval: 0.0136 s/iter. Total: 0.4074 s/iter. ETA=0:08:39\n",
            "\u001b[32m[07/27 03:34:42 d2.evaluation.evaluator]: \u001b[0mInference done 737/2000. Dataloading: 0.0033 s/iter. Inference: 0.3905 s/iter. Eval: 0.0135 s/iter. Total: 0.4074 s/iter. ETA=0:08:34\n",
            "\u001b[32m[07/27 03:34:48 d2.evaluation.evaluator]: \u001b[0mInference done 749/2000. Dataloading: 0.0034 s/iter. Inference: 0.3910 s/iter. Eval: 0.0134 s/iter. Total: 0.4079 s/iter. ETA=0:08:30\n",
            "\u001b[32m[07/27 03:34:53 d2.evaluation.evaluator]: \u001b[0mInference done 764/2000. Dataloading: 0.0033 s/iter. Inference: 0.3901 s/iter. Eval: 0.0133 s/iter. Total: 0.4068 s/iter. ETA=0:08:22\n",
            "\u001b[32m[07/27 03:34:58 d2.evaluation.evaluator]: \u001b[0mInference done 778/2000. Dataloading: 0.0033 s/iter. Inference: 0.3897 s/iter. Eval: 0.0132 s/iter. Total: 0.4063 s/iter. ETA=0:08:16\n",
            "\u001b[32m[07/27 03:35:03 d2.evaluation.evaluator]: \u001b[0mInference done 790/2000. Dataloading: 0.0033 s/iter. Inference: 0.3900 s/iter. Eval: 0.0133 s/iter. Total: 0.4067 s/iter. ETA=0:08:12\n",
            "\u001b[32m[07/27 03:35:09 d2.evaluation.evaluator]: \u001b[0mInference done 802/2000. Dataloading: 0.0033 s/iter. Inference: 0.3902 s/iter. Eval: 0.0133 s/iter. Total: 0.4070 s/iter. ETA=0:08:07\n",
            "\u001b[32m[07/27 03:35:14 d2.evaluation.evaluator]: \u001b[0mInference done 814/2000. Dataloading: 0.0033 s/iter. Inference: 0.3905 s/iter. Eval: 0.0134 s/iter. Total: 0.4073 s/iter. ETA=0:08:03\n",
            "\u001b[32m[07/27 03:35:19 d2.evaluation.evaluator]: \u001b[0mInference done 826/2000. Dataloading: 0.0033 s/iter. Inference: 0.3907 s/iter. Eval: 0.0135 s/iter. Total: 0.4076 s/iter. ETA=0:07:58\n",
            "\u001b[32m[07/27 03:35:24 d2.evaluation.evaluator]: \u001b[0mInference done 838/2000. Dataloading: 0.0033 s/iter. Inference: 0.3910 s/iter. Eval: 0.0136 s/iter. Total: 0.4079 s/iter. ETA=0:07:54\n",
            "\u001b[32m[07/27 03:35:29 d2.evaluation.evaluator]: \u001b[0mInference done 850/2000. Dataloading: 0.0033 s/iter. Inference: 0.3912 s/iter. Eval: 0.0137 s/iter. Total: 0.4083 s/iter. ETA=0:07:49\n",
            "\u001b[32m[07/27 03:35:34 d2.evaluation.evaluator]: \u001b[0mInference done 863/2000. Dataloading: 0.0033 s/iter. Inference: 0.3911 s/iter. Eval: 0.0137 s/iter. Total: 0.4082 s/iter. ETA=0:07:44\n",
            "\u001b[32m[07/27 03:35:40 d2.evaluation.evaluator]: \u001b[0mInference done 878/2000. Dataloading: 0.0033 s/iter. Inference: 0.3903 s/iter. Eval: 0.0135 s/iter. Total: 0.4072 s/iter. ETA=0:07:36\n",
            "\u001b[32m[07/27 03:35:45 d2.evaluation.evaluator]: \u001b[0mInference done 890/2000. Dataloading: 0.0033 s/iter. Inference: 0.3907 s/iter. Eval: 0.0135 s/iter. Total: 0.4076 s/iter. ETA=0:07:32\n",
            "\u001b[32m[07/27 03:35:50 d2.evaluation.evaluator]: \u001b[0mInference done 903/2000. Dataloading: 0.0033 s/iter. Inference: 0.3906 s/iter. Eval: 0.0135 s/iter. Total: 0.4074 s/iter. ETA=0:07:26\n",
            "\u001b[32m[07/27 03:35:55 d2.evaluation.evaluator]: \u001b[0mInference done 916/2000. Dataloading: 0.0033 s/iter. Inference: 0.3907 s/iter. Eval: 0.0134 s/iter. Total: 0.4075 s/iter. ETA=0:07:21\n",
            "\u001b[32m[07/27 03:36:01 d2.evaluation.evaluator]: \u001b[0mInference done 928/2000. Dataloading: 0.0033 s/iter. Inference: 0.3912 s/iter. Eval: 0.0134 s/iter. Total: 0.4080 s/iter. ETA=0:07:17\n",
            "\u001b[32m[07/27 03:36:06 d2.evaluation.evaluator]: \u001b[0mInference done 940/2000. Dataloading: 0.0033 s/iter. Inference: 0.3915 s/iter. Eval: 0.0134 s/iter. Total: 0.4083 s/iter. ETA=0:07:12\n",
            "\u001b[32m[07/27 03:36:11 d2.evaluation.evaluator]: \u001b[0mInference done 953/2000. Dataloading: 0.0033 s/iter. Inference: 0.3915 s/iter. Eval: 0.0134 s/iter. Total: 0.4083 s/iter. ETA=0:07:07\n",
            "\u001b[32m[07/27 03:36:17 d2.evaluation.evaluator]: \u001b[0mInference done 966/2000. Dataloading: 0.0033 s/iter. Inference: 0.3916 s/iter. Eval: 0.0134 s/iter. Total: 0.4084 s/iter. ETA=0:07:02\n",
            "\u001b[32m[07/27 03:36:22 d2.evaluation.evaluator]: \u001b[0mInference done 979/2000. Dataloading: 0.0033 s/iter. Inference: 0.3913 s/iter. Eval: 0.0134 s/iter. Total: 0.4081 s/iter. ETA=0:06:56\n",
            "\u001b[32m[07/27 03:36:27 d2.evaluation.evaluator]: \u001b[0mInference done 992/2000. Dataloading: 0.0033 s/iter. Inference: 0.3913 s/iter. Eval: 0.0133 s/iter. Total: 0.4081 s/iter. ETA=0:06:51\n",
            "\u001b[32m[07/27 03:36:32 d2.evaluation.evaluator]: \u001b[0mInference done 1005/2000. Dataloading: 0.0033 s/iter. Inference: 0.3914 s/iter. Eval: 0.0133 s/iter. Total: 0.4082 s/iter. ETA=0:06:46\n",
            "\u001b[32m[07/27 03:36:38 d2.evaluation.evaluator]: \u001b[0mInference done 1017/2000. Dataloading: 0.0033 s/iter. Inference: 0.3918 s/iter. Eval: 0.0134 s/iter. Total: 0.4086 s/iter. ETA=0:06:41\n",
            "\u001b[32m[07/27 03:36:43 d2.evaluation.evaluator]: \u001b[0mInference done 1030/2000. Dataloading: 0.0033 s/iter. Inference: 0.3919 s/iter. Eval: 0.0134 s/iter. Total: 0.4087 s/iter. ETA=0:06:36\n",
            "\u001b[32m[07/27 03:36:48 d2.evaluation.evaluator]: \u001b[0mInference done 1042/2000. Dataloading: 0.0033 s/iter. Inference: 0.3922 s/iter. Eval: 0.0134 s/iter. Total: 0.4090 s/iter. ETA=0:06:31\n",
            "\u001b[32m[07/27 03:36:54 d2.evaluation.evaluator]: \u001b[0mInference done 1054/2000. Dataloading: 0.0033 s/iter. Inference: 0.3925 s/iter. Eval: 0.0134 s/iter. Total: 0.4094 s/iter. ETA=0:06:27\n",
            "\u001b[32m[07/27 03:36:59 d2.evaluation.evaluator]: \u001b[0mInference done 1066/2000. Dataloading: 0.0033 s/iter. Inference: 0.3928 s/iter. Eval: 0.0134 s/iter. Total: 0.4095 s/iter. ETA=0:06:22\n",
            "\u001b[32m[07/27 03:37:04 d2.evaluation.evaluator]: \u001b[0mInference done 1078/2000. Dataloading: 0.0033 s/iter. Inference: 0.3929 s/iter. Eval: 0.0134 s/iter. Total: 0.4097 s/iter. ETA=0:06:17\n",
            "\u001b[32m[07/27 03:37:09 d2.evaluation.evaluator]: \u001b[0mInference done 1091/2000. Dataloading: 0.0033 s/iter. Inference: 0.3930 s/iter. Eval: 0.0134 s/iter. Total: 0.4098 s/iter. ETA=0:06:12\n",
            "\u001b[32m[07/27 03:37:14 d2.evaluation.evaluator]: \u001b[0mInference done 1104/2000. Dataloading: 0.0033 s/iter. Inference: 0.3928 s/iter. Eval: 0.0134 s/iter. Total: 0.4097 s/iter. ETA=0:06:07\n",
            "\u001b[32m[07/27 03:37:20 d2.evaluation.evaluator]: \u001b[0mInference done 1117/2000. Dataloading: 0.0033 s/iter. Inference: 0.3929 s/iter. Eval: 0.0134 s/iter. Total: 0.4097 s/iter. ETA=0:06:01\n",
            "\u001b[32m[07/27 03:37:25 d2.evaluation.evaluator]: \u001b[0mInference done 1129/2000. Dataloading: 0.0033 s/iter. Inference: 0.3931 s/iter. Eval: 0.0134 s/iter. Total: 0.4099 s/iter. ETA=0:05:56\n",
            "\u001b[32m[07/27 03:37:30 d2.evaluation.evaluator]: \u001b[0mInference done 1142/2000. Dataloading: 0.0033 s/iter. Inference: 0.3931 s/iter. Eval: 0.0134 s/iter. Total: 0.4098 s/iter. ETA=0:05:51\n",
            "\u001b[32m[07/27 03:37:35 d2.evaluation.evaluator]: \u001b[0mInference done 1154/2000. Dataloading: 0.0033 s/iter. Inference: 0.3933 s/iter. Eval: 0.0134 s/iter. Total: 0.4101 s/iter. ETA=0:05:46\n",
            "\u001b[32m[07/27 03:37:41 d2.evaluation.evaluator]: \u001b[0mInference done 1166/2000. Dataloading: 0.0033 s/iter. Inference: 0.3935 s/iter. Eval: 0.0136 s/iter. Total: 0.4105 s/iter. ETA=0:05:42\n",
            "\u001b[32m[07/27 03:37:46 d2.evaluation.evaluator]: \u001b[0mInference done 1178/2000. Dataloading: 0.0033 s/iter. Inference: 0.3936 s/iter. Eval: 0.0136 s/iter. Total: 0.4106 s/iter. ETA=0:05:37\n",
            "\u001b[32m[07/27 03:37:51 d2.evaluation.evaluator]: \u001b[0mInference done 1190/2000. Dataloading: 0.0033 s/iter. Inference: 0.3939 s/iter. Eval: 0.0137 s/iter. Total: 0.4109 s/iter. ETA=0:05:32\n",
            "\u001b[32m[07/27 03:37:56 d2.evaluation.evaluator]: \u001b[0mInference done 1203/2000. Dataloading: 0.0033 s/iter. Inference: 0.3937 s/iter. Eval: 0.0136 s/iter. Total: 0.4107 s/iter. ETA=0:05:27\n",
            "\u001b[32m[07/27 03:38:01 d2.evaluation.evaluator]: \u001b[0mInference done 1217/2000. Dataloading: 0.0033 s/iter. Inference: 0.3932 s/iter. Eval: 0.0135 s/iter. Total: 0.4101 s/iter. ETA=0:05:21\n",
            "\u001b[32m[07/27 03:38:07 d2.evaluation.evaluator]: \u001b[0mInference done 1229/2000. Dataloading: 0.0033 s/iter. Inference: 0.3935 s/iter. Eval: 0.0136 s/iter. Total: 0.4104 s/iter. ETA=0:05:16\n",
            "\u001b[32m[07/27 03:38:12 d2.evaluation.evaluator]: \u001b[0mInference done 1242/2000. Dataloading: 0.0033 s/iter. Inference: 0.3935 s/iter. Eval: 0.0135 s/iter. Total: 0.4104 s/iter. ETA=0:05:11\n",
            "\u001b[32m[07/27 03:38:17 d2.evaluation.evaluator]: \u001b[0mInference done 1255/2000. Dataloading: 0.0033 s/iter. Inference: 0.3936 s/iter. Eval: 0.0135 s/iter. Total: 0.4104 s/iter. ETA=0:05:05\n",
            "\u001b[32m[07/27 03:38:23 d2.evaluation.evaluator]: \u001b[0mInference done 1269/2000. Dataloading: 0.0033 s/iter. Inference: 0.3932 s/iter. Eval: 0.0135 s/iter. Total: 0.4100 s/iter. ETA=0:04:59\n",
            "\u001b[32m[07/27 03:38:28 d2.evaluation.evaluator]: \u001b[0mInference done 1282/2000. Dataloading: 0.0033 s/iter. Inference: 0.3932 s/iter. Eval: 0.0134 s/iter. Total: 0.4100 s/iter. ETA=0:04:54\n",
            "\u001b[32m[07/27 03:38:33 d2.evaluation.evaluator]: \u001b[0mInference done 1295/2000. Dataloading: 0.0033 s/iter. Inference: 0.3932 s/iter. Eval: 0.0134 s/iter. Total: 0.4099 s/iter. ETA=0:04:48\n",
            "\u001b[32m[07/27 03:38:38 d2.evaluation.evaluator]: \u001b[0mInference done 1307/2000. Dataloading: 0.0033 s/iter. Inference: 0.3933 s/iter. Eval: 0.0134 s/iter. Total: 0.4100 s/iter. ETA=0:04:44\n",
            "\u001b[32m[07/27 03:38:43 d2.evaluation.evaluator]: \u001b[0mInference done 1320/2000. Dataloading: 0.0033 s/iter. Inference: 0.3931 s/iter. Eval: 0.0134 s/iter. Total: 0.4099 s/iter. ETA=0:04:38\n",
            "\u001b[32m[07/27 03:38:48 d2.evaluation.evaluator]: \u001b[0mInference done 1332/2000. Dataloading: 0.0033 s/iter. Inference: 0.3933 s/iter. Eval: 0.0134 s/iter. Total: 0.4101 s/iter. ETA=0:04:33\n",
            "\u001b[32m[07/27 03:38:54 d2.evaluation.evaluator]: \u001b[0mInference done 1344/2000. Dataloading: 0.0033 s/iter. Inference: 0.3935 s/iter. Eval: 0.0134 s/iter. Total: 0.4103 s/iter. ETA=0:04:29\n",
            "\u001b[32m[07/27 03:38:59 d2.evaluation.evaluator]: \u001b[0mInference done 1358/2000. Dataloading: 0.0033 s/iter. Inference: 0.3933 s/iter. Eval: 0.0134 s/iter. Total: 0.4100 s/iter. ETA=0:04:23\n",
            "\u001b[32m[07/27 03:39:04 d2.evaluation.evaluator]: \u001b[0mInference done 1372/2000. Dataloading: 0.0033 s/iter. Inference: 0.3930 s/iter. Eval: 0.0133 s/iter. Total: 0.4096 s/iter. ETA=0:04:17\n",
            "\u001b[32m[07/27 03:39:10 d2.evaluation.evaluator]: \u001b[0mInference done 1384/2000. Dataloading: 0.0033 s/iter. Inference: 0.3932 s/iter. Eval: 0.0133 s/iter. Total: 0.4099 s/iter. ETA=0:04:12\n",
            "\u001b[32m[07/27 03:39:15 d2.evaluation.evaluator]: \u001b[0mInference done 1396/2000. Dataloading: 0.0033 s/iter. Inference: 0.3934 s/iter. Eval: 0.0133 s/iter. Total: 0.4101 s/iter. ETA=0:04:07\n",
            "\u001b[32m[07/27 03:39:20 d2.evaluation.evaluator]: \u001b[0mInference done 1409/2000. Dataloading: 0.0033 s/iter. Inference: 0.3933 s/iter. Eval: 0.0133 s/iter. Total: 0.4100 s/iter. ETA=0:04:02\n",
            "\u001b[32m[07/27 03:39:25 d2.evaluation.evaluator]: \u001b[0mInference done 1424/2000. Dataloading: 0.0033 s/iter. Inference: 0.3928 s/iter. Eval: 0.0132 s/iter. Total: 0.4094 s/iter. ETA=0:03:55\n",
            "\u001b[32m[07/27 03:39:30 d2.evaluation.evaluator]: \u001b[0mInference done 1436/2000. Dataloading: 0.0033 s/iter. Inference: 0.3929 s/iter. Eval: 0.0132 s/iter. Total: 0.4095 s/iter. ETA=0:03:50\n",
            "\u001b[32m[07/27 03:39:35 d2.evaluation.evaluator]: \u001b[0mInference done 1448/2000. Dataloading: 0.0033 s/iter. Inference: 0.3932 s/iter. Eval: 0.0132 s/iter. Total: 0.4097 s/iter. ETA=0:03:46\n",
            "\u001b[32m[07/27 03:39:41 d2.evaluation.evaluator]: \u001b[0mInference done 1461/2000. Dataloading: 0.0033 s/iter. Inference: 0.3932 s/iter. Eval: 0.0132 s/iter. Total: 0.4097 s/iter. ETA=0:03:40\n",
            "\u001b[32m[07/27 03:39:46 d2.evaluation.evaluator]: \u001b[0mInference done 1473/2000. Dataloading: 0.0033 s/iter. Inference: 0.3932 s/iter. Eval: 0.0132 s/iter. Total: 0.4098 s/iter. ETA=0:03:35\n",
            "\u001b[32m[07/27 03:39:51 d2.evaluation.evaluator]: \u001b[0mInference done 1486/2000. Dataloading: 0.0033 s/iter. Inference: 0.3932 s/iter. Eval: 0.0132 s/iter. Total: 0.4098 s/iter. ETA=0:03:30\n",
            "\u001b[32m[07/27 03:39:56 d2.evaluation.evaluator]: \u001b[0mInference done 1499/2000. Dataloading: 0.0033 s/iter. Inference: 0.3931 s/iter. Eval: 0.0132 s/iter. Total: 0.4097 s/iter. ETA=0:03:25\n",
            "\u001b[32m[07/27 03:40:02 d2.evaluation.evaluator]: \u001b[0mInference done 1511/2000. Dataloading: 0.0033 s/iter. Inference: 0.3934 s/iter. Eval: 0.0132 s/iter. Total: 0.4099 s/iter. ETA=0:03:20\n",
            "\u001b[32m[07/27 03:40:07 d2.evaluation.evaluator]: \u001b[0mInference done 1522/2000. Dataloading: 0.0033 s/iter. Inference: 0.3937 s/iter. Eval: 0.0133 s/iter. Total: 0.4103 s/iter. ETA=0:03:16\n",
            "\u001b[32m[07/27 03:40:12 d2.evaluation.evaluator]: \u001b[0mInference done 1534/2000. Dataloading: 0.0033 s/iter. Inference: 0.3938 s/iter. Eval: 0.0133 s/iter. Total: 0.4104 s/iter. ETA=0:03:11\n",
            "\u001b[32m[07/27 03:40:17 d2.evaluation.evaluator]: \u001b[0mInference done 1547/2000. Dataloading: 0.0033 s/iter. Inference: 0.3939 s/iter. Eval: 0.0133 s/iter. Total: 0.4105 s/iter. ETA=0:03:05\n",
            "\u001b[32m[07/27 03:40:23 d2.evaluation.evaluator]: \u001b[0mInference done 1561/2000. Dataloading: 0.0033 s/iter. Inference: 0.3936 s/iter. Eval: 0.0132 s/iter. Total: 0.4102 s/iter. ETA=0:03:00\n",
            "\u001b[32m[07/27 03:40:28 d2.evaluation.evaluator]: \u001b[0mInference done 1573/2000. Dataloading: 0.0033 s/iter. Inference: 0.3937 s/iter. Eval: 0.0133 s/iter. Total: 0.4103 s/iter. ETA=0:02:55\n",
            "\u001b[32m[07/27 03:40:33 d2.evaluation.evaluator]: \u001b[0mInference done 1585/2000. Dataloading: 0.0033 s/iter. Inference: 0.3938 s/iter. Eval: 0.0133 s/iter. Total: 0.4105 s/iter. ETA=0:02:50\n",
            "\u001b[32m[07/27 03:40:38 d2.evaluation.evaluator]: \u001b[0mInference done 1597/2000. Dataloading: 0.0033 s/iter. Inference: 0.3938 s/iter. Eval: 0.0134 s/iter. Total: 0.4105 s/iter. ETA=0:02:45\n",
            "\u001b[32m[07/27 03:40:43 d2.evaluation.evaluator]: \u001b[0mInference done 1610/2000. Dataloading: 0.0033 s/iter. Inference: 0.3937 s/iter. Eval: 0.0133 s/iter. Total: 0.4104 s/iter. ETA=0:02:40\n",
            "\u001b[32m[07/27 03:40:48 d2.evaluation.evaluator]: \u001b[0mInference done 1622/2000. Dataloading: 0.0033 s/iter. Inference: 0.3939 s/iter. Eval: 0.0133 s/iter. Total: 0.4105 s/iter. ETA=0:02:35\n",
            "\u001b[32m[07/27 03:40:53 d2.evaluation.evaluator]: \u001b[0mInference done 1634/2000. Dataloading: 0.0033 s/iter. Inference: 0.3939 s/iter. Eval: 0.0133 s/iter. Total: 0.4105 s/iter. ETA=0:02:30\n",
            "\u001b[32m[07/27 03:40:58 d2.evaluation.evaluator]: \u001b[0mInference done 1647/2000. Dataloading: 0.0033 s/iter. Inference: 0.3938 s/iter. Eval: 0.0133 s/iter. Total: 0.4104 s/iter. ETA=0:02:24\n",
            "\u001b[32m[07/27 03:41:04 d2.evaluation.evaluator]: \u001b[0mInference done 1660/2000. Dataloading: 0.0033 s/iter. Inference: 0.3939 s/iter. Eval: 0.0132 s/iter. Total: 0.4105 s/iter. ETA=0:02:19\n",
            "\u001b[32m[07/27 03:41:09 d2.evaluation.evaluator]: \u001b[0mInference done 1673/2000. Dataloading: 0.0033 s/iter. Inference: 0.3940 s/iter. Eval: 0.0132 s/iter. Total: 0.4105 s/iter. ETA=0:02:14\n",
            "\u001b[32m[07/27 03:41:14 d2.evaluation.evaluator]: \u001b[0mInference done 1685/2000. Dataloading: 0.0033 s/iter. Inference: 0.3942 s/iter. Eval: 0.0132 s/iter. Total: 0.4107 s/iter. ETA=0:02:09\n",
            "\u001b[32m[07/27 03:41:20 d2.evaluation.evaluator]: \u001b[0mInference done 1697/2000. Dataloading: 0.0033 s/iter. Inference: 0.3944 s/iter. Eval: 0.0132 s/iter. Total: 0.4109 s/iter. ETA=0:02:04\n",
            "\u001b[32m[07/27 03:41:25 d2.evaluation.evaluator]: \u001b[0mInference done 1709/2000. Dataloading: 0.0033 s/iter. Inference: 0.3944 s/iter. Eval: 0.0132 s/iter. Total: 0.4110 s/iter. ETA=0:01:59\n",
            "\u001b[32m[07/27 03:41:30 d2.evaluation.evaluator]: \u001b[0mInference done 1722/2000. Dataloading: 0.0033 s/iter. Inference: 0.3945 s/iter. Eval: 0.0132 s/iter. Total: 0.4111 s/iter. ETA=0:01:54\n",
            "\u001b[32m[07/27 03:41:35 d2.evaluation.evaluator]: \u001b[0mInference done 1734/2000. Dataloading: 0.0033 s/iter. Inference: 0.3946 s/iter. Eval: 0.0132 s/iter. Total: 0.4112 s/iter. ETA=0:01:49\n",
            "\u001b[32m[07/27 03:41:40 d2.evaluation.evaluator]: \u001b[0mInference done 1745/2000. Dataloading: 0.0033 s/iter. Inference: 0.3949 s/iter. Eval: 0.0132 s/iter. Total: 0.4115 s/iter. ETA=0:01:44\n",
            "\u001b[32m[07/27 03:41:45 d2.evaluation.evaluator]: \u001b[0mInference done 1760/2000. Dataloading: 0.0033 s/iter. Inference: 0.3944 s/iter. Eval: 0.0132 s/iter. Total: 0.4109 s/iter. ETA=0:01:38\n",
            "\u001b[32m[07/27 03:41:51 d2.evaluation.evaluator]: \u001b[0mInference done 1773/2000. Dataloading: 0.0033 s/iter. Inference: 0.3944 s/iter. Eval: 0.0131 s/iter. Total: 0.4108 s/iter. ETA=0:01:33\n",
            "\u001b[32m[07/27 03:41:56 d2.evaluation.evaluator]: \u001b[0mInference done 1785/2000. Dataloading: 0.0033 s/iter. Inference: 0.3945 s/iter. Eval: 0.0131 s/iter. Total: 0.4110 s/iter. ETA=0:01:28\n",
            "\u001b[32m[07/27 03:42:01 d2.evaluation.evaluator]: \u001b[0mInference done 1797/2000. Dataloading: 0.0033 s/iter. Inference: 0.3946 s/iter. Eval: 0.0132 s/iter. Total: 0.4111 s/iter. ETA=0:01:23\n",
            "\u001b[32m[07/27 03:42:06 d2.evaluation.evaluator]: \u001b[0mInference done 1809/2000. Dataloading: 0.0033 s/iter. Inference: 0.3946 s/iter. Eval: 0.0132 s/iter. Total: 0.4112 s/iter. ETA=0:01:18\n",
            "\u001b[32m[07/27 03:42:11 d2.evaluation.evaluator]: \u001b[0mInference done 1821/2000. Dataloading: 0.0033 s/iter. Inference: 0.3947 s/iter. Eval: 0.0133 s/iter. Total: 0.4113 s/iter. ETA=0:01:13\n",
            "\u001b[32m[07/27 03:42:16 d2.evaluation.evaluator]: \u001b[0mInference done 1833/2000. Dataloading: 0.0033 s/iter. Inference: 0.3948 s/iter. Eval: 0.0133 s/iter. Total: 0.4114 s/iter. ETA=0:01:08\n",
            "\u001b[32m[07/27 03:42:21 d2.evaluation.evaluator]: \u001b[0mInference done 1845/2000. Dataloading: 0.0033 s/iter. Inference: 0.3949 s/iter. Eval: 0.0133 s/iter. Total: 0.4115 s/iter. ETA=0:01:03\n",
            "\u001b[32m[07/27 03:42:27 d2.evaluation.evaluator]: \u001b[0mInference done 1857/2000. Dataloading: 0.0033 s/iter. Inference: 0.3949 s/iter. Eval: 0.0134 s/iter. Total: 0.4116 s/iter. ETA=0:00:58\n",
            "\u001b[32m[07/27 03:42:32 d2.evaluation.evaluator]: \u001b[0mInference done 1873/2000. Dataloading: 0.0033 s/iter. Inference: 0.3942 s/iter. Eval: 0.0133 s/iter. Total: 0.4108 s/iter. ETA=0:00:52\n",
            "\u001b[32m[07/27 03:42:37 d2.evaluation.evaluator]: \u001b[0mInference done 1885/2000. Dataloading: 0.0033 s/iter. Inference: 0.3943 s/iter. Eval: 0.0133 s/iter. Total: 0.4110 s/iter. ETA=0:00:47\n",
            "\u001b[32m[07/27 03:42:42 d2.evaluation.evaluator]: \u001b[0mInference done 1897/2000. Dataloading: 0.0033 s/iter. Inference: 0.3945 s/iter. Eval: 0.0133 s/iter. Total: 0.4111 s/iter. ETA=0:00:42\n",
            "\u001b[32m[07/27 03:42:47 d2.evaluation.evaluator]: \u001b[0mInference done 1910/2000. Dataloading: 0.0033 s/iter. Inference: 0.3944 s/iter. Eval: 0.0133 s/iter. Total: 0.4110 s/iter. ETA=0:00:36\n",
            "\u001b[32m[07/27 03:42:52 d2.evaluation.evaluator]: \u001b[0mInference done 1922/2000. Dataloading: 0.0032 s/iter. Inference: 0.3946 s/iter. Eval: 0.0133 s/iter. Total: 0.4112 s/iter. ETA=0:00:32\n",
            "\u001b[32m[07/27 03:42:58 d2.evaluation.evaluator]: \u001b[0mInference done 1934/2000. Dataloading: 0.0032 s/iter. Inference: 0.3947 s/iter. Eval: 0.0133 s/iter. Total: 0.4113 s/iter. ETA=0:00:27\n",
            "\u001b[32m[07/27 03:43:03 d2.evaluation.evaluator]: \u001b[0mInference done 1947/2000. Dataloading: 0.0033 s/iter. Inference: 0.3947 s/iter. Eval: 0.0133 s/iter. Total: 0.4113 s/iter. ETA=0:00:21\n",
            "\u001b[32m[07/27 03:43:08 d2.evaluation.evaluator]: \u001b[0mInference done 1961/2000. Dataloading: 0.0032 s/iter. Inference: 0.3945 s/iter. Eval: 0.0132 s/iter. Total: 0.4111 s/iter. ETA=0:00:16\n",
            "\u001b[32m[07/27 03:43:14 d2.evaluation.evaluator]: \u001b[0mInference done 1973/2000. Dataloading: 0.0033 s/iter. Inference: 0.3946 s/iter. Eval: 0.0133 s/iter. Total: 0.4112 s/iter. ETA=0:00:11\n",
            "\u001b[32m[07/27 03:43:19 d2.evaluation.evaluator]: \u001b[0mInference done 1986/2000. Dataloading: 0.0032 s/iter. Inference: 0.3945 s/iter. Eval: 0.0133 s/iter. Total: 0.4111 s/iter. ETA=0:00:05\n",
            "\u001b[32m[07/27 03:43:24 d2.evaluation.evaluator]: \u001b[0mInference done 1999/2000. Dataloading: 0.0032 s/iter. Inference: 0.3945 s/iter. Eval: 0.0133 s/iter. Total: 0.4110 s/iter. ETA=0:00:00\n",
            "\u001b[32m[07/27 03:43:24 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:13:40.128265 (0.411092 s / iter per device, on 1 devices)\n",
            "\u001b[32m[07/27 03:43:24 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:13:06 (0.394474 s / iter per device, on 1 devices)\n",
            "\u001b[32m[07/27 03:43:25 d2.evaluation.sem_seg_evaluation]: \u001b[0mOrderedDict([('sem_seg', {'mIoU': 0.0015418862370956603, 'fwIoU': 0.0002270866641091794, 'IoU-wall': 0.0, 'BoundaryIoU-wall': 58.54078225347201, 'min(IoU, B-Iou)-wall': 0.0, 'IoU-building': 0.0, 'BoundaryIoU-building': 0.13381279242540015, 'min(IoU, B-Iou)-building': 0.0, 'IoU-sky': 0.0, 'BoundaryIoU-sky': 0.0, 'min(IoU, B-Iou)-sky': 0.0, 'IoU-floor': 0.0, 'BoundaryIoU-floor': 0.0, 'min(IoU, B-Iou)-floor': 0.0, 'IoU-tree': 0.0, 'BoundaryIoU-tree': 0.0, 'min(IoU, B-Iou)-tree': 0.0, 'IoU-ceiling': 0.0, 'BoundaryIoU-ceiling': 0.0, 'min(IoU, B-Iou)-ceiling': 0.0, 'IoU-road, route': 0.0, 'BoundaryIoU-road, route': 0.0, 'min(IoU, B-Iou)-road, route': 0.0, 'IoU-bed': 0.0, 'BoundaryIoU-bed': 0.0, 'min(IoU, B-Iou)-bed': 0.0, 'IoU-window ': 0.0, 'BoundaryIoU-window ': 0.0, 'min(IoU, B-Iou)-window ': 0.0, 'IoU-grass': 0.0, 'BoundaryIoU-grass': 0.0, 'min(IoU, B-Iou)-grass': 0.0, 'IoU-cabinet': 0.0, 'BoundaryIoU-cabinet': 0.0, 'min(IoU, B-Iou)-cabinet': 0.0, 'IoU-sidewalk, pavement': 0.0, 'BoundaryIoU-sidewalk, pavement': 0.0, 'min(IoU, B-Iou)-sidewalk, pavement': 0.0, 'IoU-person': 0.0, 'BoundaryIoU-person': 0.0, 'min(IoU, B-Iou)-person': 0.0, 'IoU-earth, ground': 0.0, 'BoundaryIoU-earth, ground': 0.0, 'min(IoU, B-Iou)-earth, ground': 0.0, 'IoU-door': 0.0, 'BoundaryIoU-door': 0.0, 'min(IoU, B-Iou)-door': 0.0, 'IoU-table': 0.0, 'BoundaryIoU-table': 0.0, 'min(IoU, B-Iou)-table': 0.0, 'IoU-mountain, mount': 0.0, 'BoundaryIoU-mountain, mount': 0.0, 'min(IoU, B-Iou)-mountain, mount': 0.0, 'IoU-plant': 0.0, 'BoundaryIoU-plant': 0.0, 'min(IoU, B-Iou)-plant': 0.0, 'IoU-curtain': 0.0, 'BoundaryIoU-curtain': 0.0, 'min(IoU, B-Iou)-curtain': 0.0, 'IoU-chair': 0.0, 'BoundaryIoU-chair': 0.0, 'min(IoU, B-Iou)-chair': 0.0, 'IoU-car': 0.0, 'BoundaryIoU-car': 0.0, 'min(IoU, B-Iou)-car': 0.0, 'IoU-water': 0.0, 'BoundaryIoU-water': 0.0, 'min(IoU, B-Iou)-water': 0.0, 'IoU-painting, picture': 0.0, 'BoundaryIoU-painting, picture': 0.0, 'min(IoU, B-Iou)-painting, picture': 0.0, 'IoU-sofa': 0.0, 'BoundaryIoU-sofa': 0.0, 'min(IoU, B-Iou)-sofa': 0.0, 'IoU-shelf': 0.0, 'BoundaryIoU-shelf': 0.0, 'min(IoU, B-Iou)-shelf': 0.0, 'IoU-house': 0.0, 'BoundaryIoU-house': 0.0, 'min(IoU, B-Iou)-house': 0.0, 'IoU-sea': 0.0, 'BoundaryIoU-sea': 0.0, 'min(IoU, B-Iou)-sea': 0.0, 'IoU-mirror': 0.0, 'BoundaryIoU-mirror': 0.0, 'min(IoU, B-Iou)-mirror': 0.0, 'IoU-rug': 0.0, 'BoundaryIoU-rug': 0.0, 'min(IoU, B-Iou)-rug': 0.0, 'IoU-field': 0.0, 'BoundaryIoU-field': 0.0, 'min(IoU, B-Iou)-field': 0.0, 'IoU-armchair': 0.0, 'BoundaryIoU-armchair': 0.0, 'min(IoU, B-Iou)-armchair': 0.0, 'IoU-seat': 0.0, 'BoundaryIoU-seat': 0.0, 'min(IoU, B-Iou)-seat': 0.0, 'IoU-fence': 0.0, 'BoundaryIoU-fence': 0.0, 'min(IoU, B-Iou)-fence': 0.0, 'IoU-desk': 0.0, 'BoundaryIoU-desk': 0.0, 'min(IoU, B-Iou)-desk': 0.0, 'IoU-rock, stone': 0.0, 'BoundaryIoU-rock, stone': 0.0, 'min(IoU, B-Iou)-rock, stone': 0.0, 'IoU-wardrobe, closet, press': 0.0, 'BoundaryIoU-wardrobe, closet, press': 0.0, 'min(IoU, B-Iou)-wardrobe, closet, press': 0.0, 'IoU-lamp': 0.0, 'BoundaryIoU-lamp': 0.0, 'min(IoU, B-Iou)-lamp': 0.0, 'IoU-tub': 0.0, 'BoundaryIoU-tub': 0.0, 'min(IoU, B-Iou)-tub': 0.0, 'IoU-rail': 0.0, 'BoundaryIoU-rail': 0.0, 'min(IoU, B-Iou)-rail': 0.0, 'IoU-cushion': 0.0, 'BoundaryIoU-cushion': 0.0, 'min(IoU, B-Iou)-cushion': 0.0, 'IoU-base, pedestal, stand': 0.0, 'BoundaryIoU-base, pedestal, stand': 0.0, 'min(IoU, B-Iou)-base, pedestal, stand': 0.0, 'IoU-box': 0.0, 'BoundaryIoU-box': 0.0, 'min(IoU, B-Iou)-box': 0.0, 'IoU-column, pillar': 0.0, 'BoundaryIoU-column, pillar': 0.0, 'min(IoU, B-Iou)-column, pillar': 0.0, 'IoU-signboard, sign': 0.014203472543784462, 'BoundaryIoU-signboard, sign': 0.0, 'min(IoU, B-Iou)-signboard, sign': 0.0, 'IoU-chest of drawers, chest, bureau, dresser': 0.0, 'BoundaryIoU-chest of drawers, chest, bureau, dresser': 0.0, 'min(IoU, B-Iou)-chest of drawers, chest, bureau, dresser': 0.0, 'IoU-counter': 0.0, 'BoundaryIoU-counter': 0.0, 'min(IoU, B-Iou)-counter': 0.0, 'IoU-sand': 0.0, 'BoundaryIoU-sand': 0.0, 'min(IoU, B-Iou)-sand': 0.0, 'IoU-sink': 0.0, 'BoundaryIoU-sink': 0.0, 'min(IoU, B-Iou)-sink': 0.0, 'IoU-skyscraper': 0.0, 'BoundaryIoU-skyscraper': 0.0, 'min(IoU, B-Iou)-skyscraper': 0.0, 'IoU-fireplace': 0.0, 'BoundaryIoU-fireplace': 0.0, 'min(IoU, B-Iou)-fireplace': 0.0, 'IoU-refrigerator, icebox': 0.0, 'BoundaryIoU-refrigerator, icebox': 0.0, 'min(IoU, B-Iou)-refrigerator, icebox': 0.0, 'IoU-grandstand, covered stand': 0.0, 'BoundaryIoU-grandstand, covered stand': 0.0, 'min(IoU, B-Iou)-grandstand, covered stand': 0.0, 'IoU-path': 0.0, 'BoundaryIoU-path': 0.0, 'min(IoU, B-Iou)-path': 0.0, 'IoU-stairs': 0.0, 'BoundaryIoU-stairs': 0.0, 'min(IoU, B-Iou)-stairs': 0.0, 'IoU-runway': 0.0, 'BoundaryIoU-runway': 0.0, 'min(IoU, B-Iou)-runway': 0.0, 'IoU-case, display case, showcase, vitrine': 0.0, 'BoundaryIoU-case, display case, showcase, vitrine': 0.0, 'min(IoU, B-Iou)-case, display case, showcase, vitrine': 0.0, 'IoU-pool table, billiard table, snooker table': 0.0, 'BoundaryIoU-pool table, billiard table, snooker table': 0.0, 'min(IoU, B-Iou)-pool table, billiard table, snooker table': 0.0, 'IoU-pillow': 0.0, 'BoundaryIoU-pillow': 0.0, 'min(IoU, B-Iou)-pillow': 0.0, 'IoU-screen door, screen': 0.0, 'BoundaryIoU-screen door, screen': 0.0, 'min(IoU, B-Iou)-screen door, screen': 0.0, 'IoU-stairway, staircase': 0.0, 'BoundaryIoU-stairway, staircase': 0.0, 'min(IoU, B-Iou)-stairway, staircase': 0.0, 'IoU-river': 0.0, 'BoundaryIoU-river': 0.0, 'min(IoU, B-Iou)-river': 0.0, 'IoU-bridge, span': 0.0, 'BoundaryIoU-bridge, span': 0.0, 'min(IoU, B-Iou)-bridge, span': 0.0, 'IoU-bookcase': 0.0, 'BoundaryIoU-bookcase': 0.0, 'min(IoU, B-Iou)-bookcase': 0.0, 'IoU-blind, screen': 0.0, 'BoundaryIoU-blind, screen': 0.0, 'min(IoU, B-Iou)-blind, screen': 0.0, 'IoU-coffee table': 0.0, 'BoundaryIoU-coffee table': 0.0, 'min(IoU, B-Iou)-coffee table': 0.0, 'IoU-toilet, can, commode, crapper, pot, potty, stool, throne': 0.0, 'BoundaryIoU-toilet, can, commode, crapper, pot, potty, stool, throne': 0.0, 'min(IoU, B-Iou)-toilet, can, commode, crapper, pot, potty, stool, throne': 0.0, 'IoU-flower': 0.0, 'BoundaryIoU-flower': 0.0, 'min(IoU, B-Iou)-flower': 0.0, 'IoU-book': 0.0, 'BoundaryIoU-book': 0.0, 'min(IoU, B-Iou)-book': 0.0, 'IoU-hill': 0.0, 'BoundaryIoU-hill': 0.0, 'min(IoU, B-Iou)-hill': 0.0, 'IoU-bench': 0.0, 'BoundaryIoU-bench': 0.0, 'min(IoU, B-Iou)-bench': 0.0, 'IoU-countertop': 0.0, 'BoundaryIoU-countertop': 0.0, 'min(IoU, B-Iou)-countertop': 0.0, 'IoU-stove': 0.0, 'BoundaryIoU-stove': 0.0, 'min(IoU, B-Iou)-stove': 0.0, 'IoU-palm, palm tree': 0.0, 'BoundaryIoU-palm, palm tree': 0.0, 'min(IoU, B-Iou)-palm, palm tree': 0.0, 'IoU-kitchen island': 0.00824504688552969, 'BoundaryIoU-kitchen island': 0.0, 'min(IoU, B-Iou)-kitchen island': 0.0, 'IoU-computer': 0.0, 'BoundaryIoU-computer': 0.0, 'min(IoU, B-Iou)-computer': 0.0, 'IoU-swivel chair': 0.0, 'BoundaryIoU-swivel chair': 0.0, 'min(IoU, B-Iou)-swivel chair': 0.0, 'IoU-boat': 0.0, 'BoundaryIoU-boat': 0.0, 'min(IoU, B-Iou)-boat': 0.0, 'IoU-bar': 0.0337493333809357, 'BoundaryIoU-bar': 0.0, 'min(IoU, B-Iou)-bar': 0.0, 'IoU-arcade machine': 0.0, 'BoundaryIoU-arcade machine': 0.0, 'min(IoU, B-Iou)-arcade machine': 0.0, 'IoU-hovel, hut, hutch, shack, shanty': 0.0, 'BoundaryIoU-hovel, hut, hutch, shack, shanty': 0.0, 'min(IoU, B-Iou)-hovel, hut, hutch, shack, shanty': 0.0, 'IoU-bus': 0.0, 'BoundaryIoU-bus': 0.0, 'min(IoU, B-Iou)-bus': 0.0, 'IoU-towel': 0.10599610904862622, 'BoundaryIoU-towel': 0.0, 'min(IoU, B-Iou)-towel': 0.0, 'IoU-light': 0.0, 'BoundaryIoU-light': 0.0, 'min(IoU, B-Iou)-light': 0.0, 'IoU-truck': 0.0, 'BoundaryIoU-truck': 0.0, 'min(IoU, B-Iou)-truck': 0.0, 'IoU-tower': 0.0, 'BoundaryIoU-tower': 0.0, 'min(IoU, B-Iou)-tower': 0.0, 'IoU-chandelier': 0.0, 'BoundaryIoU-chandelier': 0.0, 'min(IoU, B-Iou)-chandelier': 0.0, 'IoU-awning, sunshade, sunblind': 0.0, 'BoundaryIoU-awning, sunshade, sunblind': 0.0, 'min(IoU, B-Iou)-awning, sunshade, sunblind': 0.0, 'IoU-street lamp': 0.0, 'BoundaryIoU-street lamp': 0.0, 'min(IoU, B-Iou)-street lamp': 0.0, 'IoU-booth': 0.0, 'BoundaryIoU-booth': 0.0, 'min(IoU, B-Iou)-booth': 0.0, 'IoU-tv': 0.0, 'BoundaryIoU-tv': 0.0, 'min(IoU, B-Iou)-tv': 0.0, 'IoU-plane': 0.0, 'BoundaryIoU-plane': 0.0, 'min(IoU, B-Iou)-plane': 0.0, 'IoU-dirt track': 0.0, 'BoundaryIoU-dirt track': 0.0, 'min(IoU, B-Iou)-dirt track': 0.0, 'IoU-clothes': 0.0, 'BoundaryIoU-clothes': 0.0, 'min(IoU, B-Iou)-clothes': 0.0, 'IoU-pole': 0.0, 'BoundaryIoU-pole': 0.0, 'min(IoU, B-Iou)-pole': 0.0, 'IoU-land, ground, soil': 0.0, 'BoundaryIoU-land, ground, soil': 0.0, 'min(IoU, B-Iou)-land, ground, soil': 0.0, 'IoU-bannister, banister, balustrade, balusters, handrail': 0.0, 'BoundaryIoU-bannister, banister, balustrade, balusters, handrail': 0.0, 'min(IoU, B-Iou)-bannister, banister, balustrade, balusters, handrail': 0.0, 'IoU-escalator, moving staircase, moving stairway': 0.0, 'BoundaryIoU-escalator, moving staircase, moving stairway': 0.0, 'min(IoU, B-Iou)-escalator, moving staircase, moving stairway': 0.0, 'IoU-ottoman, pouf, pouffe, puff, hassock': 0.0, 'BoundaryIoU-ottoman, pouf, pouffe, puff, hassock': 0.0, 'min(IoU, B-Iou)-ottoman, pouf, pouffe, puff, hassock': 0.0, 'IoU-bottle': 0.0, 'BoundaryIoU-bottle': 0.0, 'min(IoU, B-Iou)-bottle': 0.0, 'IoU-buffet, counter, sideboard': 0.0, 'BoundaryIoU-buffet, counter, sideboard': 0.0, 'min(IoU, B-Iou)-buffet, counter, sideboard': 0.0, 'IoU-poster, posting, placard, notice, bill, card': 0.0, 'BoundaryIoU-poster, posting, placard, notice, bill, card': 0.0, 'min(IoU, B-Iou)-poster, posting, placard, notice, bill, card': 0.0, 'IoU-stage': 0.0, 'BoundaryIoU-stage': 0.0, 'min(IoU, B-Iou)-stage': 0.0, 'IoU-van': 0.0, 'BoundaryIoU-van': 0.0, 'min(IoU, B-Iou)-van': 0.0, 'IoU-ship': 0.0, 'BoundaryIoU-ship': 0.0, 'min(IoU, B-Iou)-ship': 0.0, 'IoU-fountain': 0.0, 'BoundaryIoU-fountain': 0.0, 'min(IoU, B-Iou)-fountain': 0.0, 'IoU-conveyer belt, conveyor belt, conveyer, conveyor, transporter': 0.0, 'BoundaryIoU-conveyer belt, conveyor belt, conveyer, conveyor, transporter': 0.0, 'min(IoU, B-Iou)-conveyer belt, conveyor belt, conveyer, conveyor, transporter': 0.0, 'IoU-canopy': 0.0, 'BoundaryIoU-canopy': 0.0, 'min(IoU, B-Iou)-canopy': 0.0, 'IoU-washer, automatic washer, washing machine': 0.0, 'BoundaryIoU-washer, automatic washer, washing machine': 0.0, 'min(IoU, B-Iou)-washer, automatic washer, washing machine': 0.0, 'IoU-plaything, toy': 0.0, 'BoundaryIoU-plaything, toy': 0.0, 'min(IoU, B-Iou)-plaything, toy': 0.0, 'IoU-pool': 0.0, 'BoundaryIoU-pool': 0.0, 'min(IoU, B-Iou)-pool': 0.0, 'IoU-stool': 0.0, 'BoundaryIoU-stool': 0.0, 'min(IoU, B-Iou)-stool': 0.0, 'IoU-barrel, cask': 0.0, 'BoundaryIoU-barrel, cask': 0.0, 'min(IoU, B-Iou)-barrel, cask': 0.0, 'IoU-basket, handbasket': 0.0, 'BoundaryIoU-basket, handbasket': 0.0, 'min(IoU, B-Iou)-basket, handbasket': 0.0, 'IoU-falls': 0.0, 'BoundaryIoU-falls': 0.0, 'min(IoU, B-Iou)-falls': 0.0, 'IoU-tent': 0.0, 'BoundaryIoU-tent': 0.0, 'min(IoU, B-Iou)-tent': 0.0, 'IoU-bag': 0.0, 'BoundaryIoU-bag': 0.0, 'min(IoU, B-Iou)-bag': 0.0, 'IoU-minibike, motorbike': 0.0, 'BoundaryIoU-minibike, motorbike': 0.0, 'min(IoU, B-Iou)-minibike, motorbike': 0.0, 'IoU-cradle': 0.0, 'BoundaryIoU-cradle': 0.0, 'min(IoU, B-Iou)-cradle': 0.0, 'IoU-oven': 0.0, 'BoundaryIoU-oven': 0.0, 'min(IoU, B-Iou)-oven': 0.0, 'IoU-ball': 0.0, 'BoundaryIoU-ball': 0.0, 'min(IoU, B-Iou)-ball': 0.0, 'IoU-food, solid food': 0.0, 'BoundaryIoU-food, solid food': 0.0, 'min(IoU, B-Iou)-food, solid food': 0.0, 'IoU-step, stair': 0.0, 'BoundaryIoU-step, stair': 0.0, 'min(IoU, B-Iou)-step, stair': 0.0, 'IoU-tank, storage tank': 0.0, 'BoundaryIoU-tank, storage tank': 0.0, 'min(IoU, B-Iou)-tank, storage tank': 0.0, 'IoU-trade name': 0.0, 'BoundaryIoU-trade name': 0.0, 'min(IoU, B-Iou)-trade name': 0.0, 'IoU-microwave': 0.0, 'BoundaryIoU-microwave': 0.0, 'min(IoU, B-Iou)-microwave': 0.0, 'IoU-pot': 0.0, 'BoundaryIoU-pot': 0.0, 'min(IoU, B-Iou)-pot': 0.0, 'IoU-animal': 0.032989757577755636, 'BoundaryIoU-animal': 0.0, 'min(IoU, B-Iou)-animal': 0.0, 'IoU-bicycle': 0.0, 'BoundaryIoU-bicycle': 0.0, 'min(IoU, B-Iou)-bicycle': 0.0, 'IoU-lake': 0.0, 'BoundaryIoU-lake': 0.0, 'min(IoU, B-Iou)-lake': 0.0, 'IoU-dishwasher': 0.0, 'BoundaryIoU-dishwasher': 0.0, 'min(IoU, B-Iou)-dishwasher': 0.0, 'IoU-screen': 0.0, 'BoundaryIoU-screen': 0.0, 'min(IoU, B-Iou)-screen': 0.0, 'IoU-blanket, cover': 0.0, 'BoundaryIoU-blanket, cover': 0.0, 'min(IoU, B-Iou)-blanket, cover': 0.0, 'IoU-sculpture': 0.0, 'BoundaryIoU-sculpture': 0.0, 'min(IoU, B-Iou)-sculpture': 0.0, 'IoU-hood, exhaust hood': 0.0, 'BoundaryIoU-hood, exhaust hood': 0.0, 'min(IoU, B-Iou)-hood, exhaust hood': 0.0, 'IoU-sconce': 0.0, 'BoundaryIoU-sconce': 0.0, 'min(IoU, B-Iou)-sconce': 0.0, 'IoU-vase': 0.0, 'BoundaryIoU-vase': 0.0, 'min(IoU, B-Iou)-vase': 0.0, 'IoU-traffic light': 0.0, 'BoundaryIoU-traffic light': 0.0, 'min(IoU, B-Iou)-traffic light': 0.0, 'IoU-tray': 0.0, 'BoundaryIoU-tray': 0.0, 'min(IoU, B-Iou)-tray': 0.0, 'IoU-trash can': 0.0, 'BoundaryIoU-trash can': 0.0, 'min(IoU, B-Iou)-trash can': 0.0, 'IoU-fan': 0.0, 'BoundaryIoU-fan': 0.0, 'min(IoU, B-Iou)-fan': 0.0, 'IoU-pier': 0.0, 'BoundaryIoU-pier': 0.0, 'min(IoU, B-Iou)-pier': 0.0, 'IoU-crt screen': 0.0, 'BoundaryIoU-crt screen': 0.0, 'min(IoU, B-Iou)-crt screen': 0.0, 'IoU-plate': 0.0, 'BoundaryIoU-plate': 0.0, 'min(IoU, B-Iou)-plate': 0.0, 'IoU-monitor': 0.0, 'BoundaryIoU-monitor': 0.0, 'min(IoU, B-Iou)-monitor': 0.0, 'IoU-bulletin board': 0.0, 'BoundaryIoU-bulletin board': 0.0, 'min(IoU, B-Iou)-bulletin board': 0.0, 'IoU-shower': 0.0, 'BoundaryIoU-shower': 0.0, 'min(IoU, B-Iou)-shower': 0.0, 'IoU-radiator': 0.03609921612771736, 'BoundaryIoU-radiator': 0.0, 'min(IoU, B-Iou)-radiator': 0.0, 'IoU-glass, drinking glass': 0.0, 'BoundaryIoU-glass, drinking glass': 0.0, 'min(IoU, B-Iou)-glass, drinking glass': 0.0, 'IoU-clock': 0.0, 'BoundaryIoU-clock': 0.0, 'min(IoU, B-Iou)-clock': 0.0, 'IoU-flag': 0.0, 'BoundaryIoU-flag': 0.0, 'min(IoU, B-Iou)-flag': 0.0, 'mACC': 0.3680450004097661, 'pACC': 0.03135156426858782, 'ACC-wall': 0.0, 'ACC-building': 0.0, 'ACC-sky': 0.0, 'ACC-floor': 0.0, 'ACC-tree': 0.0, 'ACC-ceiling': 0.0, 'ACC-road, route': 0.0, 'ACC-bed': 0.0, 'ACC-window ': 0.0, 'ACC-grass': 0.0, 'ACC-cabinet': 0.0, 'ACC-sidewalk, pavement': 0.0, 'ACC-person': 0.0, 'ACC-earth, ground': 0.0, 'ACC-door': 0.0, 'ACC-table': 0.0, 'ACC-mountain, mount': 0.0, 'ACC-plant': 0.0, 'ACC-curtain': 0.0, 'ACC-chair': 0.0, 'ACC-car': 0.0, 'ACC-water': 0.0, 'ACC-painting, picture': 0.0, 'ACC-sofa': 0.0, 'ACC-shelf': 0.0, 'ACC-house': 0.0, 'ACC-sea': 0.0, 'ACC-mirror': 0.0, 'ACC-rug': 0.0, 'ACC-field': 0.0, 'ACC-armchair': 0.0, 'ACC-seat': 0.0, 'ACC-fence': 0.0, 'ACC-desk': 0.0, 'ACC-rock, stone': 0.0, 'ACC-wardrobe, closet, press': 0.0, 'ACC-lamp': 0.0, 'ACC-tub': 0.0, 'ACC-rail': 0.0, 'ACC-cushion': 0.0, 'ACC-base, pedestal, stand': 0.0, 'ACC-box': 0.0, 'ACC-column, pillar': 0.0, 'ACC-signboard, sign': 0.015766258384368622, 'ACC-chest of drawers, chest, bureau, dresser': 0.0, 'ACC-counter': 0.0, 'ACC-sand': 0.0, 'ACC-sink': 0.0, 'ACC-skyscraper': 0.0, 'ACC-fireplace': 0.0, 'ACC-refrigerator, icebox': 0.0, 'ACC-grandstand, covered stand': 0.0, 'ACC-path': 0.0, 'ACC-stairs': 0.0, 'ACC-runway': 0.0, 'ACC-case, display case, showcase, vitrine': 0.0, 'ACC-pool table, billiard table, snooker table': 0.0, 'ACC-pillow': 0.0, 'ACC-screen door, screen': 0.0, 'ACC-stairway, staircase': 0.0, 'ACC-river': 0.0, 'ACC-bridge, span': 0.0, 'ACC-bookcase': 0.0, 'ACC-blind, screen': 0.0, 'ACC-coffee table': 0.0, 'ACC-toilet, can, commode, crapper, pot, potty, stool, throne': 0.0, 'ACC-flower': 0.0, 'ACC-book': 0.0, 'ACC-hill': 0.0, 'ACC-bench': 0.0, 'ACC-countertop': 0.0, 'ACC-stove': 0.0, 'ACC-palm, palm tree': 0.0, 'ACC-kitchen island': 1.873627958166334, 'ACC-computer': 0.0, 'ACC-swivel chair': 0.0, 'ACC-boat': 0.0, 'ACC-bar': 1.0732763069618618, 'ACC-arcade machine': 0.0, 'ACC-hovel, hut, hutch, shack, shanty': 0.0, 'ACC-bus': 0.0, 'ACC-towel': 2.4108604705736383, 'ACC-light': 0.0, 'ACC-truck': 0.0, 'ACC-tower': 0.0, 'ACC-chandelier': 0.0, 'ACC-awning, sunshade, sunblind': 0.0, 'ACC-street lamp': 0.0, 'ACC-booth': 0.0, 'ACC-tv': 0.0, 'ACC-plane': 0.0, 'ACC-dirt track': 0.0, 'ACC-clothes': 0.0, 'ACC-pole': 0.0, 'ACC-land, ground, soil': 0.0, 'ACC-bannister, banister, balustrade, balusters, handrail': 0.0, 'ACC-escalator, moving staircase, moving stairway': 0.0, 'ACC-ottoman, pouf, pouffe, puff, hassock': 0.0, 'ACC-bottle': 0.0, 'ACC-buffet, counter, sideboard': 0.0, 'ACC-poster, posting, placard, notice, bill, card': 0.0, 'ACC-stage': 0.0, 'ACC-van': 0.0, 'ACC-ship': 0.0, 'ACC-fountain': 0.0, 'ACC-conveyer belt, conveyor belt, conveyer, conveyor, transporter': 0.0, 'ACC-canopy': 0.0, 'ACC-washer, automatic washer, washing machine': 0.0, 'ACC-plaything, toy': 0.0, 'ACC-pool': 0.0, 'ACC-stool': 0.0, 'ACC-barrel, cask': 0.0, 'ACC-basket, handbasket': 0.0, 'ACC-falls': 0.0, 'ACC-tent': 0.0, 'ACC-bag': 0.0, 'ACC-minibike, motorbike': 0.0, 'ACC-cradle': 0.0, 'ACC-oven': 0.0, 'ACC-ball': 0.0, 'ACC-food, solid food': 0.0, 'ACC-step, stair': 0.0, 'ACC-tank, storage tank': 0.0, 'ACC-trade name': 0.0, 'ACC-microwave': 0.0, 'ACC-pot': 0.0, 'ACC-animal': 7.437215857521349, 'ACC-bicycle': 0.0, 'ACC-lake': 0.0, 'ACC-dishwasher': 0.0, 'ACC-screen': 0.0, 'ACC-blanket, cover': 0.0, 'ACC-sculpture': 0.0, 'ACC-hood, exhaust hood': 0.0, 'ACC-sconce': 0.0, 'ACC-vase': 0.0, 'ACC-traffic light': 0.0, 'ACC-tray': 0.0, 'ACC-trash can': 0.0, 'ACC-fan': 0.0, 'ACC-pier': 0.0, 'ACC-crt screen': 0.0, 'ACC-plate': 0.0, 'ACC-monitor': 0.0, 'ACC-bulletin board': 0.0, 'ACC-shower': 0.0, 'ACC-radiator': 42.39600320985737, 'ACC-glass, drinking glass': 0.0, 'ACC-clock': 0.0, 'ACC-flag': 0.0})])\n",
            "\u001b[32m[07/27 03:43:25 d2.engine.defaults]: \u001b[0mEvaluation results for ade20k_sem_seg_val in csv format:\n",
            "\u001b[32m[07/27 03:43:25 d2.evaluation.testing]: \u001b[0mcopypaste: Task: sem_seg\n",
            "\u001b[32m[07/27 03:43:25 d2.evaluation.testing]: \u001b[0mcopypaste: mIoU,fwIoU,mACC,pACC\n",
            "\u001b[32m[07/27 03:43:25 d2.evaluation.testing]: \u001b[0mcopypaste: 0.0015,0.0002,0.3680,0.0314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MaskDINO\n",
        "!python train_net.py --num-gpus 1\\\n",
        " --config-file configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml \\\n",
        "  SOLVER.IMS_PER_BATCH 2 \\\n",
        "  SOLVER.BASE_LR 0.0001 \\\n",
        "  OUTPUT_DIR maskdino_SwinL_bs16_160k_steplr_workdir\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tmdC3bUGINYL",
        "outputId": "05e336d5-7eab-4d63-fb3b-a4891dcf38da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Command Line Args: Namespace(config_file='configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:3546', opts=['SOLVER.IMS_PER_BATCH', '2', 'SOLVER.BASE_LR', '0.0001', 'OUTPUT_DIR', 'maskdino_SwinL_bs16_160k_steplr_workdir'], EVAL_FLAG=1)\n",
            "pwd: /content/MaskDINO\n",
            "Loading config configs/ade20k/semantic-segmentation/Base-ADE20K-SemanticSegmentation.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "\u001b[32m[07/26 14:55:25 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[07/26 14:55:26 detectron2]: \u001b[0mEnvironment info:\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "sys.platform                     linux\n",
            "Python                           3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "numpy                            1.26.4\n",
            "detectron2                       0.6 @/content/detectron2/detectron2\n",
            "Compiler                         GCC 11.4\n",
            "CUDA compiler                    CUDA 12.5\n",
            "detectron2 arch flags            7.5\n",
            "DETECTRON2_ENV_MODULE            <not set>\n",
            "PyTorch                          2.1.0+cu121 @/usr/local/lib/python3.11/dist-packages/torch\n",
            "PyTorch debug build              False\n",
            "torch._C._GLIBCXX_USE_CXX11_ABI  False\n",
            "GPU available                    Yes\n",
            "GPU 0                            Tesla T4 (arch=7.5)\n",
            "Driver version                   550.54.15\n",
            "CUDA_HOME                        /usr/local/cuda\n",
            "Pillow                           11.3.0\n",
            "torchvision                      0.16.0+cu121 @/usr/local/lib/python3.11/dist-packages/torchvision\n",
            "torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0\n",
            "fvcore                           0.1.5.post20221221\n",
            "iopath                           0.1.9\n",
            "cv2                              4.12.0\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX512\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "\u001b[32m[07/26 14:55:26 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:3546', opts=['SOLVER.IMS_PER_BATCH', '2', 'SOLVER.BASE_LR', '0.0001', 'OUTPUT_DIR', 'maskdino_SwinL_bs16_160k_steplr_workdir'], EVAL_FLAG=1)\n",
            "\u001b[32m[07/26 14:55:26 detectron2]: \u001b[0mContents of args.config_file=configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml:\n",
            "\u001b[38;5;204m_BASE_\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mBase-ADE20K-SemanticSegmentation.yaml\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINO\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mD2SwinTransformer\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m192\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m18\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m6\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m12\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m24\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m48\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mpretrained/swin_large_patch4_window12_384_22k.pkl\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m123.675\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m116.280\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m103.530\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m58.395\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m57.120\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m57.375\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOHead\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mGN\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;245m# pixel decoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOEncoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres2\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres2\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINODecoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;245m# 9 decoder layers, add one for the loss on learnable query\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mseg\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR_END\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m320000\u001b[39m\n",
            "\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m(270000,300000)\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\n",
            "\u001b[32m[07/26 14:55:26 detectron2]: \u001b[0mRunning with full config:\n",
            "\u001b[38;5;204mCUDNN_BENCHMARK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;204mDATALOADER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mASPECT_RATIO_GROUPING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFILTER_EMPTY_ANNOTATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_WORKERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_SQRT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSAMPLER_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrainingSampler\u001b[39m\n",
            "\u001b[38;5;204mDATASETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141made20k_sem_seg_val\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141made20k_sem_seg_train\u001b[39m\n",
            "\u001b[38;5;204mDefault_loading\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;204mFLOAT32_PRECISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;204mGLOBAL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mHACK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;204mINPUT\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCOLOR_AUG_SSD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCROP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSINGLE_CATEGORY_MAX_AREA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mabsolute\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDATASET_MAPPER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mmask_former_semantic\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRGB\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMAGE_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_FORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mpolygon\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m307\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m358\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m409\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m460\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m563\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m614\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m665\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m716\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m819\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m870\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m921\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m972\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN_SAMPLING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mchoice\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRANDOM_FLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhorizontal\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mANCHOR_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mANGLES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-90\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m90\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPECT_RATIOS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mDefaultAnchorGenerator\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOFFSET\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m128\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFREEZE_AT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mD2SwinTransformer\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDEVICE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcuda\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFUSE_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msum\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLOAD_PROPOSALS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINO\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_BOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_CLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_DICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_GIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_MASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mseg\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NOISE_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEVAL_FLAG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mGIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIAL_PRED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLEARN_TGT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPANO_BOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRED_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TEMPERATURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.06\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TRANSFORM_EVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mTEST_FOUCUS_ON_BOX\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINODecoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPANOPTIC_FPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMBINE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCES_CONFIDENCE_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSTUFF_AREA_LIMIT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4096\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINSTANCE_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m123.675\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m116.28\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m103.53\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m58.395\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.375\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRPN\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESNETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_MODULATED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_NUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_ON_PER_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFrozenBN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES2_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES4_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_MULTI_GRID\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mbasic\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTRIDE_IN_1X1\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWIDTH_PER_GROUP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRETINANET\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_ALPHA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_GAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONVS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRIOR_PROB\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_LOSS_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOPK_CANDIDATES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_CASCADE_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id001\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOUS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id001\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_BBOX_REG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFC_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_FREQ_WEIGHT_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_NUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FC\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_ON_PRED_BOXES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_FED_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_SIGMOID_CE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRes5ROIHeads\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROPOSAL_APPEND_GT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_KEYPOINT_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_KEYPOINTS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mKRCNNConvDeconvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m17\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_MASK_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_MASK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskRCNNConvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOUNDARY_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHEAD_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mStandardRPNHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DILATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFEATURE_ORDER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhigh2low\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhard_pixel_mining\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mGN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOEncoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_DEPTHWISE_SEPARABLE_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mATTN_DROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m192\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMLP_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m24\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQKV_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQK_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_CHECKPOINT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mpretrained/swin_large_patch4_window12_384_22k.pkl\u001b[39m\n",
            "\u001b[38;5;204mOUTPUT_DIR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mmaskdino_SwinL_bs16_160k_steplr_workdir\u001b[39m\n",
            "\u001b[38;5;204mSEED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR_END\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCLIP_GRADIENTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfull_model\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mGAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m320000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMOMENTUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNESTEROV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_DECAYS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mOPTIMIZER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mADAMW\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_CONSTANT_ENDING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREFERENCE_WORLD_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESCALE_INTERVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m270000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m300000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_EMBED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAUG\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMAX_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3584\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m896\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDETECTIONS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEXPECTED_RESULTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_OKS_SIGMAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECISE_BN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m200\u001b[39m\n",
            "\u001b[38;5;204mVERSION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;204mVIS_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\n",
            "\u001b[32m[07/26 14:55:26 detectron2]: \u001b[0mFull config saved to maskdino_SwinL_bs16_160k_steplr_workdir/config.yaml\n",
            "\u001b[32m[07/26 14:55:26 d2.utils.env]: \u001b[0mUsing a generated random seed 26755410\n",
            "Command cfg: CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_SQRT: True\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('ade20k_sem_seg_val',)\n",
            "  TRAIN: ('ade20k_sem_seg_train',)\n",
            "Default_loading: True\n",
            "FLOAT32_PRECISION: \n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  COLOR_AUG_SSD: True\n",
            "  CROP:\n",
            "    ENABLED: True\n",
            "    SINGLE_CATEGORY_MAX_AREA: 1.0\n",
            "    SIZE: [512, 512]\n",
            "    TYPE: absolute\n",
            "  DATASET_MAPPER_NAME: mask_former_semantic\n",
            "  FORMAT: RGB\n",
            "  IMAGE_SIZE: 1024\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SCALE: 2.0\n",
            "  MAX_SIZE_TEST: 2048\n",
            "  MAX_SIZE_TRAIN: 2048\n",
            "  MIN_SCALE: 0.1\n",
            "  MIN_SIZE_TEST: 512\n",
            "  MIN_SIZE_TRAIN: (256, 307, 358, 409, 460, 512, 563, 614, 665, 716, 768, 819, 870, 921, 972, 1024)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "  SIZE_DIVISIBILITY: 512\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: DefaultAnchorGenerator\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[32, 64, 128, 256, 512]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 0\n",
            "    NAME: D2SwinTransformer\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: []\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: MaskDINO\n",
            "  MaskDINO:\n",
            "    BOX_LOSS: True\n",
            "    BOX_WEIGHT: 5.0\n",
            "    CLASS_WEIGHT: 4.0\n",
            "    COST_BOX_WEIGHT: 5.0\n",
            "    COST_CLASS_WEIGHT: 4.0\n",
            "    COST_DICE_WEIGHT: 5.0\n",
            "    COST_GIOU_WEIGHT: 2.0\n",
            "    COST_MASK_WEIGHT: 5.0\n",
            "    DEC_LAYERS: 9\n",
            "    DEEP_SUPERVISION: True\n",
            "    DICE_WEIGHT: 5.0\n",
            "    DIM_FEEDFORWARD: 2048\n",
            "    DN: seg\n",
            "    DN_NOISE_SCALE: 0.4\n",
            "    DN_NUM: 100\n",
            "    DROPOUT: 0.0\n",
            "    ENC_LAYERS: 0\n",
            "    ENFORCE_INPUT_PROJ: False\n",
            "    EVAL_FLAG: 1\n",
            "    GIOU_WEIGHT: 2.0\n",
            "    HIDDEN_DIM: 256\n",
            "    IMPORTANCE_SAMPLE_RATIO: 0.75\n",
            "    INITIALIZE_BOX_TYPE: no\n",
            "    INITIAL_PRED: True\n",
            "    LEARN_TGT: False\n",
            "    MASK_WEIGHT: 5.0\n",
            "    NHEADS: 8\n",
            "    NO_OBJECT_WEIGHT: 0.1\n",
            "    NUM_OBJECT_QUERIES: 100\n",
            "    OVERSAMPLE_RATIO: 3.0\n",
            "    PANO_BOX_LOSS: False\n",
            "    PRED_CONV: False\n",
            "    PRE_NORM: False\n",
            "    SEMANTIC_CE_LOSS: True\n",
            "    SIZE_DIVISIBILITY: 32\n",
            "    TEST:\n",
            "      INSTANCE_ON: False\n",
            "      OBJECT_MASK_THRESHOLD: 0.8\n",
            "      OVERLAP_THRESHOLD: 0.8\n",
            "      PANOPTIC_ON: False\n",
            "      PANO_TEMPERATURE: 0.06\n",
            "      PANO_TRANSFORM_EVAL: True\n",
            "      SEMANTIC_ON: True\n",
            "      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: False\n",
            "      TEST_FOUCUS_ON_BOX: False\n",
            "    TRAIN_NUM_POINTS: 12544\n",
            "    TRANSFORMER_DECODER_NAME: MaskDINODecoder\n",
            "    TWO_STAGE: False\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [123.675, 116.28, 103.53]\n",
            "  PIXEL_STD: [58.395, 57.12, 57.375]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES4_DILATION: 1\n",
            "    RES5_DILATION: 1\n",
            "    RES5_MULTI_GRID: [1, 1, 1]\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STEM_TYPE: basic\n",
            "    STRIDE_IN_1X1: False\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 80\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    FED_LOSS_FREQ_WEIGHT_POWER: 0.5\n",
            "    FED_LOSS_NUM_CLASSES: 50\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "    USE_FED_LOSS: False\n",
            "    USE_SIGMOID_CE: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    CONV_DIMS: [-1]\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    ASPP_CHANNELS: 256\n",
            "    ASPP_DILATIONS: [6, 12, 18]\n",
            "    ASPP_DROPOUT: 0.1\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 256\n",
            "    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8\n",
            "    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4\n",
            "    DIM_FEEDFORWARD: 1024\n",
            "    FEATURE_ORDER: high2low\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    LOSS_TYPE: hard_pixel_mining\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MASK_DIM: 256\n",
            "    NAME: MaskDINOHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 150\n",
            "    NUM_FEATURE_LEVELS: 4\n",
            "    PIXEL_DECODER_NAME: MaskDINOEncoder\n",
            "    PROJECT_CHANNELS: [48]\n",
            "    PROJECT_FEATURES: ['res2']\n",
            "    TOTAL_NUM_FEATURE_LEVELS: 5\n",
            "    TRANSFORMER_ENC_LAYERS: 6\n",
            "    USE_DEPTHWISE_SEPARABLE_CONV: False\n",
            "  SWIN:\n",
            "    APE: False\n",
            "    ATTN_DROP_RATE: 0.0\n",
            "    DEPTHS: [2, 2, 18, 2]\n",
            "    DROP_PATH_RATE: 0.3\n",
            "    DROP_RATE: 0.0\n",
            "    EMBED_DIM: 192\n",
            "    MLP_RATIO: 4.0\n",
            "    NUM_HEADS: [6, 12, 24, 48]\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    PATCH_NORM: True\n",
            "    PATCH_SIZE: 4\n",
            "    PRETRAIN_IMG_SIZE: 384\n",
            "    QKV_BIAS: True\n",
            "    QK_SCALE: None\n",
            "    USE_CHECKPOINT: False\n",
            "    WINDOW_SIZE: 12\n",
            "  WEIGHTS: pretrained/swin_large_patch4_window12_384_22k.pkl\n",
            "OUTPUT_DIR: maskdino_SwinL_bs16_160k_steplr_workdir\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: False\n",
            "  BACKBONE_MULTIPLIER: 0.1\n",
            "  BASE_LR: 0.0001\n",
            "  BASE_LR_END: 0.0\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 5000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: full_model\n",
            "    CLIP_VALUE: 0.01\n",
            "    ENABLED: True\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 2\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 320000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  NUM_DECAYS: 3\n",
            "  OPTIMIZER: ADAMW\n",
            "  POLY_LR_CONSTANT_ENDING: 0.0\n",
            "  POLY_LR_POWER: 0.9\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  RESCALE_INTERVAL: False\n",
            "  STEPS: (270000, 300000)\n",
            "  WARMUP_FACTOR: 1.0\n",
            "  WARMUP_ITERS: 10\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.05\n",
            "  WEIGHT_DECAY_BIAS: None\n",
            "  WEIGHT_DECAY_EMBED: 0.0\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 3584\n",
            "    MIN_SIZES: (256, 384, 512, 640, 768, 896)\n",
            "  DETECTIONS_PER_IMAGE: 100\n",
            "  EVAL_PERIOD: 5000\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VIS_PERIOD: 0\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "criterion.weight_dict  {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0}\n",
            "\u001b[32m[07/26 14:55:29 d2.engine.defaults]: \u001b[0mModel:\n",
            "MaskDINO(\n",
            "  (backbone): D2SwinTransformer(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
            "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.013)\n",
            "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchMerging(\n",
            "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.026)\n",
            "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.039)\n",
            "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchMerging(\n",
            "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
            "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.052)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.065)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.078)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.091)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.104)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.117)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.130)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.143)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.157)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.170)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.183)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.196)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (12): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.209)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (13): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.222)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (14): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.235)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (15): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.248)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (16): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.261)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (17): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.274)\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchMerging(\n",
            "          (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
            "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (3): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.287)\n",
            "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): SwinTransformerBlock(\n",
            "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): WindowAttention(\n",
            "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (softmax): Softmax(dim=-1)\n",
            "            )\n",
            "            (drop_path): DropPath(drop_prob=0.300)\n",
            "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (sem_seg_head): MaskDINOHead(\n",
            "    (pixel_decoder): MaskDINOEncoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (3): Sequential(\n",
            "          (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (4): Sequential(\n",
            "          (0): Conv2d(1536, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MaskDINODecoder(\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 4)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-4): 5 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=151, bias=True)\n",
            "      (label_enc): Embedding(150, 256)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (decoder): TransformerDecoder(\n",
            "        (layers): ModuleList(\n",
            "          (0-8): 9 x DeformableTransformerDecoderLayer(\n",
            "            (cross_attn): MSDeformAttn(\n",
            "              (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
            "              (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
            "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "            )\n",
            "            (dropout2): Dropout(p=0.0, inplace=False)\n",
            "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "            (dropout4): Dropout(p=0.0, inplace=False)\n",
            "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ref_point_head): MLP(\n",
            "          (layers): ModuleList(\n",
            "            (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (bbox_embed): ModuleList(\n",
            "          (0-8): 9 x MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (_bbox_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (bbox_embed): ModuleList(\n",
            "        (0-8): 9 x MLP(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 4.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks', 'boxes']\n",
            "      weight_dict: {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0}\n",
            "      num_classes: 150\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "relative_position_bias_table\n",
            "\u001b[32m[07/26 14:55:29 maskdino.data.dataset_mappers.mask_former_semantic_dataset_mapper]: \u001b[0m[MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=..., max_size=2048, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[512, 512], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7d9092778a90>, RandomFlip()]\n",
            "\u001b[32m[07/26 14:55:29 d2.data.datasets.coco]: \u001b[0mLoaded 20210 images with semantic segmentation from datasets/ADEChallengeData2016/images/training\n",
            "\u001b[32m[07/26 14:55:29 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[07/26 14:55:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[07/26 14:55:29 d2.data.common]: \u001b[0mSerializing 20210 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[07/26 14:55:29 d2.data.common]: \u001b[0mSerialized dataset takes 3.97 MiB\n",
            "\u001b[32m[07/26 14:55:29 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\u001b[32m[07/26 14:55:29 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from pretrained/swin_large_patch4_window12_384_22k.pkl ...\n",
            "\u001b[32m[07/26 14:55:29 fvcore.common.checkpoint]: \u001b[0m[Checkpointer] Loading from pretrained/swin_large_patch4_window12_384_22k.pkl ...\n",
            "\u001b[32m[07/26 14:55:30 fvcore.common.checkpoint]: \u001b[0mReading a file from 'third_party'\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:30 d2.checkpoint.c2_model_loading]: \u001b[0mShape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.pixel_decoder.adapter_1.norm.bias in model is torch.Size([256]).\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:30 d2.checkpoint.c2_model_loading]: \u001b[0mnorm.bias will not be loaded. Please double check and see if this is desired.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:30 d2.checkpoint.c2_model_loading]: \u001b[0mShape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.pixel_decoder.adapter_1.norm.weight in model is torch.Size([256]).\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:30 d2.checkpoint.c2_model_loading]: \u001b[0mnorm.weight will not be loaded. Please double check and see if this is desired.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:30 d2.checkpoint.c2_model_loading]: \u001b[0mShape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.pixel_decoder.layer_1.norm.bias in model is torch.Size([256]).\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:30 d2.checkpoint.c2_model_loading]: \u001b[0mnorm.bias will not be loaded. Please double check and see if this is desired.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:30 d2.checkpoint.c2_model_loading]: \u001b[0mShape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.pixel_decoder.layer_1.norm.weight in model is torch.Size([256]).\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:30 d2.checkpoint.c2_model_loading]: \u001b[0mnorm.weight will not be loaded. Please double check and see if this is desired.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:30 d2.checkpoint.c2_model_loading]: \u001b[0mShape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.decoder.norm.bias in model is torch.Size([256]).\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:30 d2.checkpoint.c2_model_loading]: \u001b[0mnorm.bias will not be loaded. Please double check and see if this is desired.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:30 d2.checkpoint.c2_model_loading]: \u001b[0mShape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.decoder.norm.weight in model is torch.Size([256]).\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:30 d2.checkpoint.c2_model_loading]: \u001b[0mnorm.weight will not be loaded. Please double check and see if this is desired.\n",
            "\u001b[32m[07/26 14:55:31 d2.checkpoint.c2_model_loading]: \u001b[0mFollowing weights matched with submodule backbone - Total num: 128\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:31 fvcore.common.checkpoint]: \u001b[0mSome model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mbackbone.norm0.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34mcriterion.empty_weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.adapter_1.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.3.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.3.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.4.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.4.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.layer_1.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.level_embed\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor._bbox_embed.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor._bbox_embed.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor._bbox_embed.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.0.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.0.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.0.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.1.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.1.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.1.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.2.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.2.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.2.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.3.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.3.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.3.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.4.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.4.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.4.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.5.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.5.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.5.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.6.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.6.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.6.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.7.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.7.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.7.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.8.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.8.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.8.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.class_embed.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.0.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.0.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.0.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.1.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.1.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.1.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.2.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.2.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.2.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.3.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.3.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.3.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.4.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.4.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.4.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.5.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.5.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.5.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.6.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.6.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.6.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.7.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.7.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.7.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.8.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.8.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.8.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.norm.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.ref_point_head.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.ref_point_head.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder_norm.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.label_enc.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.query_embed.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.query_feat.weight\u001b[0m\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:55:31 fvcore.common.checkpoint]: \u001b[0mThe checkpoint state_dict contains keys that are not used by the model:\n",
            "  \u001b[35mhead.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.1.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.1.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.1.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.11.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.13.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.15.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.17.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.3.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.5.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.7.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.9.attn_mask\u001b[0m\n",
            "  \u001b[35mnorm.{bias, weight}\u001b[0m\n",
            "\u001b[32m[07/26 14:55:31 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "\u001b[32m[07/26 14:56:20 d2.utils.events]: \u001b[0m eta: 7 days, 14:54:04  iter: 19  total_loss: 492.4  loss_ce: 11.31  loss_mask: 2.241  loss_dice: 4.455  loss_bbox: 2.182  loss_giou: 1.293  loss_ce_dn: 20.03  loss_mask_dn: 2.227  loss_dice_dn: 4.461  loss_bbox_dn: 1.236  loss_giou_dn: 0.8428  loss_ce_0: 19.56  loss_mask_0: 2.02  loss_dice_0: 4.334  loss_bbox_0: 2.659  loss_giou_0: 1.461  loss_ce_1: 12.79  loss_mask_1: 2.003  loss_dice_1: 4.328  loss_bbox_1: 2.419  loss_giou_1: 1.4  loss_ce_dn_1: 19.56  loss_mask_dn_1: 2.12  loss_dice_dn_1: 4.507  loss_bbox_dn_1: 1.238  loss_giou_dn_1: 0.8373  loss_ce_2: 12.24  loss_mask_2: 2.159  loss_dice_2: 4.418  loss_bbox_2: 2.236  loss_giou_2: 1.346  loss_ce_dn_2: 19.93  loss_mask_dn_2: 2.257  loss_dice_dn_2: 4.446  loss_bbox_dn_2: 1.237  loss_giou_dn_2: 0.8404  loss_ce_3: 11.09  loss_mask_3: 2.205  loss_dice_3: 4.473  loss_bbox_3: 2.171  loss_giou_3: 1.32  loss_ce_dn_3: 20.16  loss_mask_dn_3: 2.197  loss_dice_dn_3: 4.504  loss_bbox_dn_3: 1.236  loss_giou_dn_3: 0.8393  loss_ce_4: 11.26  loss_mask_4: 2.204  loss_dice_4: 4.483  loss_bbox_4: 2.162  loss_giou_4: 1.306  loss_ce_dn_4: 19.81  loss_mask_dn_4: 2.196  loss_dice_dn_4: 4.491  loss_bbox_dn_4: 1.236  loss_giou_dn_4: 0.8398  loss_ce_5: 11.55  loss_mask_5: 2.231  loss_dice_5: 4.432  loss_bbox_5: 2.144  loss_giou_5: 1.32  loss_ce_dn_5: 20.01  loss_mask_dn_5: 2.232  loss_dice_dn_5: 4.437  loss_bbox_dn_5: 1.236  loss_giou_dn_5: 0.8414  loss_ce_6: 11.4  loss_mask_6: 2.292  loss_dice_6: 4.421  loss_bbox_6: 2.171  loss_giou_6: 1.299  loss_ce_dn_6: 19.62  loss_mask_dn_6: 2.304  loss_dice_dn_6: 4.428  loss_bbox_dn_6: 1.235  loss_giou_dn_6: 0.841  loss_ce_7: 11.54  loss_mask_7: 2.324  loss_dice_7: 4.414  loss_bbox_7: 2.189  loss_giou_7: 1.28  loss_ce_dn_7: 20.13  loss_mask_dn_7: 2.321  loss_dice_dn_7: 4.408  loss_bbox_dn_7: 1.235  loss_giou_dn_7: 0.8406  loss_ce_8: 11.4  loss_mask_8: 2.174  loss_dice_8: 4.472  loss_bbox_8: 2.166  loss_giou_8: 1.29  loss_ce_dn_8: 20.06  loss_mask_dn_8: 2.159  loss_dice_dn_8: 4.469  loss_bbox_dn_8: 1.236  loss_giou_dn_8: 0.8409    time: 2.0677  last_time: 2.0703  data_time: 0.0357  last_data_time: 0.0043   lr: 0.0001  max_mem: 13215M\n",
            "2025-07-26 14:56:21.659284: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753541781.916135    9837 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753541781.991509    9837 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-26 14:56:22.534256: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m[07/26 14:57:13 d2.utils.events]: \u001b[0m eta: 7 days, 22:56:39  iter: 39  total_loss: 459.8  loss_ce: 10.96  loss_mask: 2.311  loss_dice: 4.345  loss_bbox: 2.332  loss_giou: 1.365  loss_ce_dn: 18.75  loss_mask_dn: 2.3  loss_dice_dn: 4.343  loss_bbox_dn: 1.143  loss_giou_dn: 0.881  loss_ce_0: 19.5  loss_mask_0: 1.761  loss_dice_0: 4.335  loss_bbox_0: 2.673  loss_giou_0: 1.557  loss_ce_1: 10.61  loss_mask_1: 1.992  loss_dice_1: 4.253  loss_bbox_1: 2.582  loss_giou_1: 1.491  loss_ce_dn_1: 17.6  loss_mask_dn_1: 2.186  loss_dice_dn_1: 4.389  loss_bbox_dn_1: 1.138  loss_giou_dn_1: 0.8391  loss_ce_2: 10.88  loss_mask_2: 2.156  loss_dice_2: 4.336  loss_bbox_2: 2.421  loss_giou_2: 1.459  loss_ce_dn_2: 17.39  loss_mask_dn_2: 2.281  loss_dice_dn_2: 4.337  loss_bbox_dn_2: 1.134  loss_giou_dn_2: 0.8383  loss_ce_3: 10.75  loss_mask_3: 2.302  loss_dice_3: 4.316  loss_bbox_3: 2.36  loss_giou_3: 1.39  loss_ce_dn_3: 18.17  loss_mask_dn_3: 2.327  loss_dice_dn_3: 4.321  loss_bbox_dn_3: 1.131  loss_giou_dn_3: 0.8409  loss_ce_4: 10.86  loss_mask_4: 2.302  loss_dice_4: 4.354  loss_bbox_4: 2.339  loss_giou_4: 1.387  loss_ce_dn_4: 18.58  loss_mask_dn_4: 2.259  loss_dice_dn_4: 4.354  loss_bbox_dn_4: 1.129  loss_giou_dn_4: 0.8459  loss_ce_5: 11.05  loss_mask_5: 2.152  loss_dice_5: 4.393  loss_bbox_5: 2.286  loss_giou_5: 1.383  loss_ce_dn_5: 18.7  loss_mask_dn_5: 2.132  loss_dice_dn_5: 4.403  loss_bbox_dn_5: 1.127  loss_giou_dn_5: 0.851  loss_ce_6: 11  loss_mask_6: 2.33  loss_dice_6: 4.362  loss_bbox_6: 2.287  loss_giou_6: 1.376  loss_ce_dn_6: 18.73  loss_mask_dn_6: 2.353  loss_dice_dn_6: 4.347  loss_bbox_dn_6: 1.126  loss_giou_dn_6: 0.8562  loss_ce_7: 11.23  loss_mask_7: 2.341  loss_dice_7: 4.322  loss_bbox_7: 2.331  loss_giou_7: 1.369  loss_ce_dn_7: 18.63  loss_mask_dn_7: 2.307  loss_dice_dn_7: 4.309  loss_bbox_dn_7: 1.128  loss_giou_dn_7: 0.8625  loss_ce_8: 11.21  loss_mask_8: 2.329  loss_dice_8: 4.356  loss_bbox_8: 2.3  loss_giou_8: 1.365  loss_ce_dn_8: 18.75  loss_mask_dn_8: 2.32  loss_dice_dn_8: 4.36  loss_bbox_dn_8: 1.134  loss_giou_dn_8: 0.8708    time: 2.1643  last_time: 2.3861  data_time: 0.0111  last_data_time: 0.0044   lr: 0.0001  max_mem: 13231M\n",
            "\u001b[32m[07/26 14:57:59 d2.utils.events]: \u001b[0m eta: 8 days, 6:29:29  iter: 59  total_loss: 441.6  loss_ce: 10.94  loss_mask: 2.407  loss_dice: 4.2  loss_bbox: 2.254  loss_giou: 1.427  loss_ce_dn: 17.67  loss_mask_dn: 2.281  loss_dice_dn: 4.202  loss_bbox_dn: 1.104  loss_giou_dn: 0.8645  loss_ce_0: 19.2  loss_mask_0: 1.761  loss_dice_0: 4.265  loss_bbox_0: 2.757  loss_giou_0: 1.581  loss_ce_1: 10.77  loss_mask_1: 1.888  loss_dice_1: 4.197  loss_bbox_1: 2.581  loss_giou_1: 1.553  loss_ce_dn_1: 15.32  loss_mask_dn_1: 2.131  loss_dice_dn_1: 4.324  loss_bbox_dn_1: 1.086  loss_giou_dn_1: 0.8488  loss_ce_2: 10.6  loss_mask_2: 1.915  loss_dice_2: 4.138  loss_bbox_2: 2.484  loss_giou_2: 1.52  loss_ce_dn_2: 14.71  loss_mask_dn_2: 2.119  loss_dice_dn_2: 4.25  loss_bbox_dn_2: 1.086  loss_giou_dn_2: 0.8493  loss_ce_3: 10.68  loss_mask_3: 2.049  loss_dice_3: 4.051  loss_bbox_3: 2.284  loss_giou_3: 1.45  loss_ce_dn_3: 15.76  loss_mask_dn_3: 2.095  loss_dice_dn_3: 4.194  loss_bbox_dn_3: 1.086  loss_giou_dn_3: 0.8503  loss_ce_4: 10.8  loss_mask_4: 2.268  loss_dice_4: 4.093  loss_bbox_4: 2.309  loss_giou_4: 1.443  loss_ce_dn_4: 16.95  loss_mask_dn_4: 2.272  loss_dice_dn_4: 4.18  loss_bbox_dn_4: 1.088  loss_giou_dn_4: 0.8516  loss_ce_5: 11  loss_mask_5: 2.295  loss_dice_5: 4.13  loss_bbox_5: 2.284  loss_giou_5: 1.444  loss_ce_dn_5: 17.63  loss_mask_dn_5: 2.236  loss_dice_dn_5: 4.15  loss_bbox_dn_5: 1.091  loss_giou_dn_5: 0.8533  loss_ce_6: 11  loss_mask_6: 2.385  loss_dice_6: 4.164  loss_bbox_6: 2.259  loss_giou_6: 1.407  loss_ce_dn_6: 17.6  loss_mask_dn_6: 2.314  loss_dice_dn_6: 4.195  loss_bbox_dn_6: 1.095  loss_giou_dn_6: 0.855  loss_ce_7: 10.85  loss_mask_7: 2.36  loss_dice_7: 4.154  loss_bbox_7: 2.299  loss_giou_7: 1.445  loss_ce_dn_7: 17.71  loss_mask_dn_7: 2.273  loss_dice_dn_7: 4.184  loss_bbox_dn_7: 1.098  loss_giou_dn_7: 0.857  loss_ce_8: 10.89  loss_mask_8: 2.36  loss_dice_8: 4.184  loss_bbox_8: 2.287  loss_giou_8: 1.412  loss_ce_dn_8: 17.64  loss_mask_dn_8: 2.257  loss_dice_dn_8: 4.209  loss_bbox_dn_8: 1.102  loss_giou_dn_8: 0.8599    time: 2.2099  last_time: 2.2332  data_time: 0.0140  last_data_time: 0.0142   lr: 0.0001  max_mem: 13231M\n",
            "\u001b[32m[07/26 14:58:45 d2.utils.events]: \u001b[0m eta: 8 days, 8:42:07  iter: 79  total_loss: 424  loss_ce: 10.6  loss_mask: 2.213  loss_dice: 4.097  loss_bbox: 2.214  loss_giou: 1.552  loss_ce_dn: 17.06  loss_mask_dn: 2.184  loss_dice_dn: 4.106  loss_bbox_dn: 1.127  loss_giou_dn: 0.8578  loss_ce_0: 19.07  loss_mask_0: 1.671  loss_dice_0: 4.264  loss_bbox_0: 2.791  loss_giou_0: 1.693  loss_ce_1: 10.35  loss_mask_1: 1.829  loss_dice_1: 4.085  loss_bbox_1: 2.666  loss_giou_1: 1.658  loss_ce_dn_1: 13.43  loss_mask_dn_1: 2.065  loss_dice_dn_1: 4.237  loss_bbox_dn_1: 1.132  loss_giou_dn_1: 0.8389  loss_ce_2: 9.819  loss_mask_2: 1.966  loss_dice_2: 4.07  loss_bbox_2: 2.772  loss_giou_2: 1.624  loss_ce_dn_2: 12.19  loss_mask_dn_2: 2.121  loss_dice_dn_2: 4.164  loss_bbox_dn_2: 1.128  loss_giou_dn_2: 0.8401  loss_ce_3: 9.824  loss_mask_3: 2.051  loss_dice_3: 4.009  loss_bbox_3: 2.669  loss_giou_3: 1.645  loss_ce_dn_3: 12.85  loss_mask_dn_3: 2.118  loss_dice_dn_3: 4.127  loss_bbox_dn_3: 1.125  loss_giou_dn_3: 0.8391  loss_ce_4: 10.06  loss_mask_4: 2.062  loss_dice_4: 4.021  loss_bbox_4: 2.491  loss_giou_4: 1.641  loss_ce_dn_4: 14.11  loss_mask_dn_4: 2.053  loss_dice_dn_4: 4.092  loss_bbox_dn_4: 1.124  loss_giou_dn_4: 0.8395  loss_ce_5: 10.4  loss_mask_5: 2.101  loss_dice_5: 4.031  loss_bbox_5: 2.37  loss_giou_5: 1.622  loss_ce_dn_5: 15.59  loss_mask_dn_5: 2.062  loss_dice_dn_5: 4.057  loss_bbox_dn_5: 1.123  loss_giou_dn_5: 0.8402  loss_ce_6: 10.45  loss_mask_6: 2.226  loss_dice_6: 4.031  loss_bbox_6: 2.303  loss_giou_6: 1.571  loss_ce_dn_6: 16.48  loss_mask_dn_6: 2.228  loss_dice_dn_6: 4.093  loss_bbox_dn_6: 1.123  loss_giou_dn_6: 0.8432  loss_ce_7: 10.65  loss_mask_7: 2.228  loss_dice_7: 4.061  loss_bbox_7: 2.292  loss_giou_7: 1.6  loss_ce_dn_7: 16.98  loss_mask_dn_7: 2.293  loss_dice_dn_7: 4.086  loss_bbox_dn_7: 1.124  loss_giou_dn_7: 0.8459  loss_ce_8: 10.48  loss_mask_8: 2.246  loss_dice_8: 4.103  loss_bbox_8: 2.273  loss_giou_8: 1.596  loss_ce_dn_8: 16.97  loss_mask_dn_8: 2.226  loss_dice_dn_8: 4.094  loss_bbox_dn_8: 1.125  loss_giou_dn_8: 0.8524    time: 2.2269  last_time: 2.2473  data_time: 0.0149  last_data_time: 0.0154   lr: 0.0001  max_mem: 13274M\n",
            "\u001b[32m[07/26 14:59:30 d2.utils.events]: \u001b[0m eta: 8 days, 8:55:39  iter: 99  total_loss: 397.7  loss_ce: 9.302  loss_mask: 2.167  loss_dice: 3.822  loss_bbox: 2.231  loss_giou: 1.486  loss_ce_dn: 16.99  loss_mask_dn: 2.209  loss_dice_dn: 3.845  loss_bbox_dn: 1.215  loss_giou_dn: 0.8512  loss_ce_0: 19.3  loss_mask_0: 2.002  loss_dice_0: 4.059  loss_bbox_0: 2.67  loss_giou_0: 1.566  loss_ce_1: 8.739  loss_mask_1: 2.083  loss_dice_1: 3.972  loss_bbox_1: 2.63  loss_giou_1: 1.654  loss_ce_dn_1: 12.62  loss_mask_dn_1: 2.083  loss_dice_dn_1: 4.079  loss_bbox_dn_1: 1.23  loss_giou_dn_1: 0.8421  loss_ce_2: 8.569  loss_mask_2: 2.091  loss_dice_2: 3.933  loss_bbox_2: 2.629  loss_giou_2: 1.646  loss_ce_dn_2: 11.47  loss_mask_dn_2: 2.003  loss_dice_dn_2: 3.993  loss_bbox_dn_2: 1.222  loss_giou_dn_2: 0.8418  loss_ce_3: 8.727  loss_mask_3: 2.052  loss_dice_3: 3.939  loss_bbox_3: 2.705  loss_giou_3: 1.614  loss_ce_dn_3: 11.92  loss_mask_dn_3: 2.036  loss_dice_dn_3: 3.949  loss_bbox_dn_3: 1.215  loss_giou_dn_3: 0.8403  loss_ce_4: 8.687  loss_mask_4: 2.083  loss_dice_4: 3.91  loss_bbox_4: 2.538  loss_giou_4: 1.613  loss_ce_dn_4: 12.88  loss_mask_dn_4: 2.138  loss_dice_dn_4: 3.951  loss_bbox_dn_4: 1.209  loss_giou_dn_4: 0.8403  loss_ce_5: 8.466  loss_mask_5: 2.038  loss_dice_5: 3.852  loss_bbox_5: 2.516  loss_giou_5: 1.61  loss_ce_dn_5: 14.01  loss_mask_dn_5: 2.18  loss_dice_dn_5: 3.903  loss_bbox_dn_5: 1.205  loss_giou_dn_5: 0.842  loss_ce_6: 8.568  loss_mask_6: 2.196  loss_dice_6: 3.864  loss_bbox_6: 2.276  loss_giou_6: 1.515  loss_ce_dn_6: 15.08  loss_mask_dn_6: 2.14  loss_dice_dn_6: 3.871  loss_bbox_dn_6: 1.204  loss_giou_dn_6: 0.8436  loss_ce_7: 8.77  loss_mask_7: 2.249  loss_dice_7: 3.857  loss_bbox_7: 2.216  loss_giou_7: 1.511  loss_ce_dn_7: 16.25  loss_mask_dn_7: 2.192  loss_dice_dn_7: 3.826  loss_bbox_dn_7: 1.204  loss_giou_dn_7: 0.8453  loss_ce_8: 9.092  loss_mask_8: 2.148  loss_dice_8: 3.868  loss_bbox_8: 2.247  loss_giou_8: 1.496  loss_ce_dn_8: 16.64  loss_mask_dn_8: 2.163  loss_dice_dn_8: 3.846  loss_bbox_dn_8: 1.207  loss_giou_dn_8: 0.8477    time: 2.2382  last_time: 2.2484  data_time: 0.0118  last_data_time: 0.0205   lr: 0.0001  max_mem: 13274M\n",
            "\u001b[32m[07/26 15:00:16 d2.utils.events]: \u001b[0m eta: 8 days, 8:55:26  iter: 119  total_loss: 376.8  loss_ce: 9.072  loss_mask: 2.093  loss_dice: 3.978  loss_bbox: 2.451  loss_giou: 1.429  loss_ce_dn: 15.28  loss_mask_dn: 2.099  loss_dice_dn: 3.947  loss_bbox_dn: 1.206  loss_giou_dn: 0.8655  loss_ce_0: 19.02  loss_mask_0: 1.888  loss_dice_0: 4.095  loss_bbox_0: 2.76  loss_giou_0: 1.607  loss_ce_1: 9.005  loss_mask_1: 2.123  loss_dice_1: 4.009  loss_bbox_1: 2.888  loss_giou_1: 1.626  loss_ce_dn_1: 11.42  loss_mask_dn_1: 2.083  loss_dice_dn_1: 4.129  loss_bbox_dn_1: 1.186  loss_giou_dn_1: 0.8396  loss_ce_2: 8.183  loss_mask_2: 2.109  loss_dice_2: 3.943  loss_bbox_2: 2.913  loss_giou_2: 1.609  loss_ce_dn_2: 9.753  loss_mask_dn_2: 2.028  loss_dice_dn_2: 4.033  loss_bbox_dn_2: 1.186  loss_giou_dn_2: 0.8386  loss_ce_3: 8.475  loss_mask_3: 2.008  loss_dice_3: 3.912  loss_bbox_3: 2.813  loss_giou_3: 1.602  loss_ce_dn_3: 9.881  loss_mask_dn_3: 2.045  loss_dice_dn_3: 3.938  loss_bbox_dn_3: 1.188  loss_giou_dn_3: 0.8395  loss_ce_4: 8.447  loss_mask_4: 2.031  loss_dice_4: 3.906  loss_bbox_4: 2.819  loss_giou_4: 1.524  loss_ce_dn_4: 10.47  loss_mask_dn_4: 2.042  loss_dice_dn_4: 3.984  loss_bbox_dn_4: 1.194  loss_giou_dn_4: 0.8418  loss_ce_5: 8.585  loss_mask_5: 2.052  loss_dice_5: 3.946  loss_bbox_5: 2.628  loss_giou_5: 1.516  loss_ce_dn_5: 11.21  loss_mask_dn_5: 2.074  loss_dice_dn_5: 4.025  loss_bbox_dn_5: 1.199  loss_giou_dn_5: 0.8438  loss_ce_6: 8.705  loss_mask_6: 2.059  loss_dice_6: 3.965  loss_bbox_6: 2.657  loss_giou_6: 1.523  loss_ce_dn_6: 12.28  loss_mask_dn_6: 2.103  loss_dice_dn_6: 4.016  loss_bbox_dn_6: 1.204  loss_giou_dn_6: 0.8499  loss_ce_7: 8.924  loss_mask_7: 2.151  loss_dice_7: 3.929  loss_bbox_7: 2.563  loss_giou_7: 1.485  loss_ce_dn_7: 13.6  loss_mask_dn_7: 2.2  loss_dice_dn_7: 3.994  loss_bbox_dn_7: 1.203  loss_giou_dn_7: 0.8556  loss_ce_8: 9.042  loss_mask_8: 2.124  loss_dice_8: 3.968  loss_bbox_8: 2.533  loss_giou_8: 1.461  loss_ce_dn_8: 14.64  loss_mask_dn_8: 2.135  loss_dice_dn_8: 3.965  loss_bbox_dn_8: 1.204  loss_giou_dn_8: 0.8596    time: 2.2439  last_time: 2.2710  data_time: 0.0119  last_data_time: 0.0014   lr: 0.0001  max_mem: 13274M\n",
            "\u001b[32m[07/26 15:01:02 d2.utils.events]: \u001b[0m eta: 8 days, 9:15:43  iter: 139  total_loss: 348.3  loss_ce: 7.315  loss_mask: 2.144  loss_dice: 3.626  loss_bbox: 2.472  loss_giou: 1.357  loss_ce_dn: 13.03  loss_mask_dn: 2.15  loss_dice_dn: 3.629  loss_bbox_dn: 1.249  loss_giou_dn: 0.8802  loss_ce_0: 19.02  loss_mask_0: 1.914  loss_dice_0: 3.883  loss_bbox_0: 2.848  loss_giou_0: 1.608  loss_ce_1: 7.296  loss_mask_1: 1.947  loss_dice_1: 3.585  loss_bbox_1: 3.237  loss_giou_1: 1.634  loss_ce_dn_1: 10.23  loss_mask_dn_1: 2.397  loss_dice_dn_1: 3.854  loss_bbox_dn_1: 1.215  loss_giou_dn_1: 0.836  loss_ce_2: 6.34  loss_mask_2: 1.938  loss_dice_2: 3.642  loss_bbox_2: 3.553  loss_giou_2: 1.66  loss_ce_dn_2: 8.713  loss_mask_dn_2: 2.183  loss_dice_dn_2: 3.822  loss_bbox_dn_2: 1.201  loss_giou_dn_2: 0.8365  loss_ce_3: 6.181  loss_mask_3: 1.986  loss_dice_3: 3.685  loss_bbox_3: 3.262  loss_giou_3: 1.575  loss_ce_dn_3: 8.426  loss_mask_dn_3: 2.156  loss_dice_dn_3: 3.767  loss_bbox_dn_3: 1.199  loss_giou_dn_3: 0.8356  loss_ce_4: 6.226  loss_mask_4: 1.98  loss_dice_4: 3.602  loss_bbox_4: 3.101  loss_giou_4: 1.505  loss_ce_dn_4: 8.655  loss_mask_dn_4: 2.099  loss_dice_dn_4: 3.69  loss_bbox_dn_4: 1.203  loss_giou_dn_4: 0.8362  loss_ce_5: 6.374  loss_mask_5: 1.978  loss_dice_5: 3.693  loss_bbox_5: 2.911  loss_giou_5: 1.437  loss_ce_dn_5: 9.091  loss_mask_dn_5: 2.154  loss_dice_dn_5: 3.705  loss_bbox_dn_5: 1.212  loss_giou_dn_5: 0.8413  loss_ce_6: 6.629  loss_mask_6: 2.066  loss_dice_6: 3.664  loss_bbox_6: 2.869  loss_giou_6: 1.398  loss_ce_dn_6: 9.93  loss_mask_dn_6: 2.175  loss_dice_dn_6: 3.713  loss_bbox_dn_6: 1.226  loss_giou_dn_6: 0.8485  loss_ce_7: 6.7  loss_mask_7: 2.139  loss_dice_7: 3.741  loss_bbox_7: 2.834  loss_giou_7: 1.382  loss_ce_dn_7: 10.86  loss_mask_dn_7: 2.15  loss_dice_dn_7: 3.673  loss_bbox_dn_7: 1.241  loss_giou_dn_7: 0.8576  loss_ce_8: 7.027  loss_mask_8: 2.157  loss_dice_8: 3.681  loss_bbox_8: 2.554  loss_giou_8: 1.392  loss_ce_dn_8: 12  loss_mask_dn_8: 2.105  loss_dice_dn_8: 3.654  loss_bbox_dn_8: 1.245  loss_giou_dn_8: 0.8676    time: 2.2513  last_time: 2.2888  data_time: 0.0157  last_data_time: 0.0107   lr: 0.0001  max_mem: 13274M\n",
            "\u001b[32m[07/26 15:01:47 d2.utils.events]: \u001b[0m eta: 8 days, 9:28:25  iter: 159  total_loss: 327.5  loss_ce: 7.205  loss_mask: 1.964  loss_dice: 3.746  loss_bbox: 2.446  loss_giou: 1.542  loss_ce_dn: 11.55  loss_mask_dn: 1.887  loss_dice_dn: 3.763  loss_bbox_dn: 1.123  loss_giou_dn: 0.8872  loss_ce_0: 19.17  loss_mask_0: 1.83  loss_dice_0: 3.769  loss_bbox_0: 2.681  loss_giou_0: 1.635  loss_ce_1: 7.157  loss_mask_1: 1.912  loss_dice_1: 3.704  loss_bbox_1: 3.088  loss_giou_1: 1.664  loss_ce_dn_1: 9.467  loss_mask_dn_1: 2.063  loss_dice_dn_1: 3.874  loss_bbox_dn_1: 1.14  loss_giou_dn_1: 0.8388  loss_ce_2: 6.303  loss_mask_2: 1.939  loss_dice_2: 3.731  loss_bbox_2: 3.111  loss_giou_2: 1.746  loss_ce_dn_2: 7.815  loss_mask_dn_2: 2.045  loss_dice_dn_2: 3.865  loss_bbox_dn_2: 1.116  loss_giou_dn_2: 0.8399  loss_ce_3: 6.174  loss_mask_3: 2.002  loss_dice_3: 3.778  loss_bbox_3: 3.009  loss_giou_3: 1.701  loss_ce_dn_3: 7.523  loss_mask_dn_3: 2.003  loss_dice_dn_3: 3.899  loss_bbox_dn_3: 1.096  loss_giou_dn_3: 0.8413  loss_ce_4: 6.088  loss_mask_4: 1.95  loss_dice_4: 3.763  loss_bbox_4: 2.921  loss_giou_4: 1.722  loss_ce_dn_4: 7.893  loss_mask_dn_4: 1.993  loss_dice_dn_4: 3.806  loss_bbox_dn_4: 1.087  loss_giou_dn_4: 0.8461  loss_ce_5: 6.205  loss_mask_5: 2.06  loss_dice_5: 3.744  loss_bbox_5: 2.907  loss_giou_5: 1.758  loss_ce_dn_5: 8.413  loss_mask_dn_5: 1.967  loss_dice_dn_5: 3.819  loss_bbox_dn_5: 1.083  loss_giou_dn_5: 0.8515  loss_ce_6: 6.318  loss_mask_6: 2.038  loss_dice_6: 3.766  loss_bbox_6: 2.873  loss_giou_6: 1.661  loss_ce_dn_6: 9.008  loss_mask_dn_6: 1.977  loss_dice_dn_6: 3.768  loss_bbox_dn_6: 1.085  loss_giou_dn_6: 0.8592  loss_ce_7: 6.449  loss_mask_7: 1.987  loss_dice_7: 3.719  loss_bbox_7: 2.789  loss_giou_7: 1.67  loss_ce_dn_7: 9.766  loss_mask_dn_7: 1.897  loss_dice_dn_7: 3.787  loss_bbox_dn_7: 1.092  loss_giou_dn_7: 0.8669  loss_ce_8: 6.945  loss_mask_8: 1.97  loss_dice_8: 3.714  loss_bbox_8: 2.595  loss_giou_8: 1.561  loss_ce_dn_8: 10.5  loss_mask_dn_8: 1.861  loss_dice_dn_8: 3.813  loss_bbox_dn_8: 1.107  loss_giou_dn_8: 0.8746    time: 2.2551  last_time: 2.2736  data_time: 0.0127  last_data_time: 0.0165   lr: 0.0001  max_mem: 13274M\n",
            "\u001b[32m[07/26 15:02:33 d2.utils.events]: \u001b[0m eta: 8 days, 9:32:45  iter: 179  total_loss: 330.5  loss_ce: 6.542  loss_mask: 2.205  loss_dice: 3.632  loss_bbox: 2.604  loss_giou: 1.493  loss_ce_dn: 11.73  loss_mask_dn: 2.205  loss_dice_dn: 3.578  loss_bbox_dn: 1.291  loss_giou_dn: 0.8644  loss_ce_0: 19.11  loss_mask_0: 2.156  loss_dice_0: 3.818  loss_bbox_0: 2.888  loss_giou_0: 1.549  loss_ce_1: 6.794  loss_mask_1: 2.197  loss_dice_1: 3.678  loss_bbox_1: 3.132  loss_giou_1: 1.631  loss_ce_dn_1: 10.15  loss_mask_dn_1: 2.237  loss_dice_dn_1: 3.669  loss_bbox_dn_1: 1.255  loss_giou_dn_1: 0.8254  loss_ce_2: 6.054  loss_mask_2: 2.171  loss_dice_2: 3.759  loss_bbox_2: 3.054  loss_giou_2: 1.642  loss_ce_dn_2: 8.132  loss_mask_dn_2: 2.185  loss_dice_dn_2: 3.551  loss_bbox_dn_2: 1.249  loss_giou_dn_2: 0.8267  loss_ce_3: 5.825  loss_mask_3: 2.107  loss_dice_3: 3.689  loss_bbox_3: 3.054  loss_giou_3: 1.588  loss_ce_dn_3: 7.795  loss_mask_dn_3: 2.193  loss_dice_dn_3: 3.565  loss_bbox_dn_3: 1.238  loss_giou_dn_3: 0.8297  loss_ce_4: 5.74  loss_mask_4: 2.193  loss_dice_4: 3.692  loss_bbox_4: 2.997  loss_giou_4: 1.636  loss_ce_dn_4: 8.18  loss_mask_dn_4: 2.275  loss_dice_dn_4: 3.546  loss_bbox_dn_4: 1.237  loss_giou_dn_4: 0.8344  loss_ce_5: 5.902  loss_mask_5: 2.234  loss_dice_5: 3.669  loss_bbox_5: 2.962  loss_giou_5: 1.594  loss_ce_dn_5: 8.719  loss_mask_dn_5: 2.21  loss_dice_dn_5: 3.549  loss_bbox_dn_5: 1.246  loss_giou_dn_5: 0.8401  loss_ce_6: 6.064  loss_mask_6: 2.161  loss_dice_6: 3.616  loss_bbox_6: 2.881  loss_giou_6: 1.569  loss_ce_dn_6: 9.383  loss_mask_dn_6: 2.218  loss_dice_dn_6: 3.543  loss_bbox_dn_6: 1.26  loss_giou_dn_6: 0.8471  loss_ce_7: 5.992  loss_mask_7: 2.204  loss_dice_7: 3.621  loss_bbox_7: 2.83  loss_giou_7: 1.589  loss_ce_dn_7: 9.894  loss_mask_dn_7: 2.212  loss_dice_dn_7: 3.572  loss_bbox_dn_7: 1.267  loss_giou_dn_7: 0.8538  loss_ce_8: 6.147  loss_mask_8: 2.144  loss_dice_8: 3.656  loss_bbox_8: 2.682  loss_giou_8: 1.563  loss_ce_dn_8: 10.9  loss_mask_dn_8: 2.107  loss_dice_dn_8: 3.579  loss_bbox_dn_8: 1.279  loss_giou_dn_8: 0.8576    time: 2.2578  last_time: 2.2678  data_time: 0.0129  last_data_time: 0.0206   lr: 0.0001  max_mem: 13274M\n",
            "\u001b[32m[07/26 15:03:19 d2.utils.events]: \u001b[0m eta: 8 days, 9:42:02  iter: 199  total_loss: 329.2  loss_ce: 7.017  loss_mask: 2.14  loss_dice: 3.716  loss_bbox: 2.544  loss_giou: 1.552  loss_ce_dn: 10.8  loss_mask_dn: 2.037  loss_dice_dn: 3.679  loss_bbox_dn: 1.189  loss_giou_dn: 0.8782  loss_ce_0: 19.1  loss_mask_0: 1.89  loss_dice_0: 3.8  loss_bbox_0: 2.654  loss_giou_0: 1.519  loss_ce_1: 7.156  loss_mask_1: 2.039  loss_dice_1: 3.705  loss_bbox_1: 3.137  loss_giou_1: 1.65  loss_ce_dn_1: 9.626  loss_mask_dn_1: 2.064  loss_dice_dn_1: 3.856  loss_bbox_dn_1: 1.168  loss_giou_dn_1: 0.8277  loss_ce_2: 6.289  loss_mask_2: 2.081  loss_dice_2: 3.731  loss_bbox_2: 3.228  loss_giou_2: 1.696  loss_ce_dn_2: 8.217  loss_mask_dn_2: 2.081  loss_dice_dn_2: 3.759  loss_bbox_dn_2: 1.152  loss_giou_dn_2: 0.8256  loss_ce_3: 6.102  loss_mask_3: 2.093  loss_dice_3: 3.747  loss_bbox_3: 3.142  loss_giou_3: 1.654  loss_ce_dn_3: 7.852  loss_mask_dn_3: 2.095  loss_dice_dn_3: 3.792  loss_bbox_dn_3: 1.155  loss_giou_dn_3: 0.8323  loss_ce_4: 6.03  loss_mask_4: 2.136  loss_dice_4: 3.761  loss_bbox_4: 3.068  loss_giou_4: 1.657  loss_ce_dn_4: 7.973  loss_mask_dn_4: 2.09  loss_dice_dn_4: 3.762  loss_bbox_dn_4: 1.165  loss_giou_dn_4: 0.8358  loss_ce_5: 5.946  loss_mask_5: 2.205  loss_dice_5: 3.707  loss_bbox_5: 2.84  loss_giou_5: 1.654  loss_ce_dn_5: 8.297  loss_mask_dn_5: 2.093  loss_dice_dn_5: 3.775  loss_bbox_dn_5: 1.176  loss_giou_dn_5: 0.8438  loss_ce_6: 5.909  loss_mask_6: 2.098  loss_dice_6: 3.73  loss_bbox_6: 2.944  loss_giou_6: 1.651  loss_ce_dn_6: 8.541  loss_mask_dn_6: 2.053  loss_dice_dn_6: 3.729  loss_bbox_dn_6: 1.179  loss_giou_dn_6: 0.8529  loss_ce_7: 6.275  loss_mask_7: 2.117  loss_dice_7: 3.739  loss_bbox_7: 2.76  loss_giou_7: 1.626  loss_ce_dn_7: 9.28  loss_mask_dn_7: 2.082  loss_dice_dn_7: 3.759  loss_bbox_dn_7: 1.183  loss_giou_dn_7: 0.8592  loss_ce_8: 6.737  loss_mask_8: 2.096  loss_dice_8: 3.716  loss_bbox_8: 2.685  loss_giou_8: 1.601  loss_ce_dn_8: 9.983  loss_mask_dn_8: 2.006  loss_dice_dn_8: 3.675  loss_bbox_dn_8: 1.184  loss_giou_dn_8: 0.8668    time: 2.2607  last_time: 2.3221  data_time: 0.0139  last_data_time: 0.0199   lr: 0.0001  max_mem: 13274M\n",
            "\u001b[32m[07/26 15:04:04 d2.utils.events]: \u001b[0m eta: 8 days, 9:43:39  iter: 219  total_loss: 310.3  loss_ce: 6.02  loss_mask: 2.107  loss_dice: 3.657  loss_bbox: 2.932  loss_giou: 1.514  loss_ce_dn: 8.585  loss_mask_dn: 2.075  loss_dice_dn: 3.737  loss_bbox_dn: 1.278  loss_giou_dn: 0.9266  loss_ce_0: 18.96  loss_mask_0: 1.903  loss_dice_0: 3.943  loss_bbox_0: 2.858  loss_giou_0: 1.709  loss_ce_1: 6.35  loss_mask_1: 2.097  loss_dice_1: 3.808  loss_bbox_1: 3.338  loss_giou_1: 1.752  loss_ce_dn_1: 7.958  loss_mask_dn_1: 2.097  loss_dice_dn_1: 3.767  loss_bbox_dn_1: 1.166  loss_giou_dn_1: 0.8405  loss_ce_2: 5.288  loss_mask_2: 2.058  loss_dice_2: 3.824  loss_bbox_2: 3.393  loss_giou_2: 1.708  loss_ce_dn_2: 6.21  loss_mask_dn_2: 2.047  loss_dice_dn_2: 3.776  loss_bbox_dn_2: 1.163  loss_giou_dn_2: 0.8421  loss_ce_3: 5.023  loss_mask_3: 2.13  loss_dice_3: 3.765  loss_bbox_3: 3.129  loss_giou_3: 1.715  loss_ce_dn_3: 5.827  loss_mask_dn_3: 2.006  loss_dice_dn_3: 3.792  loss_bbox_dn_3: 1.164  loss_giou_dn_3: 0.8474  loss_ce_4: 4.822  loss_mask_4: 2.097  loss_dice_4: 3.711  loss_bbox_4: 3.078  loss_giou_4: 1.7  loss_ce_dn_4: 5.851  loss_mask_dn_4: 2.042  loss_dice_dn_4: 3.796  loss_bbox_dn_4: 1.17  loss_giou_dn_4: 0.857  loss_ce_5: 4.933  loss_mask_5: 2.13  loss_dice_5: 3.752  loss_bbox_5: 3.057  loss_giou_5: 1.682  loss_ce_dn_5: 6.233  loss_mask_dn_5: 2.019  loss_dice_dn_5: 3.751  loss_bbox_dn_5: 1.206  loss_giou_dn_5: 0.8667  loss_ce_6: 5.187  loss_mask_6: 2.034  loss_dice_6: 3.814  loss_bbox_6: 2.882  loss_giou_6: 1.638  loss_ce_dn_6: 6.679  loss_mask_dn_6: 2.016  loss_dice_dn_6: 3.803  loss_bbox_dn_6: 1.235  loss_giou_dn_6: 0.881  loss_ce_7: 5.717  loss_mask_7: 2.103  loss_dice_7: 3.807  loss_bbox_7: 2.899  loss_giou_7: 1.613  loss_ce_dn_7: 7.347  loss_mask_dn_7: 2.011  loss_dice_dn_7: 3.733  loss_bbox_dn_7: 1.249  loss_giou_dn_7: 0.8922  loss_ce_8: 5.804  loss_mask_8: 2.096  loss_dice_8: 3.676  loss_bbox_8: 2.879  loss_giou_8: 1.585  loss_ce_dn_8: 7.866  loss_mask_dn_8: 2.069  loss_dice_dn_8: 3.717  loss_bbox_dn_8: 1.261  loss_giou_dn_8: 0.9093    time: 2.2629  last_time: 2.2623  data_time: 0.0142  last_data_time: 0.0092   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:04:50 d2.utils.events]: \u001b[0m eta: 8 days, 9:39:16  iter: 239  total_loss: 314.7  loss_ce: 6.384  loss_mask: 2.098  loss_dice: 3.779  loss_bbox: 3.141  loss_giou: 1.685  loss_ce_dn: 8.338  loss_mask_dn: 1.918  loss_dice_dn: 3.722  loss_bbox_dn: 1.13  loss_giou_dn: 0.944  loss_ce_0: 18.82  loss_mask_0: 1.727  loss_dice_0: 3.942  loss_bbox_0: 2.776  loss_giou_0: 1.676  loss_ce_1: 6.554  loss_mask_1: 1.992  loss_dice_1: 3.735  loss_bbox_1: 3.405  loss_giou_1: 1.665  loss_ce_dn_1: 8.103  loss_mask_dn_1: 1.936  loss_dice_dn_1: 3.847  loss_bbox_dn_1: 1.124  loss_giou_dn_1: 0.8399  loss_ce_2: 6.014  loss_mask_2: 1.99  loss_dice_2: 3.791  loss_bbox_2: 3.377  loss_giou_2: 1.711  loss_ce_dn_2: 6.442  loss_mask_dn_2: 1.938  loss_dice_dn_2: 3.767  loss_bbox_dn_2: 1.107  loss_giou_dn_2: 0.8457  loss_ce_3: 6.026  loss_mask_3: 2.086  loss_dice_3: 3.833  loss_bbox_3: 3.53  loss_giou_3: 1.712  loss_ce_dn_3: 6.08  loss_mask_dn_3: 1.889  loss_dice_dn_3: 3.77  loss_bbox_dn_3: 1.095  loss_giou_dn_3: 0.8573  loss_ce_4: 6.145  loss_mask_4: 2.041  loss_dice_4: 3.73  loss_bbox_4: 3.428  loss_giou_4: 1.709  loss_ce_dn_4: 6.097  loss_mask_dn_4: 1.967  loss_dice_dn_4: 3.789  loss_bbox_dn_4: 1.094  loss_giou_dn_4: 0.8652  loss_ce_5: 5.949  loss_mask_5: 2.102  loss_dice_5: 3.758  loss_bbox_5: 3.384  loss_giou_5: 1.713  loss_ce_dn_5: 6.294  loss_mask_dn_5: 1.974  loss_dice_dn_5: 3.728  loss_bbox_dn_5: 1.097  loss_giou_dn_5: 0.8713  loss_ce_6: 6.133  loss_mask_6: 2.107  loss_dice_6: 3.709  loss_bbox_6: 3.265  loss_giou_6: 1.685  loss_ce_dn_6: 6.683  loss_mask_dn_6: 1.96  loss_dice_dn_6: 3.715  loss_bbox_dn_6: 1.105  loss_giou_dn_6: 0.8852  loss_ce_7: 6.136  loss_mask_7: 2.08  loss_dice_7: 3.79  loss_bbox_7: 3.204  loss_giou_7: 1.663  loss_ce_dn_7: 7.131  loss_mask_dn_7: 2.032  loss_dice_dn_7: 3.715  loss_bbox_dn_7: 1.115  loss_giou_dn_7: 0.9018  loss_ce_8: 6.093  loss_mask_8: 2.062  loss_dice_8: 3.81  loss_bbox_8: 3.151  loss_giou_8: 1.659  loss_ce_dn_8: 7.624  loss_mask_dn_8: 1.926  loss_dice_dn_8: 3.753  loss_bbox_dn_8: 1.121  loss_giou_dn_8: 0.9236    time: 2.2642  last_time: 2.2900  data_time: 0.0148  last_data_time: 0.0040   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:05:35 d2.utils.events]: \u001b[0m eta: 8 days, 9:38:30  iter: 259  total_loss: 297  loss_ce: 5.8  loss_mask: 1.946  loss_dice: 3.851  loss_bbox: 2.655  loss_giou: 1.613  loss_ce_dn: 7.343  loss_mask_dn: 1.877  loss_dice_dn: 3.825  loss_bbox_dn: 1.146  loss_giou_dn: 0.896  loss_ce_0: 18.96  loss_mask_0: 1.809  loss_dice_0: 3.909  loss_bbox_0: 2.739  loss_giou_0: 1.669  loss_ce_1: 6.26  loss_mask_1: 1.792  loss_dice_1: 3.884  loss_bbox_1: 3.303  loss_giou_1: 1.774  loss_ce_dn_1: 7.08  loss_mask_dn_1: 1.869  loss_dice_dn_1: 3.919  loss_bbox_dn_1: 1.093  loss_giou_dn_1: 0.839  loss_ce_2: 5.616  loss_mask_2: 1.777  loss_dice_2: 3.815  loss_bbox_2: 3.229  loss_giou_2: 1.823  loss_ce_dn_2: 5.528  loss_mask_dn_2: 1.877  loss_dice_dn_2: 3.856  loss_bbox_dn_2: 1.094  loss_giou_dn_2: 0.8408  loss_ce_3: 5.275  loss_mask_3: 1.722  loss_dice_3: 3.893  loss_bbox_3: 3.089  loss_giou_3: 1.768  loss_ce_dn_3: 5.239  loss_mask_dn_3: 1.825  loss_dice_dn_3: 3.832  loss_bbox_dn_3: 1.101  loss_giou_dn_3: 0.8464  loss_ce_4: 5.148  loss_mask_4: 1.821  loss_dice_4: 3.874  loss_bbox_4: 3.125  loss_giou_4: 1.792  loss_ce_dn_4: 5.187  loss_mask_dn_4: 1.846  loss_dice_dn_4: 3.839  loss_bbox_dn_4: 1.106  loss_giou_dn_4: 0.8508  loss_ce_5: 5.142  loss_mask_5: 1.904  loss_dice_5: 3.91  loss_bbox_5: 3.021  loss_giou_5: 1.751  loss_ce_dn_5: 5.294  loss_mask_dn_5: 1.871  loss_dice_dn_5: 3.814  loss_bbox_dn_5: 1.114  loss_giou_dn_5: 0.8545  loss_ce_6: 5.219  loss_mask_6: 1.914  loss_dice_6: 3.905  loss_bbox_6: 2.857  loss_giou_6: 1.775  loss_ce_dn_6: 5.657  loss_mask_dn_6: 1.887  loss_dice_dn_6: 3.838  loss_bbox_dn_6: 1.125  loss_giou_dn_6: 0.8594  loss_ce_7: 5.248  loss_mask_7: 1.874  loss_dice_7: 3.854  loss_bbox_7: 2.872  loss_giou_7: 1.696  loss_ce_dn_7: 6.137  loss_mask_dn_7: 1.86  loss_dice_dn_7: 3.835  loss_bbox_dn_7: 1.136  loss_giou_dn_7: 0.8698  loss_ce_8: 5.466  loss_mask_8: 1.883  loss_dice_8: 3.853  loss_bbox_8: 2.733  loss_giou_8: 1.627  loss_ce_dn_8: 6.76  loss_mask_dn_8: 1.92  loss_dice_dn_8: 3.82  loss_bbox_dn_8: 1.141  loss_giou_dn_8: 0.881    time: 2.2646  last_time: 2.3083  data_time: 0.0143  last_data_time: 0.0099   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:06:21 d2.utils.events]: \u001b[0m eta: 8 days, 9:41:23  iter: 279  total_loss: 297.5  loss_ce: 5.251  loss_mask: 2.059  loss_dice: 3.526  loss_bbox: 2.511  loss_giou: 1.489  loss_ce_dn: 6.919  loss_mask_dn: 2.06  loss_dice_dn: 3.556  loss_bbox_dn: 1.24  loss_giou_dn: 0.8742  loss_ce_0: 18.76  loss_mask_0: 1.81  loss_dice_0: 3.771  loss_bbox_0: 2.69  loss_giou_0: 1.53  loss_ce_1: 5.677  loss_mask_1: 1.958  loss_dice_1: 3.686  loss_bbox_1: 3.271  loss_giou_1: 1.663  loss_ce_dn_1: 7.049  loss_mask_dn_1: 1.957  loss_dice_dn_1: 3.661  loss_bbox_dn_1: 1.183  loss_giou_dn_1: 0.8305  loss_ce_2: 5.038  loss_mask_2: 1.964  loss_dice_2: 3.664  loss_bbox_2: 3.115  loss_giou_2: 1.618  loss_ce_dn_2: 5.608  loss_mask_dn_2: 1.971  loss_dice_dn_2: 3.598  loss_bbox_dn_2: 1.172  loss_giou_dn_2: 0.8258  loss_ce_3: 4.64  loss_mask_3: 1.944  loss_dice_3: 3.685  loss_bbox_3: 3.057  loss_giou_3: 1.576  loss_ce_dn_3: 5.252  loss_mask_dn_3: 1.921  loss_dice_dn_3: 3.628  loss_bbox_dn_3: 1.153  loss_giou_dn_3: 0.8242  loss_ce_4: 4.62  loss_mask_4: 1.972  loss_dice_4: 3.708  loss_bbox_4: 3.026  loss_giou_4: 1.565  loss_ce_dn_4: 5.324  loss_mask_dn_4: 1.909  loss_dice_dn_4: 3.562  loss_bbox_dn_4: 1.143  loss_giou_dn_4: 0.8251  loss_ce_5: 5.032  loss_mask_5: 1.989  loss_dice_5: 3.623  loss_bbox_5: 2.907  loss_giou_5: 1.545  loss_ce_dn_5: 5.479  loss_mask_dn_5: 1.899  loss_dice_dn_5: 3.573  loss_bbox_dn_5: 1.137  loss_giou_dn_5: 0.8271  loss_ce_6: 5.027  loss_mask_6: 1.986  loss_dice_6: 3.631  loss_bbox_6: 2.842  loss_giou_6: 1.569  loss_ce_dn_6: 5.812  loss_mask_dn_6: 1.945  loss_dice_dn_6: 3.564  loss_bbox_dn_6: 1.148  loss_giou_dn_6: 0.8358  loss_ce_7: 5.057  loss_mask_7: 2.012  loss_dice_7: 3.65  loss_bbox_7: 2.796  loss_giou_7: 1.575  loss_ce_dn_7: 6.069  loss_mask_dn_7: 1.939  loss_dice_dn_7: 3.556  loss_bbox_dn_7: 1.166  loss_giou_dn_7: 0.8469  loss_ce_8: 5.084  loss_mask_8: 2.035  loss_dice_8: 3.567  loss_bbox_8: 2.648  loss_giou_8: 1.537  loss_ce_dn_8: 6.446  loss_mask_dn_8: 1.927  loss_dice_dn_8: 3.539  loss_bbox_dn_8: 1.203  loss_giou_dn_8: 0.861    time: 2.2659  last_time: 2.3418  data_time: 0.0116  last_data_time: 0.0114   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:07:07 d2.utils.events]: \u001b[0m eta: 8 days, 9:44:10  iter: 299  total_loss: 296.5  loss_ce: 5.254  loss_mask: 2.148  loss_dice: 3.503  loss_bbox: 2.953  loss_giou: 1.651  loss_ce_dn: 6.623  loss_mask_dn: 2.163  loss_dice_dn: 3.471  loss_bbox_dn: 1.189  loss_giou_dn: 0.8589  loss_ce_0: 18.87  loss_mask_0: 1.991  loss_dice_0: 3.788  loss_bbox_0: 2.645  loss_giou_0: 1.593  loss_ce_1: 5.755  loss_mask_1: 2.196  loss_dice_1: 3.455  loss_bbox_1: 3.287  loss_giou_1: 1.679  loss_ce_dn_1: 7.022  loss_mask_dn_1: 2.09  loss_dice_dn_1: 3.627  loss_bbox_dn_1: 1.182  loss_giou_dn_1: 0.8334  loss_ce_2: 4.953  loss_mask_2: 2.141  loss_dice_2: 3.529  loss_bbox_2: 3.371  loss_giou_2: 1.754  loss_ce_dn_2: 5.532  loss_mask_dn_2: 2.078  loss_dice_dn_2: 3.477  loss_bbox_dn_2: 1.17  loss_giou_dn_2: 0.8283  loss_ce_3: 4.63  loss_mask_3: 2.115  loss_dice_3: 3.538  loss_bbox_3: 3.331  loss_giou_3: 1.75  loss_ce_dn_3: 5.207  loss_mask_dn_3: 2.088  loss_dice_dn_3: 3.524  loss_bbox_dn_3: 1.158  loss_giou_dn_3: 0.8253  loss_ce_4: 4.532  loss_mask_4: 2.163  loss_dice_4: 3.571  loss_bbox_4: 3.202  loss_giou_4: 1.692  loss_ce_dn_4: 5.141  loss_mask_dn_4: 2.123  loss_dice_dn_4: 3.525  loss_bbox_dn_4: 1.148  loss_giou_dn_4: 0.8265  loss_ce_5: 4.412  loss_mask_5: 2.103  loss_dice_5: 3.617  loss_bbox_5: 3.206  loss_giou_5: 1.72  loss_ce_dn_5: 5.242  loss_mask_dn_5: 2.105  loss_dice_dn_5: 3.512  loss_bbox_dn_5: 1.145  loss_giou_dn_5: 0.8303  loss_ce_6: 4.853  loss_mask_6: 2.086  loss_dice_6: 3.545  loss_bbox_6: 3.088  loss_giou_6: 1.68  loss_ce_dn_6: 5.357  loss_mask_dn_6: 2.07  loss_dice_dn_6: 3.478  loss_bbox_dn_6: 1.144  loss_giou_dn_6: 0.8359  loss_ce_7: 4.605  loss_mask_7: 2.091  loss_dice_7: 3.545  loss_bbox_7: 3.066  loss_giou_7: 1.712  loss_ce_dn_7: 5.714  loss_mask_dn_7: 2.151  loss_dice_dn_7: 3.442  loss_bbox_dn_7: 1.151  loss_giou_dn_7: 0.8408  loss_ce_8: 4.945  loss_mask_8: 2.06  loss_dice_8: 3.519  loss_bbox_8: 2.987  loss_giou_8: 1.653  loss_ce_dn_8: 6.149  loss_mask_dn_8: 2.174  loss_dice_dn_8: 3.455  loss_bbox_dn_8: 1.171  loss_giou_dn_8: 0.8489    time: 2.2672  last_time: 2.2903  data_time: 0.0135  last_data_time: 0.0114   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:07:53 d2.utils.events]: \u001b[0m eta: 8 days, 9:43:34  iter: 319  total_loss: 294.3  loss_ce: 5.626  loss_mask: 1.861  loss_dice: 3.699  loss_bbox: 2.536  loss_giou: 1.685  loss_ce_dn: 7.059  loss_mask_dn: 1.894  loss_dice_dn: 3.649  loss_bbox_dn: 1.131  loss_giou_dn: 0.9165  loss_ce_0: 18.51  loss_mask_0: 1.768  loss_dice_0: 3.849  loss_bbox_0: 2.807  loss_giou_0: 1.664  loss_ce_1: 6.003  loss_mask_1: 1.794  loss_dice_1: 3.709  loss_bbox_1: 3.179  loss_giou_1: 1.734  loss_ce_dn_1: 7.379  loss_mask_dn_1: 1.865  loss_dice_dn_1: 3.844  loss_bbox_dn_1: 1.112  loss_giou_dn_1: 0.8454  loss_ce_2: 5.078  loss_mask_2: 1.826  loss_dice_2: 3.741  loss_bbox_2: 3.104  loss_giou_2: 1.722  loss_ce_dn_2: 6.117  loss_mask_dn_2: 1.781  loss_dice_dn_2: 3.757  loss_bbox_dn_2: 1.104  loss_giou_dn_2: 0.8453  loss_ce_3: 4.897  loss_mask_3: 1.853  loss_dice_3: 3.738  loss_bbox_3: 3.043  loss_giou_3: 1.74  loss_ce_dn_3: 5.845  loss_mask_dn_3: 1.778  loss_dice_dn_3: 3.769  loss_bbox_dn_3: 1.102  loss_giou_dn_3: 0.849  loss_ce_4: 4.94  loss_mask_4: 1.944  loss_dice_4: 3.728  loss_bbox_4: 2.949  loss_giou_4: 1.745  loss_ce_dn_4: 5.728  loss_mask_dn_4: 1.842  loss_dice_dn_4: 3.672  loss_bbox_dn_4: 1.103  loss_giou_dn_4: 0.8636  loss_ce_5: 5.086  loss_mask_5: 1.935  loss_dice_5: 3.733  loss_bbox_5: 2.924  loss_giou_5: 1.758  loss_ce_dn_5: 5.727  loss_mask_dn_5: 1.851  loss_dice_dn_5: 3.669  loss_bbox_dn_5: 1.103  loss_giou_dn_5: 0.8735  loss_ce_6: 5.02  loss_mask_6: 1.917  loss_dice_6: 3.721  loss_bbox_6: 2.902  loss_giou_6: 1.766  loss_ce_dn_6: 5.782  loss_mask_dn_6: 1.876  loss_dice_dn_6: 3.692  loss_bbox_dn_6: 1.104  loss_giou_dn_6: 0.885  loss_ce_7: 5.192  loss_mask_7: 1.885  loss_dice_7: 3.738  loss_bbox_7: 2.865  loss_giou_7: 1.717  loss_ce_dn_7: 6.246  loss_mask_dn_7: 1.839  loss_dice_dn_7: 3.693  loss_bbox_dn_7: 1.106  loss_giou_dn_7: 0.895  loss_ce_8: 5.415  loss_mask_8: 1.899  loss_dice_8: 3.715  loss_bbox_8: 2.651  loss_giou_8: 1.719  loss_ce_dn_8: 6.629  loss_mask_dn_8: 1.859  loss_dice_dn_8: 3.675  loss_bbox_dn_8: 1.114  loss_giou_dn_8: 0.9045    time: 2.2679  last_time: 2.2434  data_time: 0.0126  last_data_time: 0.0069   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:08:38 d2.utils.events]: \u001b[0m eta: 8 days, 9:43:55  iter: 339  total_loss: 296.5  loss_ce: 5.22  loss_mask: 2.134  loss_dice: 3.643  loss_bbox: 2.632  loss_giou: 1.607  loss_ce_dn: 7.349  loss_mask_dn: 2.061  loss_dice_dn: 3.543  loss_bbox_dn: 1.224  loss_giou_dn: 0.9086  loss_ce_0: 18.81  loss_mask_0: 1.843  loss_dice_0: 3.562  loss_bbox_0: 2.847  loss_giou_0: 1.622  loss_ce_1: 5.4  loss_mask_1: 2.142  loss_dice_1: 3.495  loss_bbox_1: 3.24  loss_giou_1: 1.754  loss_ce_dn_1: 6.895  loss_mask_dn_1: 2.152  loss_dice_dn_1: 3.556  loss_bbox_dn_1: 1.235  loss_giou_dn_1: 0.8252  loss_ce_2: 4.884  loss_mask_2: 2.241  loss_dice_2: 3.562  loss_bbox_2: 3.237  loss_giou_2: 1.656  loss_ce_dn_2: 5.708  loss_mask_dn_2: 2.123  loss_dice_dn_2: 3.565  loss_bbox_dn_2: 1.222  loss_giou_dn_2: 0.8285  loss_ce_3: 4.771  loss_mask_3: 2.075  loss_dice_3: 3.576  loss_bbox_3: 3.108  loss_giou_3: 1.706  loss_ce_dn_3: 5.414  loss_mask_dn_3: 2.15  loss_dice_dn_3: 3.56  loss_bbox_dn_3: 1.215  loss_giou_dn_3: 0.8383  loss_ce_4: 4.768  loss_mask_4: 2.116  loss_dice_4: 3.628  loss_bbox_4: 2.947  loss_giou_4: 1.67  loss_ce_dn_4: 5.432  loss_mask_dn_4: 2.08  loss_dice_dn_4: 3.564  loss_bbox_dn_4: 1.216  loss_giou_dn_4: 0.8478  loss_ce_5: 4.782  loss_mask_5: 2.14  loss_dice_5: 3.589  loss_bbox_5: 2.833  loss_giou_5: 1.641  loss_ce_dn_5: 5.651  loss_mask_dn_5: 2.047  loss_dice_dn_5: 3.566  loss_bbox_dn_5: 1.218  loss_giou_dn_5: 0.8553  loss_ce_6: 4.854  loss_mask_6: 2.109  loss_dice_6: 3.648  loss_bbox_6: 2.779  loss_giou_6: 1.646  loss_ce_dn_6: 6.025  loss_mask_dn_6: 2.059  loss_dice_dn_6: 3.568  loss_bbox_dn_6: 1.227  loss_giou_dn_6: 0.8702  loss_ce_7: 4.962  loss_mask_7: 2.017  loss_dice_7: 3.713  loss_bbox_7: 2.643  loss_giou_7: 1.615  loss_ce_dn_7: 6.362  loss_mask_dn_7: 2.036  loss_dice_dn_7: 3.556  loss_bbox_dn_7: 1.219  loss_giou_dn_7: 0.8825  loss_ce_8: 5.007  loss_mask_8: 2.117  loss_dice_8: 3.709  loss_bbox_8: 2.647  loss_giou_8: 1.62  loss_ce_dn_8: 6.893  loss_mask_dn_8: 2.018  loss_dice_dn_8: 3.557  loss_bbox_dn_8: 1.217  loss_giou_dn_8: 0.8946    time: 2.2695  last_time: 2.6334  data_time: 0.0126  last_data_time: 0.0240   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:09:24 d2.utils.events]: \u001b[0m eta: 8 days, 9:43:09  iter: 359  total_loss: 284.9  loss_ce: 4.817  loss_mask: 2.107  loss_dice: 3.653  loss_bbox: 2.776  loss_giou: 1.752  loss_ce_dn: 5.697  loss_mask_dn: 2.133  loss_dice_dn: 3.608  loss_bbox_dn: 1.094  loss_giou_dn: 0.9026  loss_ce_0: 18.58  loss_mask_0: 1.97  loss_dice_0: 3.74  loss_bbox_0: 2.84  loss_giou_0: 1.664  loss_ce_1: 5.027  loss_mask_1: 2.135  loss_dice_1: 3.598  loss_bbox_1: 3.317  loss_giou_1: 1.871  loss_ce_dn_1: 6.578  loss_mask_dn_1: 2.116  loss_dice_dn_1: 3.686  loss_bbox_dn_1: 1.15  loss_giou_dn_1: 0.835  loss_ce_2: 4.139  loss_mask_2: 2.153  loss_dice_2: 3.623  loss_bbox_2: 3.355  loss_giou_2: 1.836  loss_ce_dn_2: 5.328  loss_mask_dn_2: 2.103  loss_dice_dn_2: 3.619  loss_bbox_dn_2: 1.125  loss_giou_dn_2: 0.8355  loss_ce_3: 4.1  loss_mask_3: 2.12  loss_dice_3: 3.583  loss_bbox_3: 3.236  loss_giou_3: 1.771  loss_ce_dn_3: 5.061  loss_mask_dn_3: 2.098  loss_dice_dn_3: 3.63  loss_bbox_dn_3: 1.104  loss_giou_dn_3: 0.8384  loss_ce_4: 4.305  loss_mask_4: 2.119  loss_dice_4: 3.561  loss_bbox_4: 3.191  loss_giou_4: 1.779  loss_ce_dn_4: 4.885  loss_mask_dn_4: 2.088  loss_dice_dn_4: 3.627  loss_bbox_dn_4: 1.084  loss_giou_dn_4: 0.8409  loss_ce_5: 4.229  loss_mask_5: 2.136  loss_dice_5: 3.567  loss_bbox_5: 3.043  loss_giou_5: 1.772  loss_ce_dn_5: 4.901  loss_mask_dn_5: 2.045  loss_dice_dn_5: 3.554  loss_bbox_dn_5: 1.063  loss_giou_dn_5: 0.8508  loss_ce_6: 4.51  loss_mask_6: 2.173  loss_dice_6: 3.598  loss_bbox_6: 2.832  loss_giou_6: 1.72  loss_ce_dn_6: 4.974  loss_mask_dn_6: 2.137  loss_dice_dn_6: 3.571  loss_bbox_dn_6: 1.067  loss_giou_dn_6: 0.8623  loss_ce_7: 4.592  loss_mask_7: 2.118  loss_dice_7: 3.585  loss_bbox_7: 2.83  loss_giou_7: 1.729  loss_ce_dn_7: 5.338  loss_mask_dn_7: 2.078  loss_dice_dn_7: 3.626  loss_bbox_dn_7: 1.067  loss_giou_dn_7: 0.8742  loss_ce_8: 4.65  loss_mask_8: 2.107  loss_dice_8: 3.623  loss_bbox_8: 2.969  loss_giou_8: 1.75  loss_ce_dn_8: 5.434  loss_mask_dn_8: 2.149  loss_dice_dn_8: 3.603  loss_bbox_dn_8: 1.078  loss_giou_dn_8: 0.8838    time: 2.2696  last_time: 2.2811  data_time: 0.0126  last_data_time: 0.0052   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:10:10 d2.utils.events]: \u001b[0m eta: 8 days, 9:44:29  iter: 379  total_loss: 284  loss_ce: 4.365  loss_mask: 1.928  loss_dice: 3.592  loss_bbox: 2.839  loss_giou: 1.695  loss_ce_dn: 6.016  loss_mask_dn: 1.804  loss_dice_dn: 3.55  loss_bbox_dn: 1.165  loss_giou_dn: 0.9043  loss_ce_0: 18.64  loss_mask_0: 1.663  loss_dice_0: 3.791  loss_bbox_0: 2.748  loss_giou_0: 1.604  loss_ce_1: 5.178  loss_mask_1: 1.891  loss_dice_1: 3.607  loss_bbox_1: 3.372  loss_giou_1: 1.8  loss_ce_dn_1: 6.759  loss_mask_dn_1: 1.879  loss_dice_dn_1: 3.601  loss_bbox_dn_1: 1.222  loss_giou_dn_1: 0.8355  loss_ce_2: 4.355  loss_mask_2: 1.848  loss_dice_2: 3.653  loss_bbox_2: 3.345  loss_giou_2: 1.814  loss_ce_dn_2: 5.369  loss_mask_dn_2: 1.791  loss_dice_dn_2: 3.631  loss_bbox_dn_2: 1.199  loss_giou_dn_2: 0.8373  loss_ce_3: 4.111  loss_mask_3: 1.901  loss_dice_3: 3.671  loss_bbox_3: 3.193  loss_giou_3: 1.728  loss_ce_dn_3: 5.057  loss_mask_dn_3: 1.809  loss_dice_dn_3: 3.603  loss_bbox_dn_3: 1.161  loss_giou_dn_3: 0.8419  loss_ce_4: 4.258  loss_mask_4: 1.922  loss_dice_4: 3.678  loss_bbox_4: 3.059  loss_giou_4: 1.707  loss_ce_dn_4: 4.977  loss_mask_dn_4: 1.784  loss_dice_dn_4: 3.595  loss_bbox_dn_4: 1.146  loss_giou_dn_4: 0.8522  loss_ce_5: 4.205  loss_mask_5: 2.037  loss_dice_5: 3.671  loss_bbox_5: 2.916  loss_giou_5: 1.635  loss_ce_dn_5: 5.042  loss_mask_dn_5: 1.798  loss_dice_dn_5: 3.604  loss_bbox_dn_5: 1.157  loss_giou_dn_5: 0.8624  loss_ce_6: 4.23  loss_mask_6: 1.982  loss_dice_6: 3.721  loss_bbox_6: 2.835  loss_giou_6: 1.653  loss_ce_dn_6: 5.144  loss_mask_dn_6: 1.817  loss_dice_dn_6: 3.565  loss_bbox_dn_6: 1.155  loss_giou_dn_6: 0.8701  loss_ce_7: 4.142  loss_mask_7: 2.023  loss_dice_7: 3.665  loss_bbox_7: 2.821  loss_giou_7: 1.644  loss_ce_dn_7: 5.216  loss_mask_dn_7: 1.768  loss_dice_dn_7: 3.533  loss_bbox_dn_7: 1.154  loss_giou_dn_7: 0.8788  loss_ce_8: 4.228  loss_mask_8: 1.944  loss_dice_8: 3.668  loss_bbox_8: 2.882  loss_giou_8: 1.683  loss_ce_dn_8: 5.576  loss_mask_dn_8: 1.814  loss_dice_dn_8: 3.557  loss_bbox_dn_8: 1.157  loss_giou_dn_8: 0.8905    time: 2.2708  last_time: 2.2879  data_time: 0.0133  last_data_time: 0.0142   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:10:56 d2.utils.events]: \u001b[0m eta: 8 days, 9:44:50  iter: 399  total_loss: 284.3  loss_ce: 4.469  loss_mask: 2.187  loss_dice: 3.661  loss_bbox: 2.973  loss_giou: 1.669  loss_ce_dn: 6.002  loss_mask_dn: 2.152  loss_dice_dn: 3.587  loss_bbox_dn: 1.2  loss_giou_dn: 0.868  loss_ce_0: 18.47  loss_mask_0: 1.932  loss_dice_0: 3.65  loss_bbox_0: 2.957  loss_giou_0: 1.548  loss_ce_1: 4.895  loss_mask_1: 2.16  loss_dice_1: 3.596  loss_bbox_1: 3.549  loss_giou_1: 1.719  loss_ce_dn_1: 6.356  loss_mask_dn_1: 2.154  loss_dice_dn_1: 3.609  loss_bbox_dn_1: 1.179  loss_giou_dn_1: 0.8264  loss_ce_2: 4.27  loss_mask_2: 2.259  loss_dice_2: 3.579  loss_bbox_2: 3.444  loss_giou_2: 1.686  loss_ce_dn_2: 5.221  loss_mask_dn_2: 2.115  loss_dice_dn_2: 3.514  loss_bbox_dn_2: 1.181  loss_giou_dn_2: 0.8239  loss_ce_3: 4.227  loss_mask_3: 2.216  loss_dice_3: 3.527  loss_bbox_3: 3.363  loss_giou_3: 1.681  loss_ce_dn_3: 4.859  loss_mask_dn_3: 2.105  loss_dice_dn_3: 3.537  loss_bbox_dn_3: 1.182  loss_giou_dn_3: 0.8216  loss_ce_4: 4.036  loss_mask_4: 2.25  loss_dice_4: 3.507  loss_bbox_4: 3.291  loss_giou_4: 1.645  loss_ce_dn_4: 4.749  loss_mask_dn_4: 2.104  loss_dice_dn_4: 3.537  loss_bbox_dn_4: 1.17  loss_giou_dn_4: 0.8226  loss_ce_5: 4.12  loss_mask_5: 2.204  loss_dice_5: 3.556  loss_bbox_5: 3.288  loss_giou_5: 1.664  loss_ce_dn_5: 4.888  loss_mask_dn_5: 2.09  loss_dice_dn_5: 3.559  loss_bbox_dn_5: 1.169  loss_giou_dn_5: 0.8282  loss_ce_6: 4.202  loss_mask_6: 2.215  loss_dice_6: 3.551  loss_bbox_6: 3.222  loss_giou_6: 1.686  loss_ce_dn_6: 4.915  loss_mask_dn_6: 2.108  loss_dice_dn_6: 3.559  loss_bbox_dn_6: 1.168  loss_giou_dn_6: 0.8315  loss_ce_7: 4.236  loss_mask_7: 2.21  loss_dice_7: 3.594  loss_bbox_7: 3.1  loss_giou_7: 1.695  loss_ce_dn_7: 5.273  loss_mask_dn_7: 2.085  loss_dice_dn_7: 3.558  loss_bbox_dn_7: 1.163  loss_giou_dn_7: 0.8397  loss_ce_8: 4.315  loss_mask_8: 2.151  loss_dice_8: 3.627  loss_bbox_8: 3.042  loss_giou_8: 1.672  loss_ce_dn_8: 5.599  loss_mask_dn_8: 2.08  loss_dice_dn_8: 3.559  loss_bbox_dn_8: 1.172  loss_giou_dn_8: 0.8472    time: 2.2715  last_time: 2.2316  data_time: 0.0127  last_data_time: 0.0052   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:11:41 d2.utils.events]: \u001b[0m eta: 8 days, 9:43:18  iter: 419  total_loss: 289.4  loss_ce: 5.415  loss_mask: 2.153  loss_dice: 3.587  loss_bbox: 2.885  loss_giou: 1.734  loss_ce_dn: 6.234  loss_mask_dn: 1.977  loss_dice_dn: 3.537  loss_bbox_dn: 1.128  loss_giou_dn: 0.8795  loss_ce_0: 18.34  loss_mask_0: 1.815  loss_dice_0: 3.793  loss_bbox_0: 2.805  loss_giou_0: 1.639  loss_ce_1: 5.311  loss_mask_1: 2.066  loss_dice_1: 3.597  loss_bbox_1: 3.557  loss_giou_1: 1.731  loss_ce_dn_1: 6.518  loss_mask_dn_1: 2.076  loss_dice_dn_1: 3.633  loss_bbox_dn_1: 1.192  loss_giou_dn_1: 0.8429  loss_ce_2: 4.717  loss_mask_2: 2.063  loss_dice_2: 3.636  loss_bbox_2: 3.38  loss_giou_2: 1.719  loss_ce_dn_2: 5.065  loss_mask_dn_2: 1.92  loss_dice_dn_2: 3.614  loss_bbox_dn_2: 1.174  loss_giou_dn_2: 0.8455  loss_ce_3: 4.583  loss_mask_3: 2.188  loss_dice_3: 3.612  loss_bbox_3: 3.21  loss_giou_3: 1.728  loss_ce_dn_3: 4.81  loss_mask_dn_3: 1.932  loss_dice_dn_3: 3.617  loss_bbox_dn_3: 1.157  loss_giou_dn_3: 0.8463  loss_ce_4: 4.819  loss_mask_4: 2.187  loss_dice_4: 3.677  loss_bbox_4: 3.112  loss_giou_4: 1.688  loss_ce_dn_4: 4.783  loss_mask_dn_4: 1.913  loss_dice_dn_4: 3.564  loss_bbox_dn_4: 1.144  loss_giou_dn_4: 0.8477  loss_ce_5: 4.917  loss_mask_5: 2.17  loss_dice_5: 3.618  loss_bbox_5: 3.073  loss_giou_5: 1.671  loss_ce_dn_5: 4.965  loss_mask_dn_5: 1.913  loss_dice_dn_5: 3.588  loss_bbox_dn_5: 1.135  loss_giou_dn_5: 0.8484  loss_ce_6: 5.029  loss_mask_6: 2.163  loss_dice_6: 3.564  loss_bbox_6: 2.988  loss_giou_6: 1.719  loss_ce_dn_6: 4.972  loss_mask_dn_6: 1.9  loss_dice_dn_6: 3.558  loss_bbox_dn_6: 1.129  loss_giou_dn_6: 0.8526  loss_ce_7: 5.043  loss_mask_7: 2.058  loss_dice_7: 3.572  loss_bbox_7: 2.924  loss_giou_7: 1.714  loss_ce_dn_7: 5.169  loss_mask_dn_7: 1.928  loss_dice_dn_7: 3.566  loss_bbox_dn_7: 1.13  loss_giou_dn_7: 0.8589  loss_ce_8: 5.224  loss_mask_8: 2.045  loss_dice_8: 3.548  loss_bbox_8: 2.901  loss_giou_8: 1.753  loss_ce_dn_8: 5.724  loss_mask_dn_8: 1.91  loss_dice_dn_8: 3.566  loss_bbox_dn_8: 1.128  loss_giou_dn_8: 0.8675    time: 2.2718  last_time: 2.3168  data_time: 0.0110  last_data_time: 0.0122   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:12:27 d2.utils.events]: \u001b[0m eta: 8 days, 9:42:02  iter: 439  total_loss: 283.1  loss_ce: 4.514  loss_mask: 2.069  loss_dice: 3.383  loss_bbox: 3.056  loss_giou: 1.723  loss_ce_dn: 5.532  loss_mask_dn: 2.049  loss_dice_dn: 3.292  loss_bbox_dn: 1.18  loss_giou_dn: 0.8725  loss_ce_0: 18.33  loss_mask_0: 1.872  loss_dice_0: 3.641  loss_bbox_0: 2.954  loss_giou_0: 1.604  loss_ce_1: 5  loss_mask_1: 2.041  loss_dice_1: 3.444  loss_bbox_1: 3.455  loss_giou_1: 1.785  loss_ce_dn_1: 6.597  loss_mask_dn_1: 1.992  loss_dice_dn_1: 3.433  loss_bbox_dn_1: 1.191  loss_giou_dn_1: 0.8201  loss_ce_2: 4.614  loss_mask_2: 2.066  loss_dice_2: 3.466  loss_bbox_2: 3.468  loss_giou_2: 1.742  loss_ce_dn_2: 5.43  loss_mask_dn_2: 1.955  loss_dice_dn_2: 3.331  loss_bbox_dn_2: 1.189  loss_giou_dn_2: 0.8172  loss_ce_3: 4.507  loss_mask_3: 2.112  loss_dice_3: 3.502  loss_bbox_3: 3.342  loss_giou_3: 1.728  loss_ce_dn_3: 5.104  loss_mask_dn_3: 1.948  loss_dice_dn_3: 3.28  loss_bbox_dn_3: 1.18  loss_giou_dn_3: 0.8188  loss_ce_4: 4.395  loss_mask_4: 2.169  loss_dice_4: 3.445  loss_bbox_4: 3.161  loss_giou_4: 1.721  loss_ce_dn_4: 4.977  loss_mask_dn_4: 1.944  loss_dice_dn_4: 3.262  loss_bbox_dn_4: 1.16  loss_giou_dn_4: 0.8236  loss_ce_5: 4.378  loss_mask_5: 2.041  loss_dice_5: 3.456  loss_bbox_5: 3.034  loss_giou_5: 1.701  loss_ce_dn_5: 5.205  loss_mask_dn_5: 1.973  loss_dice_dn_5: 3.263  loss_bbox_dn_5: 1.171  loss_giou_dn_5: 0.8298  loss_ce_6: 4.41  loss_mask_6: 2.107  loss_dice_6: 3.387  loss_bbox_6: 3.177  loss_giou_6: 1.708  loss_ce_dn_6: 5.096  loss_mask_dn_6: 2.018  loss_dice_dn_6: 3.253  loss_bbox_dn_6: 1.167  loss_giou_dn_6: 0.8337  loss_ce_7: 4.706  loss_mask_7: 2.136  loss_dice_7: 3.375  loss_bbox_7: 3.166  loss_giou_7: 1.735  loss_ce_dn_7: 5.202  loss_mask_dn_7: 2.025  loss_dice_dn_7: 3.269  loss_bbox_dn_7: 1.164  loss_giou_dn_7: 0.8385  loss_ce_8: 4.621  loss_mask_8: 2.171  loss_dice_8: 3.405  loss_bbox_8: 3.07  loss_giou_8: 1.741  loss_ce_dn_8: 5.333  loss_mask_dn_8: 2.055  loss_dice_dn_8: 3.275  loss_bbox_dn_8: 1.169  loss_giou_dn_8: 0.8543    time: 2.2720  last_time: 2.2658  data_time: 0.0125  last_data_time: 0.0085   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:13:12 d2.utils.events]: \u001b[0m eta: 8 days, 9:38:40  iter: 459  total_loss: 273.9  loss_ce: 4.5  loss_mask: 2.105  loss_dice: 3.396  loss_bbox: 2.924  loss_giou: 1.646  loss_ce_dn: 5.793  loss_mask_dn: 2.125  loss_dice_dn: 3.446  loss_bbox_dn: 1.15  loss_giou_dn: 0.8712  loss_ce_0: 18.25  loss_mask_0: 1.892  loss_dice_0: 3.638  loss_bbox_0: 2.857  loss_giou_0: 1.624  loss_ce_1: 4.703  loss_mask_1: 1.992  loss_dice_1: 3.486  loss_bbox_1: 3.277  loss_giou_1: 1.712  loss_ce_dn_1: 6.452  loss_mask_dn_1: 2.065  loss_dice_dn_1: 3.54  loss_bbox_dn_1: 1.224  loss_giou_dn_1: 0.8307  loss_ce_2: 4.232  loss_mask_2: 2.019  loss_dice_2: 3.46  loss_bbox_2: 3.089  loss_giou_2: 1.626  loss_ce_dn_2: 5.191  loss_mask_dn_2: 2.057  loss_dice_dn_2: 3.485  loss_bbox_dn_2: 1.189  loss_giou_dn_2: 0.8227  loss_ce_3: 4.174  loss_mask_3: 2.014  loss_dice_3: 3.49  loss_bbox_3: 2.955  loss_giou_3: 1.629  loss_ce_dn_3: 4.874  loss_mask_dn_3: 2.074  loss_dice_dn_3: 3.476  loss_bbox_dn_3: 1.163  loss_giou_dn_3: 0.8159  loss_ce_4: 4.113  loss_mask_4: 2.104  loss_dice_4: 3.443  loss_bbox_4: 3.031  loss_giou_4: 1.639  loss_ce_dn_4: 4.726  loss_mask_dn_4: 2.14  loss_dice_dn_4: 3.493  loss_bbox_dn_4: 1.141  loss_giou_dn_4: 0.8203  loss_ce_5: 4.187  loss_mask_5: 2.114  loss_dice_5: 3.351  loss_bbox_5: 2.98  loss_giou_5: 1.656  loss_ce_dn_5: 4.776  loss_mask_dn_5: 2.087  loss_dice_dn_5: 3.428  loss_bbox_dn_5: 1.122  loss_giou_dn_5: 0.8361  loss_ce_6: 4.226  loss_mask_6: 2.119  loss_dice_6: 3.38  loss_bbox_6: 2.915  loss_giou_6: 1.584  loss_ce_dn_6: 4.798  loss_mask_dn_6: 2.141  loss_dice_dn_6: 3.439  loss_bbox_dn_6: 1.109  loss_giou_dn_6: 0.8468  loss_ce_7: 4.148  loss_mask_7: 2.132  loss_dice_7: 3.441  loss_bbox_7: 2.879  loss_giou_7: 1.587  loss_ce_dn_7: 4.909  loss_mask_dn_7: 2.177  loss_dice_dn_7: 3.441  loss_bbox_dn_7: 1.119  loss_giou_dn_7: 0.8571  loss_ce_8: 4.498  loss_mask_8: 2.122  loss_dice_8: 3.405  loss_bbox_8: 2.889  loss_giou_8: 1.62  loss_ce_dn_8: 5.339  loss_mask_dn_8: 2.131  loss_dice_dn_8: 3.453  loss_bbox_dn_8: 1.138  loss_giou_dn_8: 0.8648    time: 2.2722  last_time: 2.2460  data_time: 0.0133  last_data_time: 0.0063   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:13:58 d2.utils.events]: \u001b[0m eta: 8 days, 9:37:55  iter: 479  total_loss: 269.2  loss_ce: 3.927  loss_mask: 1.783  loss_dice: 3.513  loss_bbox: 2.787  loss_giou: 1.817  loss_ce_dn: 5.793  loss_mask_dn: 1.667  loss_dice_dn: 3.423  loss_bbox_dn: 1.092  loss_giou_dn: 0.9065  loss_ce_0: 18.23  loss_mask_0: 1.568  loss_dice_0: 3.639  loss_bbox_0: 3.09  loss_giou_0: 1.762  loss_ce_1: 4.446  loss_mask_1: 1.751  loss_dice_1: 3.512  loss_bbox_1: 3.571  loss_giou_1: 1.828  loss_ce_dn_1: 6.463  loss_mask_dn_1: 1.702  loss_dice_dn_1: 3.529  loss_bbox_dn_1: 1.135  loss_giou_dn_1: 0.8333  loss_ce_2: 3.942  loss_mask_2: 1.749  loss_dice_2: 3.489  loss_bbox_2: 3.505  loss_giou_2: 1.796  loss_ce_dn_2: 5.08  loss_mask_dn_2: 1.698  loss_dice_dn_2: 3.438  loss_bbox_dn_2: 1.108  loss_giou_dn_2: 0.8346  loss_ce_3: 3.974  loss_mask_3: 1.657  loss_dice_3: 3.491  loss_bbox_3: 3.254  loss_giou_3: 1.768  loss_ce_dn_3: 4.8  loss_mask_dn_3: 1.711  loss_dice_dn_3: 3.428  loss_bbox_dn_3: 1.102  loss_giou_dn_3: 0.8406  loss_ce_4: 4.024  loss_mask_4: 1.723  loss_dice_4: 3.495  loss_bbox_4: 3.114  loss_giou_4: 1.761  loss_ce_dn_4: 4.784  loss_mask_dn_4: 1.706  loss_dice_dn_4: 3.443  loss_bbox_dn_4: 1.068  loss_giou_dn_4: 0.8417  loss_ce_5: 3.934  loss_mask_5: 1.742  loss_dice_5: 3.429  loss_bbox_5: 2.946  loss_giou_5: 1.784  loss_ce_dn_5: 4.776  loss_mask_dn_5: 1.686  loss_dice_dn_5: 3.419  loss_bbox_dn_5: 1.045  loss_giou_dn_5: 0.8486  loss_ce_6: 3.856  loss_mask_6: 1.712  loss_dice_6: 3.454  loss_bbox_6: 2.896  loss_giou_6: 1.779  loss_ce_dn_6: 4.85  loss_mask_dn_6: 1.635  loss_dice_dn_6: 3.412  loss_bbox_dn_6: 1.059  loss_giou_dn_6: 0.8605  loss_ce_7: 3.901  loss_mask_7: 1.743  loss_dice_7: 3.482  loss_bbox_7: 2.929  loss_giou_7: 1.793  loss_ce_dn_7: 5.183  loss_mask_dn_7: 1.662  loss_dice_dn_7: 3.458  loss_bbox_dn_7: 1.076  loss_giou_dn_7: 0.8731  loss_ce_8: 4.032  loss_mask_8: 1.772  loss_dice_8: 3.519  loss_bbox_8: 2.847  loss_giou_8: 1.771  loss_ce_dn_8: 5.427  loss_mask_dn_8: 1.672  loss_dice_dn_8: 3.45  loss_bbox_dn_8: 1.086  loss_giou_dn_8: 0.8876    time: 2.2726  last_time: 2.2286  data_time: 0.0114  last_data_time: 0.0058   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:14:44 d2.utils.events]: \u001b[0m eta: 8 days, 9:37:51  iter: 499  total_loss: 276.9  loss_ce: 4.576  loss_mask: 2.17  loss_dice: 3.505  loss_bbox: 2.805  loss_giou: 1.723  loss_ce_dn: 5.279  loss_mask_dn: 2.09  loss_dice_dn: 3.326  loss_bbox_dn: 1.164  loss_giou_dn: 0.871  loss_ce_0: 18.19  loss_mask_0: 1.997  loss_dice_0: 3.658  loss_bbox_0: 2.961  loss_giou_0: 1.533  loss_ce_1: 4.835  loss_mask_1: 2.123  loss_dice_1: 3.38  loss_bbox_1: 3.329  loss_giou_1: 1.717  loss_ce_dn_1: 6.215  loss_mask_dn_1: 2.216  loss_dice_dn_1: 3.44  loss_bbox_dn_1: 1.178  loss_giou_dn_1: 0.8318  loss_ce_2: 4.298  loss_mask_2: 2.13  loss_dice_2: 3.385  loss_bbox_2: 3.265  loss_giou_2: 1.708  loss_ce_dn_2: 4.794  loss_mask_dn_2: 2.122  loss_dice_dn_2: 3.363  loss_bbox_dn_2: 1.164  loss_giou_dn_2: 0.8308  loss_ce_3: 4.228  loss_mask_3: 2.108  loss_dice_3: 3.39  loss_bbox_3: 3.088  loss_giou_3: 1.649  loss_ce_dn_3: 4.412  loss_mask_dn_3: 2.067  loss_dice_dn_3: 3.326  loss_bbox_dn_3: 1.172  loss_giou_dn_3: 0.834  loss_ce_4: 4.248  loss_mask_4: 2.111  loss_dice_4: 3.499  loss_bbox_4: 2.971  loss_giou_4: 1.589  loss_ce_dn_4: 4.433  loss_mask_dn_4: 2.056  loss_dice_dn_4: 3.367  loss_bbox_dn_4: 1.174  loss_giou_dn_4: 0.842  loss_ce_5: 4.327  loss_mask_5: 2.047  loss_dice_5: 3.465  loss_bbox_5: 2.889  loss_giou_5: 1.612  loss_ce_dn_5: 4.292  loss_mask_dn_5: 2.098  loss_dice_dn_5: 3.331  loss_bbox_dn_5: 1.174  loss_giou_dn_5: 0.8402  loss_ce_6: 4.305  loss_mask_6: 2.074  loss_dice_6: 3.47  loss_bbox_6: 2.884  loss_giou_6: 1.637  loss_ce_dn_6: 4.429  loss_mask_dn_6: 2.107  loss_dice_dn_6: 3.312  loss_bbox_dn_6: 1.168  loss_giou_dn_6: 0.849  loss_ce_7: 4.585  loss_mask_7: 2.096  loss_dice_7: 3.473  loss_bbox_7: 2.841  loss_giou_7: 1.673  loss_ce_dn_7: 4.483  loss_mask_dn_7: 2.085  loss_dice_dn_7: 3.294  loss_bbox_dn_7: 1.156  loss_giou_dn_7: 0.8565  loss_ce_8: 4.454  loss_mask_8: 2.126  loss_dice_8: 3.455  loss_bbox_8: 2.853  loss_giou_8: 1.712  loss_ce_dn_8: 4.831  loss_mask_dn_8: 2.067  loss_dice_dn_8: 3.288  loss_bbox_dn_8: 1.159  loss_giou_dn_8: 0.8573    time: 2.2732  last_time: 2.3885  data_time: 0.0131  last_data_time: 0.0098   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:15:29 d2.utils.events]: \u001b[0m eta: 8 days, 9:35:37  iter: 519  total_loss: 271.2  loss_ce: 4.408  loss_mask: 1.815  loss_dice: 3.274  loss_bbox: 3.198  loss_giou: 1.803  loss_ce_dn: 5.143  loss_mask_dn: 1.902  loss_dice_dn: 3.226  loss_bbox_dn: 1.202  loss_giou_dn: 0.8817  loss_ce_0: 18.33  loss_mask_0: 1.789  loss_dice_0: 3.606  loss_bbox_0: 2.979  loss_giou_0: 1.7  loss_ce_1: 5.023  loss_mask_1: 1.736  loss_dice_1: 3.28  loss_bbox_1: 3.642  loss_giou_1: 1.77  loss_ce_dn_1: 6.011  loss_mask_dn_1: 1.862  loss_dice_dn_1: 3.388  loss_bbox_dn_1: 1.183  loss_giou_dn_1: 0.833  loss_ce_2: 4.522  loss_mask_2: 1.778  loss_dice_2: 3.26  loss_bbox_2: 3.496  loss_giou_2: 1.722  loss_ce_dn_2: 4.731  loss_mask_dn_2: 1.803  loss_dice_dn_2: 3.303  loss_bbox_dn_2: 1.171  loss_giou_dn_2: 0.8323  loss_ce_3: 4.418  loss_mask_3: 1.758  loss_dice_3: 3.237  loss_bbox_3: 3.399  loss_giou_3: 1.732  loss_ce_dn_3: 4.531  loss_mask_dn_3: 1.795  loss_dice_dn_3: 3.291  loss_bbox_dn_3: 1.157  loss_giou_dn_3: 0.8329  loss_ce_4: 4.214  loss_mask_4: 1.853  loss_dice_4: 3.282  loss_bbox_4: 3.29  loss_giou_4: 1.734  loss_ce_dn_4: 4.474  loss_mask_dn_4: 1.817  loss_dice_dn_4: 3.278  loss_bbox_dn_4: 1.154  loss_giou_dn_4: 0.8373  loss_ce_5: 4.187  loss_mask_5: 1.889  loss_dice_5: 3.288  loss_bbox_5: 3.183  loss_giou_5: 1.777  loss_ce_dn_5: 4.505  loss_mask_dn_5: 1.875  loss_dice_dn_5: 3.281  loss_bbox_dn_5: 1.147  loss_giou_dn_5: 0.8398  loss_ce_6: 4.282  loss_mask_6: 1.919  loss_dice_6: 3.251  loss_bbox_6: 3.163  loss_giou_6: 1.798  loss_ce_dn_6: 4.549  loss_mask_dn_6: 1.874  loss_dice_dn_6: 3.259  loss_bbox_dn_6: 1.146  loss_giou_dn_6: 0.8434  loss_ce_7: 4.294  loss_mask_7: 1.968  loss_dice_7: 3.307  loss_bbox_7: 3.16  loss_giou_7: 1.787  loss_ce_dn_7: 4.685  loss_mask_dn_7: 1.875  loss_dice_dn_7: 3.243  loss_bbox_dn_7: 1.151  loss_giou_dn_7: 0.849  loss_ce_8: 4.369  loss_mask_8: 1.917  loss_dice_8: 3.25  loss_bbox_8: 3.227  loss_giou_8: 1.791  loss_ce_dn_8: 4.861  loss_mask_dn_8: 1.881  loss_dice_dn_8: 3.22  loss_bbox_dn_8: 1.173  loss_giou_dn_8: 0.8618    time: 2.2729  last_time: 2.2617  data_time: 0.0157  last_data_time: 0.0049   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:16:15 d2.utils.events]: \u001b[0m eta: 8 days, 9:34:06  iter: 539  total_loss: 270.4  loss_ce: 3.978  loss_mask: 2.185  loss_dice: 3.441  loss_bbox: 3.008  loss_giou: 1.726  loss_ce_dn: 5.16  loss_mask_dn: 2.071  loss_dice_dn: 3.412  loss_bbox_dn: 1.207  loss_giou_dn: 0.8853  loss_ce_0: 18.08  loss_mask_0: 1.924  loss_dice_0: 3.721  loss_bbox_0: 2.85  loss_giou_0: 1.689  loss_ce_1: 4.472  loss_mask_1: 1.99  loss_dice_1: 3.586  loss_bbox_1: 3.326  loss_giou_1: 1.843  loss_ce_dn_1: 5.972  loss_mask_dn_1: 1.942  loss_dice_dn_1: 3.532  loss_bbox_dn_1: 1.217  loss_giou_dn_1: 0.8289  loss_ce_2: 3.686  loss_mask_2: 2.082  loss_dice_2: 3.485  loss_bbox_2: 3.278  loss_giou_2: 1.804  loss_ce_dn_2: 4.816  loss_mask_dn_2: 1.962  loss_dice_dn_2: 3.466  loss_bbox_dn_2: 1.193  loss_giou_dn_2: 0.8237  loss_ce_3: 3.686  loss_mask_3: 2.099  loss_dice_3: 3.398  loss_bbox_3: 3.107  loss_giou_3: 1.747  loss_ce_dn_3: 4.497  loss_mask_dn_3: 1.966  loss_dice_dn_3: 3.424  loss_bbox_dn_3: 1.176  loss_giou_dn_3: 0.8239  loss_ce_4: 3.585  loss_mask_4: 2.133  loss_dice_4: 3.402  loss_bbox_4: 3.041  loss_giou_4: 1.736  loss_ce_dn_4: 4.48  loss_mask_dn_4: 2.03  loss_dice_dn_4: 3.365  loss_bbox_dn_4: 1.175  loss_giou_dn_4: 0.8263  loss_ce_5: 3.709  loss_mask_5: 2.059  loss_dice_5: 3.403  loss_bbox_5: 3.058  loss_giou_5: 1.713  loss_ce_dn_5: 4.502  loss_mask_dn_5: 2.01  loss_dice_dn_5: 3.34  loss_bbox_dn_5: 1.172  loss_giou_dn_5: 0.8286  loss_ce_6: 3.617  loss_mask_6: 2.069  loss_dice_6: 3.395  loss_bbox_6: 3.096  loss_giou_6: 1.705  loss_ce_dn_6: 4.549  loss_mask_dn_6: 2.006  loss_dice_dn_6: 3.341  loss_bbox_dn_6: 1.173  loss_giou_dn_6: 0.8311  loss_ce_7: 3.609  loss_mask_7: 2.112  loss_dice_7: 3.429  loss_bbox_7: 3.077  loss_giou_7: 1.724  loss_ce_dn_7: 4.738  loss_mask_dn_7: 2.022  loss_dice_dn_7: 3.408  loss_bbox_dn_7: 1.179  loss_giou_dn_7: 0.844  loss_ce_8: 3.755  loss_mask_8: 2.117  loss_dice_8: 3.436  loss_bbox_8: 3.012  loss_giou_8: 1.763  loss_ce_dn_8: 4.849  loss_mask_dn_8: 2.042  loss_dice_dn_8: 3.406  loss_bbox_dn_8: 1.189  loss_giou_dn_8: 0.8616    time: 2.2732  last_time: 2.2909  data_time: 0.0149  last_data_time: 0.0105   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:17:00 d2.utils.events]: \u001b[0m eta: 8 days, 9:33:21  iter: 559  total_loss: 246.1  loss_ce: 3.436  loss_mask: 1.718  loss_dice: 3.354  loss_bbox: 2.98  loss_giou: 1.794  loss_ce_dn: 4.138  loss_mask_dn: 1.547  loss_dice_dn: 3.351  loss_bbox_dn: 1.136  loss_giou_dn: 0.9283  loss_ce_0: 17.99  loss_mask_0: 1.512  loss_dice_0: 3.506  loss_bbox_0: 3.117  loss_giou_0: 1.696  loss_ce_1: 4.164  loss_mask_1: 1.67  loss_dice_1: 3.372  loss_bbox_1: 3.621  loss_giou_1: 1.846  loss_ce_dn_1: 5.172  loss_mask_dn_1: 1.648  loss_dice_dn_1: 3.476  loss_bbox_dn_1: 1.156  loss_giou_dn_1: 0.8308  loss_ce_2: 3.532  loss_mask_2: 1.804  loss_dice_2: 3.373  loss_bbox_2: 3.484  loss_giou_2: 1.8  loss_ce_dn_2: 4.177  loss_mask_dn_2: 1.515  loss_dice_dn_2: 3.392  loss_bbox_dn_2: 1.141  loss_giou_dn_2: 0.8361  loss_ce_3: 3.272  loss_mask_3: 1.672  loss_dice_3: 3.33  loss_bbox_3: 3.343  loss_giou_3: 1.805  loss_ce_dn_3: 3.898  loss_mask_dn_3: 1.505  loss_dice_dn_3: 3.367  loss_bbox_dn_3: 1.134  loss_giou_dn_3: 0.8389  loss_ce_4: 3.233  loss_mask_4: 1.627  loss_dice_4: 3.33  loss_bbox_4: 3.242  loss_giou_4: 1.809  loss_ce_dn_4: 3.79  loss_mask_dn_4: 1.519  loss_dice_dn_4: 3.368  loss_bbox_dn_4: 1.123  loss_giou_dn_4: 0.8541  loss_ce_5: 3.127  loss_mask_5: 1.65  loss_dice_5: 3.322  loss_bbox_5: 3.07  loss_giou_5: 1.794  loss_ce_dn_5: 3.731  loss_mask_dn_5: 1.518  loss_dice_dn_5: 3.373  loss_bbox_dn_5: 1.103  loss_giou_dn_5: 0.8664  loss_ce_6: 3.118  loss_mask_6: 1.659  loss_dice_6: 3.357  loss_bbox_6: 3.013  loss_giou_6: 1.802  loss_ce_dn_6: 3.736  loss_mask_dn_6: 1.482  loss_dice_dn_6: 3.366  loss_bbox_dn_6: 1.105  loss_giou_dn_6: 0.881  loss_ce_7: 3.195  loss_mask_7: 1.667  loss_dice_7: 3.385  loss_bbox_7: 3.035  loss_giou_7: 1.829  loss_ce_dn_7: 3.811  loss_mask_dn_7: 1.512  loss_dice_dn_7: 3.374  loss_bbox_dn_7: 1.112  loss_giou_dn_7: 0.898  loss_ce_8: 3.312  loss_mask_8: 1.638  loss_dice_8: 3.324  loss_bbox_8: 3.039  loss_giou_8: 1.811  loss_ce_dn_8: 3.957  loss_mask_dn_8: 1.56  loss_dice_dn_8: 3.357  loss_bbox_dn_8: 1.121  loss_giou_dn_8: 0.914    time: 2.2731  last_time: 2.2970  data_time: 0.0140  last_data_time: 0.0147   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:17:46 d2.utils.events]: \u001b[0m eta: 8 days, 9:32:35  iter: 579  total_loss: 264  loss_ce: 4.139  loss_mask: 1.914  loss_dice: 3.45  loss_bbox: 2.864  loss_giou: 1.809  loss_ce_dn: 4.867  loss_mask_dn: 1.764  loss_dice_dn: 3.357  loss_bbox_dn: 1.097  loss_giou_dn: 0.864  loss_ce_0: 17.92  loss_mask_0: 1.666  loss_dice_0: 3.628  loss_bbox_0: 3.013  loss_giou_0: 1.701  loss_ce_1: 4.834  loss_mask_1: 1.786  loss_dice_1: 3.41  loss_bbox_1: 3.46  loss_giou_1: 1.82  loss_ce_dn_1: 5.872  loss_mask_dn_1: 1.781  loss_dice_dn_1: 3.515  loss_bbox_dn_1: 1.1  loss_giou_dn_1: 0.8257  loss_ce_2: 4.197  loss_mask_2: 1.719  loss_dice_2: 3.368  loss_bbox_2: 3.168  loss_giou_2: 1.787  loss_ce_dn_2: 4.656  loss_mask_dn_2: 1.659  loss_dice_dn_2: 3.463  loss_bbox_dn_2: 1.087  loss_giou_dn_2: 0.8288  loss_ce_3: 4.04  loss_mask_3: 1.695  loss_dice_3: 3.394  loss_bbox_3: 3.111  loss_giou_3: 1.74  loss_ce_dn_3: 4.313  loss_mask_dn_3: 1.678  loss_dice_dn_3: 3.429  loss_bbox_dn_3: 1.069  loss_giou_dn_3: 0.8332  loss_ce_4: 4.006  loss_mask_4: 1.785  loss_dice_4: 3.437  loss_bbox_4: 3.02  loss_giou_4: 1.738  loss_ce_dn_4: 4.211  loss_mask_dn_4: 1.72  loss_dice_dn_4: 3.353  loss_bbox_dn_4: 1.066  loss_giou_dn_4: 0.8372  loss_ce_5: 4.008  loss_mask_5: 1.832  loss_dice_5: 3.431  loss_bbox_5: 2.995  loss_giou_5: 1.769  loss_ce_dn_5: 4.133  loss_mask_dn_5: 1.735  loss_dice_dn_5: 3.428  loss_bbox_dn_5: 1.065  loss_giou_dn_5: 0.8411  loss_ce_6: 3.982  loss_mask_6: 1.914  loss_dice_6: 3.424  loss_bbox_6: 2.855  loss_giou_6: 1.789  loss_ce_dn_6: 4.165  loss_mask_dn_6: 1.763  loss_dice_dn_6: 3.424  loss_bbox_dn_6: 1.07  loss_giou_dn_6: 0.8409  loss_ce_7: 3.981  loss_mask_7: 1.895  loss_dice_7: 3.426  loss_bbox_7: 2.819  loss_giou_7: 1.798  loss_ce_dn_7: 4.316  loss_mask_dn_7: 1.759  loss_dice_dn_7: 3.372  loss_bbox_dn_7: 1.076  loss_giou_dn_7: 0.8466  loss_ce_8: 4.163  loss_mask_8: 1.886  loss_dice_8: 3.426  loss_bbox_8: 2.769  loss_giou_8: 1.799  loss_ce_dn_8: 4.581  loss_mask_dn_8: 1.768  loss_dice_dn_8: 3.372  loss_bbox_dn_8: 1.084  loss_giou_dn_8: 0.8529    time: 2.2735  last_time: 2.2665  data_time: 0.0143  last_data_time: 0.0072   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:18:32 d2.utils.events]: \u001b[0m eta: 8 days, 9:32:35  iter: 599  total_loss: 262.9  loss_ce: 4.043  loss_mask: 1.57  loss_dice: 3.504  loss_bbox: 2.708  loss_giou: 1.754  loss_ce_dn: 4.996  loss_mask_dn: 1.56  loss_dice_dn: 3.468  loss_bbox_dn: 1.131  loss_giou_dn: 0.9177  loss_ce_0: 17.81  loss_mask_0: 1.486  loss_dice_0: 3.619  loss_bbox_0: 2.933  loss_giou_0: 1.592  loss_ce_1: 4.406  loss_mask_1: 1.555  loss_dice_1: 3.443  loss_bbox_1: 3.173  loss_giou_1: 1.681  loss_ce_dn_1: 5.746  loss_mask_dn_1: 1.682  loss_dice_dn_1: 3.553  loss_bbox_dn_1: 1.124  loss_giou_dn_1: 0.8344  loss_ce_2: 4.123  loss_mask_2: 1.521  loss_dice_2: 3.487  loss_bbox_2: 2.939  loss_giou_2: 1.679  loss_ce_dn_2: 4.624  loss_mask_dn_2: 1.577  loss_dice_dn_2: 3.48  loss_bbox_dn_2: 1.109  loss_giou_dn_2: 0.8354  loss_ce_3: 3.889  loss_mask_3: 1.738  loss_dice_3: 3.547  loss_bbox_3: 2.802  loss_giou_3: 1.696  loss_ce_dn_3: 4.351  loss_mask_dn_3: 1.582  loss_dice_dn_3: 3.481  loss_bbox_dn_3: 1.101  loss_giou_dn_3: 0.8367  loss_ce_4: 3.923  loss_mask_4: 1.718  loss_dice_4: 3.522  loss_bbox_4: 2.777  loss_giou_4: 1.701  loss_ce_dn_4: 4.229  loss_mask_dn_4: 1.596  loss_dice_dn_4: 3.427  loss_bbox_dn_4: 1.089  loss_giou_dn_4: 0.8433  loss_ce_5: 3.736  loss_mask_5: 1.645  loss_dice_5: 3.548  loss_bbox_5: 2.75  loss_giou_5: 1.706  loss_ce_dn_5: 4.215  loss_mask_dn_5: 1.506  loss_dice_dn_5: 3.434  loss_bbox_dn_5: 1.087  loss_giou_dn_5: 0.8586  loss_ce_6: 3.768  loss_mask_6: 1.545  loss_dice_6: 3.545  loss_bbox_6: 2.778  loss_giou_6: 1.715  loss_ce_dn_6: 4.365  loss_mask_dn_6: 1.538  loss_dice_dn_6: 3.446  loss_bbox_dn_6: 1.099  loss_giou_dn_6: 0.8698  loss_ce_7: 3.775  loss_mask_7: 1.554  loss_dice_7: 3.531  loss_bbox_7: 2.708  loss_giou_7: 1.731  loss_ce_dn_7: 4.516  loss_mask_dn_7: 1.493  loss_dice_dn_7: 3.455  loss_bbox_dn_7: 1.098  loss_giou_dn_7: 0.8825  loss_ce_8: 3.777  loss_mask_8: 1.624  loss_dice_8: 3.516  loss_bbox_8: 2.715  loss_giou_8: 1.745  loss_ce_dn_8: 4.654  loss_mask_dn_8: 1.558  loss_dice_dn_8: 3.513  loss_bbox_dn_8: 1.111  loss_giou_dn_8: 0.8999    time: 2.2737  last_time: 2.2762  data_time: 0.0115  last_data_time: 0.0043   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:19:17 d2.utils.events]: \u001b[0m eta: 8 days, 9:31:04  iter: 619  total_loss: 249.7  loss_ce: 3.448  loss_mask: 1.923  loss_dice: 3.162  loss_bbox: 2.961  loss_giou: 1.727  loss_ce_dn: 4.556  loss_mask_dn: 1.804  loss_dice_dn: 3.203  loss_bbox_dn: 1.125  loss_giou_dn: 0.8899  loss_ce_0: 17.88  loss_mask_0: 1.668  loss_dice_0: 3.538  loss_bbox_0: 2.859  loss_giou_0: 1.574  loss_ce_1: 4.105  loss_mask_1: 1.811  loss_dice_1: 3.238  loss_bbox_1: 3.234  loss_giou_1: 1.732  loss_ce_dn_1: 5.286  loss_mask_dn_1: 1.785  loss_dice_dn_1: 3.316  loss_bbox_dn_1: 1.168  loss_giou_dn_1: 0.8215  loss_ce_2: 3.519  loss_mask_2: 1.862  loss_dice_2: 3.165  loss_bbox_2: 3.268  loss_giou_2: 1.687  loss_ce_dn_2: 4.353  loss_mask_dn_2: 1.738  loss_dice_dn_2: 3.236  loss_bbox_dn_2: 1.124  loss_giou_dn_2: 0.8225  loss_ce_3: 3.293  loss_mask_3: 1.897  loss_dice_3: 3.141  loss_bbox_3: 3.301  loss_giou_3: 1.686  loss_ce_dn_3: 4.125  loss_mask_dn_3: 1.748  loss_dice_dn_3: 3.22  loss_bbox_dn_3: 1.105  loss_giou_dn_3: 0.8226  loss_ce_4: 3.338  loss_mask_4: 1.965  loss_dice_4: 3.131  loss_bbox_4: 3.089  loss_giou_4: 1.667  loss_ce_dn_4: 4.011  loss_mask_dn_4: 1.838  loss_dice_dn_4: 3.178  loss_bbox_dn_4: 1.092  loss_giou_dn_4: 0.8246  loss_ce_5: 3.337  loss_mask_5: 1.957  loss_dice_5: 3.118  loss_bbox_5: 3.003  loss_giou_5: 1.684  loss_ce_dn_5: 4.081  loss_mask_dn_5: 1.783  loss_dice_dn_5: 3.136  loss_bbox_dn_5: 1.089  loss_giou_dn_5: 0.8344  loss_ce_6: 3.452  loss_mask_6: 1.962  loss_dice_6: 3.099  loss_bbox_6: 2.957  loss_giou_6: 1.686  loss_ce_dn_6: 4.198  loss_mask_dn_6: 1.781  loss_dice_dn_6: 3.152  loss_bbox_dn_6: 1.09  loss_giou_dn_6: 0.855  loss_ce_7: 3.509  loss_mask_7: 1.907  loss_dice_7: 3.161  loss_bbox_7: 2.96  loss_giou_7: 1.692  loss_ce_dn_7: 4.35  loss_mask_dn_7: 1.793  loss_dice_dn_7: 3.194  loss_bbox_dn_7: 1.084  loss_giou_dn_7: 0.8639  loss_ce_8: 3.441  loss_mask_8: 1.907  loss_dice_8: 3.181  loss_bbox_8: 2.978  loss_giou_8: 1.697  loss_ce_dn_8: 4.332  loss_mask_dn_8: 1.856  loss_dice_dn_8: 3.177  loss_bbox_dn_8: 1.099  loss_giou_dn_8: 0.8725    time: 2.2740  last_time: 2.2671  data_time: 0.0126  last_data_time: 0.0055   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:20:03 d2.utils.events]: \u001b[0m eta: 8 days, 9:29:15  iter: 639  total_loss: 270.8  loss_ce: 4.252  loss_mask: 1.652  loss_dice: 3.331  loss_bbox: 2.867  loss_giou: 1.601  loss_ce_dn: 5.409  loss_mask_dn: 1.687  loss_dice_dn: 3.353  loss_bbox_dn: 1.131  loss_giou_dn: 0.8761  loss_ce_0: 17.93  loss_mask_0: 1.609  loss_dice_0: 3.681  loss_bbox_0: 2.911  loss_giou_0: 1.629  loss_ce_1: 4.492  loss_mask_1: 1.716  loss_dice_1: 3.339  loss_bbox_1: 3.495  loss_giou_1: 1.688  loss_ce_dn_1: 5.693  loss_mask_dn_1: 1.717  loss_dice_dn_1: 3.529  loss_bbox_dn_1: 1.103  loss_giou_dn_1: 0.8342  loss_ce_2: 4.115  loss_mask_2: 1.731  loss_dice_2: 3.326  loss_bbox_2: 3.293  loss_giou_2: 1.694  loss_ce_dn_2: 4.915  loss_mask_dn_2: 1.709  loss_dice_dn_2: 3.443  loss_bbox_dn_2: 1.117  loss_giou_dn_2: 0.829  loss_ce_3: 4.092  loss_mask_3: 1.725  loss_dice_3: 3.352  loss_bbox_3: 3.111  loss_giou_3: 1.669  loss_ce_dn_3: 4.756  loss_mask_dn_3: 1.668  loss_dice_dn_3: 3.434  loss_bbox_dn_3: 1.102  loss_giou_dn_3: 0.8235  loss_ce_4: 3.903  loss_mask_4: 1.749  loss_dice_4: 3.289  loss_bbox_4: 3.121  loss_giou_4: 1.701  loss_ce_dn_4: 4.758  loss_mask_dn_4: 1.704  loss_dice_dn_4: 3.354  loss_bbox_dn_4: 1.069  loss_giou_dn_4: 0.8232  loss_ce_5: 3.697  loss_mask_5: 1.785  loss_dice_5: 3.343  loss_bbox_5: 3.021  loss_giou_5: 1.645  loss_ce_dn_5: 4.817  loss_mask_dn_5: 1.718  loss_dice_dn_5: 3.361  loss_bbox_dn_5: 1.07  loss_giou_dn_5: 0.8283  loss_ce_6: 3.939  loss_mask_6: 1.712  loss_dice_6: 3.347  loss_bbox_6: 3.107  loss_giou_6: 1.631  loss_ce_dn_6: 4.829  loss_mask_dn_6: 1.712  loss_dice_dn_6: 3.315  loss_bbox_dn_6: 1.079  loss_giou_dn_6: 0.8338  loss_ce_7: 3.919  loss_mask_7: 1.726  loss_dice_7: 3.31  loss_bbox_7: 3.06  loss_giou_7: 1.611  loss_ce_dn_7: 4.95  loss_mask_dn_7: 1.626  loss_dice_dn_7: 3.337  loss_bbox_dn_7: 1.1  loss_giou_dn_7: 0.847  loss_ce_8: 3.893  loss_mask_8: 1.696  loss_dice_8: 3.307  loss_bbox_8: 2.848  loss_giou_8: 1.581  loss_ce_dn_8: 5.116  loss_mask_dn_8: 1.657  loss_dice_dn_8: 3.339  loss_bbox_dn_8: 1.113  loss_giou_dn_8: 0.8586    time: 2.2739  last_time: 2.2667  data_time: 0.0135  last_data_time: 0.0205   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:20:49 d2.utils.events]: \u001b[0m eta: 8 days, 9:25:33  iter: 659  total_loss: 257.1  loss_ce: 3.941  loss_mask: 1.937  loss_dice: 3.321  loss_bbox: 2.885  loss_giou: 1.789  loss_ce_dn: 5.11  loss_mask_dn: 1.914  loss_dice_dn: 3.244  loss_bbox_dn: 1.103  loss_giou_dn: 0.9105  loss_ce_0: 17.81  loss_mask_0: 1.778  loss_dice_0: 3.496  loss_bbox_0: 3.129  loss_giou_0: 1.717  loss_ce_1: 4.841  loss_mask_1: 1.997  loss_dice_1: 3.32  loss_bbox_1: 3.657  loss_giou_1: 1.769  loss_ce_dn_1: 5.853  loss_mask_dn_1: 1.858  loss_dice_dn_1: 3.359  loss_bbox_dn_1: 1.14  loss_giou_dn_1: 0.8222  loss_ce_2: 4.298  loss_mask_2: 1.971  loss_dice_2: 3.274  loss_bbox_2: 3.356  loss_giou_2: 1.809  loss_ce_dn_2: 4.548  loss_mask_dn_2: 1.861  loss_dice_dn_2: 3.262  loss_bbox_dn_2: 1.126  loss_giou_dn_2: 0.8218  loss_ce_3: 4.127  loss_mask_3: 1.974  loss_dice_3: 3.295  loss_bbox_3: 3.136  loss_giou_3: 1.815  loss_ce_dn_3: 4.206  loss_mask_dn_3: 1.854  loss_dice_dn_3: 3.31  loss_bbox_dn_3: 1.108  loss_giou_dn_3: 0.824  loss_ce_4: 4.263  loss_mask_4: 1.967  loss_dice_4: 3.342  loss_bbox_4: 3.036  loss_giou_4: 1.822  loss_ce_dn_4: 4.151  loss_mask_dn_4: 1.916  loss_dice_dn_4: 3.298  loss_bbox_dn_4: 1.092  loss_giou_dn_4: 0.833  loss_ce_5: 3.994  loss_mask_5: 2.021  loss_dice_5: 3.321  loss_bbox_5: 2.968  loss_giou_5: 1.823  loss_ce_dn_5: 4.212  loss_mask_dn_5: 1.92  loss_dice_dn_5: 3.293  loss_bbox_dn_5: 1.078  loss_giou_dn_5: 0.8366  loss_ce_6: 3.944  loss_mask_6: 1.992  loss_dice_6: 3.333  loss_bbox_6: 2.919  loss_giou_6: 1.82  loss_ce_dn_6: 4.418  loss_mask_dn_6: 1.926  loss_dice_dn_6: 3.269  loss_bbox_dn_6: 1.08  loss_giou_dn_6: 0.848  loss_ce_7: 3.804  loss_mask_7: 1.952  loss_dice_7: 3.256  loss_bbox_7: 2.879  loss_giou_7: 1.793  loss_ce_dn_7: 4.628  loss_mask_dn_7: 1.916  loss_dice_dn_7: 3.287  loss_bbox_dn_7: 1.088  loss_giou_dn_7: 0.8631  loss_ce_8: 4.038  loss_mask_8: 1.936  loss_dice_8: 3.317  loss_bbox_8: 2.889  loss_giou_8: 1.769  loss_ce_dn_8: 4.788  loss_mask_dn_8: 1.936  loss_dice_dn_8: 3.281  loss_bbox_dn_8: 1.092  loss_giou_dn_8: 0.8847    time: 2.2745  last_time: 2.2670  data_time: 0.0132  last_data_time: 0.0174   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:21:34 d2.utils.events]: \u001b[0m eta: 8 days, 9:24:03  iter: 679  total_loss: 249.2  loss_ce: 3.078  loss_mask: 2.181  loss_dice: 3.023  loss_bbox: 3.239  loss_giou: 1.573  loss_ce_dn: 3.729  loss_mask_dn: 1.993  loss_dice_dn: 3.043  loss_bbox_dn: 1.19  loss_giou_dn: 0.8902  loss_ce_0: 17.71  loss_mask_0: 2.01  loss_dice_0: 3.418  loss_bbox_0: 3.122  loss_giou_0: 1.64  loss_ce_1: 3.659  loss_mask_1: 2.029  loss_dice_1: 3.145  loss_bbox_1: 3.858  loss_giou_1: 1.725  loss_ce_dn_1: 4.708  loss_mask_dn_1: 1.998  loss_dice_dn_1: 3.131  loss_bbox_dn_1: 1.284  loss_giou_dn_1: 0.8306  loss_ce_2: 2.961  loss_mask_2: 2.022  loss_dice_2: 3.149  loss_bbox_2: 3.586  loss_giou_2: 1.678  loss_ce_dn_2: 3.592  loss_mask_dn_2: 1.998  loss_dice_dn_2: 3.087  loss_bbox_dn_2: 1.241  loss_giou_dn_2: 0.8276  loss_ce_3: 2.915  loss_mask_3: 2.081  loss_dice_3: 3.09  loss_bbox_3: 3.408  loss_giou_3: 1.579  loss_ce_dn_3: 3.336  loss_mask_dn_3: 1.999  loss_dice_dn_3: 3.067  loss_bbox_dn_3: 1.215  loss_giou_dn_3: 0.8236  loss_ce_4: 3  loss_mask_4: 2.09  loss_dice_4: 3.036  loss_bbox_4: 3.321  loss_giou_4: 1.568  loss_ce_dn_4: 3.165  loss_mask_dn_4: 2.002  loss_dice_dn_4: 3.008  loss_bbox_dn_4: 1.197  loss_giou_dn_4: 0.825  loss_ce_5: 2.788  loss_mask_5: 2.193  loss_dice_5: 2.994  loss_bbox_5: 3.138  loss_giou_5: 1.553  loss_ce_dn_5: 3.161  loss_mask_dn_5: 1.995  loss_dice_dn_5: 2.989  loss_bbox_dn_5: 1.184  loss_giou_dn_5: 0.8315  loss_ce_6: 2.745  loss_mask_6: 2.248  loss_dice_6: 3.013  loss_bbox_6: 3.085  loss_giou_6: 1.545  loss_ce_dn_6: 3.147  loss_mask_dn_6: 1.998  loss_dice_dn_6: 2.991  loss_bbox_dn_6: 1.178  loss_giou_dn_6: 0.8382  loss_ce_7: 2.875  loss_mask_7: 2.136  loss_dice_7: 3.007  loss_bbox_7: 3.204  loss_giou_7: 1.547  loss_ce_dn_7: 3.317  loss_mask_dn_7: 1.955  loss_dice_dn_7: 2.968  loss_bbox_dn_7: 1.172  loss_giou_dn_7: 0.8536  loss_ce_8: 3.049  loss_mask_8: 2.217  loss_dice_8: 3.03  loss_bbox_8: 3.21  loss_giou_8: 1.551  loss_ce_dn_8: 3.495  loss_mask_dn_8: 1.957  loss_dice_dn_8: 2.988  loss_bbox_dn_8: 1.178  loss_giou_dn_8: 0.8657    time: 2.2745  last_time: 2.2530  data_time: 0.0115  last_data_time: 0.0049   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:22:20 d2.utils.events]: \u001b[0m eta: 8 days, 9:23:17  iter: 699  total_loss: 259.8  loss_ce: 4.154  loss_mask: 1.95  loss_dice: 3.288  loss_bbox: 3.002  loss_giou: 1.767  loss_ce_dn: 5.121  loss_mask_dn: 1.902  loss_dice_dn: 3.377  loss_bbox_dn: 1.121  loss_giou_dn: 0.8719  loss_ce_0: 17.54  loss_mask_0: 1.753  loss_dice_0: 3.577  loss_bbox_0: 3.073  loss_giou_0: 1.629  loss_ce_1: 4.088  loss_mask_1: 1.934  loss_dice_1: 3.314  loss_bbox_1: 3.634  loss_giou_1: 1.786  loss_ce_dn_1: 5.777  loss_mask_dn_1: 1.934  loss_dice_dn_1: 3.454  loss_bbox_dn_1: 1.179  loss_giou_dn_1: 0.829  loss_ce_2: 3.595  loss_mask_2: 1.88  loss_dice_2: 3.256  loss_bbox_2: 3.319  loss_giou_2: 1.722  loss_ce_dn_2: 4.793  loss_mask_dn_2: 1.894  loss_dice_dn_2: 3.401  loss_bbox_dn_2: 1.157  loss_giou_dn_2: 0.8225  loss_ce_3: 3.518  loss_mask_3: 1.877  loss_dice_3: 3.219  loss_bbox_3: 3.123  loss_giou_3: 1.752  loss_ce_dn_3: 4.567  loss_mask_dn_3: 1.943  loss_dice_dn_3: 3.402  loss_bbox_dn_3: 1.13  loss_giou_dn_3: 0.8189  loss_ce_4: 3.514  loss_mask_4: 1.845  loss_dice_4: 3.243  loss_bbox_4: 3.041  loss_giou_4: 1.761  loss_ce_dn_4: 4.4  loss_mask_dn_4: 1.9  loss_dice_dn_4: 3.396  loss_bbox_dn_4: 1.106  loss_giou_dn_4: 0.8178  loss_ce_5: 3.722  loss_mask_5: 1.864  loss_dice_5: 3.274  loss_bbox_5: 3.071  loss_giou_5: 1.757  loss_ce_dn_5: 4.376  loss_mask_dn_5: 1.868  loss_dice_dn_5: 3.373  loss_bbox_dn_5: 1.103  loss_giou_dn_5: 0.8266  loss_ce_6: 3.667  loss_mask_6: 1.903  loss_dice_6: 3.273  loss_bbox_6: 3.026  loss_giou_6: 1.725  loss_ce_dn_6: 4.581  loss_mask_dn_6: 1.872  loss_dice_dn_6: 3.343  loss_bbox_dn_6: 1.109  loss_giou_dn_6: 0.8391  loss_ce_7: 3.894  loss_mask_7: 1.938  loss_dice_7: 3.268  loss_bbox_7: 3.042  loss_giou_7: 1.727  loss_ce_dn_7: 4.669  loss_mask_dn_7: 1.903  loss_dice_dn_7: 3.388  loss_bbox_dn_7: 1.116  loss_giou_dn_7: 0.8498  loss_ce_8: 3.938  loss_mask_8: 1.938  loss_dice_8: 3.296  loss_bbox_8: 3.07  loss_giou_8: 1.775  loss_ce_dn_8: 4.834  loss_mask_dn_8: 1.899  loss_dice_dn_8: 3.383  loss_bbox_dn_8: 1.116  loss_giou_dn_8: 0.8602    time: 2.2749  last_time: 2.2271  data_time: 0.0132  last_data_time: 0.0065   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:23:05 d2.utils.events]: \u001b[0m eta: 8 days, 9:21:19  iter: 719  total_loss: 255.9  loss_ce: 3.834  loss_mask: 1.69  loss_dice: 3.334  loss_bbox: 2.736  loss_giou: 1.881  loss_ce_dn: 4.891  loss_mask_dn: 1.6  loss_dice_dn: 3.306  loss_bbox_dn: 1.05  loss_giou_dn: 0.8835  loss_ce_0: 17.36  loss_mask_0: 1.541  loss_dice_0: 3.747  loss_bbox_0: 3.009  loss_giou_0: 1.744  loss_ce_1: 4.428  loss_mask_1: 1.678  loss_dice_1: 3.394  loss_bbox_1: 3.506  loss_giou_1: 1.823  loss_ce_dn_1: 5.705  loss_mask_dn_1: 1.618  loss_dice_dn_1: 3.503  loss_bbox_dn_1: 1.014  loss_giou_dn_1: 0.8373  loss_ce_2: 4.167  loss_mask_2: 1.791  loss_dice_2: 3.336  loss_bbox_2: 3.199  loss_giou_2: 1.858  loss_ce_dn_2: 4.484  loss_mask_dn_2: 1.545  loss_dice_dn_2: 3.41  loss_bbox_dn_2: 1.013  loss_giou_dn_2: 0.837  loss_ce_3: 3.714  loss_mask_3: 1.653  loss_dice_3: 3.299  loss_bbox_3: 3.059  loss_giou_3: 1.85  loss_ce_dn_3: 4.301  loss_mask_dn_3: 1.568  loss_dice_dn_3: 3.439  loss_bbox_dn_3: 1.009  loss_giou_dn_3: 0.8375  loss_ce_4: 3.73  loss_mask_4: 1.67  loss_dice_4: 3.251  loss_bbox_4: 2.911  loss_giou_4: 1.833  loss_ce_dn_4: 4.329  loss_mask_dn_4: 1.565  loss_dice_dn_4: 3.389  loss_bbox_dn_4: 1.011  loss_giou_dn_4: 0.8386  loss_ce_5: 3.585  loss_mask_5: 1.67  loss_dice_5: 3.223  loss_bbox_5: 2.872  loss_giou_5: 1.852  loss_ce_dn_5: 4.357  loss_mask_dn_5: 1.55  loss_dice_dn_5: 3.377  loss_bbox_dn_5: 1.016  loss_giou_dn_5: 0.8382  loss_ce_6: 3.579  loss_mask_6: 1.678  loss_dice_6: 3.288  loss_bbox_6: 2.736  loss_giou_6: 1.858  loss_ce_dn_6: 4.325  loss_mask_dn_6: 1.539  loss_dice_dn_6: 3.36  loss_bbox_dn_6: 1.023  loss_giou_dn_6: 0.849  loss_ce_7: 3.504  loss_mask_7: 1.642  loss_dice_7: 3.252  loss_bbox_7: 2.758  loss_giou_7: 1.852  loss_ce_dn_7: 4.495  loss_mask_dn_7: 1.635  loss_dice_dn_7: 3.367  loss_bbox_dn_7: 1.019  loss_giou_dn_7: 0.8599  loss_ce_8: 3.544  loss_mask_8: 1.682  loss_dice_8: 3.281  loss_bbox_8: 2.765  loss_giou_8: 1.86  loss_ce_dn_8: 4.59  loss_mask_dn_8: 1.575  loss_dice_dn_8: 3.341  loss_bbox_dn_8: 1.029  loss_giou_dn_8: 0.8721    time: 2.2749  last_time: 2.3028  data_time: 0.0121  last_data_time: 0.0048   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:23:51 d2.utils.events]: \u001b[0m eta: 8 days, 9:21:36  iter: 739  total_loss: 263  loss_ce: 3.718  loss_mask: 2.136  loss_dice: 3.255  loss_bbox: 2.86  loss_giou: 1.76  loss_ce_dn: 4.91  loss_mask_dn: 2.067  loss_dice_dn: 3.176  loss_bbox_dn: 1.176  loss_giou_dn: 0.8828  loss_ce_0: 17.57  loss_mask_0: 1.947  loss_dice_0: 3.417  loss_bbox_0: 3.024  loss_giou_0: 1.632  loss_ce_1: 3.874  loss_mask_1: 2.169  loss_dice_1: 3.245  loss_bbox_1: 3.47  loss_giou_1: 1.768  loss_ce_dn_1: 5.585  loss_mask_dn_1: 2.117  loss_dice_dn_1: 3.31  loss_bbox_dn_1: 1.217  loss_giou_dn_1: 0.8282  loss_ce_2: 3.465  loss_mask_2: 2.158  loss_dice_2: 3.26  loss_bbox_2: 3.304  loss_giou_2: 1.714  loss_ce_dn_2: 4.76  loss_mask_dn_2: 2.042  loss_dice_dn_2: 3.225  loss_bbox_dn_2: 1.182  loss_giou_dn_2: 0.8285  loss_ce_3: 3.391  loss_mask_3: 2.191  loss_dice_3: 3.227  loss_bbox_3: 3.259  loss_giou_3: 1.737  loss_ce_dn_3: 4.6  loss_mask_dn_3: 2.004  loss_dice_dn_3: 3.204  loss_bbox_dn_3: 1.158  loss_giou_dn_3: 0.8235  loss_ce_4: 3.543  loss_mask_4: 2.166  loss_dice_4: 3.229  loss_bbox_4: 3.036  loss_giou_4: 1.76  loss_ce_dn_4: 4.493  loss_mask_dn_4: 2.008  loss_dice_dn_4: 3.18  loss_bbox_dn_4: 1.138  loss_giou_dn_4: 0.8266  loss_ce_5: 3.441  loss_mask_5: 2.186  loss_dice_5: 3.275  loss_bbox_5: 3.151  loss_giou_5: 1.744  loss_ce_dn_5: 4.374  loss_mask_dn_5: 2.042  loss_dice_dn_5: 3.193  loss_bbox_dn_5: 1.133  loss_giou_dn_5: 0.8385  loss_ce_6: 3.699  loss_mask_6: 2.193  loss_dice_6: 3.272  loss_bbox_6: 3.03  loss_giou_6: 1.756  loss_ce_dn_6: 4.363  loss_mask_dn_6: 2.101  loss_dice_dn_6: 3.179  loss_bbox_dn_6: 1.124  loss_giou_dn_6: 0.8454  loss_ce_7: 3.688  loss_mask_7: 2.057  loss_dice_7: 3.226  loss_bbox_7: 2.897  loss_giou_7: 1.726  loss_ce_dn_7: 4.526  loss_mask_dn_7: 2.064  loss_dice_dn_7: 3.132  loss_bbox_dn_7: 1.133  loss_giou_dn_7: 0.8529  loss_ce_8: 3.748  loss_mask_8: 2.075  loss_dice_8: 3.277  loss_bbox_8: 2.84  loss_giou_8: 1.716  loss_ce_dn_8: 4.743  loss_mask_dn_8: 2.081  loss_dice_dn_8: 3.154  loss_bbox_dn_8: 1.147  loss_giou_dn_8: 0.8669    time: 2.2752  last_time: 2.2457  data_time: 0.0123  last_data_time: 0.0047   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:24:37 d2.utils.events]: \u001b[0m eta: 8 days, 9:20:29  iter: 759  total_loss: 245.8  loss_ce: 3.498  loss_mask: 1.595  loss_dice: 3.291  loss_bbox: 2.864  loss_giou: 1.803  loss_ce_dn: 4.61  loss_mask_dn: 1.644  loss_dice_dn: 3.292  loss_bbox_dn: 1.006  loss_giou_dn: 0.8982  loss_ce_0: 17.17  loss_mask_0: 1.557  loss_dice_0: 3.534  loss_bbox_0: 3.022  loss_giou_0: 1.692  loss_ce_1: 4.279  loss_mask_1: 1.696  loss_dice_1: 3.243  loss_bbox_1: 3.514  loss_giou_1: 1.751  loss_ce_dn_1: 5.096  loss_mask_dn_1: 1.613  loss_dice_dn_1: 3.369  loss_bbox_dn_1: 1.112  loss_giou_dn_1: 0.8338  loss_ce_2: 3.949  loss_mask_2: 1.627  loss_dice_2: 3.287  loss_bbox_2: 3.205  loss_giou_2: 1.777  loss_ce_dn_2: 4.074  loss_mask_dn_2: 1.635  loss_dice_dn_2: 3.334  loss_bbox_dn_2: 1.073  loss_giou_dn_2: 0.8342  loss_ce_3: 3.791  loss_mask_3: 1.518  loss_dice_3: 3.29  loss_bbox_3: 3.16  loss_giou_3: 1.77  loss_ce_dn_3: 3.872  loss_mask_dn_3: 1.64  loss_dice_dn_3: 3.354  loss_bbox_dn_3: 1.027  loss_giou_dn_3: 0.8309  loss_ce_4: 3.681  loss_mask_4: 1.627  loss_dice_4: 3.284  loss_bbox_4: 3.061  loss_giou_4: 1.804  loss_ce_dn_4: 3.761  loss_mask_dn_4: 1.612  loss_dice_dn_4: 3.316  loss_bbox_dn_4: 1.026  loss_giou_dn_4: 0.8339  loss_ce_5: 3.74  loss_mask_5: 1.646  loss_dice_5: 3.277  loss_bbox_5: 2.977  loss_giou_5: 1.746  loss_ce_dn_5: 3.756  loss_mask_dn_5: 1.615  loss_dice_dn_5: 3.312  loss_bbox_dn_5: 1.015  loss_giou_dn_5: 0.8407  loss_ce_6: 3.455  loss_mask_6: 1.661  loss_dice_6: 3.254  loss_bbox_6: 2.948  loss_giou_6: 1.772  loss_ce_dn_6: 3.787  loss_mask_dn_6: 1.604  loss_dice_dn_6: 3.299  loss_bbox_dn_6: 1.015  loss_giou_dn_6: 0.8421  loss_ce_7: 3.59  loss_mask_7: 1.678  loss_dice_7: 3.234  loss_bbox_7: 2.908  loss_giou_7: 1.761  loss_ce_dn_7: 3.998  loss_mask_dn_7: 1.618  loss_dice_dn_7: 3.31  loss_bbox_dn_7: 1.01  loss_giou_dn_7: 0.8622  loss_ce_8: 3.559  loss_mask_8: 1.64  loss_dice_8: 3.279  loss_bbox_8: 2.858  loss_giou_8: 1.768  loss_ce_dn_8: 4.207  loss_mask_dn_8: 1.632  loss_dice_dn_8: 3.297  loss_bbox_dn_8: 1.006  loss_giou_dn_8: 0.8796    time: 2.2751  last_time: 2.2599  data_time: 0.0128  last_data_time: 0.0060   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:25:22 d2.utils.events]: \u001b[0m eta: 8 days, 9:18:52  iter: 779  total_loss: 250.3  loss_ce: 3.514  loss_mask: 1.827  loss_dice: 3.479  loss_bbox: 3.412  loss_giou: 1.747  loss_ce_dn: 4.127  loss_mask_dn: 1.833  loss_dice_dn: 3.346  loss_bbox_dn: 1.097  loss_giou_dn: 0.8504  loss_ce_0: 17.33  loss_mask_0: 1.586  loss_dice_0: 3.712  loss_bbox_0: 3.174  loss_giou_0: 1.636  loss_ce_1: 4.302  loss_mask_1: 1.695  loss_dice_1: 3.369  loss_bbox_1: 3.626  loss_giou_1: 1.841  loss_ce_dn_1: 5.127  loss_mask_dn_1: 1.732  loss_dice_dn_1: 3.448  loss_bbox_dn_1: 1.119  loss_giou_dn_1: 0.8338  loss_ce_2: 3.902  loss_mask_2: 1.757  loss_dice_2: 3.313  loss_bbox_2: 3.487  loss_giou_2: 1.768  loss_ce_dn_2: 3.837  loss_mask_dn_2: 1.747  loss_dice_dn_2: 3.369  loss_bbox_dn_2: 1.105  loss_giou_dn_2: 0.8307  loss_ce_3: 3.55  loss_mask_3: 1.794  loss_dice_3: 3.275  loss_bbox_3: 3.38  loss_giou_3: 1.751  loss_ce_dn_3: 3.504  loss_mask_dn_3: 1.75  loss_dice_dn_3: 3.348  loss_bbox_dn_3: 1.079  loss_giou_dn_3: 0.8265  loss_ce_4: 3.497  loss_mask_4: 1.791  loss_dice_4: 3.336  loss_bbox_4: 3.334  loss_giou_4: 1.809  loss_ce_dn_4: 3.429  loss_mask_dn_4: 1.778  loss_dice_dn_4: 3.305  loss_bbox_dn_4: 1.055  loss_giou_dn_4: 0.8229  loss_ce_5: 3.536  loss_mask_5: 1.785  loss_dice_5: 3.378  loss_bbox_5: 3.37  loss_giou_5: 1.805  loss_ce_dn_5: 3.496  loss_mask_dn_5: 1.824  loss_dice_dn_5: 3.333  loss_bbox_dn_5: 1.051  loss_giou_dn_5: 0.8248  loss_ce_6: 3.458  loss_mask_6: 1.782  loss_dice_6: 3.386  loss_bbox_6: 3.36  loss_giou_6: 1.783  loss_ce_dn_6: 3.553  loss_mask_dn_6: 1.779  loss_dice_dn_6: 3.327  loss_bbox_dn_6: 1.073  loss_giou_dn_6: 0.829  loss_ce_7: 3.58  loss_mask_7: 1.824  loss_dice_7: 3.425  loss_bbox_7: 3.295  loss_giou_7: 1.713  loss_ce_dn_7: 3.662  loss_mask_dn_7: 1.825  loss_dice_dn_7: 3.35  loss_bbox_dn_7: 1.089  loss_giou_dn_7: 0.8322  loss_ce_8: 3.57  loss_mask_8: 1.87  loss_dice_8: 3.43  loss_bbox_8: 3.355  loss_giou_8: 1.708  loss_ce_dn_8: 3.884  loss_mask_dn_8: 1.894  loss_dice_dn_8: 3.352  loss_bbox_dn_8: 1.094  loss_giou_dn_8: 0.8375    time: 2.2749  last_time: 2.2526  data_time: 0.0124  last_data_time: 0.0141   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:26:08 d2.utils.events]: \u001b[0m eta: 8 days, 9:16:17  iter: 799  total_loss: 238  loss_ce: 3.694  loss_mask: 1.653  loss_dice: 3.048  loss_bbox: 2.731  loss_giou: 1.654  loss_ce_dn: 4.53  loss_mask_dn: 1.688  loss_dice_dn: 3.123  loss_bbox_dn: 1.069  loss_giou_dn: 0.8914  loss_ce_0: 17.27  loss_mask_0: 1.502  loss_dice_0: 3.43  loss_bbox_0: 3.018  loss_giou_0: 1.654  loss_ce_1: 4.217  loss_mask_1: 1.594  loss_dice_1: 3.15  loss_bbox_1: 3.242  loss_giou_1: 1.739  loss_ce_dn_1: 5.289  loss_mask_dn_1: 1.706  loss_dice_dn_1: 3.261  loss_bbox_dn_1: 1.121  loss_giou_dn_1: 0.8324  loss_ce_2: 3.772  loss_mask_2: 1.6  loss_dice_2: 3.111  loss_bbox_2: 2.991  loss_giou_2: 1.678  loss_ce_dn_2: 4.077  loss_mask_dn_2: 1.648  loss_dice_dn_2: 3.171  loss_bbox_dn_2: 1.098  loss_giou_dn_2: 0.8321  loss_ce_3: 3.717  loss_mask_3: 1.76  loss_dice_3: 3.115  loss_bbox_3: 2.864  loss_giou_3: 1.622  loss_ce_dn_3: 3.768  loss_mask_dn_3: 1.639  loss_dice_dn_3: 3.17  loss_bbox_dn_3: 1.067  loss_giou_dn_3: 0.8306  loss_ce_4: 3.514  loss_mask_4: 1.82  loss_dice_4: 3.123  loss_bbox_4: 2.802  loss_giou_4: 1.686  loss_ce_dn_4: 3.663  loss_mask_dn_4: 1.654  loss_dice_dn_4: 3.146  loss_bbox_dn_4: 1.061  loss_giou_dn_4: 0.8357  loss_ce_5: 3.515  loss_mask_5: 1.741  loss_dice_5: 3.029  loss_bbox_5: 2.793  loss_giou_5: 1.677  loss_ce_dn_5: 3.76  loss_mask_dn_5: 1.591  loss_dice_dn_5: 3.124  loss_bbox_dn_5: 1.061  loss_giou_dn_5: 0.8432  loss_ce_6: 3.573  loss_mask_6: 1.688  loss_dice_6: 3.036  loss_bbox_6: 2.784  loss_giou_6: 1.688  loss_ce_dn_6: 3.834  loss_mask_dn_6: 1.577  loss_dice_dn_6: 3.113  loss_bbox_dn_6: 1.059  loss_giou_dn_6: 0.8558  loss_ce_7: 3.633  loss_mask_7: 1.676  loss_dice_7: 3.063  loss_bbox_7: 2.758  loss_giou_7: 1.653  loss_ce_dn_7: 4.137  loss_mask_dn_7: 1.586  loss_dice_dn_7: 3.145  loss_bbox_dn_7: 1.064  loss_giou_dn_7: 0.8657  loss_ce_8: 3.497  loss_mask_8: 1.686  loss_dice_8: 3.034  loss_bbox_8: 2.721  loss_giou_8: 1.644  loss_ce_dn_8: 4.324  loss_mask_dn_8: 1.644  loss_dice_dn_8: 3.13  loss_bbox_dn_8: 1.065  loss_giou_dn_8: 0.8718    time: 2.2748  last_time: 2.2632  data_time: 0.0123  last_data_time: 0.0150   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:26:53 d2.utils.events]: \u001b[0m eta: 8 days, 9:15:32  iter: 819  total_loss: 243.9  loss_ce: 3.479  loss_mask: 1.742  loss_dice: 3.024  loss_bbox: 3.038  loss_giou: 1.685  loss_ce_dn: 4.546  loss_mask_dn: 1.724  loss_dice_dn: 2.997  loss_bbox_dn: 1.145  loss_giou_dn: 0.8551  loss_ce_0: 17.45  loss_mask_0: 1.611  loss_dice_0: 3.162  loss_bbox_0: 3.208  loss_giou_0: 1.615  loss_ce_1: 3.892  loss_mask_1: 1.74  loss_dice_1: 3.004  loss_bbox_1: 3.501  loss_giou_1: 1.707  loss_ce_dn_1: 4.879  loss_mask_dn_1: 1.881  loss_dice_dn_1: 2.975  loss_bbox_dn_1: 1.186  loss_giou_dn_1: 0.8263  loss_ce_2: 3.535  loss_mask_2: 1.774  loss_dice_2: 3.038  loss_bbox_2: 3.209  loss_giou_2: 1.731  loss_ce_dn_2: 4.196  loss_mask_dn_2: 1.811  loss_dice_dn_2: 2.907  loss_bbox_dn_2: 1.151  loss_giou_dn_2: 0.8167  loss_ce_3: 3.505  loss_mask_3: 1.744  loss_dice_3: 3.033  loss_bbox_3: 3.153  loss_giou_3: 1.689  loss_ce_dn_3: 3.947  loss_mask_dn_3: 1.817  loss_dice_dn_3: 2.872  loss_bbox_dn_3: 1.134  loss_giou_dn_3: 0.8069  loss_ce_4: 3.348  loss_mask_4: 1.743  loss_dice_4: 2.999  loss_bbox_4: 3.039  loss_giou_4: 1.632  loss_ce_dn_4: 3.898  loss_mask_dn_4: 1.846  loss_dice_dn_4: 2.876  loss_bbox_dn_4: 1.124  loss_giou_dn_4: 0.8065  loss_ce_5: 3.28  loss_mask_5: 1.748  loss_dice_5: 3.008  loss_bbox_5: 3.008  loss_giou_5: 1.624  loss_ce_dn_5: 3.982  loss_mask_dn_5: 1.8  loss_dice_dn_5: 2.908  loss_bbox_dn_5: 1.13  loss_giou_dn_5: 0.8134  loss_ce_6: 3.277  loss_mask_6: 1.733  loss_dice_6: 3.05  loss_bbox_6: 3.017  loss_giou_6: 1.634  loss_ce_dn_6: 3.965  loss_mask_dn_6: 1.752  loss_dice_dn_6: 2.887  loss_bbox_dn_6: 1.134  loss_giou_dn_6: 0.824  loss_ce_7: 3.318  loss_mask_7: 1.74  loss_dice_7: 3.062  loss_bbox_7: 3.022  loss_giou_7: 1.641  loss_ce_dn_7: 4.111  loss_mask_dn_7: 1.766  loss_dice_dn_7: 2.856  loss_bbox_dn_7: 1.143  loss_giou_dn_7: 0.8348  loss_ce_8: 3.401  loss_mask_8: 1.753  loss_dice_8: 3.06  loss_bbox_8: 2.981  loss_giou_8: 1.646  loss_ce_dn_8: 4.179  loss_mask_dn_8: 1.734  loss_dice_dn_8: 2.891  loss_bbox_dn_8: 1.14  loss_giou_dn_8: 0.8414    time: 2.2749  last_time: 2.2750  data_time: 0.0113  last_data_time: 0.0247   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:27:39 d2.utils.events]: \u001b[0m eta: 8 days, 9:14:35  iter: 839  total_loss: 253.7  loss_ce: 3.888  loss_mask: 1.778  loss_dice: 3.31  loss_bbox: 2.888  loss_giou: 1.778  loss_ce_dn: 4.766  loss_mask_dn: 1.666  loss_dice_dn: 3.329  loss_bbox_dn: 1.06  loss_giou_dn: 0.8584  loss_ce_0: 16.6  loss_mask_0: 1.7  loss_dice_0: 3.583  loss_bbox_0: 3.129  loss_giou_0: 1.856  loss_ce_1: 3.97  loss_mask_1: 1.632  loss_dice_1: 3.231  loss_bbox_1: 3.423  loss_giou_1: 1.855  loss_ce_dn_1: 5.53  loss_mask_dn_1: 1.733  loss_dice_dn_1: 3.411  loss_bbox_dn_1: 1.025  loss_giou_dn_1: 0.8351  loss_ce_2: 3.572  loss_mask_2: 1.755  loss_dice_2: 3.165  loss_bbox_2: 3.049  loss_giou_2: 1.795  loss_ce_dn_2: 4.427  loss_mask_dn_2: 1.707  loss_dice_dn_2: 3.327  loss_bbox_dn_2: 1.026  loss_giou_dn_2: 0.8315  loss_ce_3: 3.641  loss_mask_3: 1.753  loss_dice_3: 3.292  loss_bbox_3: 2.793  loss_giou_3: 1.799  loss_ce_dn_3: 4.179  loss_mask_dn_3: 1.689  loss_dice_dn_3: 3.302  loss_bbox_dn_3: 1.02  loss_giou_dn_3: 0.8274  loss_ce_4: 3.677  loss_mask_4: 1.744  loss_dice_4: 3.189  loss_bbox_4: 2.889  loss_giou_4: 1.822  loss_ce_dn_4: 4.09  loss_mask_dn_4: 1.675  loss_dice_dn_4: 3.265  loss_bbox_dn_4: 1.018  loss_giou_dn_4: 0.8268  loss_ce_5: 3.627  loss_mask_5: 1.733  loss_dice_5: 3.265  loss_bbox_5: 2.891  loss_giou_5: 1.803  loss_ce_dn_5: 4.253  loss_mask_dn_5: 1.628  loss_dice_dn_5: 3.284  loss_bbox_dn_5: 1.017  loss_giou_dn_5: 0.8307  loss_ce_6: 3.659  loss_mask_6: 1.732  loss_dice_6: 3.209  loss_bbox_6: 2.83  loss_giou_6: 1.811  loss_ce_dn_6: 4.334  loss_mask_dn_6: 1.66  loss_dice_dn_6: 3.271  loss_bbox_dn_6: 1.027  loss_giou_dn_6: 0.8419  loss_ce_7: 3.604  loss_mask_7: 1.732  loss_dice_7: 3.349  loss_bbox_7: 2.777  loss_giou_7: 1.781  loss_ce_dn_7: 4.458  loss_mask_dn_7: 1.64  loss_dice_dn_7: 3.314  loss_bbox_dn_7: 1.035  loss_giou_dn_7: 0.8528  loss_ce_8: 3.682  loss_mask_8: 1.779  loss_dice_8: 3.32  loss_bbox_8: 2.801  loss_giou_8: 1.793  loss_ce_dn_8: 4.632  loss_mask_dn_8: 1.635  loss_dice_dn_8: 3.284  loss_bbox_dn_8: 1.044  loss_giou_dn_8: 0.8559    time: 2.2750  last_time: 2.2634  data_time: 0.0122  last_data_time: 0.0097   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:28:24 d2.utils.events]: \u001b[0m eta: 8 days, 9:13:49  iter: 859  total_loss: 253.5  loss_ce: 4.023  loss_mask: 1.909  loss_dice: 3.071  loss_bbox: 2.881  loss_giou: 1.69  loss_ce_dn: 4.391  loss_mask_dn: 1.792  loss_dice_dn: 3.102  loss_bbox_dn: 1.105  loss_giou_dn: 0.8638  loss_ce_0: 17.02  loss_mask_0: 1.746  loss_dice_0: 3.324  loss_bbox_0: 3.262  loss_giou_0: 1.734  loss_ce_1: 3.973  loss_mask_1: 1.952  loss_dice_1: 3.167  loss_bbox_1: 3.662  loss_giou_1: 1.806  loss_ce_dn_1: 4.74  loss_mask_dn_1: 1.824  loss_dice_dn_1: 3.242  loss_bbox_dn_1: 1.155  loss_giou_dn_1: 0.8288  loss_ce_2: 3.699  loss_mask_2: 1.876  loss_dice_2: 3.142  loss_bbox_2: 3.375  loss_giou_2: 1.733  loss_ce_dn_2: 3.72  loss_mask_dn_2: 1.789  loss_dice_dn_2: 3.151  loss_bbox_dn_2: 1.13  loss_giou_dn_2: 0.8191  loss_ce_3: 3.658  loss_mask_3: 1.907  loss_dice_3: 3.178  loss_bbox_3: 3.22  loss_giou_3: 1.739  loss_ce_dn_3: 3.475  loss_mask_dn_3: 1.852  loss_dice_dn_3: 3.091  loss_bbox_dn_3: 1.11  loss_giou_dn_3: 0.8135  loss_ce_4: 3.677  loss_mask_4: 1.924  loss_dice_4: 3.166  loss_bbox_4: 3.107  loss_giou_4: 1.736  loss_ce_dn_4: 3.404  loss_mask_dn_4: 1.889  loss_dice_dn_4: 3.055  loss_bbox_dn_4: 1.101  loss_giou_dn_4: 0.8066  loss_ce_5: 3.767  loss_mask_5: 1.945  loss_dice_5: 3.164  loss_bbox_5: 3.05  loss_giou_5: 1.715  loss_ce_dn_5: 3.401  loss_mask_dn_5: 1.87  loss_dice_dn_5: 3.075  loss_bbox_dn_5: 1.092  loss_giou_dn_5: 0.8044  loss_ce_6: 3.786  loss_mask_6: 1.991  loss_dice_6: 3.147  loss_bbox_6: 2.961  loss_giou_6: 1.727  loss_ce_dn_6: 3.504  loss_mask_dn_6: 1.858  loss_dice_dn_6: 3.144  loss_bbox_dn_6: 1.098  loss_giou_dn_6: 0.8189  loss_ce_7: 3.872  loss_mask_7: 1.953  loss_dice_7: 3.175  loss_bbox_7: 2.958  loss_giou_7: 1.703  loss_ce_dn_7: 3.783  loss_mask_dn_7: 1.794  loss_dice_dn_7: 3.142  loss_bbox_dn_7: 1.105  loss_giou_dn_7: 0.8291  loss_ce_8: 3.775  loss_mask_8: 1.887  loss_dice_8: 3.087  loss_bbox_8: 2.857  loss_giou_8: 1.718  loss_ce_dn_8: 3.932  loss_mask_dn_8: 1.809  loss_dice_dn_8: 3.125  loss_bbox_dn_8: 1.11  loss_giou_dn_8: 0.8472    time: 2.2749  last_time: 2.2983  data_time: 0.0096  last_data_time: 0.0149   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:29:10 d2.utils.events]: \u001b[0m eta: 8 days, 9:12:55  iter: 879  total_loss: 241.2  loss_ce: 3.307  loss_mask: 1.688  loss_dice: 3.168  loss_bbox: 2.909  loss_giou: 1.748  loss_ce_dn: 3.951  loss_mask_dn: 1.546  loss_dice_dn: 2.996  loss_bbox_dn: 1.102  loss_giou_dn: 0.8617  loss_ce_0: 16.83  loss_mask_0: 1.593  loss_dice_0: 3.36  loss_bbox_0: 3.133  loss_giou_0: 1.704  loss_ce_1: 3.985  loss_mask_1: 1.647  loss_dice_1: 3.173  loss_bbox_1: 3.319  loss_giou_1: 1.8  loss_ce_dn_1: 4.778  loss_mask_dn_1: 1.559  loss_dice_dn_1: 3.238  loss_bbox_dn_1: 1.113  loss_giou_dn_1: 0.829  loss_ce_2: 3.665  loss_mask_2: 1.682  loss_dice_2: 3.13  loss_bbox_2: 3.256  loss_giou_2: 1.745  loss_ce_dn_2: 3.805  loss_mask_dn_2: 1.516  loss_dice_dn_2: 3.103  loss_bbox_dn_2: 1.084  loss_giou_dn_2: 0.8273  loss_ce_3: 3.324  loss_mask_3: 1.656  loss_dice_3: 3.111  loss_bbox_3: 3.122  loss_giou_3: 1.743  loss_ce_dn_3: 3.692  loss_mask_dn_3: 1.476  loss_dice_dn_3: 3.101  loss_bbox_dn_3: 1.05  loss_giou_dn_3: 0.8191  loss_ce_4: 3.234  loss_mask_4: 1.671  loss_dice_4: 3.188  loss_bbox_4: 3.003  loss_giou_4: 1.718  loss_ce_dn_4: 3.604  loss_mask_dn_4: 1.498  loss_dice_dn_4: 3.064  loss_bbox_dn_4: 1.044  loss_giou_dn_4: 0.8074  loss_ce_5: 3.29  loss_mask_5: 1.693  loss_dice_5: 3.192  loss_bbox_5: 2.981  loss_giou_5: 1.75  loss_ce_dn_5: 3.572  loss_mask_dn_5: 1.471  loss_dice_dn_5: 3.028  loss_bbox_dn_5: 1.053  loss_giou_dn_5: 0.8096  loss_ce_6: 3.139  loss_mask_6: 1.674  loss_dice_6: 3.132  loss_bbox_6: 2.965  loss_giou_6: 1.766  loss_ce_dn_6: 3.559  loss_mask_dn_6: 1.553  loss_dice_dn_6: 2.983  loss_bbox_dn_6: 1.062  loss_giou_dn_6: 0.8198  loss_ce_7: 3.243  loss_mask_7: 1.68  loss_dice_7: 3.138  loss_bbox_7: 2.927  loss_giou_7: 1.749  loss_ce_dn_7: 3.736  loss_mask_dn_7: 1.54  loss_dice_dn_7: 3.017  loss_bbox_dn_7: 1.073  loss_giou_dn_7: 0.8275  loss_ce_8: 3.264  loss_mask_8: 1.666  loss_dice_8: 3.102  loss_bbox_8: 2.885  loss_giou_8: 1.732  loss_ce_dn_8: 3.829  loss_mask_dn_8: 1.523  loss_dice_dn_8: 2.98  loss_bbox_dn_8: 1.073  loss_giou_dn_8: 0.8414    time: 2.2748  last_time: 2.2402  data_time: 0.0141  last_data_time: 0.0082   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:29:55 d2.utils.events]: \u001b[0m eta: 8 days, 9:12:52  iter: 899  total_loss: 250.8  loss_ce: 3.162  loss_mask: 2.101  loss_dice: 3.006  loss_bbox: 3.189  loss_giou: 1.589  loss_ce_dn: 4.497  loss_mask_dn: 1.928  loss_dice_dn: 3.022  loss_bbox_dn: 1.164  loss_giou_dn: 0.8597  loss_ce_0: 17.33  loss_mask_0: 1.725  loss_dice_0: 3.329  loss_bbox_0: 3.061  loss_giou_0: 1.618  loss_ce_1: 3.785  loss_mask_1: 2.048  loss_dice_1: 3.095  loss_bbox_1: 3.892  loss_giou_1: 1.661  loss_ce_dn_1: 5.044  loss_mask_dn_1: 1.916  loss_dice_dn_1: 3.151  loss_bbox_dn_1: 1.195  loss_giou_dn_1: 0.8201  loss_ce_2: 3.265  loss_mask_2: 2.045  loss_dice_2: 3.01  loss_bbox_2: 3.534  loss_giou_2: 1.626  loss_ce_dn_2: 4.107  loss_mask_dn_2: 1.954  loss_dice_dn_2: 3.051  loss_bbox_dn_2: 1.17  loss_giou_dn_2: 0.814  loss_ce_3: 3.078  loss_mask_3: 2.015  loss_dice_3: 3.079  loss_bbox_3: 3.439  loss_giou_3: 1.6  loss_ce_dn_3: 3.854  loss_mask_dn_3: 1.858  loss_dice_dn_3: 3.051  loss_bbox_dn_3: 1.133  loss_giou_dn_3: 0.8043  loss_ce_4: 3.038  loss_mask_4: 2.061  loss_dice_4: 3.092  loss_bbox_4: 3.199  loss_giou_4: 1.606  loss_ce_dn_4: 3.841  loss_mask_dn_4: 1.939  loss_dice_dn_4: 3.058  loss_bbox_dn_4: 1.132  loss_giou_dn_4: 0.7981  loss_ce_5: 3.013  loss_mask_5: 2.15  loss_dice_5: 3.059  loss_bbox_5: 3.237  loss_giou_5: 1.59  loss_ce_dn_5: 3.795  loss_mask_dn_5: 1.916  loss_dice_dn_5: 3.027  loss_bbox_dn_5: 1.121  loss_giou_dn_5: 0.8021  loss_ce_6: 2.968  loss_mask_6: 2.08  loss_dice_6: 3.055  loss_bbox_6: 3.222  loss_giou_6: 1.586  loss_ce_dn_6: 3.841  loss_mask_dn_6: 1.916  loss_dice_dn_6: 3.024  loss_bbox_dn_6: 1.128  loss_giou_dn_6: 0.817  loss_ce_7: 3.051  loss_mask_7: 2.07  loss_dice_7: 3.079  loss_bbox_7: 3.241  loss_giou_7: 1.579  loss_ce_dn_7: 3.964  loss_mask_dn_7: 1.901  loss_dice_dn_7: 2.976  loss_bbox_dn_7: 1.135  loss_giou_dn_7: 0.8278  loss_ce_8: 3.185  loss_mask_8: 2.111  loss_dice_8: 3.048  loss_bbox_8: 3.272  loss_giou_8: 1.571  loss_ce_dn_8: 4.13  loss_mask_dn_8: 1.944  loss_dice_dn_8: 2.974  loss_bbox_dn_8: 1.145  loss_giou_dn_8: 0.8386    time: 2.2751  last_time: 2.2588  data_time: 0.0124  last_data_time: 0.0176   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:30:41 d2.utils.events]: \u001b[0m eta: 8 days, 9:11:24  iter: 919  total_loss: 245.6  loss_ce: 3.313  loss_mask: 1.696  loss_dice: 3.379  loss_bbox: 3.049  loss_giou: 1.832  loss_ce_dn: 3.871  loss_mask_dn: 1.638  loss_dice_dn: 3.2  loss_bbox_dn: 0.9791  loss_giou_dn: 0.8559  loss_ce_0: 16.76  loss_mask_0: 1.505  loss_dice_0: 3.482  loss_bbox_0: 3.169  loss_giou_0: 1.841  loss_ce_1: 3.726  loss_mask_1: 1.723  loss_dice_1: 3.343  loss_bbox_1: 3.413  loss_giou_1: 1.943  loss_ce_dn_1: 4.557  loss_mask_dn_1: 1.663  loss_dice_dn_1: 3.294  loss_bbox_dn_1: 1.049  loss_giou_dn_1: 0.8404  loss_ce_2: 3.236  loss_mask_2: 1.757  loss_dice_2: 3.302  loss_bbox_2: 3.34  loss_giou_2: 1.935  loss_ce_dn_2: 3.902  loss_mask_dn_2: 1.695  loss_dice_dn_2: 3.22  loss_bbox_dn_2: 1.025  loss_giou_dn_2: 0.8378  loss_ce_3: 3.035  loss_mask_3: 1.686  loss_dice_3: 3.343  loss_bbox_3: 3.295  loss_giou_3: 1.921  loss_ce_dn_3: 3.678  loss_mask_dn_3: 1.672  loss_dice_dn_3: 3.213  loss_bbox_dn_3: 0.9909  loss_giou_dn_3: 0.8238  loss_ce_4: 3.155  loss_mask_4: 1.736  loss_dice_4: 3.343  loss_bbox_4: 3.143  loss_giou_4: 1.916  loss_ce_dn_4: 3.497  loss_mask_dn_4: 1.702  loss_dice_dn_4: 3.214  loss_bbox_dn_4: 0.97  loss_giou_dn_4: 0.8232  loss_ce_5: 2.995  loss_mask_5: 1.791  loss_dice_5: 3.362  loss_bbox_5: 3.165  loss_giou_5: 1.916  loss_ce_dn_5: 3.459  loss_mask_dn_5: 1.621  loss_dice_dn_5: 3.249  loss_bbox_dn_5: 0.9643  loss_giou_dn_5: 0.8265  loss_ce_6: 3.137  loss_mask_6: 1.768  loss_dice_6: 3.422  loss_bbox_6: 3.106  loss_giou_6: 1.9  loss_ce_dn_6: 3.594  loss_mask_dn_6: 1.623  loss_dice_dn_6: 3.237  loss_bbox_dn_6: 0.9629  loss_giou_dn_6: 0.8299  loss_ce_7: 3.097  loss_mask_7: 1.727  loss_dice_7: 3.363  loss_bbox_7: 3.106  loss_giou_7: 1.888  loss_ce_dn_7: 3.615  loss_mask_dn_7: 1.699  loss_dice_dn_7: 3.267  loss_bbox_dn_7: 0.96  loss_giou_dn_7: 0.8412  loss_ce_8: 3.159  loss_mask_8: 1.668  loss_dice_8: 3.381  loss_bbox_8: 3.102  loss_giou_8: 1.847  loss_ce_dn_8: 3.7  loss_mask_dn_8: 1.693  loss_dice_dn_8: 3.2  loss_bbox_dn_8: 0.9691  loss_giou_dn_8: 0.8546    time: 2.2748  last_time: 2.2304  data_time: 0.0116  last_data_time: 0.0058   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:31:26 d2.utils.events]: \u001b[0m eta: 8 days, 9:10:39  iter: 939  total_loss: 258  loss_ce: 3.999  loss_mask: 1.973  loss_dice: 3.097  loss_bbox: 3.132  loss_giou: 1.873  loss_ce_dn: 4.475  loss_mask_dn: 1.875  loss_dice_dn: 3.176  loss_bbox_dn: 1.096  loss_giou_dn: 0.8781  loss_ce_0: 16.67  loss_mask_0: 1.711  loss_dice_0: 3.623  loss_bbox_0: 3.271  loss_giou_0: 1.814  loss_ce_1: 4.206  loss_mask_1: 1.893  loss_dice_1: 3.197  loss_bbox_1: 3.592  loss_giou_1: 1.86  loss_ce_dn_1: 4.817  loss_mask_dn_1: 1.857  loss_dice_dn_1: 3.376  loss_bbox_dn_1: 1.163  loss_giou_dn_1: 0.8317  loss_ce_2: 3.98  loss_mask_2: 1.994  loss_dice_2: 3.118  loss_bbox_2: 3.374  loss_giou_2: 1.858  loss_ce_dn_2: 4.004  loss_mask_dn_2: 1.752  loss_dice_dn_2: 3.309  loss_bbox_dn_2: 1.129  loss_giou_dn_2: 0.8219  loss_ce_3: 3.74  loss_mask_3: 1.974  loss_dice_3: 3.121  loss_bbox_3: 3.225  loss_giou_3: 1.86  loss_ce_dn_3: 3.794  loss_mask_dn_3: 1.733  loss_dice_dn_3: 3.241  loss_bbox_dn_3: 1.097  loss_giou_dn_3: 0.8158  loss_ce_4: 3.634  loss_mask_4: 1.962  loss_dice_4: 3.167  loss_bbox_4: 3.247  loss_giou_4: 1.846  loss_ce_dn_4: 3.755  loss_mask_dn_4: 1.809  loss_dice_dn_4: 3.195  loss_bbox_dn_4: 1.076  loss_giou_dn_4: 0.8166  loss_ce_5: 3.901  loss_mask_5: 1.95  loss_dice_5: 3.084  loss_bbox_5: 3.186  loss_giou_5: 1.857  loss_ce_dn_5: 3.757  loss_mask_dn_5: 1.777  loss_dice_dn_5: 3.198  loss_bbox_dn_5: 1.067  loss_giou_dn_5: 0.8226  loss_ce_6: 3.809  loss_mask_6: 2.004  loss_dice_6: 3.082  loss_bbox_6: 3.198  loss_giou_6: 1.865  loss_ce_dn_6: 3.757  loss_mask_dn_6: 1.838  loss_dice_dn_6: 3.176  loss_bbox_dn_6: 1.07  loss_giou_dn_6: 0.8353  loss_ce_7: 3.765  loss_mask_7: 1.995  loss_dice_7: 3.086  loss_bbox_7: 3.171  loss_giou_7: 1.817  loss_ce_dn_7: 3.853  loss_mask_dn_7: 1.792  loss_dice_dn_7: 3.173  loss_bbox_dn_7: 1.071  loss_giou_dn_7: 0.8453  loss_ce_8: 3.914  loss_mask_8: 1.937  loss_dice_8: 3.101  loss_bbox_8: 3.109  loss_giou_8: 1.86  loss_ce_dn_8: 4.072  loss_mask_dn_8: 1.843  loss_dice_dn_8: 3.165  loss_bbox_dn_8: 1.084  loss_giou_dn_8: 0.86    time: 2.2749  last_time: 2.3019  data_time: 0.0143  last_data_time: 0.0163   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:32:12 d2.utils.events]: \u001b[0m eta: 8 days, 9:09:36  iter: 959  total_loss: 242.2  loss_ce: 3.383  loss_mask: 1.568  loss_dice: 3.239  loss_bbox: 3.062  loss_giou: 1.806  loss_ce_dn: 4.006  loss_mask_dn: 1.538  loss_dice_dn: 3.127  loss_bbox_dn: 1.034  loss_giou_dn: 0.842  loss_ce_0: 16.54  loss_mask_0: 1.507  loss_dice_0: 3.469  loss_bbox_0: 3.404  loss_giou_0: 1.744  loss_ce_1: 4.009  loss_mask_1: 1.605  loss_dice_1: 3.267  loss_bbox_1: 3.406  loss_giou_1: 1.784  loss_ce_dn_1: 4.663  loss_mask_dn_1: 1.598  loss_dice_dn_1: 3.269  loss_bbox_dn_1: 1.093  loss_giou_dn_1: 0.8255  loss_ce_2: 3.594  loss_mask_2: 1.547  loss_dice_2: 3.221  loss_bbox_2: 3.327  loss_giou_2: 1.737  loss_ce_dn_2: 3.664  loss_mask_dn_2: 1.583  loss_dice_dn_2: 3.195  loss_bbox_dn_2: 1.059  loss_giou_dn_2: 0.8106  loss_ce_3: 3.307  loss_mask_3: 1.506  loss_dice_3: 3.164  loss_bbox_3: 3.157  loss_giou_3: 1.757  loss_ce_dn_3: 3.434  loss_mask_dn_3: 1.629  loss_dice_dn_3: 3.209  loss_bbox_dn_3: 1.022  loss_giou_dn_3: 0.8022  loss_ce_4: 3.33  loss_mask_4: 1.48  loss_dice_4: 3.208  loss_bbox_4: 3.073  loss_giou_4: 1.764  loss_ce_dn_4: 3.288  loss_mask_dn_4: 1.569  loss_dice_dn_4: 3.189  loss_bbox_dn_4: 0.9898  loss_giou_dn_4: 0.7993  loss_ce_5: 3.318  loss_mask_5: 1.543  loss_dice_5: 3.164  loss_bbox_5: 3.044  loss_giou_5: 1.754  loss_ce_dn_5: 3.295  loss_mask_dn_5: 1.524  loss_dice_dn_5: 3.172  loss_bbox_dn_5: 0.9796  loss_giou_dn_5: 0.8014  loss_ce_6: 3.293  loss_mask_6: 1.561  loss_dice_6: 3.166  loss_bbox_6: 3.047  loss_giou_6: 1.77  loss_ce_dn_6: 3.356  loss_mask_dn_6: 1.545  loss_dice_dn_6: 3.116  loss_bbox_dn_6: 0.9873  loss_giou_dn_6: 0.8083  loss_ce_7: 3.445  loss_mask_7: 1.553  loss_dice_7: 3.213  loss_bbox_7: 3.043  loss_giou_7: 1.784  loss_ce_dn_7: 3.47  loss_mask_dn_7: 1.522  loss_dice_dn_7: 3.119  loss_bbox_dn_7: 0.9897  loss_giou_dn_7: 0.8157  loss_ce_8: 3.515  loss_mask_8: 1.536  loss_dice_8: 3.214  loss_bbox_8: 3.043  loss_giou_8: 1.804  loss_ce_dn_8: 3.728  loss_mask_dn_8: 1.532  loss_dice_dn_8: 3.111  loss_bbox_dn_8: 1.002  loss_giou_dn_8: 0.8265    time: 2.2752  last_time: 2.2713  data_time: 0.0143  last_data_time: 0.0143   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:32:58 d2.utils.events]: \u001b[0m eta: 8 days, 9:08:12  iter: 979  total_loss: 238.7  loss_ce: 3.558  loss_mask: 1.667  loss_dice: 3.219  loss_bbox: 2.739  loss_giou: 1.841  loss_ce_dn: 4.66  loss_mask_dn: 1.471  loss_dice_dn: 3.178  loss_bbox_dn: 0.9711  loss_giou_dn: 0.8508  loss_ce_0: 16.51  loss_mask_0: 1.485  loss_dice_0: 3.351  loss_bbox_0: 3.329  loss_giou_0: 1.789  loss_ce_1: 3.673  loss_mask_1: 1.583  loss_dice_1: 3.174  loss_bbox_1: 3.497  loss_giou_1: 1.839  loss_ce_dn_1: 5.063  loss_mask_dn_1: 1.58  loss_dice_dn_1: 3.287  loss_bbox_dn_1: 1.05  loss_giou_dn_1: 0.8306  loss_ce_2: 3.551  loss_mask_2: 1.647  loss_dice_2: 3.155  loss_bbox_2: 3.18  loss_giou_2: 1.797  loss_ce_dn_2: 4.018  loss_mask_dn_2: 1.618  loss_dice_dn_2: 3.197  loss_bbox_dn_2: 1.044  loss_giou_dn_2: 0.824  loss_ce_3: 3.347  loss_mask_3: 1.597  loss_dice_3: 3.125  loss_bbox_3: 2.959  loss_giou_3: 1.83  loss_ce_dn_3: 3.746  loss_mask_dn_3: 1.583  loss_dice_dn_3: 3.152  loss_bbox_dn_3: 1.01  loss_giou_dn_3: 0.8109  loss_ce_4: 3.365  loss_mask_4: 1.576  loss_dice_4: 3.163  loss_bbox_4: 2.714  loss_giou_4: 1.805  loss_ce_dn_4: 3.708  loss_mask_dn_4: 1.525  loss_dice_dn_4: 3.16  loss_bbox_dn_4: 0.9768  loss_giou_dn_4: 0.8052  loss_ce_5: 3.503  loss_mask_5: 1.653  loss_dice_5: 3.123  loss_bbox_5: 2.761  loss_giou_5: 1.823  loss_ce_dn_5: 3.88  loss_mask_dn_5: 1.51  loss_dice_dn_5: 3.152  loss_bbox_dn_5: 0.965  loss_giou_dn_5: 0.8013  loss_ce_6: 3.389  loss_mask_6: 1.61  loss_dice_6: 3.172  loss_bbox_6: 2.699  loss_giou_6: 1.846  loss_ce_dn_6: 3.965  loss_mask_dn_6: 1.504  loss_dice_dn_6: 3.142  loss_bbox_dn_6: 0.9631  loss_giou_dn_6: 0.8143  loss_ce_7: 3.435  loss_mask_7: 1.662  loss_dice_7: 3.21  loss_bbox_7: 2.711  loss_giou_7: 1.847  loss_ce_dn_7: 4.109  loss_mask_dn_7: 1.491  loss_dice_dn_7: 3.158  loss_bbox_dn_7: 0.9636  loss_giou_dn_7: 0.8233  loss_ce_8: 3.494  loss_mask_8: 1.688  loss_dice_8: 3.177  loss_bbox_8: 2.732  loss_giou_8: 1.816  loss_ce_dn_8: 4.242  loss_mask_dn_8: 1.492  loss_dice_dn_8: 3.153  loss_bbox_dn_8: 0.9615  loss_giou_dn_8: 0.832    time: 2.2753  last_time: 2.2503  data_time: 0.0139  last_data_time: 0.0070   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:33:43 d2.utils.events]: \u001b[0m eta: 8 days, 9:06:24  iter: 999  total_loss: 235.3  loss_ce: 3.291  loss_mask: 1.588  loss_dice: 3.02  loss_bbox: 2.621  loss_giou: 1.654  loss_ce_dn: 3.898  loss_mask_dn: 1.555  loss_dice_dn: 2.933  loss_bbox_dn: 1.108  loss_giou_dn: 0.834  loss_ce_0: 16.43  loss_mask_0: 1.488  loss_dice_0: 3.281  loss_bbox_0: 3.051  loss_giou_0: 1.721  loss_ce_1: 3.583  loss_mask_1: 1.492  loss_dice_1: 3.023  loss_bbox_1: 3.387  loss_giou_1: 1.759  loss_ce_dn_1: 4.322  loss_mask_dn_1: 1.619  loss_dice_dn_1: 3.112  loss_bbox_dn_1: 1.175  loss_giou_dn_1: 0.8217  loss_ce_2: 3.001  loss_mask_2: 1.529  loss_dice_2: 3  loss_bbox_2: 3.035  loss_giou_2: 1.777  loss_ce_dn_2: 3.441  loss_mask_dn_2: 1.568  loss_dice_dn_2: 3.013  loss_bbox_dn_2: 1.133  loss_giou_dn_2: 0.8164  loss_ce_3: 2.836  loss_mask_3: 1.524  loss_dice_3: 2.978  loss_bbox_3: 2.806  loss_giou_3: 1.764  loss_ce_dn_3: 3.274  loss_mask_dn_3: 1.571  loss_dice_dn_3: 2.995  loss_bbox_dn_3: 1.092  loss_giou_dn_3: 0.8017  loss_ce_4: 2.867  loss_mask_4: 1.589  loss_dice_4: 3.002  loss_bbox_4: 2.82  loss_giou_4: 1.694  loss_ce_dn_4: 3.313  loss_mask_dn_4: 1.592  loss_dice_dn_4: 3.009  loss_bbox_dn_4: 1.064  loss_giou_dn_4: 0.7964  loss_ce_5: 3.072  loss_mask_5: 1.549  loss_dice_5: 2.983  loss_bbox_5: 2.758  loss_giou_5: 1.656  loss_ce_dn_5: 3.389  loss_mask_dn_5: 1.585  loss_dice_dn_5: 2.995  loss_bbox_dn_5: 1.059  loss_giou_dn_5: 0.7984  loss_ce_6: 3.003  loss_mask_6: 1.55  loss_dice_6: 2.949  loss_bbox_6: 2.722  loss_giou_6: 1.641  loss_ce_dn_6: 3.523  loss_mask_dn_6: 1.597  loss_dice_dn_6: 2.967  loss_bbox_dn_6: 1.071  loss_giou_dn_6: 0.8026  loss_ce_7: 3.079  loss_mask_7: 1.567  loss_dice_7: 2.941  loss_bbox_7: 2.741  loss_giou_7: 1.649  loss_ce_dn_7: 3.606  loss_mask_dn_7: 1.587  loss_dice_dn_7: 2.972  loss_bbox_dn_7: 1.081  loss_giou_dn_7: 0.8064  loss_ce_8: 3.187  loss_mask_8: 1.567  loss_dice_8: 2.985  loss_bbox_8: 2.743  loss_giou_8: 1.651  loss_ce_dn_8: 3.705  loss_mask_dn_8: 1.585  loss_dice_dn_8: 2.932  loss_bbox_dn_8: 1.09  loss_giou_dn_8: 0.8178    time: 2.2753  last_time: 2.3127  data_time: 0.0127  last_data_time: 0.0068   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:34:29 d2.utils.events]: \u001b[0m eta: 8 days, 9:07:43  iter: 1019  total_loss: 225.4  loss_ce: 3.14  loss_mask: 1.821  loss_dice: 2.907  loss_bbox: 2.711  loss_giou: 1.572  loss_ce_dn: 3.539  loss_mask_dn: 1.686  loss_dice_dn: 2.882  loss_bbox_dn: 1.086  loss_giou_dn: 0.816  loss_ce_0: 16.52  loss_mask_0: 1.934  loss_dice_0: 3.273  loss_bbox_0: 3.014  loss_giou_0: 1.624  loss_ce_1: 3.23  loss_mask_1: 1.875  loss_dice_1: 3.001  loss_bbox_1: 3.122  loss_giou_1: 1.647  loss_ce_dn_1: 4.054  loss_mask_dn_1: 1.823  loss_dice_dn_1: 3.069  loss_bbox_dn_1: 1.135  loss_giou_dn_1: 0.8316  loss_ce_2: 2.777  loss_mask_2: 1.854  loss_dice_2: 2.978  loss_bbox_2: 2.81  loss_giou_2: 1.613  loss_ce_dn_2: 3.381  loss_mask_dn_2: 1.713  loss_dice_dn_2: 2.978  loss_bbox_dn_2: 1.126  loss_giou_dn_2: 0.8164  loss_ce_3: 2.787  loss_mask_3: 1.833  loss_dice_3: 2.939  loss_bbox_3: 2.782  loss_giou_3: 1.588  loss_ce_dn_3: 3.229  loss_mask_dn_3: 1.715  loss_dice_dn_3: 2.953  loss_bbox_dn_3: 1.104  loss_giou_dn_3: 0.7968  loss_ce_4: 2.723  loss_mask_4: 1.817  loss_dice_4: 2.916  loss_bbox_4: 2.743  loss_giou_4: 1.602  loss_ce_dn_4: 3.216  loss_mask_dn_4: 1.705  loss_dice_dn_4: 2.932  loss_bbox_dn_4: 1.09  loss_giou_dn_4: 0.7882  loss_ce_5: 2.715  loss_mask_5: 1.839  loss_dice_5: 2.929  loss_bbox_5: 2.7  loss_giou_5: 1.598  loss_ce_dn_5: 3.128  loss_mask_dn_5: 1.688  loss_dice_dn_5: 2.922  loss_bbox_dn_5: 1.08  loss_giou_dn_5: 0.7869  loss_ce_6: 2.762  loss_mask_6: 1.848  loss_dice_6: 2.906  loss_bbox_6: 2.713  loss_giou_6: 1.594  loss_ce_dn_6: 3.165  loss_mask_dn_6: 1.667  loss_dice_dn_6: 2.918  loss_bbox_dn_6: 1.07  loss_giou_dn_6: 0.7944  loss_ce_7: 2.89  loss_mask_7: 1.874  loss_dice_7: 2.905  loss_bbox_7: 2.712  loss_giou_7: 1.551  loss_ce_dn_7: 3.235  loss_mask_dn_7: 1.695  loss_dice_dn_7: 2.893  loss_bbox_dn_7: 1.059  loss_giou_dn_7: 0.7966  loss_ce_8: 3.035  loss_mask_8: 1.859  loss_dice_8: 2.915  loss_bbox_8: 2.702  loss_giou_8: 1.56  loss_ce_dn_8: 3.393  loss_mask_dn_8: 1.686  loss_dice_dn_8: 2.878  loss_bbox_dn_8: 1.069  loss_giou_dn_8: 0.8017    time: 2.2752  last_time: 2.2668  data_time: 0.0116  last_data_time: 0.0045   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:35:15 d2.utils.events]: \u001b[0m eta: 8 days, 9:07:34  iter: 1039  total_loss: 226.4  loss_ce: 2.547  loss_mask: 1.916  loss_dice: 2.832  loss_bbox: 2.912  loss_giou: 1.534  loss_ce_dn: 3.27  loss_mask_dn: 1.808  loss_dice_dn: 2.697  loss_bbox_dn: 1.084  loss_giou_dn: 0.8278  loss_ce_0: 16.3  loss_mask_0: 1.816  loss_dice_0: 3.191  loss_bbox_0: 3.121  loss_giou_0: 1.761  loss_ce_1: 2.697  loss_mask_1: 1.846  loss_dice_1: 2.841  loss_bbox_1: 3.457  loss_giou_1: 1.699  loss_ce_dn_1: 3.996  loss_mask_dn_1: 1.819  loss_dice_dn_1: 2.803  loss_bbox_dn_1: 1.212  loss_giou_dn_1: 0.8223  loss_ce_2: 2.37  loss_mask_2: 1.81  loss_dice_2: 2.807  loss_bbox_2: 3.215  loss_giou_2: 1.64  loss_ce_dn_2: 3.363  loss_mask_dn_2: 1.785  loss_dice_dn_2: 2.811  loss_bbox_dn_2: 1.174  loss_giou_dn_2: 0.8131  loss_ce_3: 2.362  loss_mask_3: 1.857  loss_dice_3: 2.768  loss_bbox_3: 3.011  loss_giou_3: 1.564  loss_ce_dn_3: 3.206  loss_mask_dn_3: 1.744  loss_dice_dn_3: 2.825  loss_bbox_dn_3: 1.134  loss_giou_dn_3: 0.8014  loss_ce_4: 2.503  loss_mask_4: 1.852  loss_dice_4: 2.759  loss_bbox_4: 2.962  loss_giou_4: 1.532  loss_ce_dn_4: 3.144  loss_mask_dn_4: 1.776  loss_dice_dn_4: 2.77  loss_bbox_dn_4: 1.098  loss_giou_dn_4: 0.7985  loss_ce_5: 2.571  loss_mask_5: 1.875  loss_dice_5: 2.763  loss_bbox_5: 3.021  loss_giou_5: 1.534  loss_ce_dn_5: 3.099  loss_mask_dn_5: 1.672  loss_dice_dn_5: 2.707  loss_bbox_dn_5: 1.084  loss_giou_dn_5: 0.8003  loss_ce_6: 2.555  loss_mask_6: 1.896  loss_dice_6: 2.761  loss_bbox_6: 3.028  loss_giou_6: 1.497  loss_ce_dn_6: 3.093  loss_mask_dn_6: 1.683  loss_dice_dn_6: 2.697  loss_bbox_dn_6: 1.081  loss_giou_dn_6: 0.8061  loss_ce_7: 2.536  loss_mask_7: 1.88  loss_dice_7: 2.762  loss_bbox_7: 2.955  loss_giou_7: 1.508  loss_ce_dn_7: 3.049  loss_mask_dn_7: 1.69  loss_dice_dn_7: 2.694  loss_bbox_dn_7: 1.072  loss_giou_dn_7: 0.8125  loss_ce_8: 2.461  loss_mask_8: 1.862  loss_dice_8: 2.818  loss_bbox_8: 2.923  loss_giou_8: 1.533  loss_ce_dn_8: 3.042  loss_mask_dn_8: 1.703  loss_dice_dn_8: 2.707  loss_bbox_dn_8: 1.074  loss_giou_dn_8: 0.8176    time: 2.2754  last_time: 2.2776  data_time: 0.0133  last_data_time: 0.0078   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:36:00 d2.utils.events]: \u001b[0m eta: 8 days, 9:06:15  iter: 1059  total_loss: 238.9  loss_ce: 3.315  loss_mask: 1.844  loss_dice: 3.291  loss_bbox: 2.768  loss_giou: 1.653  loss_ce_dn: 4.064  loss_mask_dn: 1.696  loss_dice_dn: 3.245  loss_bbox_dn: 1.068  loss_giou_dn: 0.818  loss_ce_0: 16.24  loss_mask_0: 1.652  loss_dice_0: 3.559  loss_bbox_0: 3.196  loss_giou_0: 1.767  loss_ce_1: 3.763  loss_mask_1: 1.824  loss_dice_1: 3.366  loss_bbox_1: 3.353  loss_giou_1: 1.87  loss_ce_dn_1: 4.785  loss_mask_dn_1: 1.618  loss_dice_dn_1: 3.37  loss_bbox_dn_1: 1.113  loss_giou_dn_1: 0.8311  loss_ce_2: 3.379  loss_mask_2: 1.819  loss_dice_2: 3.36  loss_bbox_2: 3.091  loss_giou_2: 1.817  loss_ce_dn_2: 3.967  loss_mask_dn_2: 1.61  loss_dice_dn_2: 3.303  loss_bbox_dn_2: 1.072  loss_giou_dn_2: 0.8184  loss_ce_3: 3.292  loss_mask_3: 1.851  loss_dice_3: 3.329  loss_bbox_3: 2.92  loss_giou_3: 1.796  loss_ce_dn_3: 3.819  loss_mask_dn_3: 1.618  loss_dice_dn_3: 3.25  loss_bbox_dn_3: 1.03  loss_giou_dn_3: 0.8021  loss_ce_4: 3.125  loss_mask_4: 1.916  loss_dice_4: 3.33  loss_bbox_4: 2.805  loss_giou_4: 1.739  loss_ce_dn_4: 3.749  loss_mask_dn_4: 1.63  loss_dice_dn_4: 3.256  loss_bbox_dn_4: 1.013  loss_giou_dn_4: 0.7902  loss_ce_5: 3.09  loss_mask_5: 1.873  loss_dice_5: 3.33  loss_bbox_5: 2.76  loss_giou_5: 1.691  loss_ce_dn_5: 3.774  loss_mask_dn_5: 1.635  loss_dice_dn_5: 3.252  loss_bbox_dn_5: 1.015  loss_giou_dn_5: 0.7873  loss_ce_6: 3.158  loss_mask_6: 1.909  loss_dice_6: 3.315  loss_bbox_6: 2.724  loss_giou_6: 1.64  loss_ce_dn_6: 3.804  loss_mask_dn_6: 1.632  loss_dice_dn_6: 3.24  loss_bbox_dn_6: 1.027  loss_giou_dn_6: 0.7925  loss_ce_7: 3.067  loss_mask_7: 1.844  loss_dice_7: 3.302  loss_bbox_7: 2.704  loss_giou_7: 1.664  loss_ce_dn_7: 3.774  loss_mask_dn_7: 1.614  loss_dice_dn_7: 3.237  loss_bbox_dn_7: 1.041  loss_giou_dn_7: 0.7968  loss_ce_8: 3.226  loss_mask_8: 1.853  loss_dice_8: 3.31  loss_bbox_8: 2.766  loss_giou_8: 1.656  loss_ce_dn_8: 3.837  loss_mask_dn_8: 1.642  loss_dice_dn_8: 3.239  loss_bbox_dn_8: 1.06  loss_giou_dn_8: 0.8012    time: 2.2753  last_time: 2.2289  data_time: 0.0138  last_data_time: 0.0072   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:36:45 d2.utils.events]: \u001b[0m eta: 8 days, 9:04:04  iter: 1079  total_loss: 233  loss_ce: 2.599  loss_mask: 1.898  loss_dice: 3.025  loss_bbox: 2.952  loss_giou: 1.833  loss_ce_dn: 3.569  loss_mask_dn: 1.811  loss_dice_dn: 2.924  loss_bbox_dn: 1.016  loss_giou_dn: 0.8159  loss_ce_0: 16.17  loss_mask_0: 1.709  loss_dice_0: 3.32  loss_bbox_0: 3.34  loss_giou_0: 1.798  loss_ce_1: 2.791  loss_mask_1: 1.847  loss_dice_1: 3.053  loss_bbox_1: 3.585  loss_giou_1: 1.807  loss_ce_dn_1: 4.29  loss_mask_dn_1: 1.804  loss_dice_dn_1: 3.1  loss_bbox_dn_1: 1.102  loss_giou_dn_1: 0.8157  loss_ce_2: 2.71  loss_mask_2: 1.886  loss_dice_2: 3.066  loss_bbox_2: 3.29  loss_giou_2: 1.725  loss_ce_dn_2: 3.426  loss_mask_dn_2: 1.767  loss_dice_dn_2: 3.019  loss_bbox_dn_2: 1.056  loss_giou_dn_2: 0.8058  loss_ce_3: 2.435  loss_mask_3: 1.955  loss_dice_3: 3.003  loss_bbox_3: 3.084  loss_giou_3: 1.71  loss_ce_dn_3: 3.254  loss_mask_dn_3: 1.802  loss_dice_dn_3: 2.963  loss_bbox_dn_3: 1.023  loss_giou_dn_3: 0.7987  loss_ce_4: 2.567  loss_mask_4: 2.023  loss_dice_4: 2.991  loss_bbox_4: 3.035  loss_giou_4: 1.775  loss_ce_dn_4: 3.199  loss_mask_dn_4: 1.8  loss_dice_dn_4: 2.901  loss_bbox_dn_4: 1.012  loss_giou_dn_4: 0.7936  loss_ce_5: 2.552  loss_mask_5: 1.952  loss_dice_5: 3.021  loss_bbox_5: 2.994  loss_giou_5: 1.793  loss_ce_dn_5: 3.155  loss_mask_dn_5: 1.802  loss_dice_dn_5: 2.882  loss_bbox_dn_5: 1.004  loss_giou_dn_5: 0.7889  loss_ce_6: 2.593  loss_mask_6: 1.98  loss_dice_6: 3.032  loss_bbox_6: 3.024  loss_giou_6: 1.806  loss_ce_dn_6: 3.206  loss_mask_dn_6: 1.815  loss_dice_dn_6: 2.902  loss_bbox_dn_6: 1.004  loss_giou_dn_6: 0.7916  loss_ce_7: 2.554  loss_mask_7: 1.948  loss_dice_7: 3.05  loss_bbox_7: 2.971  loss_giou_7: 1.8  loss_ce_dn_7: 3.292  loss_mask_dn_7: 1.797  loss_dice_dn_7: 2.89  loss_bbox_dn_7: 1.001  loss_giou_dn_7: 0.7937  loss_ce_8: 2.549  loss_mask_8: 1.892  loss_dice_8: 3.047  loss_bbox_8: 2.963  loss_giou_8: 1.82  loss_ce_dn_8: 3.44  loss_mask_dn_8: 1.799  loss_dice_dn_8: 2.88  loss_bbox_dn_8: 1.008  loss_giou_dn_8: 0.8004    time: 2.2752  last_time: 2.2600  data_time: 0.0130  last_data_time: 0.0206   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:37:31 d2.utils.events]: \u001b[0m eta: 8 days, 9:02:25  iter: 1099  total_loss: 229  loss_ce: 3.234  loss_mask: 1.714  loss_dice: 3.068  loss_bbox: 2.956  loss_giou: 1.723  loss_ce_dn: 3.918  loss_mask_dn: 1.507  loss_dice_dn: 2.851  loss_bbox_dn: 1.037  loss_giou_dn: 0.8457  loss_ce_0: 16.16  loss_mask_0: 1.581  loss_dice_0: 3.35  loss_bbox_0: 3.465  loss_giou_0: 1.793  loss_ce_1: 3.613  loss_mask_1: 1.604  loss_dice_1: 3.132  loss_bbox_1: 3.561  loss_giou_1: 1.807  loss_ce_dn_1: 4.599  loss_mask_dn_1: 1.583  loss_dice_dn_1: 3.105  loss_bbox_dn_1: 1.048  loss_giou_dn_1: 0.8343  loss_ce_2: 3.172  loss_mask_2: 1.692  loss_dice_2: 3.078  loss_bbox_2: 3.127  loss_giou_2: 1.716  loss_ce_dn_2: 3.772  loss_mask_dn_2: 1.528  loss_dice_dn_2: 2.994  loss_bbox_dn_2: 1.031  loss_giou_dn_2: 0.8145  loss_ce_3: 3.155  loss_mask_3: 1.711  loss_dice_3: 3.061  loss_bbox_3: 2.99  loss_giou_3: 1.721  loss_ce_dn_3: 3.619  loss_mask_dn_3: 1.489  loss_dice_dn_3: 2.974  loss_bbox_dn_3: 1.009  loss_giou_dn_3: 0.7968  loss_ce_4: 3.123  loss_mask_4: 1.763  loss_dice_4: 3.026  loss_bbox_4: 2.98  loss_giou_4: 1.705  loss_ce_dn_4: 3.526  loss_mask_dn_4: 1.49  loss_dice_dn_4: 2.918  loss_bbox_dn_4: 0.9931  loss_giou_dn_4: 0.7925  loss_ce_5: 3.283  loss_mask_5: 1.693  loss_dice_5: 3.042  loss_bbox_5: 2.947  loss_giou_5: 1.695  loss_ce_dn_5: 3.556  loss_mask_dn_5: 1.484  loss_dice_dn_5: 2.911  loss_bbox_dn_5: 0.9849  loss_giou_dn_5: 0.8047  loss_ce_6: 3.397  loss_mask_6: 1.618  loss_dice_6: 3.075  loss_bbox_6: 2.939  loss_giou_6: 1.714  loss_ce_dn_6: 3.588  loss_mask_dn_6: 1.481  loss_dice_dn_6: 2.867  loss_bbox_dn_6: 0.9913  loss_giou_dn_6: 0.815  loss_ce_7: 3.248  loss_mask_7: 1.712  loss_dice_7: 3.094  loss_bbox_7: 2.879  loss_giou_7: 1.691  loss_ce_dn_7: 3.632  loss_mask_dn_7: 1.444  loss_dice_dn_7: 2.882  loss_bbox_dn_7: 0.9959  loss_giou_dn_7: 0.8149  loss_ce_8: 3.185  loss_mask_8: 1.748  loss_dice_8: 3.097  loss_bbox_8: 2.884  loss_giou_8: 1.711  loss_ce_dn_8: 3.804  loss_mask_dn_8: 1.463  loss_dice_dn_8: 2.859  loss_bbox_dn_8: 1.011  loss_giou_dn_8: 0.8262    time: 2.2753  last_time: 2.2427  data_time: 0.0129  last_data_time: 0.0196   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:38:17 d2.utils.events]: \u001b[0m eta: 8 days, 9:02:04  iter: 1119  total_loss: 228.2  loss_ce: 3.111  loss_mask: 1.95  loss_dice: 2.876  loss_bbox: 2.947  loss_giou: 1.545  loss_ce_dn: 3.901  loss_mask_dn: 1.727  loss_dice_dn: 2.783  loss_bbox_dn: 1.058  loss_giou_dn: 0.8037  loss_ce_0: 16.1  loss_mask_0: 1.757  loss_dice_0: 3.182  loss_bbox_0: 3.345  loss_giou_0: 1.835  loss_ce_1: 3.483  loss_mask_1: 1.793  loss_dice_1: 2.885  loss_bbox_1: 3.434  loss_giou_1: 1.764  loss_ce_dn_1: 4.125  loss_mask_dn_1: 1.796  loss_dice_dn_1: 2.92  loss_bbox_dn_1: 1.134  loss_giou_dn_1: 0.8173  loss_ce_2: 2.923  loss_mask_2: 1.799  loss_dice_2: 2.892  loss_bbox_2: 3.134  loss_giou_2: 1.611  loss_ce_dn_2: 3.239  loss_mask_dn_2: 1.713  loss_dice_dn_2: 2.835  loss_bbox_dn_2: 1.091  loss_giou_dn_2: 0.7997  loss_ce_3: 2.921  loss_mask_3: 1.875  loss_dice_3: 2.839  loss_bbox_3: 2.925  loss_giou_3: 1.559  loss_ce_dn_3: 3.047  loss_mask_dn_3: 1.75  loss_dice_dn_3: 2.842  loss_bbox_dn_3: 1.057  loss_giou_dn_3: 0.7748  loss_ce_4: 2.948  loss_mask_4: 1.939  loss_dice_4: 2.892  loss_bbox_4: 2.932  loss_giou_4: 1.565  loss_ce_dn_4: 3.047  loss_mask_dn_4: 1.728  loss_dice_dn_4: 2.806  loss_bbox_dn_4: 1.024  loss_giou_dn_4: 0.7654  loss_ce_5: 2.975  loss_mask_5: 1.934  loss_dice_5: 2.876  loss_bbox_5: 3.003  loss_giou_5: 1.565  loss_ce_dn_5: 3.088  loss_mask_dn_5: 1.773  loss_dice_dn_5: 2.758  loss_bbox_dn_5: 1.031  loss_giou_dn_5: 0.7739  loss_ce_6: 3.068  loss_mask_6: 1.962  loss_dice_6: 2.914  loss_bbox_6: 2.964  loss_giou_6: 1.56  loss_ce_dn_6: 3.333  loss_mask_dn_6: 1.75  loss_dice_dn_6: 2.73  loss_bbox_dn_6: 1.047  loss_giou_dn_6: 0.7807  loss_ce_7: 3.071  loss_mask_7: 1.978  loss_dice_7: 2.93  loss_bbox_7: 2.935  loss_giou_7: 1.573  loss_ce_dn_7: 3.612  loss_mask_dn_7: 1.734  loss_dice_dn_7: 2.77  loss_bbox_dn_7: 1.04  loss_giou_dn_7: 0.787  loss_ce_8: 3.114  loss_mask_8: 1.943  loss_dice_8: 2.873  loss_bbox_8: 2.942  loss_giou_8: 1.561  loss_ce_dn_8: 3.726  loss_mask_dn_8: 1.781  loss_dice_dn_8: 2.759  loss_bbox_dn_8: 1.04  loss_giou_dn_8: 0.7933    time: 2.2753  last_time: 2.2832  data_time: 0.0144  last_data_time: 0.0021   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:39:02 d2.utils.events]: \u001b[0m eta: 8 days, 9:00:03  iter: 1139  total_loss: 237.6  loss_ce: 2.82  loss_mask: 1.855  loss_dice: 2.933  loss_bbox: 3.101  loss_giou: 1.619  loss_ce_dn: 4.062  loss_mask_dn: 1.704  loss_dice_dn: 2.932  loss_bbox_dn: 1.037  loss_giou_dn: 0.7863  loss_ce_0: 16.14  loss_mask_0: 1.606  loss_dice_0: 3.271  loss_bbox_0: 3.197  loss_giou_0: 1.793  loss_ce_1: 3.174  loss_mask_1: 1.729  loss_dice_1: 2.989  loss_bbox_1: 3.499  loss_giou_1: 1.808  loss_ce_dn_1: 4.633  loss_mask_dn_1: 1.688  loss_dice_dn_1: 3.14  loss_bbox_dn_1: 1.214  loss_giou_dn_1: 0.8179  loss_ce_2: 2.815  loss_mask_2: 1.675  loss_dice_2: 2.88  loss_bbox_2: 3.274  loss_giou_2: 1.657  loss_ce_dn_2: 3.878  loss_mask_dn_2: 1.733  loss_dice_dn_2: 3.07  loss_bbox_dn_2: 1.177  loss_giou_dn_2: 0.796  loss_ce_3: 2.843  loss_mask_3: 1.748  loss_dice_3: 2.868  loss_bbox_3: 3.124  loss_giou_3: 1.638  loss_ce_dn_3: 3.689  loss_mask_dn_3: 1.694  loss_dice_dn_3: 2.997  loss_bbox_dn_3: 1.083  loss_giou_dn_3: 0.7783  loss_ce_4: 2.802  loss_mask_4: 1.811  loss_dice_4: 2.892  loss_bbox_4: 3.057  loss_giou_4: 1.578  loss_ce_dn_4: 3.643  loss_mask_dn_4: 1.651  loss_dice_dn_4: 2.976  loss_bbox_dn_4: 1.034  loss_giou_dn_4: 0.7676  loss_ce_5: 2.765  loss_mask_5: 1.814  loss_dice_5: 2.838  loss_bbox_5: 3.134  loss_giou_5: 1.54  loss_ce_dn_5: 3.741  loss_mask_dn_5: 1.656  loss_dice_dn_5: 2.988  loss_bbox_dn_5: 1.024  loss_giou_dn_5: 0.7651  loss_ce_6: 2.901  loss_mask_6: 1.838  loss_dice_6: 2.87  loss_bbox_6: 3.154  loss_giou_6: 1.541  loss_ce_dn_6: 3.779  loss_mask_dn_6: 1.689  loss_dice_dn_6: 2.918  loss_bbox_dn_6: 1.022  loss_giou_dn_6: 0.7691  loss_ce_7: 2.75  loss_mask_7: 1.852  loss_dice_7: 2.89  loss_bbox_7: 3.133  loss_giou_7: 1.543  loss_ce_dn_7: 3.858  loss_mask_dn_7: 1.708  loss_dice_dn_7: 2.96  loss_bbox_dn_7: 1.017  loss_giou_dn_7: 0.7715  loss_ce_8: 2.774  loss_mask_8: 1.792  loss_dice_8: 2.957  loss_bbox_8: 2.994  loss_giou_8: 1.581  loss_ce_dn_8: 3.943  loss_mask_dn_8: 1.699  loss_dice_dn_8: 2.954  loss_bbox_dn_8: 1.022  loss_giou_dn_8: 0.7747    time: 2.2752  last_time: 2.2901  data_time: 0.0129  last_data_time: 0.0191   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:39:48 d2.utils.events]: \u001b[0m eta: 8 days, 8:57:10  iter: 1159  total_loss: 222.9  loss_ce: 3.228  loss_mask: 1.61  loss_dice: 2.884  loss_bbox: 2.881  loss_giou: 1.586  loss_ce_dn: 4.034  loss_mask_dn: 1.521  loss_dice_dn: 2.819  loss_bbox_dn: 0.9601  loss_giou_dn: 0.8032  loss_ce_0: 15.86  loss_mask_0: 1.473  loss_dice_0: 3.336  loss_bbox_0: 3.296  loss_giou_0: 1.801  loss_ce_1: 3.498  loss_mask_1: 1.586  loss_dice_1: 2.989  loss_bbox_1: 3.523  loss_giou_1: 1.785  loss_ce_dn_1: 4.801  loss_mask_dn_1: 1.542  loss_dice_dn_1: 3.059  loss_bbox_dn_1: 1.122  loss_giou_dn_1: 0.8345  loss_ce_2: 3.182  loss_mask_2: 1.635  loss_dice_2: 2.942  loss_bbox_2: 3.094  loss_giou_2: 1.735  loss_ce_dn_2: 3.981  loss_mask_dn_2: 1.516  loss_dice_dn_2: 2.895  loss_bbox_dn_2: 1.061  loss_giou_dn_2: 0.8204  loss_ce_3: 2.982  loss_mask_3: 1.733  loss_dice_3: 2.967  loss_bbox_3: 2.938  loss_giou_3: 1.718  loss_ce_dn_3: 3.784  loss_mask_dn_3: 1.492  loss_dice_dn_3: 2.923  loss_bbox_dn_3: 1.028  loss_giou_dn_3: 0.7981  loss_ce_4: 2.95  loss_mask_4: 1.641  loss_dice_4: 2.932  loss_bbox_4: 2.905  loss_giou_4: 1.697  loss_ce_dn_4: 3.695  loss_mask_dn_4: 1.477  loss_dice_dn_4: 2.894  loss_bbox_dn_4: 1.002  loss_giou_dn_4: 0.7865  loss_ce_5: 3.016  loss_mask_5: 1.597  loss_dice_5: 2.914  loss_bbox_5: 2.887  loss_giou_5: 1.647  loss_ce_dn_5: 3.643  loss_mask_dn_5: 1.482  loss_dice_dn_5: 2.865  loss_bbox_dn_5: 0.9815  loss_giou_dn_5: 0.7846  loss_ce_6: 3.127  loss_mask_6: 1.628  loss_dice_6: 2.881  loss_bbox_6: 2.859  loss_giou_6: 1.628  loss_ce_dn_6: 3.632  loss_mask_dn_6: 1.509  loss_dice_dn_6: 2.833  loss_bbox_dn_6: 0.9718  loss_giou_dn_6: 0.788  loss_ce_7: 3.244  loss_mask_7: 1.63  loss_dice_7: 2.874  loss_bbox_7: 2.897  loss_giou_7: 1.59  loss_ce_dn_7: 3.753  loss_mask_dn_7: 1.53  loss_dice_dn_7: 2.809  loss_bbox_dn_7: 0.952  loss_giou_dn_7: 0.7912  loss_ce_8: 3.319  loss_mask_8: 1.6  loss_dice_8: 2.889  loss_bbox_8: 2.884  loss_giou_8: 1.583  loss_ce_dn_8: 3.935  loss_mask_dn_8: 1.502  loss_dice_dn_8: 2.803  loss_bbox_dn_8: 0.9495  loss_giou_dn_8: 0.7958    time: 2.2751  last_time: 2.2600  data_time: 0.0130  last_data_time: 0.0051   lr: 0.0001  max_mem: 13292M\n",
            "\u001b[32m[07/26 15:40:33 d2.utils.events]: \u001b[0m eta: 8 days, 8:56:06  iter: 1179  total_loss: 237.6  loss_ce: 3.409  loss_mask: 1.768  loss_dice: 3.073  loss_bbox: 2.598  loss_giou: 1.741  loss_ce_dn: 4.355  loss_mask_dn: 1.501  loss_dice_dn: 3.015  loss_bbox_dn: 0.9238  loss_giou_dn: 0.8178  loss_ce_0: 15.87  loss_mask_0: 1.415  loss_dice_0: 3.361  loss_bbox_0: 3.351  loss_giou_0: 1.87  loss_ce_1: 3.955  loss_mask_1: 1.612  loss_dice_1: 3.101  loss_bbox_1: 3.268  loss_giou_1: 1.858  loss_ce_dn_1: 4.596  loss_mask_dn_1: 1.552  loss_dice_dn_1: 3.126  loss_bbox_dn_1: 1.039  loss_giou_dn_1: 0.8202  loss_ce_2: 3.512  loss_mask_2: 1.657  loss_dice_2: 3.048  loss_bbox_2: 2.869  loss_giou_2: 1.82  loss_ce_dn_2: 3.969  loss_mask_dn_2: 1.5  loss_dice_dn_2: 3.017  loss_bbox_dn_2: 1.005  loss_giou_dn_2: 0.8102  loss_ce_3: 3.431  loss_mask_3: 1.702  loss_dice_3: 3.006  loss_bbox_3: 2.81  loss_giou_3: 1.752  loss_ce_dn_3: 3.754  loss_mask_dn_3: 1.485  loss_dice_dn_3: 3.018  loss_bbox_dn_3: 0.9552  loss_giou_dn_3: 0.7958  loss_ce_4: 3.484  loss_mask_4: 1.692  loss_dice_4: 3.028  loss_bbox_4: 2.774  loss_giou_4: 1.71  loss_ce_dn_4: 3.658  loss_mask_dn_4: 1.467  loss_dice_dn_4: 2.977  loss_bbox_dn_4: 0.9349  loss_giou_dn_4: 0.7918  loss_ce_5: 3.636  loss_mask_5: 1.706  loss_dice_5: 3.009  loss_bbox_5: 2.751  loss_giou_5: 1.697  loss_ce_dn_5: 3.689  loss_mask_dn_5: 1.502  loss_dice_dn_5: 3.038  loss_bbox_dn_5: 0.9333  loss_giou_dn_5: 0.7942  loss_ce_6: 3.432  loss_mask_6: 1.711  loss_dice_6: 3.052  loss_bbox_6: 2.661  loss_giou_6: 1.67  loss_ce_dn_6: 3.665  loss_mask_dn_6: 1.5  loss_dice_dn_6: 3.047  loss_bbox_dn_6: 0.9322  loss_giou_dn_6: 0.8007  loss_ce_7: 3.536  loss_mask_7: 1.717  loss_dice_7: 3.068  loss_bbox_7: 2.674  loss_giou_7: 1.685  loss_ce_dn_7: 3.813  loss_mask_dn_7: 1.46  loss_dice_dn_7: 3.083  loss_bbox_dn_7: 0.9288  loss_giou_dn_7: 0.8034  loss_ce_8: 3.657  loss_mask_8: 1.766  loss_dice_8: 3.033  loss_bbox_8: 2.645  loss_giou_8: 1.704  loss_ce_dn_8: 4.112  loss_mask_dn_8: 1.461  loss_dice_dn_8: 3.031  loss_bbox_dn_8: 0.9238  loss_giou_dn_8: 0.8093    time: 2.2749  last_time: 2.2898  data_time: 0.0122  last_data_time: 0.0205   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:41:18 d2.utils.events]: \u001b[0m eta: 8 days, 8:55:14  iter: 1199  total_loss: 235.4  loss_ce: 3.259  loss_mask: 1.74  loss_dice: 3.168  loss_bbox: 2.871  loss_giou: 1.807  loss_ce_dn: 3.37  loss_mask_dn: 1.621  loss_dice_dn: 2.998  loss_bbox_dn: 0.9525  loss_giou_dn: 0.8507  loss_ce_0: 15.71  loss_mask_0: 1.578  loss_dice_0: 3.373  loss_bbox_0: 3.353  loss_giou_0: 1.809  loss_ce_1: 3.882  loss_mask_1: 1.778  loss_dice_1: 3.201  loss_bbox_1: 3.45  loss_giou_1: 1.893  loss_ce_dn_1: 4.44  loss_mask_dn_1: 1.668  loss_dice_dn_1: 3.187  loss_bbox_dn_1: 1.009  loss_giou_dn_1: 0.8355  loss_ce_2: 3.306  loss_mask_2: 1.74  loss_dice_2: 3.251  loss_bbox_2: 3.083  loss_giou_2: 1.829  loss_ce_dn_2: 3.553  loss_mask_dn_2: 1.671  loss_dice_dn_2: 3.072  loss_bbox_dn_2: 0.9801  loss_giou_dn_2: 0.8167  loss_ce_3: 3.264  loss_mask_3: 1.746  loss_dice_3: 3.211  loss_bbox_3: 2.956  loss_giou_3: 1.797  loss_ce_dn_3: 3.281  loss_mask_dn_3: 1.665  loss_dice_dn_3: 3.067  loss_bbox_dn_3: 0.9508  loss_giou_dn_3: 0.8022  loss_ce_4: 3.192  loss_mask_4: 1.774  loss_dice_4: 3.235  loss_bbox_4: 2.913  loss_giou_4: 1.867  loss_ce_dn_4: 3.169  loss_mask_dn_4: 1.656  loss_dice_dn_4: 3.056  loss_bbox_dn_4: 0.9404  loss_giou_dn_4: 0.8107  loss_ce_5: 3.161  loss_mask_5: 1.805  loss_dice_5: 3.194  loss_bbox_5: 2.903  loss_giou_5: 1.83  loss_ce_dn_5: 3.12  loss_mask_dn_5: 1.593  loss_dice_dn_5: 3.018  loss_bbox_dn_5: 0.9303  loss_giou_dn_5: 0.8098  loss_ce_6: 3.21  loss_mask_6: 1.767  loss_dice_6: 3.172  loss_bbox_6: 2.895  loss_giou_6: 1.838  loss_ce_dn_6: 3.096  loss_mask_dn_6: 1.6  loss_dice_dn_6: 2.997  loss_bbox_dn_6: 0.9303  loss_giou_dn_6: 0.8174  loss_ce_7: 3.07  loss_mask_7: 1.746  loss_dice_7: 3.213  loss_bbox_7: 2.861  loss_giou_7: 1.834  loss_ce_dn_7: 3.07  loss_mask_dn_7: 1.605  loss_dice_dn_7: 2.971  loss_bbox_dn_7: 0.9277  loss_giou_dn_7: 0.8228  loss_ce_8: 3.22  loss_mask_8: 1.781  loss_dice_8: 3.188  loss_bbox_8: 2.938  loss_giou_8: 1.802  loss_ce_dn_8: 3.187  loss_mask_dn_8: 1.614  loss_dice_dn_8: 2.971  loss_bbox_dn_8: 0.938  loss_giou_dn_8: 0.8364    time: 2.2750  last_time: 2.2842  data_time: 0.0126  last_data_time: 0.0296   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:42:04 d2.utils.events]: \u001b[0m eta: 8 days, 8:53:55  iter: 1219  total_loss: 213.3  loss_ce: 2.532  loss_mask: 1.832  loss_dice: 2.695  loss_bbox: 2.795  loss_giou: 1.471  loss_ce_dn: 3.329  loss_mask_dn: 1.711  loss_dice_dn: 2.556  loss_bbox_dn: 1.082  loss_giou_dn: 0.7833  loss_ce_0: 15.65  loss_mask_0: 1.567  loss_dice_0: 2.954  loss_bbox_0: 2.977  loss_giou_0: 1.672  loss_ce_1: 2.71  loss_mask_1: 1.778  loss_dice_1: 2.681  loss_bbox_1: 3.042  loss_giou_1: 1.65  loss_ce_dn_1: 4.004  loss_mask_dn_1: 1.789  loss_dice_dn_1: 2.786  loss_bbox_dn_1: 1.156  loss_giou_dn_1: 0.8145  loss_ce_2: 2.337  loss_mask_2: 1.742  loss_dice_2: 2.663  loss_bbox_2: 2.857  loss_giou_2: 1.541  loss_ce_dn_2: 3.223  loss_mask_dn_2: 1.717  loss_dice_dn_2: 2.657  loss_bbox_dn_2: 1.101  loss_giou_dn_2: 0.7968  loss_ce_3: 2.305  loss_mask_3: 1.755  loss_dice_3: 2.676  loss_bbox_3: 2.997  loss_giou_3: 1.474  loss_ce_dn_3: 2.98  loss_mask_dn_3: 1.728  loss_dice_dn_3: 2.575  loss_bbox_dn_3: 1.089  loss_giou_dn_3: 0.7724  loss_ce_4: 2.194  loss_mask_4: 1.852  loss_dice_4: 2.597  loss_bbox_4: 2.945  loss_giou_4: 1.506  loss_ce_dn_4: 2.921  loss_mask_dn_4: 1.698  loss_dice_dn_4: 2.545  loss_bbox_dn_4: 1.075  loss_giou_dn_4: 0.7686  loss_ce_5: 2.406  loss_mask_5: 1.829  loss_dice_5: 2.666  loss_bbox_5: 2.76  loss_giou_5: 1.428  loss_ce_dn_5: 2.949  loss_mask_dn_5: 1.673  loss_dice_dn_5: 2.526  loss_bbox_dn_5: 1.069  loss_giou_dn_5: 0.7653  loss_ce_6: 2.246  loss_mask_6: 1.83  loss_dice_6: 2.737  loss_bbox_6: 2.762  loss_giou_6: 1.474  loss_ce_dn_6: 2.938  loss_mask_dn_6: 1.701  loss_dice_dn_6: 2.545  loss_bbox_dn_6: 1.068  loss_giou_dn_6: 0.7685  loss_ce_7: 2.326  loss_mask_7: 1.865  loss_dice_7: 2.67  loss_bbox_7: 2.77  loss_giou_7: 1.458  loss_ce_dn_7: 2.939  loss_mask_dn_7: 1.701  loss_dice_dn_7: 2.575  loss_bbox_dn_7: 1.072  loss_giou_dn_7: 0.7634  loss_ce_8: 2.414  loss_mask_8: 1.834  loss_dice_8: 2.68  loss_bbox_8: 2.877  loss_giou_8: 1.462  loss_ce_dn_8: 3.153  loss_mask_dn_8: 1.72  loss_dice_dn_8: 2.557  loss_bbox_dn_8: 1.071  loss_giou_dn_8: 0.7649    time: 2.2750  last_time: 2.3234  data_time: 0.0124  last_data_time: 0.0249   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:42:49 d2.utils.events]: \u001b[0m eta: 8 days, 8:53:43  iter: 1239  total_loss: 228.2  loss_ce: 2.623  loss_mask: 1.651  loss_dice: 2.884  loss_bbox: 2.835  loss_giou: 1.698  loss_ce_dn: 3.708  loss_mask_dn: 1.615  loss_dice_dn: 2.786  loss_bbox_dn: 0.9412  loss_giou_dn: 0.8256  loss_ce_0: 15.62  loss_mask_0: 1.459  loss_dice_0: 3.253  loss_bbox_0: 3.122  loss_giou_0: 1.83  loss_ce_1: 3.545  loss_mask_1: 1.553  loss_dice_1: 2.979  loss_bbox_1: 3.271  loss_giou_1: 1.79  loss_ce_dn_1: 4.564  loss_mask_dn_1: 1.456  loss_dice_dn_1: 3.004  loss_bbox_dn_1: 1.11  loss_giou_dn_1: 0.8385  loss_ce_2: 2.929  loss_mask_2: 1.555  loss_dice_2: 2.892  loss_bbox_2: 3.069  loss_giou_2: 1.701  loss_ce_dn_2: 3.571  loss_mask_dn_2: 1.433  loss_dice_dn_2: 2.889  loss_bbox_dn_2: 1.067  loss_giou_dn_2: 0.8347  loss_ce_3: 2.641  loss_mask_3: 1.608  loss_dice_3: 2.914  loss_bbox_3: 2.946  loss_giou_3: 1.686  loss_ce_dn_3: 3.435  loss_mask_dn_3: 1.472  loss_dice_dn_3: 2.882  loss_bbox_dn_3: 1.007  loss_giou_dn_3: 0.8226  loss_ce_4: 2.629  loss_mask_4: 1.681  loss_dice_4: 2.91  loss_bbox_4: 2.901  loss_giou_4: 1.664  loss_ce_dn_4: 3.456  loss_mask_dn_4: 1.521  loss_dice_dn_4: 2.867  loss_bbox_dn_4: 0.9657  loss_giou_dn_4: 0.8171  loss_ce_5: 2.563  loss_mask_5: 1.627  loss_dice_5: 2.933  loss_bbox_5: 2.865  loss_giou_5: 1.628  loss_ce_dn_5: 3.447  loss_mask_dn_5: 1.505  loss_dice_dn_5: 2.877  loss_bbox_dn_5: 0.948  loss_giou_dn_5: 0.816  loss_ce_6: 2.586  loss_mask_6: 1.655  loss_dice_6: 2.931  loss_bbox_6: 2.874  loss_giou_6: 1.66  loss_ce_dn_6: 3.455  loss_mask_dn_6: 1.534  loss_dice_dn_6: 2.816  loss_bbox_dn_6: 0.9437  loss_giou_dn_6: 0.821  loss_ce_7: 2.638  loss_mask_7: 1.663  loss_dice_7: 2.97  loss_bbox_7: 2.868  loss_giou_7: 1.654  loss_ce_dn_7: 3.564  loss_mask_dn_7: 1.612  loss_dice_dn_7: 2.838  loss_bbox_dn_7: 0.9289  loss_giou_dn_7: 0.821  loss_ce_8: 2.66  loss_mask_8: 1.671  loss_dice_8: 2.945  loss_bbox_8: 2.897  loss_giou_8: 1.682  loss_ce_dn_8: 3.642  loss_mask_dn_8: 1.591  loss_dice_dn_8: 2.835  loss_bbox_dn_8: 0.9299  loss_giou_dn_8: 0.8242    time: 2.2749  last_time: 2.2471  data_time: 0.0123  last_data_time: 0.0079   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:43:35 d2.utils.events]: \u001b[0m eta: 8 days, 8:53:02  iter: 1259  total_loss: 216.2  loss_ce: 2.175  loss_mask: 1.83  loss_dice: 2.747  loss_bbox: 3.198  loss_giou: 1.557  loss_ce_dn: 3.313  loss_mask_dn: 1.709  loss_dice_dn: 2.704  loss_bbox_dn: 1.17  loss_giou_dn: 0.8072  loss_ce_0: 15.5  loss_mask_0: 1.714  loss_dice_0: 3.022  loss_bbox_0: 3.233  loss_giou_0: 1.791  loss_ce_1: 2.459  loss_mask_1: 1.742  loss_dice_1: 2.626  loss_bbox_1: 3.408  loss_giou_1: 1.738  loss_ce_dn_1: 4.054  loss_mask_dn_1: 1.824  loss_dice_dn_1: 2.767  loss_bbox_dn_1: 1.208  loss_giou_dn_1: 0.8227  loss_ce_2: 2.146  loss_mask_2: 1.77  loss_dice_2: 2.606  loss_bbox_2: 3.384  loss_giou_2: 1.623  loss_ce_dn_2: 3.355  loss_mask_dn_2: 1.784  loss_dice_dn_2: 2.669  loss_bbox_dn_2: 1.159  loss_giou_dn_2: 0.8009  loss_ce_3: 2.303  loss_mask_3: 1.78  loss_dice_3: 2.692  loss_bbox_3: 3.31  loss_giou_3: 1.554  loss_ce_dn_3: 3.164  loss_mask_dn_3: 1.752  loss_dice_dn_3: 2.651  loss_bbox_dn_3: 1.133  loss_giou_dn_3: 0.7816  loss_ce_4: 2.227  loss_mask_4: 1.805  loss_dice_4: 2.72  loss_bbox_4: 3.278  loss_giou_4: 1.54  loss_ce_dn_4: 3.114  loss_mask_dn_4: 1.682  loss_dice_dn_4: 2.667  loss_bbox_dn_4: 1.121  loss_giou_dn_4: 0.7682  loss_ce_5: 2.107  loss_mask_5: 1.845  loss_dice_5: 2.668  loss_bbox_5: 3.222  loss_giou_5: 1.512  loss_ce_dn_5: 3.126  loss_mask_dn_5: 1.711  loss_dice_dn_5: 2.646  loss_bbox_dn_5: 1.13  loss_giou_dn_5: 0.7671  loss_ce_6: 2.085  loss_mask_6: 1.786  loss_dice_6: 2.668  loss_bbox_6: 3.219  loss_giou_6: 1.486  loss_ce_dn_6: 3.169  loss_mask_dn_6: 1.724  loss_dice_dn_6: 2.628  loss_bbox_dn_6: 1.142  loss_giou_dn_6: 0.7757  loss_ce_7: 2.15  loss_mask_7: 1.779  loss_dice_7: 2.74  loss_bbox_7: 3.151  loss_giou_7: 1.487  loss_ce_dn_7: 3.154  loss_mask_dn_7: 1.717  loss_dice_dn_7: 2.654  loss_bbox_dn_7: 1.142  loss_giou_dn_7: 0.7819  loss_ce_8: 2.118  loss_mask_8: 1.817  loss_dice_8: 2.703  loss_bbox_8: 3.184  loss_giou_8: 1.523  loss_ce_dn_8: 3.235  loss_mask_dn_8: 1.717  loss_dice_dn_8: 2.665  loss_bbox_dn_8: 1.158  loss_giou_dn_8: 0.7909    time: 2.2753  last_time: 2.2742  data_time: 0.0131  last_data_time: 0.0142   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:44:21 d2.utils.events]: \u001b[0m eta: 8 days, 8:52:11  iter: 1279  total_loss: 229.9  loss_ce: 3.273  loss_mask: 1.859  loss_dice: 3.08  loss_bbox: 3.022  loss_giou: 1.661  loss_ce_dn: 3.674  loss_mask_dn: 1.668  loss_dice_dn: 3.02  loss_bbox_dn: 1.075  loss_giou_dn: 0.7964  loss_ce_0: 15.48  loss_mask_0: 1.816  loss_dice_0: 3.244  loss_bbox_0: 3.218  loss_giou_0: 1.679  loss_ce_1: 3.576  loss_mask_1: 1.711  loss_dice_1: 3.098  loss_bbox_1: 3.295  loss_giou_1: 1.692  loss_ce_dn_1: 4.296  loss_mask_dn_1: 1.655  loss_dice_dn_1: 3.184  loss_bbox_dn_1: 1.148  loss_giou_dn_1: 0.8292  loss_ce_2: 3.176  loss_mask_2: 1.719  loss_dice_2: 3.053  loss_bbox_2: 3.024  loss_giou_2: 1.646  loss_ce_dn_2: 3.633  loss_mask_dn_2: 1.629  loss_dice_dn_2: 3.095  loss_bbox_dn_2: 1.108  loss_giou_dn_2: 0.8145  loss_ce_3: 3.128  loss_mask_3: 1.766  loss_dice_3: 3.039  loss_bbox_3: 3.081  loss_giou_3: 1.63  loss_ce_dn_3: 3.54  loss_mask_dn_3: 1.693  loss_dice_dn_3: 3.084  loss_bbox_dn_3: 1.073  loss_giou_dn_3: 0.7949  loss_ce_4: 3.084  loss_mask_4: 1.819  loss_dice_4: 3.104  loss_bbox_4: 3.159  loss_giou_4: 1.642  loss_ce_dn_4: 3.558  loss_mask_dn_4: 1.681  loss_dice_dn_4: 3.082  loss_bbox_dn_4: 1.061  loss_giou_dn_4: 0.7869  loss_ce_5: 2.921  loss_mask_5: 1.816  loss_dice_5: 3.022  loss_bbox_5: 3.262  loss_giou_5: 1.647  loss_ce_dn_5: 3.44  loss_mask_dn_5: 1.708  loss_dice_dn_5: 3.052  loss_bbox_dn_5: 1.065  loss_giou_dn_5: 0.7913  loss_ce_6: 3.097  loss_mask_6: 1.858  loss_dice_6: 2.976  loss_bbox_6: 3.289  loss_giou_6: 1.667  loss_ce_dn_6: 3.433  loss_mask_dn_6: 1.663  loss_dice_dn_6: 3.002  loss_bbox_dn_6: 1.076  loss_giou_dn_6: 0.7913  loss_ce_7: 2.97  loss_mask_7: 1.854  loss_dice_7: 3.069  loss_bbox_7: 3.18  loss_giou_7: 1.651  loss_ce_dn_7: 3.356  loss_mask_dn_7: 1.666  loss_dice_dn_7: 3.034  loss_bbox_dn_7: 1.076  loss_giou_dn_7: 0.7852  loss_ce_8: 2.991  loss_mask_8: 1.845  loss_dice_8: 3.074  loss_bbox_8: 3.102  loss_giou_8: 1.653  loss_ce_dn_8: 3.479  loss_mask_dn_8: 1.641  loss_dice_dn_8: 2.981  loss_bbox_dn_8: 1.074  loss_giou_dn_8: 0.7876    time: 2.2754  last_time: 2.2384  data_time: 0.0137  last_data_time: 0.0115   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:45:07 d2.utils.events]: \u001b[0m eta: 8 days, 8:50:53  iter: 1299  total_loss: 232.4  loss_ce: 3.737  loss_mask: 1.718  loss_dice: 2.921  loss_bbox: 2.656  loss_giou: 1.742  loss_ce_dn: 4.174  loss_mask_dn: 1.718  loss_dice_dn: 2.795  loss_bbox_dn: 1.027  loss_giou_dn: 0.8377  loss_ce_0: 15.31  loss_mask_0: 1.368  loss_dice_0: 3.153  loss_bbox_0: 3.294  loss_giou_0: 1.856  loss_ce_1: 3.853  loss_mask_1: 1.546  loss_dice_1: 3.113  loss_bbox_1: 3.402  loss_giou_1: 1.797  loss_ce_dn_1: 4.648  loss_mask_dn_1: 1.581  loss_dice_dn_1: 2.978  loss_bbox_dn_1: 1.068  loss_giou_dn_1: 0.8339  loss_ce_2: 3.614  loss_mask_2: 1.616  loss_dice_2: 3.024  loss_bbox_2: 3.033  loss_giou_2: 1.712  loss_ce_dn_2: 3.739  loss_mask_dn_2: 1.595  loss_dice_dn_2: 2.841  loss_bbox_dn_2: 1.045  loss_giou_dn_2: 0.8201  loss_ce_3: 3.295  loss_mask_3: 1.68  loss_dice_3: 2.999  loss_bbox_3: 2.959  loss_giou_3: 1.713  loss_ce_dn_3: 3.499  loss_mask_dn_3: 1.582  loss_dice_dn_3: 2.824  loss_bbox_dn_3: 1.034  loss_giou_dn_3: 0.8092  loss_ce_4: 3.374  loss_mask_4: 1.702  loss_dice_4: 3.001  loss_bbox_4: 2.836  loss_giou_4: 1.644  loss_ce_dn_4: 3.512  loss_mask_dn_4: 1.607  loss_dice_dn_4: 2.793  loss_bbox_dn_4: 1.025  loss_giou_dn_4: 0.8  loss_ce_5: 3.356  loss_mask_5: 1.634  loss_dice_5: 3.026  loss_bbox_5: 2.707  loss_giou_5: 1.603  loss_ce_dn_5: 3.558  loss_mask_dn_5: 1.617  loss_dice_dn_5: 2.778  loss_bbox_dn_5: 1.024  loss_giou_dn_5: 0.8016  loss_ce_6: 3.331  loss_mask_6: 1.749  loss_dice_6: 2.923  loss_bbox_6: 2.643  loss_giou_6: 1.66  loss_ce_dn_6: 3.589  loss_mask_dn_6: 1.645  loss_dice_dn_6: 2.776  loss_bbox_dn_6: 1.024  loss_giou_dn_6: 0.8109  loss_ce_7: 3.533  loss_mask_7: 1.704  loss_dice_7: 2.972  loss_bbox_7: 2.639  loss_giou_7: 1.66  loss_ce_dn_7: 3.86  loss_mask_dn_7: 1.644  loss_dice_dn_7: 2.778  loss_bbox_dn_7: 1.018  loss_giou_dn_7: 0.8179  loss_ce_8: 3.593  loss_mask_8: 1.76  loss_dice_8: 2.892  loss_bbox_8: 2.603  loss_giou_8: 1.685  loss_ce_dn_8: 3.94  loss_mask_dn_8: 1.682  loss_dice_dn_8: 2.792  loss_bbox_dn_8: 1.021  loss_giou_dn_8: 0.827    time: 2.2754  last_time: 2.2815  data_time: 0.0135  last_data_time: 0.0184   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:45:52 d2.utils.events]: \u001b[0m eta: 8 days, 8:48:27  iter: 1319  total_loss: 229.3  loss_ce: 3.216  loss_mask: 1.668  loss_dice: 2.929  loss_bbox: 2.966  loss_giou: 1.631  loss_ce_dn: 4.021  loss_mask_dn: 1.591  loss_dice_dn: 2.766  loss_bbox_dn: 0.9778  loss_giou_dn: 0.8001  loss_ce_0: 15.44  loss_mask_0: 1.546  loss_dice_0: 3.267  loss_bbox_0: 3.252  loss_giou_0: 1.824  loss_ce_1: 3.7  loss_mask_1: 1.678  loss_dice_1: 3.027  loss_bbox_1: 3.536  loss_giou_1: 1.776  loss_ce_dn_1: 4.251  loss_mask_dn_1: 1.516  loss_dice_dn_1: 2.985  loss_bbox_dn_1: 1.084  loss_giou_dn_1: 0.8091  loss_ce_2: 3.256  loss_mask_2: 1.641  loss_dice_2: 3.033  loss_bbox_2: 3.282  loss_giou_2: 1.705  loss_ce_dn_2: 3.466  loss_mask_dn_2: 1.514  loss_dice_dn_2: 2.871  loss_bbox_dn_2: 1.048  loss_giou_dn_2: 0.7957  loss_ce_3: 3.165  loss_mask_3: 1.63  loss_dice_3: 2.936  loss_bbox_3: 3.23  loss_giou_3: 1.676  loss_ce_dn_3: 3.374  loss_mask_dn_3: 1.51  loss_dice_dn_3: 2.84  loss_bbox_dn_3: 0.9736  loss_giou_dn_3: 0.7719  loss_ce_4: 3.274  loss_mask_4: 1.664  loss_dice_4: 2.961  loss_bbox_4: 3.117  loss_giou_4: 1.672  loss_ce_dn_4: 3.401  loss_mask_dn_4: 1.537  loss_dice_dn_4: 2.811  loss_bbox_dn_4: 0.9645  loss_giou_dn_4: 0.7699  loss_ce_5: 3.245  loss_mask_5: 1.607  loss_dice_5: 2.97  loss_bbox_5: 3.092  loss_giou_5: 1.665  loss_ce_dn_5: 3.508  loss_mask_dn_5: 1.532  loss_dice_dn_5: 2.8  loss_bbox_dn_5: 0.9628  loss_giou_dn_5: 0.773  loss_ce_6: 3.26  loss_mask_6: 1.615  loss_dice_6: 2.97  loss_bbox_6: 3.071  loss_giou_6: 1.647  loss_ce_dn_6: 3.616  loss_mask_dn_6: 1.572  loss_dice_dn_6: 2.748  loss_bbox_dn_6: 0.9704  loss_giou_dn_6: 0.7777  loss_ce_7: 3.253  loss_mask_7: 1.632  loss_dice_7: 2.997  loss_bbox_7: 3.055  loss_giou_7: 1.631  loss_ce_dn_7: 3.661  loss_mask_dn_7: 1.574  loss_dice_dn_7: 2.765  loss_bbox_dn_7: 0.9614  loss_giou_dn_7: 0.7793  loss_ce_8: 3.194  loss_mask_8: 1.643  loss_dice_8: 2.975  loss_bbox_8: 3.034  loss_giou_8: 1.6  loss_ce_dn_8: 3.853  loss_mask_dn_8: 1.578  loss_dice_dn_8: 2.767  loss_bbox_dn_8: 0.9699  loss_giou_dn_8: 0.7904    time: 2.2753  last_time: 2.2347  data_time: 0.0115  last_data_time: 0.0083   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:46:38 d2.utils.events]: \u001b[0m eta: 8 days, 8:48:35  iter: 1339  total_loss: 217.1  loss_ce: 3.046  loss_mask: 1.891  loss_dice: 2.714  loss_bbox: 3.064  loss_giou: 1.698  loss_ce_dn: 3.511  loss_mask_dn: 1.761  loss_dice_dn: 2.604  loss_bbox_dn: 1.096  loss_giou_dn: 0.8016  loss_ce_0: 15.42  loss_mask_0: 1.806  loss_dice_0: 3.163  loss_bbox_0: 3.146  loss_giou_0: 1.694  loss_ce_1: 3.448  loss_mask_1: 1.838  loss_dice_1: 2.739  loss_bbox_1: 3.041  loss_giou_1: 1.683  loss_ce_dn_1: 4.168  loss_mask_dn_1: 1.776  loss_dice_dn_1: 2.811  loss_bbox_dn_1: 1.217  loss_giou_dn_1: 0.821  loss_ce_2: 2.893  loss_mask_2: 1.861  loss_dice_2: 2.744  loss_bbox_2: 2.82  loss_giou_2: 1.617  loss_ce_dn_2: 3.661  loss_mask_dn_2: 1.755  loss_dice_dn_2: 2.698  loss_bbox_dn_2: 1.174  loss_giou_dn_2: 0.7975  loss_ce_3: 2.992  loss_mask_3: 1.903  loss_dice_3: 2.731  loss_bbox_3: 2.914  loss_giou_3: 1.536  loss_ce_dn_3: 3.501  loss_mask_dn_3: 1.758  loss_dice_dn_3: 2.634  loss_bbox_dn_3: 1.119  loss_giou_dn_3: 0.7761  loss_ce_4: 2.979  loss_mask_4: 1.844  loss_dice_4: 2.694  loss_bbox_4: 3.001  loss_giou_4: 1.58  loss_ce_dn_4: 3.346  loss_mask_dn_4: 1.812  loss_dice_dn_4: 2.631  loss_bbox_dn_4: 1.08  loss_giou_dn_4: 0.767  loss_ce_5: 3.055  loss_mask_5: 1.879  loss_dice_5: 2.648  loss_bbox_5: 3.063  loss_giou_5: 1.614  loss_ce_dn_5: 3.288  loss_mask_dn_5: 1.771  loss_dice_dn_5: 2.623  loss_bbox_dn_5: 1.071  loss_giou_dn_5: 0.7751  loss_ce_6: 2.901  loss_mask_6: 1.885  loss_dice_6: 2.785  loss_bbox_6: 3.092  loss_giou_6: 1.646  loss_ce_dn_6: 3.277  loss_mask_dn_6: 1.772  loss_dice_dn_6: 2.609  loss_bbox_dn_6: 1.076  loss_giou_dn_6: 0.7789  loss_ce_7: 2.889  loss_mask_7: 1.891  loss_dice_7: 2.739  loss_bbox_7: 3.087  loss_giou_7: 1.662  loss_ce_dn_7: 3.409  loss_mask_dn_7: 1.796  loss_dice_dn_7: 2.586  loss_bbox_dn_7: 1.066  loss_giou_dn_7: 0.7764  loss_ce_8: 3.042  loss_mask_8: 1.895  loss_dice_8: 2.676  loss_bbox_8: 3.08  loss_giou_8: 1.688  loss_ce_dn_8: 3.487  loss_mask_dn_8: 1.75  loss_dice_dn_8: 2.578  loss_bbox_dn_8: 1.075  loss_giou_dn_8: 0.783    time: 2.2753  last_time: 2.2794  data_time: 0.0129  last_data_time: 0.0145   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:47:23 d2.utils.events]: \u001b[0m eta: 8 days, 8:48:04  iter: 1359  total_loss: 226.1  loss_ce: 3.047  loss_mask: 1.789  loss_dice: 2.934  loss_bbox: 2.994  loss_giou: 1.612  loss_ce_dn: 4.093  loss_mask_dn: 1.688  loss_dice_dn: 2.785  loss_bbox_dn: 0.9804  loss_giou_dn: 0.8026  loss_ce_0: 15.14  loss_mask_0: 1.746  loss_dice_0: 3.229  loss_bbox_0: 3.22  loss_giou_0: 1.833  loss_ce_1: 3.488  loss_mask_1: 1.66  loss_dice_1: 3.036  loss_bbox_1: 3.424  loss_giou_1: 1.727  loss_ce_dn_1: 4.529  loss_mask_dn_1: 1.676  loss_dice_dn_1: 2.832  loss_bbox_dn_1: 1.028  loss_giou_dn_1: 0.8169  loss_ce_2: 2.938  loss_mask_2: 1.694  loss_dice_2: 3.019  loss_bbox_2: 3.2  loss_giou_2: 1.612  loss_ce_dn_2: 3.789  loss_mask_dn_2: 1.679  loss_dice_dn_2: 2.807  loss_bbox_dn_2: 0.9968  loss_giou_dn_2: 0.8021  loss_ce_3: 2.922  loss_mask_3: 1.769  loss_dice_3: 2.918  loss_bbox_3: 3.075  loss_giou_3: 1.603  loss_ce_dn_3: 3.56  loss_mask_dn_3: 1.635  loss_dice_dn_3: 2.719  loss_bbox_dn_3: 0.9987  loss_giou_dn_3: 0.7841  loss_ce_4: 3.048  loss_mask_4: 1.748  loss_dice_4: 2.896  loss_bbox_4: 3.002  loss_giou_4: 1.597  loss_ce_dn_4: 3.513  loss_mask_dn_4: 1.655  loss_dice_dn_4: 2.726  loss_bbox_dn_4: 1.007  loss_giou_dn_4: 0.7734  loss_ce_5: 3.056  loss_mask_5: 1.772  loss_dice_5: 2.946  loss_bbox_5: 3.001  loss_giou_5: 1.596  loss_ce_dn_5: 3.482  loss_mask_dn_5: 1.618  loss_dice_dn_5: 2.684  loss_bbox_dn_5: 0.9979  loss_giou_dn_5: 0.7714  loss_ce_6: 3.004  loss_mask_6: 1.775  loss_dice_6: 2.943  loss_bbox_6: 3.006  loss_giou_6: 1.613  loss_ce_dn_6: 3.575  loss_mask_dn_6: 1.63  loss_dice_dn_6: 2.694  loss_bbox_dn_6: 0.99  loss_giou_dn_6: 0.7707  loss_ce_7: 3.124  loss_mask_7: 1.808  loss_dice_7: 2.951  loss_bbox_7: 2.993  loss_giou_7: 1.577  loss_ce_dn_7: 3.64  loss_mask_dn_7: 1.663  loss_dice_dn_7: 2.776  loss_bbox_dn_7: 0.9717  loss_giou_dn_7: 0.7817  loss_ce_8: 3.105  loss_mask_8: 1.776  loss_dice_8: 2.936  loss_bbox_8: 2.991  loss_giou_8: 1.61  loss_ce_dn_8: 3.863  loss_mask_dn_8: 1.662  loss_dice_dn_8: 2.749  loss_bbox_dn_8: 0.9727  loss_giou_dn_8: 0.792    time: 2.2753  last_time: 2.2270  data_time: 0.0108  last_data_time: 0.0090   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:48:09 d2.utils.events]: \u001b[0m eta: 8 days, 8:45:28  iter: 1379  total_loss: 226.2  loss_ce: 2.847  loss_mask: 1.775  loss_dice: 3.016  loss_bbox: 2.865  loss_giou: 1.74  loss_ce_dn: 3.498  loss_mask_dn: 1.605  loss_dice_dn: 2.904  loss_bbox_dn: 0.9733  loss_giou_dn: 0.7948  loss_ce_0: 15.01  loss_mask_0: 1.655  loss_dice_0: 3.223  loss_bbox_0: 3.591  loss_giou_0: 1.788  loss_ce_1: 3.276  loss_mask_1: 1.731  loss_dice_1: 3.017  loss_bbox_1: 3.496  loss_giou_1: 1.821  loss_ce_dn_1: 4.037  loss_mask_dn_1: 1.643  loss_dice_dn_1: 2.976  loss_bbox_dn_1: 1.05  loss_giou_dn_1: 0.8233  loss_ce_2: 2.745  loss_mask_2: 1.695  loss_dice_2: 2.938  loss_bbox_2: 3.109  loss_giou_2: 1.801  loss_ce_dn_2: 3.245  loss_mask_dn_2: 1.595  loss_dice_dn_2: 2.847  loss_bbox_dn_2: 1.029  loss_giou_dn_2: 0.8118  loss_ce_3: 2.675  loss_mask_3: 1.731  loss_dice_3: 2.936  loss_bbox_3: 2.948  loss_giou_3: 1.815  loss_ce_dn_3: 3.043  loss_mask_dn_3: 1.644  loss_dice_dn_3: 2.793  loss_bbox_dn_3: 0.9832  loss_giou_dn_3: 0.7919  loss_ce_4: 2.665  loss_mask_4: 1.699  loss_dice_4: 2.952  loss_bbox_4: 2.915  loss_giou_4: 1.803  loss_ce_dn_4: 3.075  loss_mask_dn_4: 1.64  loss_dice_dn_4: 2.795  loss_bbox_dn_4: 0.9607  loss_giou_dn_4: 0.7909  loss_ce_5: 2.538  loss_mask_5: 1.719  loss_dice_5: 2.959  loss_bbox_5: 2.947  loss_giou_5: 1.777  loss_ce_dn_5: 2.976  loss_mask_dn_5: 1.604  loss_dice_dn_5: 2.792  loss_bbox_dn_5: 0.9518  loss_giou_dn_5: 0.7902  loss_ce_6: 2.567  loss_mask_6: 1.746  loss_dice_6: 2.966  loss_bbox_6: 2.975  loss_giou_6: 1.737  loss_ce_dn_6: 2.994  loss_mask_dn_6: 1.625  loss_dice_dn_6: 2.841  loss_bbox_dn_6: 0.9511  loss_giou_dn_6: 0.7942  loss_ce_7: 2.683  loss_mask_7: 1.852  loss_dice_7: 2.974  loss_bbox_7: 2.841  loss_giou_7: 1.702  loss_ce_dn_7: 3.138  loss_mask_dn_7: 1.61  loss_dice_dn_7: 2.868  loss_bbox_dn_7: 0.9468  loss_giou_dn_7: 0.7862  loss_ce_8: 2.737  loss_mask_8: 1.829  loss_dice_8: 2.99  loss_bbox_8: 2.858  loss_giou_8: 1.722  loss_ce_dn_8: 3.221  loss_mask_dn_8: 1.601  loss_dice_dn_8: 2.871  loss_bbox_dn_8: 0.9607  loss_giou_dn_8: 0.7848    time: 2.2752  last_time: 2.2714  data_time: 0.0123  last_data_time: 0.0054   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:48:54 d2.utils.events]: \u001b[0m eta: 8 days, 8:42:04  iter: 1399  total_loss: 227.3  loss_ce: 3.08  loss_mask: 1.73  loss_dice: 2.973  loss_bbox: 3.075  loss_giou: 1.671  loss_ce_dn: 3.734  loss_mask_dn: 1.734  loss_dice_dn: 2.76  loss_bbox_dn: 1.023  loss_giou_dn: 0.7683  loss_ce_0: 14.99  loss_mask_0: 1.703  loss_dice_0: 3.236  loss_bbox_0: 3.533  loss_giou_0: 1.848  loss_ce_1: 3.444  loss_mask_1: 1.719  loss_dice_1: 2.973  loss_bbox_1: 3.602  loss_giou_1: 1.81  loss_ce_dn_1: 3.937  loss_mask_dn_1: 1.688  loss_dice_dn_1: 2.994  loss_bbox_dn_1: 1.157  loss_giou_dn_1: 0.8244  loss_ce_2: 3.209  loss_mask_2: 1.697  loss_dice_2: 2.964  loss_bbox_2: 3.358  loss_giou_2: 1.798  loss_ce_dn_2: 3.509  loss_mask_dn_2: 1.679  loss_dice_dn_2: 2.92  loss_bbox_dn_2: 1.1  loss_giou_dn_2: 0.8048  loss_ce_3: 3.049  loss_mask_3: 1.748  loss_dice_3: 2.976  loss_bbox_3: 3.308  loss_giou_3: 1.759  loss_ce_dn_3: 3.415  loss_mask_dn_3: 1.673  loss_dice_dn_3: 2.86  loss_bbox_dn_3: 1.084  loss_giou_dn_3: 0.7752  loss_ce_4: 2.929  loss_mask_4: 1.784  loss_dice_4: 2.934  loss_bbox_4: 3.184  loss_giou_4: 1.703  loss_ce_dn_4: 3.383  loss_mask_dn_4: 1.658  loss_dice_dn_4: 2.799  loss_bbox_dn_4: 1.073  loss_giou_dn_4: 0.7682  loss_ce_5: 2.997  loss_mask_5: 1.755  loss_dice_5: 2.959  loss_bbox_5: 3.148  loss_giou_5: 1.685  loss_ce_dn_5: 3.359  loss_mask_dn_5: 1.665  loss_dice_dn_5: 2.789  loss_bbox_dn_5: 1.057  loss_giou_dn_5: 0.7662  loss_ce_6: 2.981  loss_mask_6: 1.778  loss_dice_6: 2.985  loss_bbox_6: 3.116  loss_giou_6: 1.679  loss_ce_dn_6: 3.291  loss_mask_dn_6: 1.663  loss_dice_dn_6: 2.788  loss_bbox_dn_6: 1.048  loss_giou_dn_6: 0.7682  loss_ce_7: 3.034  loss_mask_7: 1.761  loss_dice_7: 2.933  loss_bbox_7: 3.092  loss_giou_7: 1.678  loss_ce_dn_7: 3.438  loss_mask_dn_7: 1.726  loss_dice_dn_7: 2.774  loss_bbox_dn_7: 1.032  loss_giou_dn_7: 0.7659  loss_ce_8: 3.017  loss_mask_8: 1.789  loss_dice_8: 2.919  loss_bbox_8: 3.108  loss_giou_8: 1.653  loss_ce_dn_8: 3.571  loss_mask_dn_8: 1.711  loss_dice_dn_8: 2.75  loss_bbox_dn_8: 1.023  loss_giou_dn_8: 0.7632    time: 2.2749  last_time: 2.2660  data_time: 0.0115  last_data_time: 0.0055   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:49:39 d2.utils.events]: \u001b[0m eta: 8 days, 8:41:19  iter: 1419  total_loss: 212.4  loss_ce: 2.617  loss_mask: 1.856  loss_dice: 2.572  loss_bbox: 2.978  loss_giou: 1.673  loss_ce_dn: 3.292  loss_mask_dn: 1.675  loss_dice_dn: 2.489  loss_bbox_dn: 1.046  loss_giou_dn: 0.7558  loss_ce_0: 14.98  loss_mask_0: 1.573  loss_dice_0: 3.002  loss_bbox_0: 3.338  loss_giou_0: 1.751  loss_ce_1: 2.816  loss_mask_1: 1.646  loss_dice_1: 2.725  loss_bbox_1: 3.304  loss_giou_1: 1.653  loss_ce_dn_1: 4.061  loss_mask_dn_1: 1.658  loss_dice_dn_1: 2.694  loss_bbox_dn_1: 1.179  loss_giou_dn_1: 0.8058  loss_ce_2: 2.547  loss_mask_2: 1.795  loss_dice_2: 2.636  loss_bbox_2: 3.071  loss_giou_2: 1.583  loss_ce_dn_2: 3.42  loss_mask_dn_2: 1.706  loss_dice_dn_2: 2.559  loss_bbox_dn_2: 1.128  loss_giou_dn_2: 0.7804  loss_ce_3: 2.516  loss_mask_3: 1.716  loss_dice_3: 2.626  loss_bbox_3: 2.967  loss_giou_3: 1.593  loss_ce_dn_3: 3.217  loss_mask_dn_3: 1.705  loss_dice_dn_3: 2.476  loss_bbox_dn_3: 1.059  loss_giou_dn_3: 0.7652  loss_ce_4: 2.515  loss_mask_4: 1.776  loss_dice_4: 2.589  loss_bbox_4: 2.984  loss_giou_4: 1.517  loss_ce_dn_4: 3.116  loss_mask_dn_4: 1.7  loss_dice_dn_4: 2.491  loss_bbox_dn_4: 1.056  loss_giou_dn_4: 0.7564  loss_ce_5: 2.565  loss_mask_5: 1.798  loss_dice_5: 2.581  loss_bbox_5: 2.965  loss_giou_5: 1.556  loss_ce_dn_5: 3.061  loss_mask_dn_5: 1.709  loss_dice_dn_5: 2.432  loss_bbox_dn_5: 1.056  loss_giou_dn_5: 0.7521  loss_ce_6: 2.59  loss_mask_6: 1.824  loss_dice_6: 2.568  loss_bbox_6: 2.975  loss_giou_6: 1.601  loss_ce_dn_6: 3.051  loss_mask_dn_6: 1.701  loss_dice_dn_6: 2.416  loss_bbox_dn_6: 1.062  loss_giou_dn_6: 0.753  loss_ce_7: 2.577  loss_mask_7: 1.817  loss_dice_7: 2.572  loss_bbox_7: 2.976  loss_giou_7: 1.644  loss_ce_dn_7: 3.092  loss_mask_dn_7: 1.701  loss_dice_dn_7: 2.404  loss_bbox_dn_7: 1.055  loss_giou_dn_7: 0.7485  loss_ce_8: 2.611  loss_mask_8: 1.908  loss_dice_8: 2.548  loss_bbox_8: 2.914  loss_giou_8: 1.601  loss_ce_dn_8: 3.18  loss_mask_dn_8: 1.691  loss_dice_dn_8: 2.467  loss_bbox_dn_8: 1.047  loss_giou_dn_8: 0.7517    time: 2.2750  last_time: 2.2502  data_time: 0.0117  last_data_time: 0.0123   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:50:25 d2.utils.events]: \u001b[0m eta: 8 days, 8:40:22  iter: 1439  total_loss: 215.8  loss_ce: 2.577  loss_mask: 1.751  loss_dice: 2.742  loss_bbox: 2.976  loss_giou: 1.678  loss_ce_dn: 3.117  loss_mask_dn: 1.651  loss_dice_dn: 2.672  loss_bbox_dn: 0.9268  loss_giou_dn: 0.8145  loss_ce_0: 14.9  loss_mask_0: 1.708  loss_dice_0: 3.066  loss_bbox_0: 3.399  loss_giou_0: 1.744  loss_ce_1: 3.429  loss_mask_1: 1.608  loss_dice_1: 2.874  loss_bbox_1: 3.316  loss_giou_1: 1.68  loss_ce_dn_1: 3.603  loss_mask_dn_1: 1.586  loss_dice_dn_1: 2.819  loss_bbox_dn_1: 1.099  loss_giou_dn_1: 0.8314  loss_ce_2: 2.812  loss_mask_2: 1.678  loss_dice_2: 2.77  loss_bbox_2: 3.11  loss_giou_2: 1.642  loss_ce_dn_2: 3.105  loss_mask_dn_2: 1.57  loss_dice_dn_2: 2.747  loss_bbox_dn_2: 1.064  loss_giou_dn_2: 0.8153  loss_ce_3: 2.523  loss_mask_3: 1.742  loss_dice_3: 2.702  loss_bbox_3: 3.071  loss_giou_3: 1.635  loss_ce_dn_3: 2.96  loss_mask_dn_3: 1.613  loss_dice_dn_3: 2.73  loss_bbox_dn_3: 1.036  loss_giou_dn_3: 0.7881  loss_ce_4: 2.517  loss_mask_4: 1.715  loss_dice_4: 2.737  loss_bbox_4: 3.053  loss_giou_4: 1.663  loss_ce_dn_4: 2.948  loss_mask_dn_4: 1.666  loss_dice_dn_4: 2.685  loss_bbox_dn_4: 1.002  loss_giou_dn_4: 0.7797  loss_ce_5: 2.344  loss_mask_5: 1.736  loss_dice_5: 2.731  loss_bbox_5: 3.079  loss_giou_5: 1.657  loss_ce_dn_5: 2.981  loss_mask_dn_5: 1.68  loss_dice_dn_5: 2.648  loss_bbox_dn_5: 0.9889  loss_giou_dn_5: 0.7818  loss_ce_6: 2.364  loss_mask_6: 1.742  loss_dice_6: 2.769  loss_bbox_6: 3.063  loss_giou_6: 1.666  loss_ce_dn_6: 3.015  loss_mask_dn_6: 1.708  loss_dice_dn_6: 2.667  loss_bbox_dn_6: 0.9821  loss_giou_dn_6: 0.7927  loss_ce_7: 2.39  loss_mask_7: 1.688  loss_dice_7: 2.781  loss_bbox_7: 3.043  loss_giou_7: 1.659  loss_ce_dn_7: 2.896  loss_mask_dn_7: 1.656  loss_dice_dn_7: 2.681  loss_bbox_dn_7: 0.9566  loss_giou_dn_7: 0.7945  loss_ce_8: 2.486  loss_mask_8: 1.756  loss_dice_8: 2.725  loss_bbox_8: 2.985  loss_giou_8: 1.651  loss_ce_dn_8: 2.971  loss_mask_dn_8: 1.649  loss_dice_dn_8: 2.656  loss_bbox_dn_8: 0.94  loss_giou_dn_8: 0.8049    time: 2.2749  last_time: 2.3185  data_time: 0.0122  last_data_time: 0.0274   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:51:10 d2.utils.events]: \u001b[0m eta: 8 days, 8:39:26  iter: 1459  total_loss: 220.6  loss_ce: 2.421  loss_mask: 1.572  loss_dice: 2.914  loss_bbox: 3.065  loss_giou: 1.677  loss_ce_dn: 3.056  loss_mask_dn: 1.439  loss_dice_dn: 2.803  loss_bbox_dn: 0.9882  loss_giou_dn: 0.7961  loss_ce_0: 14.77  loss_mask_0: 1.537  loss_dice_0: 3.319  loss_bbox_0: 3.453  loss_giou_0: 1.804  loss_ce_1: 2.863  loss_mask_1: 1.522  loss_dice_1: 3.021  loss_bbox_1: 3.794  loss_giou_1: 1.774  loss_ce_dn_1: 3.729  loss_mask_dn_1: 1.509  loss_dice_dn_1: 2.906  loss_bbox_dn_1: 1.098  loss_giou_dn_1: 0.8236  loss_ce_2: 2.532  loss_mask_2: 1.609  loss_dice_2: 2.922  loss_bbox_2: 3.393  loss_giou_2: 1.73  loss_ce_dn_2: 3.078  loss_mask_dn_2: 1.415  loss_dice_dn_2: 2.836  loss_bbox_dn_2: 1.045  loss_giou_dn_2: 0.8121  loss_ce_3: 2.421  loss_mask_3: 1.603  loss_dice_3: 2.91  loss_bbox_3: 3.344  loss_giou_3: 1.675  loss_ce_dn_3: 2.911  loss_mask_dn_3: 1.445  loss_dice_dn_3: 2.795  loss_bbox_dn_3: 1.005  loss_giou_dn_3: 0.8017  loss_ce_4: 2.335  loss_mask_4: 1.616  loss_dice_4: 2.89  loss_bbox_4: 3.268  loss_giou_4: 1.661  loss_ce_dn_4: 2.683  loss_mask_dn_4: 1.399  loss_dice_dn_4: 2.788  loss_bbox_dn_4: 1.002  loss_giou_dn_4: 0.7935  loss_ce_5: 2.391  loss_mask_5: 1.602  loss_dice_5: 2.885  loss_bbox_5: 3.223  loss_giou_5: 1.648  loss_ce_dn_5: 2.697  loss_mask_dn_5: 1.369  loss_dice_dn_5: 2.771  loss_bbox_dn_5: 1.006  loss_giou_dn_5: 0.7905  loss_ce_6: 2.403  loss_mask_6: 1.593  loss_dice_6: 2.899  loss_bbox_6: 3.161  loss_giou_6: 1.665  loss_ce_dn_6: 2.751  loss_mask_dn_6: 1.38  loss_dice_dn_6: 2.762  loss_bbox_dn_6: 1.007  loss_giou_dn_6: 0.7888  loss_ce_7: 2.497  loss_mask_7: 1.634  loss_dice_7: 2.905  loss_bbox_7: 3.085  loss_giou_7: 1.678  loss_ce_dn_7: 2.855  loss_mask_dn_7: 1.426  loss_dice_dn_7: 2.827  loss_bbox_dn_7: 0.9889  loss_giou_dn_7: 0.7906  loss_ce_8: 2.408  loss_mask_8: 1.583  loss_dice_8: 2.887  loss_bbox_8: 3.085  loss_giou_8: 1.664  loss_ce_dn_8: 2.836  loss_mask_dn_8: 1.447  loss_dice_dn_8: 2.834  loss_bbox_dn_8: 0.9837  loss_giou_dn_8: 0.7935    time: 2.2748  last_time: 2.2510  data_time: 0.0123  last_data_time: 0.0048   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:51:56 d2.utils.events]: \u001b[0m eta: 8 days, 8:38:11  iter: 1479  total_loss: 223.6  loss_ce: 3.126  loss_mask: 2.021  loss_dice: 2.789  loss_bbox: 2.82  loss_giou: 1.461  loss_ce_dn: 3.569  loss_mask_dn: 1.851  loss_dice_dn: 2.758  loss_bbox_dn: 1.005  loss_giou_dn: 0.7677  loss_ce_0: 14.86  loss_mask_0: 1.807  loss_dice_0: 3.063  loss_bbox_0: 3.589  loss_giou_0: 1.786  loss_ce_1: 3.744  loss_mask_1: 1.996  loss_dice_1: 2.769  loss_bbox_1: 3.143  loss_giou_1: 1.598  loss_ce_dn_1: 4.18  loss_mask_dn_1: 1.919  loss_dice_dn_1: 2.919  loss_bbox_dn_1: 1.177  loss_giou_dn_1: 0.808  loss_ce_2: 3.442  loss_mask_2: 2.008  loss_dice_2: 2.732  loss_bbox_2: 2.872  loss_giou_2: 1.547  loss_ce_dn_2: 3.474  loss_mask_dn_2: 1.89  loss_dice_dn_2: 2.831  loss_bbox_dn_2: 1.095  loss_giou_dn_2: 0.788  loss_ce_3: 3.239  loss_mask_3: 2.056  loss_dice_3: 2.728  loss_bbox_3: 2.881  loss_giou_3: 1.556  loss_ce_dn_3: 3.226  loss_mask_dn_3: 1.795  loss_dice_dn_3: 2.738  loss_bbox_dn_3: 1.077  loss_giou_dn_3: 0.7726  loss_ce_4: 3.19  loss_mask_4: 2.095  loss_dice_4: 2.739  loss_bbox_4: 2.857  loss_giou_4: 1.542  loss_ce_dn_4: 3.164  loss_mask_dn_4: 1.834  loss_dice_dn_4: 2.709  loss_bbox_dn_4: 1.042  loss_giou_dn_4: 0.766  loss_ce_5: 3.296  loss_mask_5: 2.098  loss_dice_5: 2.76  loss_bbox_5: 2.863  loss_giou_5: 1.526  loss_ce_dn_5: 3.155  loss_mask_dn_5: 1.786  loss_dice_dn_5: 2.691  loss_bbox_dn_5: 1.037  loss_giou_dn_5: 0.7682  loss_ce_6: 3.263  loss_mask_6: 2.045  loss_dice_6: 2.818  loss_bbox_6: 2.845  loss_giou_6: 1.513  loss_ce_dn_6: 3.225  loss_mask_dn_6: 1.816  loss_dice_dn_6: 2.696  loss_bbox_dn_6: 1.036  loss_giou_dn_6: 0.7719  loss_ce_7: 3.124  loss_mask_7: 2.108  loss_dice_7: 2.804  loss_bbox_7: 2.832  loss_giou_7: 1.465  loss_ce_dn_7: 3.224  loss_mask_dn_7: 1.85  loss_dice_dn_7: 2.749  loss_bbox_dn_7: 1.018  loss_giou_dn_7: 0.77  loss_ce_8: 3.036  loss_mask_8: 2.15  loss_dice_8: 2.795  loss_bbox_8: 2.84  loss_giou_8: 1.461  loss_ce_dn_8: 3.315  loss_mask_dn_8: 1.82  loss_dice_dn_8: 2.699  loss_bbox_dn_8: 1.015  loss_giou_dn_8: 0.7651    time: 2.2749  last_time: 2.2826  data_time: 0.0129  last_data_time: 0.0099   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:52:41 d2.utils.events]: \u001b[0m eta: 8 days, 8:36:32  iter: 1499  total_loss: 230.7  loss_ce: 3.005  loss_mask: 1.678  loss_dice: 3.094  loss_bbox: 2.873  loss_giou: 1.657  loss_ce_dn: 3.822  loss_mask_dn: 1.653  loss_dice_dn: 2.95  loss_bbox_dn: 0.9767  loss_giou_dn: 0.8153  loss_ce_0: 14.81  loss_mask_0: 1.524  loss_dice_0: 3.247  loss_bbox_0: 3.494  loss_giou_0: 1.846  loss_ce_1: 3.875  loss_mask_1: 1.655  loss_dice_1: 3.072  loss_bbox_1: 3.386  loss_giou_1: 1.807  loss_ce_dn_1: 4.29  loss_mask_dn_1: 1.539  loss_dice_dn_1: 3.108  loss_bbox_dn_1: 1.009  loss_giou_dn_1: 0.8298  loss_ce_2: 3.429  loss_mask_2: 1.76  loss_dice_2: 3.005  loss_bbox_2: 3.021  loss_giou_2: 1.75  loss_ce_dn_2: 3.568  loss_mask_dn_2: 1.527  loss_dice_dn_2: 2.986  loss_bbox_dn_2: 0.9828  loss_giou_dn_2: 0.8197  loss_ce_3: 3.168  loss_mask_3: 1.734  loss_dice_3: 3.031  loss_bbox_3: 2.864  loss_giou_3: 1.752  loss_ce_dn_3: 3.354  loss_mask_dn_3: 1.57  loss_dice_dn_3: 2.959  loss_bbox_dn_3: 0.9684  loss_giou_dn_3: 0.8093  loss_ce_4: 3.11  loss_mask_4: 1.708  loss_dice_4: 3.04  loss_bbox_4: 2.812  loss_giou_4: 1.744  loss_ce_dn_4: 3.353  loss_mask_dn_4: 1.533  loss_dice_dn_4: 2.924  loss_bbox_dn_4: 0.958  loss_giou_dn_4: 0.8034  loss_ce_5: 3.041  loss_mask_5: 1.704  loss_dice_5: 3.079  loss_bbox_5: 2.826  loss_giou_5: 1.735  loss_ce_dn_5: 3.392  loss_mask_dn_5: 1.563  loss_dice_dn_5: 2.945  loss_bbox_dn_5: 0.9511  loss_giou_dn_5: 0.7993  loss_ce_6: 3.13  loss_mask_6: 1.676  loss_dice_6: 3.099  loss_bbox_6: 2.821  loss_giou_6: 1.703  loss_ce_dn_6: 3.479  loss_mask_dn_6: 1.578  loss_dice_dn_6: 2.91  loss_bbox_dn_6: 0.951  loss_giou_dn_6: 0.7962  loss_ce_7: 3.042  loss_mask_7: 1.651  loss_dice_7: 3.163  loss_bbox_7: 2.847  loss_giou_7: 1.666  loss_ce_dn_7: 3.488  loss_mask_dn_7: 1.607  loss_dice_dn_7: 2.918  loss_bbox_dn_7: 0.9563  loss_giou_dn_7: 0.8018  loss_ce_8: 3.003  loss_mask_8: 1.695  loss_dice_8: 3.141  loss_bbox_8: 2.859  loss_giou_8: 1.659  loss_ce_dn_8: 3.648  loss_mask_dn_8: 1.617  loss_dice_dn_8: 2.945  loss_bbox_dn_8: 0.964  loss_giou_dn_8: 0.8088    time: 2.2749  last_time: 2.2726  data_time: 0.0133  last_data_time: 0.0243   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:53:27 d2.utils.events]: \u001b[0m eta: 8 days, 8:36:54  iter: 1519  total_loss: 220.1  loss_ce: 2.977  loss_mask: 1.701  loss_dice: 2.878  loss_bbox: 2.608  loss_giou: 1.628  loss_ce_dn: 3.541  loss_mask_dn: 1.574  loss_dice_dn: 2.704  loss_bbox_dn: 0.9356  loss_giou_dn: 0.7402  loss_ce_0: 14.48  loss_mask_0: 1.676  loss_dice_0: 3.162  loss_bbox_0: 3.567  loss_giou_0: 1.78  loss_ce_1: 3.49  loss_mask_1: 1.671  loss_dice_1: 2.85  loss_bbox_1: 3.109  loss_giou_1: 1.668  loss_ce_dn_1: 4.453  loss_mask_dn_1: 1.643  loss_dice_dn_1: 2.778  loss_bbox_dn_1: 1.144  loss_giou_dn_1: 0.8073  loss_ce_2: 3.087  loss_mask_2: 1.629  loss_dice_2: 2.837  loss_bbox_2: 2.853  loss_giou_2: 1.629  loss_ce_dn_2: 3.38  loss_mask_dn_2: 1.593  loss_dice_dn_2: 2.632  loss_bbox_dn_2: 1.095  loss_giou_dn_2: 0.7871  loss_ce_3: 2.802  loss_mask_3: 1.631  loss_dice_3: 2.906  loss_bbox_3: 2.808  loss_giou_3: 1.647  loss_ce_dn_3: 3.309  loss_mask_dn_3: 1.594  loss_dice_dn_3: 2.601  loss_bbox_dn_3: 1.026  loss_giou_dn_3: 0.7689  loss_ce_4: 2.84  loss_mask_4: 1.657  loss_dice_4: 2.938  loss_bbox_4: 2.803  loss_giou_4: 1.658  loss_ce_dn_4: 3.249  loss_mask_dn_4: 1.571  loss_dice_dn_4: 2.64  loss_bbox_dn_4: 1.001  loss_giou_dn_4: 0.7567  loss_ce_5: 2.745  loss_mask_5: 1.698  loss_dice_5: 2.918  loss_bbox_5: 2.718  loss_giou_5: 1.612  loss_ce_dn_5: 3.282  loss_mask_dn_5: 1.544  loss_dice_dn_5: 2.623  loss_bbox_dn_5: 0.9767  loss_giou_dn_5: 0.7515  loss_ce_6: 2.662  loss_mask_6: 1.675  loss_dice_6: 2.888  loss_bbox_6: 2.678  loss_giou_6: 1.613  loss_ce_dn_6: 3.2  loss_mask_dn_6: 1.582  loss_dice_dn_6: 2.646  loss_bbox_dn_6: 0.9814  loss_giou_dn_6: 0.7495  loss_ce_7: 2.666  loss_mask_7: 1.626  loss_dice_7: 2.865  loss_bbox_7: 2.616  loss_giou_7: 1.635  loss_ce_dn_7: 3.259  loss_mask_dn_7: 1.54  loss_dice_dn_7: 2.596  loss_bbox_dn_7: 0.9553  loss_giou_dn_7: 0.7427  loss_ce_8: 2.705  loss_mask_8: 1.655  loss_dice_8: 2.827  loss_bbox_8: 2.603  loss_giou_8: 1.564  loss_ce_dn_8: 3.451  loss_mask_dn_8: 1.584  loss_dice_dn_8: 2.607  loss_bbox_dn_8: 0.9419  loss_giou_dn_8: 0.741    time: 2.2750  last_time: 2.2916  data_time: 0.0113  last_data_time: 0.0050   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:54:13 d2.utils.events]: \u001b[0m eta: 8 days, 8:36:09  iter: 1539  total_loss: 218.3  loss_ce: 2.898  loss_mask: 1.657  loss_dice: 2.731  loss_bbox: 3.175  loss_giou: 1.675  loss_ce_dn: 3.479  loss_mask_dn: 1.55  loss_dice_dn: 2.633  loss_bbox_dn: 1.059  loss_giou_dn: 0.8045  loss_ce_0: 14.3  loss_mask_0: 1.604  loss_dice_0: 3.064  loss_bbox_0: 3.325  loss_giou_0: 1.795  loss_ce_1: 3.381  loss_mask_1: 1.843  loss_dice_1: 2.843  loss_bbox_1: 3.539  loss_giou_1: 1.764  loss_ce_dn_1: 3.86  loss_mask_dn_1: 1.615  loss_dice_dn_1: 2.882  loss_bbox_dn_1: 1.122  loss_giou_dn_1: 0.8196  loss_ce_2: 3.071  loss_mask_2: 1.746  loss_dice_2: 2.811  loss_bbox_2: 3.234  loss_giou_2: 1.71  loss_ce_dn_2: 3.077  loss_mask_dn_2: 1.552  loss_dice_dn_2: 2.707  loss_bbox_dn_2: 1.056  loss_giou_dn_2: 0.8061  loss_ce_3: 2.881  loss_mask_3: 1.723  loss_dice_3: 2.783  loss_bbox_3: 3.211  loss_giou_3: 1.656  loss_ce_dn_3: 2.937  loss_mask_dn_3: 1.6  loss_dice_dn_3: 2.641  loss_bbox_dn_3: 1.044  loss_giou_dn_3: 0.7894  loss_ce_4: 2.629  loss_mask_4: 1.775  loss_dice_4: 2.773  loss_bbox_4: 3.236  loss_giou_4: 1.65  loss_ce_dn_4: 2.925  loss_mask_dn_4: 1.578  loss_dice_dn_4: 2.624  loss_bbox_dn_4: 1.018  loss_giou_dn_4: 0.7808  loss_ce_5: 2.625  loss_mask_5: 1.767  loss_dice_5: 2.782  loss_bbox_5: 3.205  loss_giou_5: 1.664  loss_ce_dn_5: 3.001  loss_mask_dn_5: 1.56  loss_dice_dn_5: 2.649  loss_bbox_dn_5: 1.031  loss_giou_dn_5: 0.785  loss_ce_6: 2.835  loss_mask_6: 1.699  loss_dice_6: 2.798  loss_bbox_6: 3.05  loss_giou_6: 1.671  loss_ce_dn_6: 3.063  loss_mask_dn_6: 1.584  loss_dice_dn_6: 2.671  loss_bbox_dn_6: 1.042  loss_giou_dn_6: 0.7892  loss_ce_7: 2.845  loss_mask_7: 1.751  loss_dice_7: 2.841  loss_bbox_7: 3.195  loss_giou_7: 1.659  loss_ce_dn_7: 3.185  loss_mask_dn_7: 1.536  loss_dice_dn_7: 2.645  loss_bbox_dn_7: 1.047  loss_giou_dn_7: 0.7908  loss_ce_8: 2.877  loss_mask_8: 1.663  loss_dice_8: 2.736  loss_bbox_8: 3.163  loss_giou_8: 1.667  loss_ce_dn_8: 3.266  loss_mask_dn_8: 1.565  loss_dice_dn_8: 2.632  loss_bbox_dn_8: 1.052  loss_giou_dn_8: 0.7964    time: 2.2750  last_time: 2.2598  data_time: 0.0136  last_data_time: 0.0072   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:54:58 d2.utils.events]: \u001b[0m eta: 8 days, 8:33:51  iter: 1559  total_loss: 211.3  loss_ce: 2.353  loss_mask: 1.836  loss_dice: 3.047  loss_bbox: 2.868  loss_giou: 1.652  loss_ce_dn: 3.205  loss_mask_dn: 1.609  loss_dice_dn: 2.843  loss_bbox_dn: 1.06  loss_giou_dn: 0.8106  loss_ce_0: 14.48  loss_mask_0: 1.682  loss_dice_0: 3.199  loss_bbox_0: 3.417  loss_giou_0: 1.849  loss_ce_1: 2.534  loss_mask_1: 1.709  loss_dice_1: 3.067  loss_bbox_1: 3.242  loss_giou_1: 1.757  loss_ce_dn_1: 3.649  loss_mask_dn_1: 1.662  loss_dice_dn_1: 2.986  loss_bbox_dn_1: 1.161  loss_giou_dn_1: 0.8289  loss_ce_2: 2.315  loss_mask_2: 1.726  loss_dice_2: 3.09  loss_bbox_2: 2.927  loss_giou_2: 1.772  loss_ce_dn_2: 2.916  loss_mask_dn_2: 1.612  loss_dice_dn_2: 2.893  loss_bbox_dn_2: 1.125  loss_giou_dn_2: 0.8087  loss_ce_3: 2.205  loss_mask_3: 1.759  loss_dice_3: 3.046  loss_bbox_3: 2.895  loss_giou_3: 1.747  loss_ce_dn_3: 2.816  loss_mask_dn_3: 1.65  loss_dice_dn_3: 2.823  loss_bbox_dn_3: 1.087  loss_giou_dn_3: 0.8005  loss_ce_4: 2.22  loss_mask_4: 1.811  loss_dice_4: 2.991  loss_bbox_4: 2.87  loss_giou_4: 1.68  loss_ce_dn_4: 2.801  loss_mask_dn_4: 1.718  loss_dice_dn_4: 2.829  loss_bbox_dn_4: 1.082  loss_giou_dn_4: 0.7944  loss_ce_5: 2.166  loss_mask_5: 1.771  loss_dice_5: 3.041  loss_bbox_5: 2.937  loss_giou_5: 1.675  loss_ce_dn_5: 2.763  loss_mask_dn_5: 1.644  loss_dice_dn_5: 2.845  loss_bbox_dn_5: 1.08  loss_giou_dn_5: 0.8002  loss_ce_6: 2.306  loss_mask_6: 1.778  loss_dice_6: 3.097  loss_bbox_6: 2.885  loss_giou_6: 1.622  loss_ce_dn_6: 2.812  loss_mask_dn_6: 1.629  loss_dice_dn_6: 2.866  loss_bbox_dn_6: 1.079  loss_giou_dn_6: 0.8069  loss_ce_7: 2.267  loss_mask_7: 1.8  loss_dice_7: 2.978  loss_bbox_7: 2.955  loss_giou_7: 1.591  loss_ce_dn_7: 2.852  loss_mask_dn_7: 1.628  loss_dice_dn_7: 2.884  loss_bbox_dn_7: 1.068  loss_giou_dn_7: 0.8098  loss_ce_8: 2.337  loss_mask_8: 1.778  loss_dice_8: 2.996  loss_bbox_8: 2.851  loss_giou_8: 1.597  loss_ce_dn_8: 3.015  loss_mask_dn_8: 1.603  loss_dice_dn_8: 2.859  loss_bbox_dn_8: 1.061  loss_giou_dn_8: 0.81    time: 2.2752  last_time: 2.2661  data_time: 0.0141  last_data_time: 0.0161   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:55:44 d2.utils.events]: \u001b[0m eta: 8 days, 8:32:48  iter: 1579  total_loss: 210.4  loss_ce: 3.025  loss_mask: 1.548  loss_dice: 2.772  loss_bbox: 2.777  loss_giou: 1.65  loss_ce_dn: 3.187  loss_mask_dn: 1.351  loss_dice_dn: 2.549  loss_bbox_dn: 0.9002  loss_giou_dn: 0.7618  loss_ce_0: 14.19  loss_mask_0: 1.374  loss_dice_0: 3.043  loss_bbox_0: 3.296  loss_giou_0: 1.883  loss_ce_1: 3.25  loss_mask_1: 1.474  loss_dice_1: 2.785  loss_bbox_1: 3.146  loss_giou_1: 1.725  loss_ce_dn_1: 4.041  loss_mask_dn_1: 1.438  loss_dice_dn_1: 2.746  loss_bbox_dn_1: 1.047  loss_giou_dn_1: 0.8104  loss_ce_2: 3.09  loss_mask_2: 1.427  loss_dice_2: 2.821  loss_bbox_2: 2.789  loss_giou_2: 1.683  loss_ce_dn_2: 3.17  loss_mask_dn_2: 1.344  loss_dice_dn_2: 2.636  loss_bbox_dn_2: 0.9776  loss_giou_dn_2: 0.7839  loss_ce_3: 3.052  loss_mask_3: 1.416  loss_dice_3: 2.744  loss_bbox_3: 2.687  loss_giou_3: 1.615  loss_ce_dn_3: 2.907  loss_mask_dn_3: 1.343  loss_dice_dn_3: 2.613  loss_bbox_dn_3: 0.9317  loss_giou_dn_3: 0.7615  loss_ce_4: 3.006  loss_mask_4: 1.453  loss_dice_4: 2.788  loss_bbox_4: 2.649  loss_giou_4: 1.588  loss_ce_dn_4: 2.785  loss_mask_dn_4: 1.338  loss_dice_dn_4: 2.585  loss_bbox_dn_4: 0.9167  loss_giou_dn_4: 0.7505  loss_ce_5: 2.982  loss_mask_5: 1.454  loss_dice_5: 2.741  loss_bbox_5: 2.653  loss_giou_5: 1.6  loss_ce_dn_5: 2.789  loss_mask_dn_5: 1.344  loss_dice_dn_5: 2.574  loss_bbox_dn_5: 0.9063  loss_giou_dn_5: 0.7533  loss_ce_6: 2.877  loss_mask_6: 1.5  loss_dice_6: 2.734  loss_bbox_6: 2.734  loss_giou_6: 1.604  loss_ce_dn_6: 2.846  loss_mask_dn_6: 1.342  loss_dice_dn_6: 2.585  loss_bbox_dn_6: 0.8996  loss_giou_dn_6: 0.7506  loss_ce_7: 2.928  loss_mask_7: 1.528  loss_dice_7: 2.784  loss_bbox_7: 2.752  loss_giou_7: 1.599  loss_ce_dn_7: 2.908  loss_mask_dn_7: 1.358  loss_dice_dn_7: 2.59  loss_bbox_dn_7: 0.8877  loss_giou_dn_7: 0.7473  loss_ce_8: 3.051  loss_mask_8: 1.525  loss_dice_8: 2.782  loss_bbox_8: 2.762  loss_giou_8: 1.606  loss_ce_dn_8: 2.996  loss_mask_dn_8: 1.359  loss_dice_dn_8: 2.565  loss_bbox_dn_8: 0.8872  loss_giou_dn_8: 0.751    time: 2.2750  last_time: 2.2374  data_time: 0.0104  last_data_time: 0.0054   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:56:29 d2.utils.events]: \u001b[0m eta: 8 days, 8:31:02  iter: 1599  total_loss: 221.6  loss_ce: 2.786  loss_mask: 1.631  loss_dice: 3.017  loss_bbox: 3.074  loss_giou: 1.738  loss_ce_dn: 3.727  loss_mask_dn: 1.52  loss_dice_dn: 2.728  loss_bbox_dn: 0.9467  loss_giou_dn: 0.7644  loss_ce_0: 14.34  loss_mask_0: 1.522  loss_dice_0: 3.145  loss_bbox_0: 3.361  loss_giou_0: 1.806  loss_ce_1: 3.405  loss_mask_1: 1.651  loss_dice_1: 3.002  loss_bbox_1: 3.38  loss_giou_1: 1.752  loss_ce_dn_1: 4.287  loss_mask_dn_1: 1.542  loss_dice_dn_1: 3.046  loss_bbox_dn_1: 1.041  loss_giou_dn_1: 0.8135  loss_ce_2: 2.927  loss_mask_2: 1.616  loss_dice_2: 2.969  loss_bbox_2: 3.082  loss_giou_2: 1.607  loss_ce_dn_2: 3.464  loss_mask_dn_2: 1.511  loss_dice_dn_2: 2.828  loss_bbox_dn_2: 1.02  loss_giou_dn_2: 0.7943  loss_ce_3: 2.909  loss_mask_3: 1.648  loss_dice_3: 2.962  loss_bbox_3: 2.955  loss_giou_3: 1.588  loss_ce_dn_3: 3.247  loss_mask_dn_3: 1.5  loss_dice_dn_3: 2.768  loss_bbox_dn_3: 0.9779  loss_giou_dn_3: 0.7774  loss_ce_4: 2.813  loss_mask_4: 1.6  loss_dice_4: 2.921  loss_bbox_4: 3.002  loss_giou_4: 1.643  loss_ce_dn_4: 3.266  loss_mask_dn_4: 1.504  loss_dice_dn_4: 2.76  loss_bbox_dn_4: 0.9584  loss_giou_dn_4: 0.7768  loss_ce_5: 2.738  loss_mask_5: 1.629  loss_dice_5: 2.972  loss_bbox_5: 3.017  loss_giou_5: 1.677  loss_ce_dn_5: 3.327  loss_mask_dn_5: 1.487  loss_dice_dn_5: 2.764  loss_bbox_dn_5: 0.9603  loss_giou_dn_5: 0.7772  loss_ce_6: 2.849  loss_mask_6: 1.623  loss_dice_6: 2.95  loss_bbox_6: 3.048  loss_giou_6: 1.706  loss_ce_dn_6: 3.348  loss_mask_dn_6: 1.481  loss_dice_dn_6: 2.736  loss_bbox_dn_6: 0.9588  loss_giou_dn_6: 0.7756  loss_ce_7: 2.714  loss_mask_7: 1.607  loss_dice_7: 2.992  loss_bbox_7: 3.007  loss_giou_7: 1.683  loss_ce_dn_7: 3.466  loss_mask_dn_7: 1.483  loss_dice_dn_7: 2.712  loss_bbox_dn_7: 0.9529  loss_giou_dn_7: 0.764  loss_ce_8: 2.652  loss_mask_8: 1.619  loss_dice_8: 2.983  loss_bbox_8: 3.064  loss_giou_8: 1.723  loss_ce_dn_8: 3.565  loss_mask_dn_8: 1.485  loss_dice_dn_8: 2.746  loss_bbox_dn_8: 0.9453  loss_giou_dn_8: 0.7638    time: 2.2751  last_time: 2.3777  data_time: 0.0130  last_data_time: 0.0145   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:57:15 d2.utils.events]: \u001b[0m eta: 8 days, 8:31:26  iter: 1619  total_loss: 212.9  loss_ce: 2.361  loss_mask: 1.668  loss_dice: 2.84  loss_bbox: 2.686  loss_giou: 1.698  loss_ce_dn: 2.627  loss_mask_dn: 1.325  loss_dice_dn: 2.728  loss_bbox_dn: 0.9155  loss_giou_dn: 0.8376  loss_ce_0: 14.1  loss_mask_0: 1.422  loss_dice_0: 3.248  loss_bbox_0: 3.614  loss_giou_0: 1.973  loss_ce_1: 2.778  loss_mask_1: 1.626  loss_dice_1: 3.04  loss_bbox_1: 3.223  loss_giou_1: 1.825  loss_ce_dn_1: 3.338  loss_mask_dn_1: 1.41  loss_dice_dn_1: 2.928  loss_bbox_dn_1: 0.9935  loss_giou_dn_1: 0.8227  loss_ce_2: 2.509  loss_mask_2: 1.745  loss_dice_2: 2.936  loss_bbox_2: 2.934  loss_giou_2: 1.755  loss_ce_dn_2: 2.737  loss_mask_dn_2: 1.408  loss_dice_dn_2: 2.804  loss_bbox_dn_2: 0.9908  loss_giou_dn_2: 0.8123  loss_ce_3: 2.389  loss_mask_3: 1.784  loss_dice_3: 2.934  loss_bbox_3: 2.865  loss_giou_3: 1.731  loss_ce_dn_3: 2.592  loss_mask_dn_3: 1.451  loss_dice_dn_3: 2.741  loss_bbox_dn_3: 0.9745  loss_giou_dn_3: 0.8026  loss_ce_4: 2.286  loss_mask_4: 1.724  loss_dice_4: 2.866  loss_bbox_4: 2.867  loss_giou_4: 1.703  loss_ce_dn_4: 2.514  loss_mask_dn_4: 1.406  loss_dice_dn_4: 2.69  loss_bbox_dn_4: 0.9581  loss_giou_dn_4: 0.8009  loss_ce_5: 2.306  loss_mask_5: 1.712  loss_dice_5: 2.853  loss_bbox_5: 2.797  loss_giou_5: 1.685  loss_ce_dn_5: 2.447  loss_mask_dn_5: 1.352  loss_dice_dn_5: 2.705  loss_bbox_dn_5: 0.9493  loss_giou_dn_5: 0.7981  loss_ce_6: 2.396  loss_mask_6: 1.674  loss_dice_6: 2.834  loss_bbox_6: 2.754  loss_giou_6: 1.707  loss_ce_dn_6: 2.463  loss_mask_dn_6: 1.348  loss_dice_dn_6: 2.705  loss_bbox_dn_6: 0.9425  loss_giou_dn_6: 0.8005  loss_ce_7: 2.373  loss_mask_7: 1.726  loss_dice_7: 2.843  loss_bbox_7: 2.668  loss_giou_7: 1.68  loss_ce_dn_7: 2.446  loss_mask_dn_7: 1.392  loss_dice_dn_7: 2.71  loss_bbox_dn_7: 0.9199  loss_giou_dn_7: 0.8064  loss_ce_8: 2.354  loss_mask_8: 1.701  loss_dice_8: 2.83  loss_bbox_8: 2.646  loss_giou_8: 1.699  loss_ce_dn_8: 2.514  loss_mask_dn_8: 1.323  loss_dice_dn_8: 2.704  loss_bbox_dn_8: 0.9163  loss_giou_dn_8: 0.8227    time: 2.2752  last_time: 2.2613  data_time: 0.0136  last_data_time: 0.0074   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:58:00 d2.utils.events]: \u001b[0m eta: 8 days, 8:31:03  iter: 1639  total_loss: 215.4  loss_ce: 2.551  loss_mask: 1.631  loss_dice: 2.88  loss_bbox: 2.855  loss_giou: 1.624  loss_ce_dn: 3.474  loss_mask_dn: 1.309  loss_dice_dn: 2.836  loss_bbox_dn: 0.925  loss_giou_dn: 0.8021  loss_ce_0: 14.34  loss_mask_0: 1.263  loss_dice_0: 3.06  loss_bbox_0: 3.665  loss_giou_0: 1.905  loss_ce_1: 3.245  loss_mask_1: 1.532  loss_dice_1: 2.827  loss_bbox_1: 3.25  loss_giou_1: 1.816  loss_ce_dn_1: 3.801  loss_mask_dn_1: 1.367  loss_dice_dn_1: 2.826  loss_bbox_dn_1: 1.017  loss_giou_dn_1: 0.8219  loss_ce_2: 3.064  loss_mask_2: 1.497  loss_dice_2: 2.755  loss_bbox_2: 2.92  loss_giou_2: 1.713  loss_ce_dn_2: 3.07  loss_mask_dn_2: 1.289  loss_dice_dn_2: 2.78  loss_bbox_dn_2: 0.9793  loss_giou_dn_2: 0.7924  loss_ce_3: 2.838  loss_mask_3: 1.482  loss_dice_3: 2.794  loss_bbox_3: 2.762  loss_giou_3: 1.675  loss_ce_dn_3: 2.931  loss_mask_dn_3: 1.273  loss_dice_dn_3: 2.73  loss_bbox_dn_3: 0.9426  loss_giou_dn_3: 0.772  loss_ce_4: 2.542  loss_mask_4: 1.57  loss_dice_4: 2.793  loss_bbox_4: 2.817  loss_giou_4: 1.75  loss_ce_dn_4: 2.903  loss_mask_dn_4: 1.291  loss_dice_dn_4: 2.761  loss_bbox_dn_4: 0.9336  loss_giou_dn_4: 0.7614  loss_ce_5: 2.408  loss_mask_5: 1.557  loss_dice_5: 2.817  loss_bbox_5: 2.866  loss_giou_5: 1.749  loss_ce_dn_5: 2.983  loss_mask_dn_5: 1.25  loss_dice_dn_5: 2.731  loss_bbox_dn_5: 0.928  loss_giou_dn_5: 0.7676  loss_ce_6: 2.398  loss_mask_6: 1.593  loss_dice_6: 2.85  loss_bbox_6: 2.849  loss_giou_6: 1.718  loss_ce_dn_6: 3.006  loss_mask_dn_6: 1.284  loss_dice_dn_6: 2.758  loss_bbox_dn_6: 0.9361  loss_giou_dn_6: 0.7762  loss_ce_7: 2.472  loss_mask_7: 1.604  loss_dice_7: 2.832  loss_bbox_7: 2.847  loss_giou_7: 1.69  loss_ce_dn_7: 3.09  loss_mask_dn_7: 1.312  loss_dice_dn_7: 2.799  loss_bbox_dn_7: 0.9199  loss_giou_dn_7: 0.7822  loss_ce_8: 2.542  loss_mask_8: 1.619  loss_dice_8: 2.883  loss_bbox_8: 2.844  loss_giou_8: 1.66  loss_ce_dn_8: 3.309  loss_mask_dn_8: 1.326  loss_dice_dn_8: 2.796  loss_bbox_dn_8: 0.9198  loss_giou_dn_8: 0.7909    time: 2.2751  last_time: 2.2447  data_time: 0.0128  last_data_time: 0.0074   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:58:46 d2.utils.events]: \u001b[0m eta: 8 days, 8:30:04  iter: 1659  total_loss: 219.2  loss_ce: 2.317  loss_mask: 1.943  loss_dice: 2.882  loss_bbox: 3.301  loss_giou: 1.632  loss_ce_dn: 2.951  loss_mask_dn: 1.718  loss_dice_dn: 2.726  loss_bbox_dn: 0.9219  loss_giou_dn: 0.7808  loss_ce_0: 14.16  loss_mask_0: 1.61  loss_dice_0: 3.09  loss_bbox_0: 3.408  loss_giou_0: 1.835  loss_ce_1: 3.083  loss_mask_1: 1.748  loss_dice_1: 2.807  loss_bbox_1: 3.195  loss_giou_1: 1.744  loss_ce_dn_1: 3.534  loss_mask_dn_1: 1.604  loss_dice_dn_1: 2.841  loss_bbox_dn_1: 1.034  loss_giou_dn_1: 0.8134  loss_ce_2: 2.525  loss_mask_2: 1.828  loss_dice_2: 2.848  loss_bbox_2: 3.154  loss_giou_2: 1.684  loss_ce_dn_2: 2.825  loss_mask_dn_2: 1.639  loss_dice_dn_2: 2.721  loss_bbox_dn_2: 0.9848  loss_giou_dn_2: 0.7965  loss_ce_3: 2.401  loss_mask_3: 1.876  loss_dice_3: 2.822  loss_bbox_3: 3.182  loss_giou_3: 1.667  loss_ce_dn_3: 2.666  loss_mask_dn_3: 1.658  loss_dice_dn_3: 2.681  loss_bbox_dn_3: 0.9403  loss_giou_dn_3: 0.7675  loss_ce_4: 2.267  loss_mask_4: 1.866  loss_dice_4: 2.881  loss_bbox_4: 3.165  loss_giou_4: 1.636  loss_ce_dn_4: 2.558  loss_mask_dn_4: 1.696  loss_dice_dn_4: 2.692  loss_bbox_dn_4: 0.9181  loss_giou_dn_4: 0.7588  loss_ce_5: 2.235  loss_mask_5: 1.913  loss_dice_5: 2.884  loss_bbox_5: 3.201  loss_giou_5: 1.643  loss_ce_dn_5: 2.688  loss_mask_dn_5: 1.667  loss_dice_dn_5: 2.701  loss_bbox_dn_5: 0.914  loss_giou_dn_5: 0.7632  loss_ce_6: 2.302  loss_mask_6: 1.875  loss_dice_6: 2.856  loss_bbox_6: 3.256  loss_giou_6: 1.637  loss_ce_dn_6: 2.671  loss_mask_dn_6: 1.668  loss_dice_dn_6: 2.662  loss_bbox_dn_6: 0.9096  loss_giou_dn_6: 0.7724  loss_ce_7: 2.253  loss_mask_7: 1.966  loss_dice_7: 2.837  loss_bbox_7: 3.269  loss_giou_7: 1.622  loss_ce_dn_7: 2.754  loss_mask_dn_7: 1.667  loss_dice_dn_7: 2.654  loss_bbox_dn_7: 0.9  loss_giou_dn_7: 0.7649  loss_ce_8: 2.277  loss_mask_8: 1.942  loss_dice_8: 2.928  loss_bbox_8: 3.288  loss_giou_8: 1.63  loss_ce_dn_8: 2.904  loss_mask_dn_8: 1.665  loss_dice_dn_8: 2.675  loss_bbox_dn_8: 0.9071  loss_giou_dn_8: 0.768    time: 2.2750  last_time: 2.2546  data_time: 0.0123  last_data_time: 0.0182   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 15:59:31 d2.utils.events]: \u001b[0m eta: 8 days, 8:28:13  iter: 1679  total_loss: 215.8  loss_ce: 2.873  loss_mask: 1.888  loss_dice: 2.868  loss_bbox: 2.751  loss_giou: 1.749  loss_ce_dn: 3.053  loss_mask_dn: 1.634  loss_dice_dn: 2.774  loss_bbox_dn: 0.9657  loss_giou_dn: 0.7425  loss_ce_0: 14.11  loss_mask_0: 1.671  loss_dice_0: 3.116  loss_bbox_0: 3.3  loss_giou_0: 1.875  loss_ce_1: 3.252  loss_mask_1: 1.716  loss_dice_1: 2.942  loss_bbox_1: 2.985  loss_giou_1: 1.763  loss_ce_dn_1: 3.663  loss_mask_dn_1: 1.704  loss_dice_dn_1: 2.938  loss_bbox_dn_1: 1.107  loss_giou_dn_1: 0.8128  loss_ce_2: 2.865  loss_mask_2: 1.778  loss_dice_2: 2.842  loss_bbox_2: 2.894  loss_giou_2: 1.713  loss_ce_dn_2: 3.082  loss_mask_dn_2: 1.694  loss_dice_dn_2: 2.831  loss_bbox_dn_2: 1.063  loss_giou_dn_2: 0.7921  loss_ce_3: 2.63  loss_mask_3: 1.768  loss_dice_3: 2.886  loss_bbox_3: 2.891  loss_giou_3: 1.731  loss_ce_dn_3: 2.877  loss_mask_dn_3: 1.662  loss_dice_dn_3: 2.803  loss_bbox_dn_3: 1.026  loss_giou_dn_3: 0.7733  loss_ce_4: 2.619  loss_mask_4: 1.767  loss_dice_4: 2.859  loss_bbox_4: 2.863  loss_giou_4: 1.725  loss_ce_dn_4: 2.819  loss_mask_dn_4: 1.689  loss_dice_dn_4: 2.793  loss_bbox_dn_4: 1.016  loss_giou_dn_4: 0.765  loss_ce_5: 2.628  loss_mask_5: 1.797  loss_dice_5: 2.88  loss_bbox_5: 2.834  loss_giou_5: 1.722  loss_ce_dn_5: 2.887  loss_mask_dn_5: 1.676  loss_dice_dn_5: 2.818  loss_bbox_dn_5: 0.9999  loss_giou_dn_5: 0.7622  loss_ce_6: 2.853  loss_mask_6: 1.781  loss_dice_6: 2.845  loss_bbox_6: 2.789  loss_giou_6: 1.768  loss_ce_dn_6: 2.856  loss_mask_dn_6: 1.711  loss_dice_dn_6: 2.788  loss_bbox_dn_6: 0.9946  loss_giou_dn_6: 0.7569  loss_ce_7: 2.789  loss_mask_7: 1.816  loss_dice_7: 2.92  loss_bbox_7: 2.75  loss_giou_7: 1.766  loss_ce_dn_7: 2.89  loss_mask_dn_7: 1.746  loss_dice_dn_7: 2.79  loss_bbox_dn_7: 0.9635  loss_giou_dn_7: 0.7473  loss_ce_8: 2.801  loss_mask_8: 1.891  loss_dice_8: 2.867  loss_bbox_8: 2.75  loss_giou_8: 1.741  loss_ce_dn_8: 2.939  loss_mask_dn_8: 1.679  loss_dice_dn_8: 2.749  loss_bbox_dn_8: 0.9652  loss_giou_dn_8: 0.7434    time: 2.2748  last_time: 2.2790  data_time: 0.0123  last_data_time: 0.0229   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:00:17 d2.utils.events]: \u001b[0m eta: 8 days, 8:27:15  iter: 1699  total_loss: 221.8  loss_ce: 2.923  loss_mask: 1.715  loss_dice: 2.834  loss_bbox: 2.866  loss_giou: 1.457  loss_ce_dn: 3.823  loss_mask_dn: 1.65  loss_dice_dn: 2.728  loss_bbox_dn: 1.015  loss_giou_dn: 0.7691  loss_ce_0: 13.95  loss_mask_0: 1.648  loss_dice_0: 3.068  loss_bbox_0: 3.507  loss_giou_0: 1.853  loss_ce_1: 3.391  loss_mask_1: 1.637  loss_dice_1: 2.784  loss_bbox_1: 3.25  loss_giou_1: 1.645  loss_ce_dn_1: 4.377  loss_mask_dn_1: 1.709  loss_dice_dn_1: 2.926  loss_bbox_dn_1: 1.16  loss_giou_dn_1: 0.8198  loss_ce_2: 3.024  loss_mask_2: 1.703  loss_dice_2: 2.787  loss_bbox_2: 3.138  loss_giou_2: 1.582  loss_ce_dn_2: 3.838  loss_mask_dn_2: 1.682  loss_dice_dn_2: 2.818  loss_bbox_dn_2: 1.102  loss_giou_dn_2: 0.7938  loss_ce_3: 3.028  loss_mask_3: 1.734  loss_dice_3: 2.777  loss_bbox_3: 3.056  loss_giou_3: 1.524  loss_ce_dn_3: 3.538  loss_mask_dn_3: 1.677  loss_dice_dn_3: 2.762  loss_bbox_dn_3: 1.049  loss_giou_dn_3: 0.7751  loss_ce_4: 2.878  loss_mask_4: 1.739  loss_dice_4: 2.84  loss_bbox_4: 2.954  loss_giou_4: 1.517  loss_ce_dn_4: 3.345  loss_mask_dn_4: 1.663  loss_dice_dn_4: 2.75  loss_bbox_dn_4: 1.033  loss_giou_dn_4: 0.7719  loss_ce_5: 2.884  loss_mask_5: 1.738  loss_dice_5: 2.83  loss_bbox_5: 2.946  loss_giou_5: 1.56  loss_ce_dn_5: 3.358  loss_mask_dn_5: 1.639  loss_dice_dn_5: 2.757  loss_bbox_dn_5: 1.018  loss_giou_dn_5: 0.7698  loss_ce_6: 2.847  loss_mask_6: 1.734  loss_dice_6: 2.876  loss_bbox_6: 2.835  loss_giou_6: 1.499  loss_ce_dn_6: 3.413  loss_mask_dn_6: 1.648  loss_dice_dn_6: 2.731  loss_bbox_dn_6: 1.017  loss_giou_dn_6: 0.771  loss_ce_7: 2.89  loss_mask_7: 1.76  loss_dice_7: 2.891  loss_bbox_7: 2.837  loss_giou_7: 1.477  loss_ce_dn_7: 3.477  loss_mask_dn_7: 1.715  loss_dice_dn_7: 2.748  loss_bbox_dn_7: 1.004  loss_giou_dn_7: 0.7632  loss_ce_8: 2.866  loss_mask_8: 1.763  loss_dice_8: 2.901  loss_bbox_8: 2.858  loss_giou_8: 1.471  loss_ce_dn_8: 3.577  loss_mask_dn_8: 1.627  loss_dice_dn_8: 2.734  loss_bbox_dn_8: 1.008  loss_giou_dn_8: 0.7646    time: 2.2748  last_time: 2.2668  data_time: 0.0149  last_data_time: 0.0181   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:01:02 d2.utils.events]: \u001b[0m eta: 8 days, 8:26:42  iter: 1719  total_loss: 214  loss_ce: 2.735  loss_mask: 1.539  loss_dice: 2.785  loss_bbox: 2.741  loss_giou: 1.764  loss_ce_dn: 3.351  loss_mask_dn: 1.437  loss_dice_dn: 2.695  loss_bbox_dn: 0.927  loss_giou_dn: 0.7769  loss_ce_0: 13.83  loss_mask_0: 1.432  loss_dice_0: 3.187  loss_bbox_0: 3.681  loss_giou_0: 1.839  loss_ce_1: 3.311  loss_mask_1: 1.467  loss_dice_1: 2.822  loss_bbox_1: 3.241  loss_giou_1: 1.874  loss_ce_dn_1: 4.015  loss_mask_dn_1: 1.519  loss_dice_dn_1: 2.843  loss_bbox_dn_1: 1.039  loss_giou_dn_1: 0.8148  loss_ce_2: 2.765  loss_mask_2: 1.464  loss_dice_2: 2.844  loss_bbox_2: 3.068  loss_giou_2: 1.859  loss_ce_dn_2: 3.265  loss_mask_dn_2: 1.475  loss_dice_dn_2: 2.753  loss_bbox_dn_2: 0.988  loss_giou_dn_2: 0.8007  loss_ce_3: 2.621  loss_mask_3: 1.502  loss_dice_3: 2.85  loss_bbox_3: 2.928  loss_giou_3: 1.852  loss_ce_dn_3: 3.068  loss_mask_dn_3: 1.482  loss_dice_dn_3: 2.742  loss_bbox_dn_3: 0.9563  loss_giou_dn_3: 0.7862  loss_ce_4: 2.509  loss_mask_4: 1.496  loss_dice_4: 2.817  loss_bbox_4: 2.889  loss_giou_4: 1.816  loss_ce_dn_4: 3.002  loss_mask_dn_4: 1.455  loss_dice_dn_4: 2.667  loss_bbox_dn_4: 0.9515  loss_giou_dn_4: 0.783  loss_ce_5: 2.554  loss_mask_5: 1.396  loss_dice_5: 2.88  loss_bbox_5: 2.841  loss_giou_5: 1.866  loss_ce_dn_5: 2.999  loss_mask_dn_5: 1.439  loss_dice_dn_5: 2.631  loss_bbox_dn_5: 0.9387  loss_giou_dn_5: 0.778  loss_ce_6: 2.508  loss_mask_6: 1.423  loss_dice_6: 2.966  loss_bbox_6: 2.797  loss_giou_6: 1.855  loss_ce_dn_6: 3.023  loss_mask_dn_6: 1.454  loss_dice_dn_6: 2.598  loss_bbox_dn_6: 0.9337  loss_giou_dn_6: 0.7787  loss_ce_7: 2.638  loss_mask_7: 1.433  loss_dice_7: 2.828  loss_bbox_7: 2.783  loss_giou_7: 1.827  loss_ce_dn_7: 3.062  loss_mask_dn_7: 1.414  loss_dice_dn_7: 2.63  loss_bbox_dn_7: 0.9152  loss_giou_dn_7: 0.7695  loss_ce_8: 2.681  loss_mask_8: 1.438  loss_dice_8: 2.819  loss_bbox_8: 2.736  loss_giou_8: 1.765  loss_ce_dn_8: 3.169  loss_mask_dn_8: 1.454  loss_dice_dn_8: 2.665  loss_bbox_dn_8: 0.9201  loss_giou_dn_8: 0.7697    time: 2.2748  last_time: 2.2223  data_time: 0.0115  last_data_time: 0.0064   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:01:48 d2.utils.events]: \u001b[0m eta: 8 days, 8:24:47  iter: 1739  total_loss: 200.5  loss_ce: 2.651  loss_mask: 1.455  loss_dice: 2.936  loss_bbox: 2.687  loss_giou: 1.637  loss_ce_dn: 3.499  loss_mask_dn: 1.337  loss_dice_dn: 2.791  loss_bbox_dn: 0.9527  loss_giou_dn: 0.7912  loss_ce_0: 13.64  loss_mask_0: 1.375  loss_dice_0: 3.046  loss_bbox_0: 3.638  loss_giou_0: 1.919  loss_ce_1: 3.04  loss_mask_1: 1.455  loss_dice_1: 2.802  loss_bbox_1: 3.224  loss_giou_1: 1.748  loss_ce_dn_1: 3.84  loss_mask_dn_1: 1.362  loss_dice_dn_1: 2.877  loss_bbox_dn_1: 1.08  loss_giou_dn_1: 0.8184  loss_ce_2: 2.8  loss_mask_2: 1.475  loss_dice_2: 2.78  loss_bbox_2: 3.101  loss_giou_2: 1.699  loss_ce_dn_2: 3.178  loss_mask_dn_2: 1.235  loss_dice_dn_2: 2.759  loss_bbox_dn_2: 1.017  loss_giou_dn_2: 0.8059  loss_ce_3: 2.65  loss_mask_3: 1.429  loss_dice_3: 2.826  loss_bbox_3: 3.023  loss_giou_3: 1.687  loss_ce_dn_3: 2.961  loss_mask_dn_3: 1.226  loss_dice_dn_3: 2.696  loss_bbox_dn_3: 0.9704  loss_giou_dn_3: 0.7818  loss_ce_4: 2.7  loss_mask_4: 1.429  loss_dice_4: 2.812  loss_bbox_4: 2.958  loss_giou_4: 1.68  loss_ce_dn_4: 2.99  loss_mask_dn_4: 1.252  loss_dice_dn_4: 2.673  loss_bbox_dn_4: 0.9564  loss_giou_dn_4: 0.7714  loss_ce_5: 2.557  loss_mask_5: 1.476  loss_dice_5: 2.789  loss_bbox_5: 2.942  loss_giou_5: 1.671  loss_ce_dn_5: 3.101  loss_mask_dn_5: 1.28  loss_dice_dn_5: 2.701  loss_bbox_dn_5: 0.9529  loss_giou_dn_5: 0.7712  loss_ce_6: 2.484  loss_mask_6: 1.453  loss_dice_6: 2.804  loss_bbox_6: 2.899  loss_giou_6: 1.624  loss_ce_dn_6: 3.165  loss_mask_dn_6: 1.298  loss_dice_dn_6: 2.654  loss_bbox_dn_6: 0.9535  loss_giou_dn_6: 0.7721  loss_ce_7: 2.532  loss_mask_7: 1.445  loss_dice_7: 2.814  loss_bbox_7: 2.796  loss_giou_7: 1.651  loss_ce_dn_7: 3.166  loss_mask_dn_7: 1.306  loss_dice_dn_7: 2.702  loss_bbox_dn_7: 0.9342  loss_giou_dn_7: 0.7757  loss_ce_8: 2.626  loss_mask_8: 1.462  loss_dice_8: 2.794  loss_bbox_8: 2.662  loss_giou_8: 1.636  loss_ce_dn_8: 3.256  loss_mask_dn_8: 1.267  loss_dice_dn_8: 2.729  loss_bbox_dn_8: 0.9328  loss_giou_dn_8: 0.7877    time: 2.2748  last_time: 2.2660  data_time: 0.0136  last_data_time: 0.0147   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:02:33 d2.utils.events]: \u001b[0m eta: 8 days, 8:22:44  iter: 1759  total_loss: 221.2  loss_ce: 2.976  loss_mask: 1.584  loss_dice: 2.844  loss_bbox: 2.799  loss_giou: 1.642  loss_ce_dn: 3.853  loss_mask_dn: 1.52  loss_dice_dn: 2.704  loss_bbox_dn: 0.9446  loss_giou_dn: 0.7775  loss_ce_0: 13.61  loss_mask_0: 1.407  loss_dice_0: 3.145  loss_bbox_0: 3.575  loss_giou_0: 1.848  loss_ce_1: 3.676  loss_mask_1: 1.522  loss_dice_1: 2.962  loss_bbox_1: 3.096  loss_giou_1: 1.713  loss_ce_dn_1: 4.283  loss_mask_dn_1: 1.573  loss_dice_dn_1: 2.895  loss_bbox_dn_1: 1.08  loss_giou_dn_1: 0.8091  loss_ce_2: 2.988  loss_mask_2: 1.534  loss_dice_2: 2.865  loss_bbox_2: 2.87  loss_giou_2: 1.58  loss_ce_dn_2: 3.544  loss_mask_dn_2: 1.523  loss_dice_dn_2: 2.82  loss_bbox_dn_2: 1.044  loss_giou_dn_2: 0.7858  loss_ce_3: 2.822  loss_mask_3: 1.523  loss_dice_3: 2.866  loss_bbox_3: 2.826  loss_giou_3: 1.607  loss_ce_dn_3: 3.396  loss_mask_dn_3: 1.479  loss_dice_dn_3: 2.752  loss_bbox_dn_3: 0.9954  loss_giou_dn_3: 0.7719  loss_ce_4: 2.792  loss_mask_4: 1.606  loss_dice_4: 2.858  loss_bbox_4: 2.776  loss_giou_4: 1.635  loss_ce_dn_4: 3.421  loss_mask_dn_4: 1.442  loss_dice_dn_4: 2.692  loss_bbox_dn_4: 0.9784  loss_giou_dn_4: 0.7686  loss_ce_5: 2.726  loss_mask_5: 1.567  loss_dice_5: 2.876  loss_bbox_5: 2.786  loss_giou_5: 1.658  loss_ce_dn_5: 3.493  loss_mask_dn_5: 1.434  loss_dice_dn_5: 2.753  loss_bbox_dn_5: 0.9681  loss_giou_dn_5: 0.7701  loss_ce_6: 2.739  loss_mask_6: 1.608  loss_dice_6: 2.944  loss_bbox_6: 2.813  loss_giou_6: 1.675  loss_ce_dn_6: 3.601  loss_mask_dn_6: 1.494  loss_dice_dn_6: 2.748  loss_bbox_dn_6: 0.9645  loss_giou_dn_6: 0.7751  loss_ce_7: 2.865  loss_mask_7: 1.591  loss_dice_7: 2.878  loss_bbox_7: 2.781  loss_giou_7: 1.651  loss_ce_dn_7: 3.546  loss_mask_dn_7: 1.424  loss_dice_dn_7: 2.724  loss_bbox_dn_7: 0.9377  loss_giou_dn_7: 0.7713  loss_ce_8: 2.932  loss_mask_8: 1.591  loss_dice_8: 2.871  loss_bbox_8: 2.76  loss_giou_8: 1.637  loss_ce_dn_8: 3.736  loss_mask_dn_8: 1.445  loss_dice_dn_8: 2.674  loss_bbox_dn_8: 0.9413  loss_giou_dn_8: 0.7728    time: 2.2747  last_time: 2.2587  data_time: 0.0113  last_data_time: 0.0104   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:03:19 d2.utils.events]: \u001b[0m eta: 8 days, 8:22:44  iter: 1779  total_loss: 199.4  loss_ce: 2.313  loss_mask: 1.505  loss_dice: 2.672  loss_bbox: 2.824  loss_giou: 1.694  loss_ce_dn: 3.169  loss_mask_dn: 1.567  loss_dice_dn: 2.404  loss_bbox_dn: 0.9396  loss_giou_dn: 0.777  loss_ce_0: 13.55  loss_mask_0: 1.451  loss_dice_0: 2.821  loss_bbox_0: 3.534  loss_giou_0: 1.761  loss_ce_1: 2.587  loss_mask_1: 1.377  loss_dice_1: 2.766  loss_bbox_1: 3.389  loss_giou_1: 1.673  loss_ce_dn_1: 3.629  loss_mask_dn_1: 1.617  loss_dice_dn_1: 2.659  loss_bbox_dn_1: 1.119  loss_giou_dn_1: 0.8013  loss_ce_2: 2.346  loss_mask_2: 1.509  loss_dice_2: 2.685  loss_bbox_2: 2.984  loss_giou_2: 1.637  loss_ce_dn_2: 3.022  loss_mask_dn_2: 1.544  loss_dice_dn_2: 2.468  loss_bbox_dn_2: 1.029  loss_giou_dn_2: 0.7781  loss_ce_3: 2.182  loss_mask_3: 1.49  loss_dice_3: 2.753  loss_bbox_3: 2.911  loss_giou_3: 1.675  loss_ce_dn_3: 2.935  loss_mask_dn_3: 1.525  loss_dice_dn_3: 2.416  loss_bbox_dn_3: 0.9297  loss_giou_dn_3: 0.7564  loss_ce_4: 2.136  loss_mask_4: 1.465  loss_dice_4: 2.781  loss_bbox_4: 2.88  loss_giou_4: 1.709  loss_ce_dn_4: 2.902  loss_mask_dn_4: 1.524  loss_dice_dn_4: 2.386  loss_bbox_dn_4: 0.9029  loss_giou_dn_4: 0.7563  loss_ce_5: 2.071  loss_mask_5: 1.493  loss_dice_5: 2.728  loss_bbox_5: 2.831  loss_giou_5: 1.695  loss_ce_dn_5: 2.961  loss_mask_dn_5: 1.535  loss_dice_dn_5: 2.356  loss_bbox_dn_5: 0.9031  loss_giou_dn_5: 0.7584  loss_ce_6: 2.093  loss_mask_6: 1.506  loss_dice_6: 2.699  loss_bbox_6: 2.875  loss_giou_6: 1.678  loss_ce_dn_6: 2.978  loss_mask_dn_6: 1.572  loss_dice_dn_6: 2.376  loss_bbox_dn_6: 0.9092  loss_giou_dn_6: 0.7603  loss_ce_7: 2.227  loss_mask_7: 1.532  loss_dice_7: 2.685  loss_bbox_7: 2.791  loss_giou_7: 1.656  loss_ce_dn_7: 2.963  loss_mask_dn_7: 1.565  loss_dice_dn_7: 2.36  loss_bbox_dn_7: 0.9004  loss_giou_dn_7: 0.7522  loss_ce_8: 2.143  loss_mask_8: 1.538  loss_dice_8: 2.663  loss_bbox_8: 2.819  loss_giou_8: 1.658  loss_ce_dn_8: 3.027  loss_mask_dn_8: 1.53  loss_dice_dn_8: 2.375  loss_bbox_dn_8: 0.9158  loss_giou_dn_8: 0.7615    time: 2.2747  last_time: 2.2714  data_time: 0.0122  last_data_time: 0.0122   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:04:04 d2.utils.events]: \u001b[0m eta: 8 days, 8:22:50  iter: 1799  total_loss: 211.3  loss_ce: 2.576  loss_mask: 1.709  loss_dice: 2.582  loss_bbox: 3.055  loss_giou: 1.766  loss_ce_dn: 3.056  loss_mask_dn: 1.621  loss_dice_dn: 2.499  loss_bbox_dn: 0.989  loss_giou_dn: 0.7782  loss_ce_0: 13.36  loss_mask_0: 1.576  loss_dice_0: 3.024  loss_bbox_0: 3.673  loss_giou_0: 1.821  loss_ce_1: 2.981  loss_mask_1: 1.712  loss_dice_1: 2.715  loss_bbox_1: 3.18  loss_giou_1: 1.775  loss_ce_dn_1: 3.796  loss_mask_dn_1: 1.602  loss_dice_dn_1: 2.732  loss_bbox_dn_1: 1.044  loss_giou_dn_1: 0.8112  loss_ce_2: 2.542  loss_mask_2: 1.672  loss_dice_2: 2.587  loss_bbox_2: 3.164  loss_giou_2: 1.763  loss_ce_dn_2: 3.11  loss_mask_dn_2: 1.563  loss_dice_dn_2: 2.541  loss_bbox_dn_2: 1  loss_giou_dn_2: 0.7942  loss_ce_3: 2.614  loss_mask_3: 1.537  loss_dice_3: 2.67  loss_bbox_3: 2.981  loss_giou_3: 1.717  loss_ce_dn_3: 2.938  loss_mask_dn_3: 1.568  loss_dice_dn_3: 2.468  loss_bbox_dn_3: 0.9782  loss_giou_dn_3: 0.7814  loss_ce_4: 2.613  loss_mask_4: 1.568  loss_dice_4: 2.623  loss_bbox_4: 3.045  loss_giou_4: 1.753  loss_ce_dn_4: 2.998  loss_mask_dn_4: 1.57  loss_dice_dn_4: 2.479  loss_bbox_dn_4: 0.9839  loss_giou_dn_4: 0.7769  loss_ce_5: 2.574  loss_mask_5: 1.562  loss_dice_5: 2.594  loss_bbox_5: 3.026  loss_giou_5: 1.752  loss_ce_dn_5: 2.895  loss_mask_dn_5: 1.532  loss_dice_dn_5: 2.462  loss_bbox_dn_5: 0.979  loss_giou_dn_5: 0.7745  loss_ce_6: 2.637  loss_mask_6: 1.784  loss_dice_6: 2.54  loss_bbox_6: 3.054  loss_giou_6: 1.76  loss_ce_dn_6: 2.95  loss_mask_dn_6: 1.589  loss_dice_dn_6: 2.473  loss_bbox_dn_6: 0.9798  loss_giou_dn_6: 0.7776  loss_ce_7: 2.618  loss_mask_7: 1.798  loss_dice_7: 2.517  loss_bbox_7: 3.081  loss_giou_7: 1.772  loss_ce_dn_7: 2.909  loss_mask_dn_7: 1.623  loss_dice_dn_7: 2.499  loss_bbox_dn_7: 0.9758  loss_giou_dn_7: 0.7718  loss_ce_8: 2.642  loss_mask_8: 1.667  loss_dice_8: 2.533  loss_bbox_8: 3.11  loss_giou_8: 1.782  loss_ce_dn_8: 2.918  loss_mask_dn_8: 1.594  loss_dice_dn_8: 2.486  loss_bbox_dn_8: 0.9788  loss_giou_dn_8: 0.7729    time: 2.2746  last_time: 2.2515  data_time: 0.0124  last_data_time: 0.0105   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:04:49 d2.utils.events]: \u001b[0m eta: 8 days, 8:21:13  iter: 1819  total_loss: 212.8  loss_ce: 2.536  loss_mask: 1.776  loss_dice: 2.85  loss_bbox: 2.607  loss_giou: 1.667  loss_ce_dn: 2.89  loss_mask_dn: 1.587  loss_dice_dn: 2.552  loss_bbox_dn: 0.9294  loss_giou_dn: 0.7667  loss_ce_0: 13.34  loss_mask_0: 1.612  loss_dice_0: 3.123  loss_bbox_0: 3.465  loss_giou_0: 1.829  loss_ce_1: 3.033  loss_mask_1: 1.73  loss_dice_1: 2.88  loss_bbox_1: 3.08  loss_giou_1: 1.714  loss_ce_dn_1: 3.528  loss_mask_dn_1: 1.598  loss_dice_dn_1: 2.739  loss_bbox_dn_1: 1.061  loss_giou_dn_1: 0.8193  loss_ce_2: 2.653  loss_mask_2: 1.737  loss_dice_2: 2.821  loss_bbox_2: 2.894  loss_giou_2: 1.623  loss_ce_dn_2: 3.02  loss_mask_dn_2: 1.606  loss_dice_dn_2: 2.6  loss_bbox_dn_2: 1.017  loss_giou_dn_2: 0.8061  loss_ce_3: 2.544  loss_mask_3: 1.82  loss_dice_3: 2.837  loss_bbox_3: 2.708  loss_giou_3: 1.631  loss_ce_dn_3: 2.818  loss_mask_dn_3: 1.622  loss_dice_dn_3: 2.555  loss_bbox_dn_3: 0.9797  loss_giou_dn_3: 0.7908  loss_ce_4: 2.437  loss_mask_4: 1.829  loss_dice_4: 2.84  loss_bbox_4: 2.629  loss_giou_4: 1.623  loss_ce_dn_4: 2.66  loss_mask_dn_4: 1.544  loss_dice_dn_4: 2.489  loss_bbox_dn_4: 0.9652  loss_giou_dn_4: 0.7831  loss_ce_5: 2.532  loss_mask_5: 1.87  loss_dice_5: 2.821  loss_bbox_5: 2.605  loss_giou_5: 1.619  loss_ce_dn_5: 2.688  loss_mask_dn_5: 1.531  loss_dice_dn_5: 2.51  loss_bbox_dn_5: 0.9558  loss_giou_dn_5: 0.778  loss_ce_6: 2.481  loss_mask_6: 1.842  loss_dice_6: 2.862  loss_bbox_6: 2.644  loss_giou_6: 1.632  loss_ce_dn_6: 2.701  loss_mask_dn_6: 1.597  loss_dice_dn_6: 2.487  loss_bbox_dn_6: 0.9529  loss_giou_dn_6: 0.7776  loss_ce_7: 2.506  loss_mask_7: 1.81  loss_dice_7: 2.819  loss_bbox_7: 2.659  loss_giou_7: 1.649  loss_ce_dn_7: 2.731  loss_mask_dn_7: 1.588  loss_dice_dn_7: 2.525  loss_bbox_dn_7: 0.928  loss_giou_dn_7: 0.7708  loss_ce_8: 2.526  loss_mask_8: 1.869  loss_dice_8: 2.85  loss_bbox_8: 2.642  loss_giou_8: 1.655  loss_ce_dn_8: 2.816  loss_mask_dn_8: 1.638  loss_dice_dn_8: 2.536  loss_bbox_dn_8: 0.927  loss_giou_dn_8: 0.7694    time: 2.2746  last_time: 2.2413  data_time: 0.0138  last_data_time: 0.0056   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:05:35 d2.utils.events]: \u001b[0m eta: 8 days, 8:19:57  iter: 1839  total_loss: 207.3  loss_ce: 2.085  loss_mask: 1.597  loss_dice: 2.756  loss_bbox: 2.791  loss_giou: 1.667  loss_ce_dn: 2.765  loss_mask_dn: 1.466  loss_dice_dn: 2.583  loss_bbox_dn: 0.8663  loss_giou_dn: 0.7801  loss_ce_0: 13.32  loss_mask_0: 1.418  loss_dice_0: 3.041  loss_bbox_0: 3.6  loss_giou_0: 1.941  loss_ce_1: 3.052  loss_mask_1: 1.411  loss_dice_1: 2.829  loss_bbox_1: 3.056  loss_giou_1: 1.803  loss_ce_dn_1: 3.548  loss_mask_dn_1: 1.419  loss_dice_dn_1: 2.755  loss_bbox_dn_1: 0.995  loss_giou_dn_1: 0.8007  loss_ce_2: 2.454  loss_mask_2: 1.491  loss_dice_2: 2.808  loss_bbox_2: 2.821  loss_giou_2: 1.727  loss_ce_dn_2: 3.062  loss_mask_dn_2: 1.408  loss_dice_dn_2: 2.656  loss_bbox_dn_2: 0.9345  loss_giou_dn_2: 0.7803  loss_ce_3: 2.114  loss_mask_3: 1.558  loss_dice_3: 2.757  loss_bbox_3: 2.753  loss_giou_3: 1.744  loss_ce_dn_3: 2.84  loss_mask_dn_3: 1.403  loss_dice_dn_3: 2.587  loss_bbox_dn_3: 0.9019  loss_giou_dn_3: 0.7637  loss_ce_4: 1.984  loss_mask_4: 1.56  loss_dice_4: 2.744  loss_bbox_4: 2.782  loss_giou_4: 1.702  loss_ce_dn_4: 2.666  loss_mask_dn_4: 1.421  loss_dice_dn_4: 2.614  loss_bbox_dn_4: 0.8833  loss_giou_dn_4: 0.7576  loss_ce_5: 1.968  loss_mask_5: 1.581  loss_dice_5: 2.716  loss_bbox_5: 2.813  loss_giou_5: 1.693  loss_ce_dn_5: 2.651  loss_mask_dn_5: 1.392  loss_dice_dn_5: 2.582  loss_bbox_dn_5: 0.8822  loss_giou_dn_5: 0.7576  loss_ce_6: 1.998  loss_mask_6: 1.616  loss_dice_6: 2.72  loss_bbox_6: 2.839  loss_giou_6: 1.691  loss_ce_dn_6: 2.689  loss_mask_dn_6: 1.403  loss_dice_dn_6: 2.582  loss_bbox_dn_6: 0.8872  loss_giou_dn_6: 0.7599  loss_ce_7: 2.02  loss_mask_7: 1.564  loss_dice_7: 2.728  loss_bbox_7: 2.75  loss_giou_7: 1.667  loss_ce_dn_7: 2.66  loss_mask_dn_7: 1.415  loss_dice_dn_7: 2.589  loss_bbox_dn_7: 0.871  loss_giou_dn_7: 0.7616  loss_ce_8: 2.018  loss_mask_8: 1.589  loss_dice_8: 2.75  loss_bbox_8: 2.764  loss_giou_8: 1.668  loss_ce_dn_8: 2.775  loss_mask_dn_8: 1.453  loss_dice_dn_8: 2.584  loss_bbox_dn_8: 0.8668  loss_giou_dn_8: 0.7703    time: 2.2745  last_time: 2.2556  data_time: 0.0164  last_data_time: 0.0087   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:06:20 d2.utils.events]: \u001b[0m eta: 8 days, 8:19:12  iter: 1859  total_loss: 209.8  loss_ce: 2.66  loss_mask: 1.529  loss_dice: 2.7  loss_bbox: 2.684  loss_giou: 1.532  loss_ce_dn: 3.153  loss_mask_dn: 1.476  loss_dice_dn: 2.61  loss_bbox_dn: 0.9342  loss_giou_dn: 0.7593  loss_ce_0: 13.17  loss_mask_0: 1.554  loss_dice_0: 2.956  loss_bbox_0: 3.384  loss_giou_0: 1.762  loss_ce_1: 3.01  loss_mask_1: 1.575  loss_dice_1: 2.772  loss_bbox_1: 3.064  loss_giou_1: 1.692  loss_ce_dn_1: 3.674  loss_mask_dn_1: 1.574  loss_dice_dn_1: 2.857  loss_bbox_dn_1: 1.091  loss_giou_dn_1: 0.7942  loss_ce_2: 2.577  loss_mask_2: 1.602  loss_dice_2: 2.677  loss_bbox_2: 2.884  loss_giou_2: 1.597  loss_ce_dn_2: 3.09  loss_mask_dn_2: 1.517  loss_dice_dn_2: 2.707  loss_bbox_dn_2: 1.042  loss_giou_dn_2: 0.7754  loss_ce_3: 2.42  loss_mask_3: 1.577  loss_dice_3: 2.648  loss_bbox_3: 2.867  loss_giou_3: 1.521  loss_ce_dn_3: 2.856  loss_mask_dn_3: 1.496  loss_dice_dn_3: 2.61  loss_bbox_dn_3: 1.012  loss_giou_dn_3: 0.7608  loss_ce_4: 2.611  loss_mask_4: 1.582  loss_dice_4: 2.644  loss_bbox_4: 2.795  loss_giou_4: 1.488  loss_ce_dn_4: 2.81  loss_mask_dn_4: 1.525  loss_dice_dn_4: 2.579  loss_bbox_dn_4: 0.9967  loss_giou_dn_4: 0.7542  loss_ce_5: 2.594  loss_mask_5: 1.594  loss_dice_5: 2.643  loss_bbox_5: 2.744  loss_giou_5: 1.505  loss_ce_dn_5: 2.805  loss_mask_dn_5: 1.522  loss_dice_dn_5: 2.55  loss_bbox_dn_5: 0.9872  loss_giou_dn_5: 0.7545  loss_ce_6: 2.59  loss_mask_6: 1.577  loss_dice_6: 2.673  loss_bbox_6: 2.715  loss_giou_6: 1.509  loss_ce_dn_6: 2.844  loss_mask_dn_6: 1.516  loss_dice_dn_6: 2.576  loss_bbox_dn_6: 0.9801  loss_giou_dn_6: 0.7522  loss_ce_7: 2.483  loss_mask_7: 1.533  loss_dice_7: 2.671  loss_bbox_7: 2.724  loss_giou_7: 1.514  loss_ce_dn_7: 2.846  loss_mask_dn_7: 1.488  loss_dice_dn_7: 2.597  loss_bbox_dn_7: 0.9277  loss_giou_dn_7: 0.7517  loss_ce_8: 2.698  loss_mask_8: 1.539  loss_dice_8: 2.694  loss_bbox_8: 2.705  loss_giou_8: 1.514  loss_ce_dn_8: 2.995  loss_mask_dn_8: 1.498  loss_dice_dn_8: 2.587  loss_bbox_dn_8: 0.9255  loss_giou_dn_8: 0.7509    time: 2.2746  last_time: 2.2837  data_time: 0.0129  last_data_time: 0.0289   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:07:06 d2.utils.events]: \u001b[0m eta: 8 days, 8:18:31  iter: 1879  total_loss: 206  loss_ce: 2.439  loss_mask: 1.422  loss_dice: 2.614  loss_bbox: 2.684  loss_giou: 1.692  loss_ce_dn: 2.654  loss_mask_dn: 1.426  loss_dice_dn: 2.477  loss_bbox_dn: 0.9297  loss_giou_dn: 0.772  loss_ce_0: 13.01  loss_mask_0: 1.395  loss_dice_0: 3.061  loss_bbox_0: 3.307  loss_giou_0: 1.88  loss_ce_1: 3.007  loss_mask_1: 1.437  loss_dice_1: 2.722  loss_bbox_1: 2.957  loss_giou_1: 1.707  loss_ce_dn_1: 3.427  loss_mask_dn_1: 1.438  loss_dice_dn_1: 2.684  loss_bbox_dn_1: 1.048  loss_giou_dn_1: 0.821  loss_ce_2: 2.558  loss_mask_2: 1.455  loss_dice_2: 2.732  loss_bbox_2: 2.938  loss_giou_2: 1.656  loss_ce_dn_2: 2.891  loss_mask_dn_2: 1.393  loss_dice_dn_2: 2.514  loss_bbox_dn_2: 0.9878  loss_giou_dn_2: 0.803  loss_ce_3: 2.384  loss_mask_3: 1.557  loss_dice_3: 2.738  loss_bbox_3: 2.964  loss_giou_3: 1.689  loss_ce_dn_3: 2.763  loss_mask_dn_3: 1.371  loss_dice_dn_3: 2.491  loss_bbox_dn_3: 0.9433  loss_giou_dn_3: 0.7913  loss_ce_4: 2.232  loss_mask_4: 1.484  loss_dice_4: 2.689  loss_bbox_4: 2.86  loss_giou_4: 1.681  loss_ce_dn_4: 2.69  loss_mask_dn_4: 1.397  loss_dice_dn_4: 2.504  loss_bbox_dn_4: 0.9176  loss_giou_dn_4: 0.782  loss_ce_5: 2.158  loss_mask_5: 1.459  loss_dice_5: 2.659  loss_bbox_5: 2.861  loss_giou_5: 1.683  loss_ce_dn_5: 2.622  loss_mask_dn_5: 1.387  loss_dice_dn_5: 2.452  loss_bbox_dn_5: 0.9075  loss_giou_dn_5: 0.7803  loss_ce_6: 2.214  loss_mask_6: 1.455  loss_dice_6: 2.661  loss_bbox_6: 2.717  loss_giou_6: 1.709  loss_ce_dn_6: 2.567  loss_mask_dn_6: 1.365  loss_dice_dn_6: 2.436  loss_bbox_dn_6: 0.9085  loss_giou_dn_6: 0.778  loss_ce_7: 2.331  loss_mask_7: 1.455  loss_dice_7: 2.651  loss_bbox_7: 2.662  loss_giou_7: 1.699  loss_ce_dn_7: 2.64  loss_mask_dn_7: 1.403  loss_dice_dn_7: 2.459  loss_bbox_dn_7: 0.9034  loss_giou_dn_7: 0.7676  loss_ce_8: 2.365  loss_mask_8: 1.42  loss_dice_8: 2.648  loss_bbox_8: 2.694  loss_giou_8: 1.697  loss_ce_dn_8: 2.618  loss_mask_dn_8: 1.404  loss_dice_dn_8: 2.497  loss_bbox_dn_8: 0.9116  loss_giou_dn_8: 0.769    time: 2.2746  last_time: 2.2829  data_time: 0.0138  last_data_time: 0.0054   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:07:52 d2.utils.events]: \u001b[0m eta: 8 days, 8:16:45  iter: 1899  total_loss: 203.4  loss_ce: 2.18  loss_mask: 1.802  loss_dice: 2.739  loss_bbox: 2.894  loss_giou: 1.608  loss_ce_dn: 2.49  loss_mask_dn: 1.675  loss_dice_dn: 2.498  loss_bbox_dn: 0.885  loss_giou_dn: 0.7389  loss_ce_0: 12.94  loss_mask_0: 1.605  loss_dice_0: 3.028  loss_bbox_0: 3.539  loss_giou_0: 1.82  loss_ce_1: 2.624  loss_mask_1: 1.717  loss_dice_1: 2.735  loss_bbox_1: 3.196  loss_giou_1: 1.713  loss_ce_dn_1: 3.144  loss_mask_dn_1: 1.644  loss_dice_dn_1: 2.677  loss_bbox_dn_1: 1.039  loss_giou_dn_1: 0.7965  loss_ce_2: 2.346  loss_mask_2: 1.733  loss_dice_2: 2.725  loss_bbox_2: 2.991  loss_giou_2: 1.635  loss_ce_dn_2: 2.51  loss_mask_dn_2: 1.605  loss_dice_dn_2: 2.518  loss_bbox_dn_2: 0.9501  loss_giou_dn_2: 0.7718  loss_ce_3: 2.227  loss_mask_3: 1.823  loss_dice_3: 2.694  loss_bbox_3: 2.973  loss_giou_3: 1.634  loss_ce_dn_3: 2.315  loss_mask_dn_3: 1.608  loss_dice_dn_3: 2.482  loss_bbox_dn_3: 0.9111  loss_giou_dn_3: 0.7544  loss_ce_4: 2.148  loss_mask_4: 1.807  loss_dice_4: 2.711  loss_bbox_4: 2.904  loss_giou_4: 1.629  loss_ce_dn_4: 2.146  loss_mask_dn_4: 1.62  loss_dice_dn_4: 2.487  loss_bbox_dn_4: 0.8888  loss_giou_dn_4: 0.7473  loss_ce_5: 2.141  loss_mask_5: 1.824  loss_dice_5: 2.757  loss_bbox_5: 2.877  loss_giou_5: 1.621  loss_ce_dn_5: 2.034  loss_mask_dn_5: 1.682  loss_dice_dn_5: 2.434  loss_bbox_dn_5: 0.8853  loss_giou_dn_5: 0.7445  loss_ce_6: 2.178  loss_mask_6: 1.805  loss_dice_6: 2.748  loss_bbox_6: 2.909  loss_giou_6: 1.606  loss_ce_dn_6: 2.037  loss_mask_dn_6: 1.663  loss_dice_dn_6: 2.447  loss_bbox_dn_6: 0.8835  loss_giou_dn_6: 0.7438  loss_ce_7: 2.157  loss_mask_7: 1.781  loss_dice_7: 2.694  loss_bbox_7: 2.917  loss_giou_7: 1.584  loss_ce_dn_7: 2.104  loss_mask_dn_7: 1.666  loss_dice_dn_7: 2.491  loss_bbox_dn_7: 0.8764  loss_giou_dn_7: 0.7381  loss_ce_8: 2.178  loss_mask_8: 1.788  loss_dice_8: 2.708  loss_bbox_8: 2.902  loss_giou_8: 1.597  loss_ce_dn_8: 2.222  loss_mask_dn_8: 1.68  loss_dice_dn_8: 2.475  loss_bbox_dn_8: 0.8753  loss_giou_dn_8: 0.7368    time: 2.2746  last_time: 2.2650  data_time: 0.0130  last_data_time: 0.0122   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:08:37 d2.utils.events]: \u001b[0m eta: 8 days, 8:17:27  iter: 1919  total_loss: 204.9  loss_ce: 2.392  loss_mask: 1.756  loss_dice: 2.741  loss_bbox: 2.862  loss_giou: 1.67  loss_ce_dn: 3.059  loss_mask_dn: 1.675  loss_dice_dn: 2.599  loss_bbox_dn: 0.9408  loss_giou_dn: 0.7588  loss_ce_0: 12.75  loss_mask_0: 1.695  loss_dice_0: 3.072  loss_bbox_0: 3.876  loss_giou_0: 1.987  loss_ce_1: 2.783  loss_mask_1: 1.681  loss_dice_1: 2.85  loss_bbox_1: 3.225  loss_giou_1: 1.721  loss_ce_dn_1: 3.497  loss_mask_dn_1: 1.609  loss_dice_dn_1: 2.868  loss_bbox_dn_1: 1.043  loss_giou_dn_1: 0.8037  loss_ce_2: 2.299  loss_mask_2: 1.8  loss_dice_2: 2.717  loss_bbox_2: 3.054  loss_giou_2: 1.73  loss_ce_dn_2: 2.87  loss_mask_dn_2: 1.586  loss_dice_dn_2: 2.648  loss_bbox_dn_2: 0.9818  loss_giou_dn_2: 0.778  loss_ce_3: 2.183  loss_mask_3: 1.782  loss_dice_3: 2.72  loss_bbox_3: 3.02  loss_giou_3: 1.697  loss_ce_dn_3: 2.777  loss_mask_dn_3: 1.598  loss_dice_dn_3: 2.565  loss_bbox_dn_3: 0.9602  loss_giou_dn_3: 0.7603  loss_ce_4: 2.252  loss_mask_4: 1.787  loss_dice_4: 2.74  loss_bbox_4: 3.001  loss_giou_4: 1.754  loss_ce_dn_4: 2.777  loss_mask_dn_4: 1.57  loss_dice_dn_4: 2.534  loss_bbox_dn_4: 0.9539  loss_giou_dn_4: 0.7574  loss_ce_5: 2.227  loss_mask_5: 1.787  loss_dice_5: 2.706  loss_bbox_5: 2.955  loss_giou_5: 1.719  loss_ce_dn_5: 2.799  loss_mask_dn_5: 1.604  loss_dice_dn_5: 2.508  loss_bbox_dn_5: 0.9516  loss_giou_dn_5: 0.7553  loss_ce_6: 2.453  loss_mask_6: 1.762  loss_dice_6: 2.748  loss_bbox_6: 2.963  loss_giou_6: 1.686  loss_ce_dn_6: 2.792  loss_mask_dn_6: 1.622  loss_dice_dn_6: 2.529  loss_bbox_dn_6: 0.9593  loss_giou_dn_6: 0.76  loss_ce_7: 2.424  loss_mask_7: 1.741  loss_dice_7: 2.759  loss_bbox_7: 2.946  loss_giou_7: 1.676  loss_ce_dn_7: 2.807  loss_mask_dn_7: 1.604  loss_dice_dn_7: 2.554  loss_bbox_dn_7: 0.9383  loss_giou_dn_7: 0.7499  loss_ce_8: 2.426  loss_mask_8: 1.716  loss_dice_8: 2.738  loss_bbox_8: 2.903  loss_giou_8: 1.662  loss_ce_dn_8: 2.92  loss_mask_dn_8: 1.638  loss_dice_dn_8: 2.572  loss_bbox_dn_8: 0.9401  loss_giou_dn_8: 0.7531    time: 2.2747  last_time: 2.2860  data_time: 0.0134  last_data_time: 0.0202   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:09:23 d2.utils.events]: \u001b[0m eta: 8 days, 8:16:10  iter: 1939  total_loss: 202.9  loss_ce: 2.545  loss_mask: 1.499  loss_dice: 2.684  loss_bbox: 2.815  loss_giou: 1.739  loss_ce_dn: 2.7  loss_mask_dn: 1.359  loss_dice_dn: 2.373  loss_bbox_dn: 0.9217  loss_giou_dn: 0.8051  loss_ce_0: 12.83  loss_mask_0: 1.345  loss_dice_0: 3.051  loss_bbox_0: 3.412  loss_giou_0: 1.885  loss_ce_1: 3.038  loss_mask_1: 1.462  loss_dice_1: 2.677  loss_bbox_1: 2.967  loss_giou_1: 1.743  loss_ce_dn_1: 3.303  loss_mask_dn_1: 1.35  loss_dice_dn_1: 2.65  loss_bbox_dn_1: 1.01  loss_giou_dn_1: 0.8189  loss_ce_2: 2.431  loss_mask_2: 1.41  loss_dice_2: 2.661  loss_bbox_2: 2.861  loss_giou_2: 1.746  loss_ce_dn_2: 2.657  loss_mask_dn_2: 1.316  loss_dice_dn_2: 2.466  loss_bbox_dn_2: 0.9681  loss_giou_dn_2: 0.802  loss_ce_3: 2.357  loss_mask_3: 1.426  loss_dice_3: 2.694  loss_bbox_3: 2.868  loss_giou_3: 1.765  loss_ce_dn_3: 2.505  loss_mask_dn_3: 1.326  loss_dice_dn_3: 2.404  loss_bbox_dn_3: 0.946  loss_giou_dn_3: 0.7881  loss_ce_4: 2.458  loss_mask_4: 1.498  loss_dice_4: 2.714  loss_bbox_4: 2.832  loss_giou_4: 1.743  loss_ce_dn_4: 2.52  loss_mask_dn_4: 1.29  loss_dice_dn_4: 2.401  loss_bbox_dn_4: 0.9319  loss_giou_dn_4: 0.794  loss_ce_5: 2.225  loss_mask_5: 1.508  loss_dice_5: 2.71  loss_bbox_5: 2.782  loss_giou_5: 1.746  loss_ce_dn_5: 2.538  loss_mask_dn_5: 1.31  loss_dice_dn_5: 2.401  loss_bbox_dn_5: 0.9195  loss_giou_dn_5: 0.794  loss_ce_6: 2.17  loss_mask_6: 1.557  loss_dice_6: 2.715  loss_bbox_6: 2.801  loss_giou_6: 1.725  loss_ce_dn_6: 2.649  loss_mask_dn_6: 1.32  loss_dice_dn_6: 2.391  loss_bbox_dn_6: 0.9162  loss_giou_dn_6: 0.7987  loss_ce_7: 2.355  loss_mask_7: 1.521  loss_dice_7: 2.689  loss_bbox_7: 2.8  loss_giou_7: 1.736  loss_ce_dn_7: 2.743  loss_mask_dn_7: 1.318  loss_dice_dn_7: 2.387  loss_bbox_dn_7: 0.906  loss_giou_dn_7: 0.7986  loss_ce_8: 2.497  loss_mask_8: 1.516  loss_dice_8: 2.664  loss_bbox_8: 2.822  loss_giou_8: 1.737  loss_ce_dn_8: 2.755  loss_mask_dn_8: 1.346  loss_dice_dn_8: 2.398  loss_bbox_dn_8: 0.9114  loss_giou_dn_8: 0.7999    time: 2.2747  last_time: 2.2859  data_time: 0.0116  last_data_time: 0.0047   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:10:08 d2.utils.events]: \u001b[0m eta: 8 days, 8:15:30  iter: 1959  total_loss: 194.3  loss_ce: 2.181  loss_mask: 1.652  loss_dice: 2.633  loss_bbox: 2.629  loss_giou: 1.576  loss_ce_dn: 2.566  loss_mask_dn: 1.606  loss_dice_dn: 2.535  loss_bbox_dn: 0.9139  loss_giou_dn: 0.7622  loss_ce_0: 12.69  loss_mask_0: 1.569  loss_dice_0: 2.94  loss_bbox_0: 3.871  loss_giou_0: 1.871  loss_ce_1: 2.304  loss_mask_1: 1.499  loss_dice_1: 2.59  loss_bbox_1: 3.191  loss_giou_1: 1.684  loss_ce_dn_1: 3.028  loss_mask_dn_1: 1.596  loss_dice_dn_1: 2.661  loss_bbox_dn_1: 1.11  loss_giou_dn_1: 0.8038  loss_ce_2: 2.116  loss_mask_2: 1.619  loss_dice_2: 2.574  loss_bbox_2: 2.938  loss_giou_2: 1.68  loss_ce_dn_2: 2.318  loss_mask_dn_2: 1.547  loss_dice_dn_2: 2.482  loss_bbox_dn_2: 1.029  loss_giou_dn_2: 0.7747  loss_ce_3: 2.172  loss_mask_3: 1.688  loss_dice_3: 2.585  loss_bbox_3: 2.884  loss_giou_3: 1.688  loss_ce_dn_3: 2.246  loss_mask_dn_3: 1.548  loss_dice_dn_3: 2.453  loss_bbox_dn_3: 0.9833  loss_giou_dn_3: 0.766  loss_ce_4: 2.075  loss_mask_4: 1.628  loss_dice_4: 2.583  loss_bbox_4: 2.795  loss_giou_4: 1.671  loss_ce_dn_4: 2.174  loss_mask_dn_4: 1.493  loss_dice_dn_4: 2.441  loss_bbox_dn_4: 0.9596  loss_giou_dn_4: 0.7654  loss_ce_5: 2.135  loss_mask_5: 1.664  loss_dice_5: 2.607  loss_bbox_5: 2.668  loss_giou_5: 1.648  loss_ce_dn_5: 2.189  loss_mask_dn_5: 1.551  loss_dice_dn_5: 2.442  loss_bbox_dn_5: 0.9436  loss_giou_dn_5: 0.7728  loss_ce_6: 2.244  loss_mask_6: 1.591  loss_dice_6: 2.598  loss_bbox_6: 2.655  loss_giou_6: 1.587  loss_ce_dn_6: 2.205  loss_mask_dn_6: 1.616  loss_dice_dn_6: 2.467  loss_bbox_dn_6: 0.9313  loss_giou_dn_6: 0.7774  loss_ce_7: 2.285  loss_mask_7: 1.553  loss_dice_7: 2.606  loss_bbox_7: 2.597  loss_giou_7: 1.55  loss_ce_dn_7: 2.287  loss_mask_dn_7: 1.618  loss_dice_dn_7: 2.473  loss_bbox_dn_7: 0.9127  loss_giou_dn_7: 0.7653  loss_ce_8: 2.287  loss_mask_8: 1.584  loss_dice_8: 2.603  loss_bbox_8: 2.601  loss_giou_8: 1.534  loss_ce_dn_8: 2.346  loss_mask_dn_8: 1.599  loss_dice_dn_8: 2.511  loss_bbox_dn_8: 0.9098  loss_giou_dn_8: 0.762    time: 2.2748  last_time: 2.3501  data_time: 0.0137  last_data_time: 0.0304   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:10:54 d2.utils.events]: \u001b[0m eta: 8 days, 8:14:26  iter: 1979  total_loss: 194.5  loss_ce: 2.476  loss_mask: 1.418  loss_dice: 2.674  loss_bbox: 2.861  loss_giou: 1.641  loss_ce_dn: 3.131  loss_mask_dn: 1.328  loss_dice_dn: 2.535  loss_bbox_dn: 0.9873  loss_giou_dn: 0.7691  loss_ce_0: 12.68  loss_mask_0: 1.52  loss_dice_0: 3.093  loss_bbox_0: 3.616  loss_giou_0: 1.892  loss_ce_1: 2.88  loss_mask_1: 1.411  loss_dice_1: 2.752  loss_bbox_1: 3.233  loss_giou_1: 1.811  loss_ce_dn_1: 3.749  loss_mask_dn_1: 1.446  loss_dice_dn_1: 2.736  loss_bbox_dn_1: 1.12  loss_giou_dn_1: 0.818  loss_ce_2: 2.619  loss_mask_2: 1.48  loss_dice_2: 2.675  loss_bbox_2: 2.943  loss_giou_2: 1.716  loss_ce_dn_2: 3.259  loss_mask_dn_2: 1.494  loss_dice_dn_2: 2.64  loss_bbox_dn_2: 1.083  loss_giou_dn_2: 0.7998  loss_ce_3: 2.761  loss_mask_3: 1.495  loss_dice_3: 2.656  loss_bbox_3: 2.985  loss_giou_3: 1.674  loss_ce_dn_3: 3.004  loss_mask_dn_3: 1.483  loss_dice_dn_3: 2.639  loss_bbox_dn_3: 1.068  loss_giou_dn_3: 0.7913  loss_ce_4: 2.582  loss_mask_4: 1.447  loss_dice_4: 2.686  loss_bbox_4: 2.995  loss_giou_4: 1.679  loss_ce_dn_4: 3.026  loss_mask_dn_4: 1.453  loss_dice_dn_4: 2.592  loss_bbox_dn_4: 1.054  loss_giou_dn_4: 0.7838  loss_ce_5: 2.555  loss_mask_5: 1.473  loss_dice_5: 2.704  loss_bbox_5: 2.974  loss_giou_5: 1.666  loss_ce_dn_5: 2.986  loss_mask_dn_5: 1.374  loss_dice_dn_5: 2.573  loss_bbox_dn_5: 1.038  loss_giou_dn_5: 0.7825  loss_ce_6: 2.514  loss_mask_6: 1.466  loss_dice_6: 2.697  loss_bbox_6: 2.947  loss_giou_6: 1.673  loss_ce_dn_6: 2.994  loss_mask_dn_6: 1.373  loss_dice_dn_6: 2.543  loss_bbox_dn_6: 1.026  loss_giou_dn_6: 0.7797  loss_ce_7: 2.562  loss_mask_7: 1.453  loss_dice_7: 2.678  loss_bbox_7: 2.918  loss_giou_7: 1.629  loss_ce_dn_7: 2.996  loss_mask_dn_7: 1.392  loss_dice_dn_7: 2.563  loss_bbox_dn_7: 0.9881  loss_giou_dn_7: 0.7692  loss_ce_8: 2.462  loss_mask_8: 1.477  loss_dice_8: 2.662  loss_bbox_8: 2.863  loss_giou_8: 1.634  loss_ce_dn_8: 3.017  loss_mask_dn_8: 1.341  loss_dice_dn_8: 2.556  loss_bbox_dn_8: 0.9885  loss_giou_dn_8: 0.7669    time: 2.2746  last_time: 2.2507  data_time: 0.0118  last_data_time: 0.0162   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:11:39 d2.utils.events]: \u001b[0m eta: 8 days, 8:14:26  iter: 1999  total_loss: 197.4  loss_ce: 2.763  loss_mask: 1.335  loss_dice: 2.646  loss_bbox: 2.907  loss_giou: 1.69  loss_ce_dn: 3.162  loss_mask_dn: 1.33  loss_dice_dn: 2.555  loss_bbox_dn: 0.8284  loss_giou_dn: 0.76  loss_ce_0: 12.81  loss_mask_0: 1.305  loss_dice_0: 2.834  loss_bbox_0: 3.746  loss_giou_0: 1.891  loss_ce_1: 3.179  loss_mask_1: 1.408  loss_dice_1: 2.677  loss_bbox_1: 3.158  loss_giou_1: 1.715  loss_ce_dn_1: 3.798  loss_mask_dn_1: 1.305  loss_dice_dn_1: 2.695  loss_bbox_dn_1: 0.9807  loss_giou_dn_1: 0.8037  loss_ce_2: 2.836  loss_mask_2: 1.46  loss_dice_2: 2.619  loss_bbox_2: 2.988  loss_giou_2: 1.714  loss_ce_dn_2: 3.107  loss_mask_dn_2: 1.296  loss_dice_dn_2: 2.556  loss_bbox_dn_2: 0.9168  loss_giou_dn_2: 0.7834  loss_ce_3: 2.743  loss_mask_3: 1.427  loss_dice_3: 2.582  loss_bbox_3: 2.945  loss_giou_3: 1.692  loss_ce_dn_3: 2.961  loss_mask_dn_3: 1.285  loss_dice_dn_3: 2.484  loss_bbox_dn_3: 0.8753  loss_giou_dn_3: 0.7737  loss_ce_4: 2.752  loss_mask_4: 1.407  loss_dice_4: 2.57  loss_bbox_4: 2.957  loss_giou_4: 1.704  loss_ce_dn_4: 2.86  loss_mask_dn_4: 1.265  loss_dice_dn_4: 2.495  loss_bbox_dn_4: 0.8684  loss_giou_dn_4: 0.7688  loss_ce_5: 2.509  loss_mask_5: 1.395  loss_dice_5: 2.646  loss_bbox_5: 2.906  loss_giou_5: 1.73  loss_ce_dn_5: 2.928  loss_mask_dn_5: 1.228  loss_dice_dn_5: 2.483  loss_bbox_dn_5: 0.8591  loss_giou_dn_5: 0.7682  loss_ce_6: 2.598  loss_mask_6: 1.357  loss_dice_6: 2.598  loss_bbox_6: 2.849  loss_giou_6: 1.687  loss_ce_dn_6: 2.976  loss_mask_dn_6: 1.244  loss_dice_dn_6: 2.497  loss_bbox_dn_6: 0.8616  loss_giou_dn_6: 0.7673  loss_ce_7: 2.593  loss_mask_7: 1.315  loss_dice_7: 2.603  loss_bbox_7: 2.888  loss_giou_7: 1.664  loss_ce_dn_7: 2.999  loss_mask_dn_7: 1.266  loss_dice_dn_7: 2.525  loss_bbox_dn_7: 0.8372  loss_giou_dn_7: 0.7576  loss_ce_8: 2.734  loss_mask_8: 1.386  loss_dice_8: 2.607  loss_bbox_8: 2.907  loss_giou_8: 1.73  loss_ce_dn_8: 3.035  loss_mask_dn_8: 1.295  loss_dice_dn_8: 2.529  loss_bbox_dn_8: 0.8341  loss_giou_dn_8: 0.7597    time: 2.2747  last_time: 2.2426  data_time: 0.0154  last_data_time: 0.0148   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:12:25 d2.utils.events]: \u001b[0m eta: 8 days, 8:14:20  iter: 2019  total_loss: 204.3  loss_ce: 2.225  loss_mask: 1.72  loss_dice: 2.574  loss_bbox: 3.196  loss_giou: 1.68  loss_ce_dn: 2.446  loss_mask_dn: 1.464  loss_dice_dn: 2.533  loss_bbox_dn: 0.9249  loss_giou_dn: 0.7636  loss_ce_0: 12.66  loss_mask_0: 1.493  loss_dice_0: 2.885  loss_bbox_0: 3.696  loss_giou_0: 1.892  loss_ce_1: 2.646  loss_mask_1: 1.542  loss_dice_1: 2.587  loss_bbox_1: 3.44  loss_giou_1: 1.792  loss_ce_dn_1: 3.058  loss_mask_dn_1: 1.386  loss_dice_dn_1: 2.788  loss_bbox_dn_1: 1.037  loss_giou_dn_1: 0.7979  loss_ce_2: 2.259  loss_mask_2: 1.568  loss_dice_2: 2.568  loss_bbox_2: 3.324  loss_giou_2: 1.761  loss_ce_dn_2: 2.626  loss_mask_dn_2: 1.448  loss_dice_dn_2: 2.585  loss_bbox_dn_2: 0.9882  loss_giou_dn_2: 0.7752  loss_ce_3: 2.073  loss_mask_3: 1.576  loss_dice_3: 2.576  loss_bbox_3: 3.21  loss_giou_3: 1.713  loss_ce_dn_3: 2.457  loss_mask_dn_3: 1.393  loss_dice_dn_3: 2.544  loss_bbox_dn_3: 0.9409  loss_giou_dn_3: 0.7531  loss_ce_4: 2.045  loss_mask_4: 1.588  loss_dice_4: 2.561  loss_bbox_4: 3.222  loss_giou_4: 1.759  loss_ce_dn_4: 2.257  loss_mask_dn_4: 1.388  loss_dice_dn_4: 2.491  loss_bbox_dn_4: 0.9261  loss_giou_dn_4: 0.748  loss_ce_5: 2.055  loss_mask_5: 1.612  loss_dice_5: 2.562  loss_bbox_5: 3.232  loss_giou_5: 1.735  loss_ce_dn_5: 2.264  loss_mask_dn_5: 1.383  loss_dice_dn_5: 2.473  loss_bbox_dn_5: 0.9259  loss_giou_dn_5: 0.7461  loss_ce_6: 2.198  loss_mask_6: 1.611  loss_dice_6: 2.577  loss_bbox_6: 3.215  loss_giou_6: 1.703  loss_ce_dn_6: 2.303  loss_mask_dn_6: 1.453  loss_dice_dn_6: 2.467  loss_bbox_dn_6: 0.9256  loss_giou_dn_6: 0.7496  loss_ce_7: 2.136  loss_mask_7: 1.606  loss_dice_7: 2.571  loss_bbox_7: 3.161  loss_giou_7: 1.691  loss_ce_dn_7: 2.338  loss_mask_dn_7: 1.441  loss_dice_dn_7: 2.51  loss_bbox_dn_7: 0.9146  loss_giou_dn_7: 0.7529  loss_ce_8: 2.05  loss_mask_8: 1.693  loss_dice_8: 2.547  loss_bbox_8: 3.073  loss_giou_8: 1.646  loss_ce_dn_8: 2.406  loss_mask_dn_8: 1.466  loss_dice_dn_8: 2.493  loss_bbox_dn_8: 0.9183  loss_giou_dn_8: 0.758    time: 2.2747  last_time: 2.2787  data_time: 0.0151  last_data_time: 0.0165   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:13:11 d2.utils.events]: \u001b[0m eta: 8 days, 8:13:01  iter: 2039  total_loss: 207.1  loss_ce: 2.192  loss_mask: 1.799  loss_dice: 2.874  loss_bbox: 2.894  loss_giou: 1.709  loss_ce_dn: 3.274  loss_mask_dn: 1.606  loss_dice_dn: 2.658  loss_bbox_dn: 0.896  loss_giou_dn: 0.7587  loss_ce_0: 12.79  loss_mask_0: 1.565  loss_dice_0: 3.033  loss_bbox_0: 3.631  loss_giou_0: 1.921  loss_ce_1: 2.495  loss_mask_1: 1.617  loss_dice_1: 2.835  loss_bbox_1: 3.273  loss_giou_1: 1.744  loss_ce_dn_1: 3.612  loss_mask_dn_1: 1.545  loss_dice_dn_1: 2.842  loss_bbox_dn_1: 1.025  loss_giou_dn_1: 0.816  loss_ce_2: 2.292  loss_mask_2: 1.672  loss_dice_2: 2.791  loss_bbox_2: 3.057  loss_giou_2: 1.706  loss_ce_dn_2: 3.158  loss_mask_dn_2: 1.522  loss_dice_dn_2: 2.72  loss_bbox_dn_2: 0.9606  loss_giou_dn_2: 0.8003  loss_ce_3: 2.143  loss_mask_3: 1.671  loss_dice_3: 2.8  loss_bbox_3: 3.102  loss_giou_3: 1.72  loss_ce_dn_3: 2.987  loss_mask_dn_3: 1.511  loss_dice_dn_3: 2.694  loss_bbox_dn_3: 0.9278  loss_giou_dn_3: 0.7845  loss_ce_4: 2.017  loss_mask_4: 1.772  loss_dice_4: 2.819  loss_bbox_4: 3.046  loss_giou_4: 1.742  loss_ce_dn_4: 2.972  loss_mask_dn_4: 1.582  loss_dice_dn_4: 2.628  loss_bbox_dn_4: 0.9165  loss_giou_dn_4: 0.7763  loss_ce_5: 2.116  loss_mask_5: 1.82  loss_dice_5: 2.83  loss_bbox_5: 3.052  loss_giou_5: 1.722  loss_ce_dn_5: 2.912  loss_mask_dn_5: 1.545  loss_dice_dn_5: 2.651  loss_bbox_dn_5: 0.9107  loss_giou_dn_5: 0.7759  loss_ce_6: 2.132  loss_mask_6: 1.809  loss_dice_6: 2.862  loss_bbox_6: 2.996  loss_giou_6: 1.748  loss_ce_dn_6: 2.891  loss_mask_dn_6: 1.544  loss_dice_dn_6: 2.634  loss_bbox_dn_6: 0.9089  loss_giou_dn_6: 0.7716  loss_ce_7: 2.198  loss_mask_7: 1.811  loss_dice_7: 2.882  loss_bbox_7: 3.024  loss_giou_7: 1.745  loss_ce_dn_7: 2.986  loss_mask_dn_7: 1.585  loss_dice_dn_7: 2.66  loss_bbox_dn_7: 0.8925  loss_giou_dn_7: 0.7612  loss_ce_8: 2.117  loss_mask_8: 1.821  loss_dice_8: 2.852  loss_bbox_8: 2.939  loss_giou_8: 1.717  loss_ce_dn_8: 3.143  loss_mask_dn_8: 1.597  loss_dice_dn_8: 2.612  loss_bbox_dn_8: 0.8927  loss_giou_dn_8: 0.7612    time: 2.2748  last_time: 2.3203  data_time: 0.0127  last_data_time: 0.0191   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:13:56 d2.utils.events]: \u001b[0m eta: 8 days, 8:12:49  iter: 2059  total_loss: 200.4  loss_ce: 2.204  loss_mask: 1.466  loss_dice: 2.723  loss_bbox: 3.015  loss_giou: 1.637  loss_ce_dn: 2.964  loss_mask_dn: 1.428  loss_dice_dn: 2.486  loss_bbox_dn: 0.9926  loss_giou_dn: 0.7644  loss_ce_0: 12.5  loss_mask_0: 1.424  loss_dice_0: 2.981  loss_bbox_0: 3.636  loss_giou_0: 1.821  loss_ce_1: 2.461  loss_mask_1: 1.406  loss_dice_1: 2.864  loss_bbox_1: 3.371  loss_giou_1: 1.76  loss_ce_dn_1: 3.532  loss_mask_dn_1: 1.472  loss_dice_dn_1: 2.817  loss_bbox_dn_1: 1.076  loss_giou_dn_1: 0.8068  loss_ce_2: 2.201  loss_mask_2: 1.455  loss_dice_2: 2.735  loss_bbox_2: 3.146  loss_giou_2: 1.691  loss_ce_dn_2: 2.954  loss_mask_dn_2: 1.452  loss_dice_dn_2: 2.648  loss_bbox_dn_2: 1.072  loss_giou_dn_2: 0.7867  loss_ce_3: 2.235  loss_mask_3: 1.445  loss_dice_3: 2.736  loss_bbox_3: 3.065  loss_giou_3: 1.656  loss_ce_dn_3: 2.765  loss_mask_dn_3: 1.409  loss_dice_dn_3: 2.578  loss_bbox_dn_3: 1.044  loss_giou_dn_3: 0.7746  loss_ce_4: 2.101  loss_mask_4: 1.484  loss_dice_4: 2.748  loss_bbox_4: 3.04  loss_giou_4: 1.666  loss_ce_dn_4: 2.745  loss_mask_dn_4: 1.431  loss_dice_dn_4: 2.546  loss_bbox_dn_4: 1.04  loss_giou_dn_4: 0.7666  loss_ce_5: 2.038  loss_mask_5: 1.467  loss_dice_5: 2.758  loss_bbox_5: 3.042  loss_giou_5: 1.65  loss_ce_dn_5: 2.727  loss_mask_dn_5: 1.409  loss_dice_dn_5: 2.525  loss_bbox_dn_5: 1.037  loss_giou_dn_5: 0.7681  loss_ce_6: 2.067  loss_mask_6: 1.478  loss_dice_6: 2.775  loss_bbox_6: 3.021  loss_giou_6: 1.64  loss_ce_dn_6: 2.811  loss_mask_dn_6: 1.389  loss_dice_dn_6: 2.514  loss_bbox_dn_6: 1.035  loss_giou_dn_6: 0.7676  loss_ce_7: 2.171  loss_mask_7: 1.437  loss_dice_7: 2.745  loss_bbox_7: 2.993  loss_giou_7: 1.626  loss_ce_dn_7: 2.879  loss_mask_dn_7: 1.392  loss_dice_dn_7: 2.502  loss_bbox_dn_7: 1  loss_giou_dn_7: 0.7656  loss_ce_8: 2.245  loss_mask_8: 1.456  loss_dice_8: 2.733  loss_bbox_8: 3.006  loss_giou_8: 1.614  loss_ce_dn_8: 2.887  loss_mask_dn_8: 1.398  loss_dice_dn_8: 2.479  loss_bbox_dn_8: 0.9931  loss_giou_dn_8: 0.7629    time: 2.2748  last_time: 2.2924  data_time: 0.0121  last_data_time: 0.0092   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:14:42 d2.utils.events]: \u001b[0m eta: 8 days, 8:13:06  iter: 2079  total_loss: 207.7  loss_ce: 2.384  loss_mask: 1.422  loss_dice: 2.742  loss_bbox: 2.905  loss_giou: 1.587  loss_ce_dn: 3.202  loss_mask_dn: 1.399  loss_dice_dn: 2.567  loss_bbox_dn: 0.9984  loss_giou_dn: 0.7736  loss_ce_0: 12.69  loss_mask_0: 1.672  loss_dice_0: 2.942  loss_bbox_0: 3.5  loss_giou_0: 1.835  loss_ce_1: 2.899  loss_mask_1: 1.648  loss_dice_1: 2.722  loss_bbox_1: 2.987  loss_giou_1: 1.599  loss_ce_dn_1: 3.678  loss_mask_dn_1: 1.624  loss_dice_dn_1: 2.78  loss_bbox_dn_1: 1.136  loss_giou_dn_1: 0.8075  loss_ce_2: 2.371  loss_mask_2: 1.57  loss_dice_2: 2.69  loss_bbox_2: 2.919  loss_giou_2: 1.583  loss_ce_dn_2: 3.031  loss_mask_dn_2: 1.625  loss_dice_dn_2: 2.626  loss_bbox_dn_2: 1.019  loss_giou_dn_2: 0.7853  loss_ce_3: 2.247  loss_mask_3: 1.603  loss_dice_3: 2.683  loss_bbox_3: 2.863  loss_giou_3: 1.597  loss_ce_dn_3: 2.821  loss_mask_dn_3: 1.586  loss_dice_dn_3: 2.596  loss_bbox_dn_3: 1.01  loss_giou_dn_3: 0.772  loss_ce_4: 2.194  loss_mask_4: 1.51  loss_dice_4: 2.677  loss_bbox_4: 2.846  loss_giou_4: 1.596  loss_ce_dn_4: 2.773  loss_mask_dn_4: 1.501  loss_dice_dn_4: 2.546  loss_bbox_dn_4: 1.006  loss_giou_dn_4: 0.773  loss_ce_5: 2.232  loss_mask_5: 1.542  loss_dice_5: 2.642  loss_bbox_5: 2.862  loss_giou_5: 1.573  loss_ce_dn_5: 2.826  loss_mask_dn_5: 1.519  loss_dice_dn_5: 2.564  loss_bbox_dn_5: 1.005  loss_giou_dn_5: 0.7738  loss_ce_6: 2.331  loss_mask_6: 1.468  loss_dice_6: 2.665  loss_bbox_6: 2.859  loss_giou_6: 1.556  loss_ce_dn_6: 2.894  loss_mask_dn_6: 1.477  loss_dice_dn_6: 2.592  loss_bbox_dn_6: 1.003  loss_giou_dn_6: 0.7757  loss_ce_7: 2.507  loss_mask_7: 1.476  loss_dice_7: 2.68  loss_bbox_7: 2.853  loss_giou_7: 1.555  loss_ce_dn_7: 3.007  loss_mask_dn_7: 1.392  loss_dice_dn_7: 2.6  loss_bbox_dn_7: 0.9823  loss_giou_dn_7: 0.7688  loss_ce_8: 2.359  loss_mask_8: 1.462  loss_dice_8: 2.671  loss_bbox_8: 2.88  loss_giou_8: 1.572  loss_ce_dn_8: 3.021  loss_mask_dn_8: 1.451  loss_dice_dn_8: 2.585  loss_bbox_dn_8: 0.9866  loss_giou_dn_8: 0.7694    time: 2.2749  last_time: 2.2651  data_time: 0.0157  last_data_time: 0.0079   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:15:28 d2.utils.events]: \u001b[0m eta: 8 days, 8:13:02  iter: 2099  total_loss: 209  loss_ce: 1.996  loss_mask: 1.642  loss_dice: 2.792  loss_bbox: 3.139  loss_giou: 1.751  loss_ce_dn: 2.878  loss_mask_dn: 1.625  loss_dice_dn: 2.596  loss_bbox_dn: 1.025  loss_giou_dn: 0.7489  loss_ce_0: 12.49  loss_mask_0: 1.61  loss_dice_0: 3.054  loss_bbox_0: 3.861  loss_giou_0: 1.882  loss_ce_1: 2.433  loss_mask_1: 1.601  loss_dice_1: 2.727  loss_bbox_1: 3.398  loss_giou_1: 1.805  loss_ce_dn_1: 3.279  loss_mask_dn_1: 1.608  loss_dice_dn_1: 2.742  loss_bbox_dn_1: 1.11  loss_giou_dn_1: 0.8012  loss_ce_2: 2.026  loss_mask_2: 1.577  loss_dice_2: 2.636  loss_bbox_2: 3.161  loss_giou_2: 1.724  loss_ce_dn_2: 2.81  loss_mask_dn_2: 1.538  loss_dice_dn_2: 2.623  loss_bbox_dn_2: 1.053  loss_giou_dn_2: 0.7656  loss_ce_3: 2.011  loss_mask_3: 1.609  loss_dice_3: 2.679  loss_bbox_3: 3.101  loss_giou_3: 1.687  loss_ce_dn_3: 2.742  loss_mask_dn_3: 1.546  loss_dice_dn_3: 2.588  loss_bbox_dn_3: 1.059  loss_giou_dn_3: 0.7538  loss_ce_4: 1.904  loss_mask_4: 1.57  loss_dice_4: 2.658  loss_bbox_4: 3.086  loss_giou_4: 1.717  loss_ce_dn_4: 2.797  loss_mask_dn_4: 1.563  loss_dice_dn_4: 2.578  loss_bbox_dn_4: 1.048  loss_giou_dn_4: 0.756  loss_ce_5: 1.904  loss_mask_5: 1.64  loss_dice_5: 2.781  loss_bbox_5: 3.158  loss_giou_5: 1.738  loss_ce_dn_5: 2.723  loss_mask_dn_5: 1.629  loss_dice_dn_5: 2.604  loss_bbox_dn_5: 1.039  loss_giou_dn_5: 0.7656  loss_ce_6: 1.921  loss_mask_6: 1.614  loss_dice_6: 2.732  loss_bbox_6: 3.144  loss_giou_6: 1.756  loss_ce_dn_6: 2.754  loss_mask_dn_6: 1.601  loss_dice_dn_6: 2.589  loss_bbox_dn_6: 1.029  loss_giou_dn_6: 0.7637  loss_ce_7: 1.912  loss_mask_7: 1.585  loss_dice_7: 2.803  loss_bbox_7: 3.11  loss_giou_7: 1.727  loss_ce_dn_7: 2.729  loss_mask_dn_7: 1.569  loss_dice_dn_7: 2.604  loss_bbox_dn_7: 1.021  loss_giou_dn_7: 0.7474  loss_ce_8: 1.958  loss_mask_8: 1.578  loss_dice_8: 2.768  loss_bbox_8: 3.103  loss_giou_8: 1.729  loss_ce_dn_8: 2.807  loss_mask_dn_8: 1.573  loss_dice_dn_8: 2.597  loss_bbox_dn_8: 1.023  loss_giou_dn_8: 0.7444    time: 2.2749  last_time: 2.2741  data_time: 0.0137  last_data_time: 0.0067   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:16:13 d2.utils.events]: \u001b[0m eta: 8 days, 8:12:41  iter: 2119  total_loss: 202  loss_ce: 2.421  loss_mask: 1.722  loss_dice: 2.679  loss_bbox: 3.136  loss_giou: 1.601  loss_ce_dn: 3.116  loss_mask_dn: 1.49  loss_dice_dn: 2.392  loss_bbox_dn: 0.9308  loss_giou_dn: 0.7636  loss_ce_0: 12.16  loss_mask_0: 1.525  loss_dice_0: 2.716  loss_bbox_0: 3.634  loss_giou_0: 1.811  loss_ce_1: 3.087  loss_mask_1: 1.677  loss_dice_1: 2.67  loss_bbox_1: 3.098  loss_giou_1: 1.721  loss_ce_dn_1: 3.689  loss_mask_dn_1: 1.472  loss_dice_dn_1: 2.609  loss_bbox_dn_1: 1.144  loss_giou_dn_1: 0.8008  loss_ce_2: 2.802  loss_mask_2: 1.633  loss_dice_2: 2.671  loss_bbox_2: 3.021  loss_giou_2: 1.674  loss_ce_dn_2: 3.065  loss_mask_dn_2: 1.528  loss_dice_dn_2: 2.455  loss_bbox_dn_2: 1.05  loss_giou_dn_2: 0.7834  loss_ce_3: 2.375  loss_mask_3: 1.596  loss_dice_3: 2.67  loss_bbox_3: 2.965  loss_giou_3: 1.657  loss_ce_dn_3: 2.936  loss_mask_dn_3: 1.504  loss_dice_dn_3: 2.45  loss_bbox_dn_3: 0.9853  loss_giou_dn_3: 0.7663  loss_ce_4: 2.329  loss_mask_4: 1.608  loss_dice_4: 2.696  loss_bbox_4: 2.961  loss_giou_4: 1.646  loss_ce_dn_4: 2.965  loss_mask_dn_4: 1.491  loss_dice_dn_4: 2.406  loss_bbox_dn_4: 0.9661  loss_giou_dn_4: 0.7624  loss_ce_5: 2.381  loss_mask_5: 1.801  loss_dice_5: 2.716  loss_bbox_5: 2.964  loss_giou_5: 1.622  loss_ce_dn_5: 3.029  loss_mask_dn_5: 1.426  loss_dice_dn_5: 2.382  loss_bbox_dn_5: 0.9604  loss_giou_dn_5: 0.7615  loss_ce_6: 2.339  loss_mask_6: 1.801  loss_dice_6: 2.652  loss_bbox_6: 3.131  loss_giou_6: 1.61  loss_ce_dn_6: 3.021  loss_mask_dn_6: 1.435  loss_dice_dn_6: 2.369  loss_bbox_dn_6: 0.9614  loss_giou_dn_6: 0.7668  loss_ce_7: 2.265  loss_mask_7: 1.784  loss_dice_7: 2.688  loss_bbox_7: 3.076  loss_giou_7: 1.566  loss_ce_dn_7: 3.014  loss_mask_dn_7: 1.465  loss_dice_dn_7: 2.39  loss_bbox_dn_7: 0.9424  loss_giou_dn_7: 0.7625  loss_ce_8: 2.423  loss_mask_8: 1.764  loss_dice_8: 2.679  loss_bbox_8: 3.156  loss_giou_8: 1.601  loss_ce_dn_8: 2.975  loss_mask_dn_8: 1.476  loss_dice_dn_8: 2.37  loss_bbox_dn_8: 0.9378  loss_giou_dn_8: 0.7664    time: 2.2750  last_time: 2.2838  data_time: 0.0131  last_data_time: 0.0132   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:16:59 d2.utils.events]: \u001b[0m eta: 8 days, 8:13:04  iter: 2139  total_loss: 212.1  loss_ce: 2.291  loss_mask: 1.543  loss_dice: 2.901  loss_bbox: 2.802  loss_giou: 1.685  loss_ce_dn: 3.141  loss_mask_dn: 1.502  loss_dice_dn: 2.867  loss_bbox_dn: 0.9023  loss_giou_dn: 0.7645  loss_ce_0: 12.27  loss_mask_0: 1.434  loss_dice_0: 3.077  loss_bbox_0: 3.754  loss_giou_0: 1.818  loss_ce_1: 2.95  loss_mask_1: 1.481  loss_dice_1: 2.972  loss_bbox_1: 3.371  loss_giou_1: 1.743  loss_ce_dn_1: 3.434  loss_mask_dn_1: 1.406  loss_dice_dn_1: 2.966  loss_bbox_dn_1: 1.027  loss_giou_dn_1: 0.8062  loss_ce_2: 2.442  loss_mask_2: 1.45  loss_dice_2: 2.844  loss_bbox_2: 3.09  loss_giou_2: 1.717  loss_ce_dn_2: 2.966  loss_mask_dn_2: 1.361  loss_dice_dn_2: 2.85  loss_bbox_dn_2: 0.9794  loss_giou_dn_2: 0.7873  loss_ce_3: 2.355  loss_mask_3: 1.464  loss_dice_3: 2.842  loss_bbox_3: 3.04  loss_giou_3: 1.719  loss_ce_dn_3: 2.862  loss_mask_dn_3: 1.372  loss_dice_dn_3: 2.843  loss_bbox_dn_3: 0.9516  loss_giou_dn_3: 0.7695  loss_ce_4: 2.166  loss_mask_4: 1.499  loss_dice_4: 2.95  loss_bbox_4: 3.07  loss_giou_4: 1.646  loss_ce_dn_4: 2.702  loss_mask_dn_4: 1.381  loss_dice_dn_4: 2.839  loss_bbox_dn_4: 0.9387  loss_giou_dn_4: 0.7634  loss_ce_5: 2.24  loss_mask_5: 1.587  loss_dice_5: 2.931  loss_bbox_5: 2.954  loss_giou_5: 1.646  loss_ce_dn_5: 2.838  loss_mask_dn_5: 1.416  loss_dice_dn_5: 2.8  loss_bbox_dn_5: 0.9164  loss_giou_dn_5: 0.7602  loss_ce_6: 2.253  loss_mask_6: 1.466  loss_dice_6: 2.904  loss_bbox_6: 2.895  loss_giou_6: 1.668  loss_ce_dn_6: 2.869  loss_mask_dn_6: 1.453  loss_dice_dn_6: 2.826  loss_bbox_dn_6: 0.9109  loss_giou_dn_6: 0.7635  loss_ce_7: 2.236  loss_mask_7: 1.471  loss_dice_7: 2.914  loss_bbox_7: 2.87  loss_giou_7: 1.676  loss_ce_dn_7: 2.817  loss_mask_dn_7: 1.461  loss_dice_dn_7: 2.785  loss_bbox_dn_7: 0.9055  loss_giou_dn_7: 0.7633  loss_ce_8: 2.293  loss_mask_8: 1.461  loss_dice_8: 2.873  loss_bbox_8: 2.804  loss_giou_8: 1.68  loss_ce_dn_8: 2.971  loss_mask_dn_8: 1.473  loss_dice_dn_8: 2.911  loss_bbox_dn_8: 0.8995  loss_giou_dn_8: 0.7636    time: 2.2751  last_time: 2.2912  data_time: 0.0129  last_data_time: 0.0144   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:17:44 d2.utils.events]: \u001b[0m eta: 8 days, 8:12:30  iter: 2159  total_loss: 196.3  loss_ce: 2.239  loss_mask: 1.634  loss_dice: 2.66  loss_bbox: 2.99  loss_giou: 1.701  loss_ce_dn: 2.957  loss_mask_dn: 1.427  loss_dice_dn: 2.503  loss_bbox_dn: 0.9948  loss_giou_dn: 0.7906  loss_ce_0: 12.05  loss_mask_0: 1.376  loss_dice_0: 2.924  loss_bbox_0: 3.947  loss_giou_0: 1.897  loss_ce_1: 2.497  loss_mask_1: 1.379  loss_dice_1: 2.723  loss_bbox_1: 3.243  loss_giou_1: 1.772  loss_ce_dn_1: 3.216  loss_mask_dn_1: 1.563  loss_dice_dn_1: 2.671  loss_bbox_dn_1: 1.062  loss_giou_dn_1: 0.815  loss_ce_2: 2.08  loss_mask_2: 1.443  loss_dice_2: 2.728  loss_bbox_2: 3.05  loss_giou_2: 1.699  loss_ce_dn_2: 2.689  loss_mask_dn_2: 1.512  loss_dice_dn_2: 2.55  loss_bbox_dn_2: 1.016  loss_giou_dn_2: 0.7957  loss_ce_3: 2.079  loss_mask_3: 1.458  loss_dice_3: 2.692  loss_bbox_3: 3.01  loss_giou_3: 1.686  loss_ce_dn_3: 2.557  loss_mask_dn_3: 1.458  loss_dice_dn_3: 2.516  loss_bbox_dn_3: 0.996  loss_giou_dn_3: 0.7837  loss_ce_4: 1.989  loss_mask_4: 1.445  loss_dice_4: 2.657  loss_bbox_4: 2.95  loss_giou_4: 1.701  loss_ce_dn_4: 2.631  loss_mask_dn_4: 1.439  loss_dice_dn_4: 2.502  loss_bbox_dn_4: 0.9962  loss_giou_dn_4: 0.7825  loss_ce_5: 2.124  loss_mask_5: 1.49  loss_dice_5: 2.644  loss_bbox_5: 3.021  loss_giou_5: 1.7  loss_ce_dn_5: 2.632  loss_mask_dn_5: 1.367  loss_dice_dn_5: 2.48  loss_bbox_dn_5: 0.9901  loss_giou_dn_5: 0.7884  loss_ce_6: 2.018  loss_mask_6: 1.544  loss_dice_6: 2.683  loss_bbox_6: 3.025  loss_giou_6: 1.719  loss_ce_dn_6: 2.672  loss_mask_dn_6: 1.379  loss_dice_dn_6: 2.501  loss_bbox_dn_6: 0.998  loss_giou_dn_6: 0.7905  loss_ce_7: 2.167  loss_mask_7: 1.58  loss_dice_7: 2.693  loss_bbox_7: 3.008  loss_giou_7: 1.71  loss_ce_dn_7: 2.822  loss_mask_dn_7: 1.414  loss_dice_dn_7: 2.513  loss_bbox_dn_7: 0.9959  loss_giou_dn_7: 0.7907  loss_ce_8: 2.296  loss_mask_8: 1.601  loss_dice_8: 2.66  loss_bbox_8: 2.998  loss_giou_8: 1.708  loss_ce_dn_8: 2.896  loss_mask_dn_8: 1.454  loss_dice_dn_8: 2.518  loss_bbox_dn_8: 0.9947  loss_giou_dn_8: 0.7882    time: 2.2750  last_time: 2.2581  data_time: 0.0123  last_data_time: 0.0197   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:18:30 d2.utils.events]: \u001b[0m eta: 8 days, 8:11:33  iter: 2179  total_loss: 206.9  loss_ce: 2.311  loss_mask: 1.678  loss_dice: 2.917  loss_bbox: 2.782  loss_giou: 1.715  loss_ce_dn: 3.095  loss_mask_dn: 1.536  loss_dice_dn: 2.62  loss_bbox_dn: 0.8826  loss_giou_dn: 0.7875  loss_ce_0: 12.5  loss_mask_0: 1.54  loss_dice_0: 3.083  loss_bbox_0: 3.567  loss_giou_0: 1.964  loss_ce_1: 3.011  loss_mask_1: 1.601  loss_dice_1: 2.911  loss_bbox_1: 3.164  loss_giou_1: 1.772  loss_ce_dn_1: 3.712  loss_mask_dn_1: 1.617  loss_dice_dn_1: 2.803  loss_bbox_dn_1: 0.9591  loss_giou_dn_1: 0.8081  loss_ce_2: 2.589  loss_mask_2: 1.691  loss_dice_2: 2.829  loss_bbox_2: 2.915  loss_giou_2: 1.792  loss_ce_dn_2: 3.126  loss_mask_dn_2: 1.48  loss_dice_dn_2: 2.671  loss_bbox_dn_2: 0.9092  loss_giou_dn_2: 0.7913  loss_ce_3: 2.338  loss_mask_3: 1.633  loss_dice_3: 2.833  loss_bbox_3: 2.806  loss_giou_3: 1.761  loss_ce_dn_3: 3.014  loss_mask_dn_3: 1.48  loss_dice_dn_3: 2.597  loss_bbox_dn_3: 0.8825  loss_giou_dn_3: 0.7869  loss_ce_4: 2.249  loss_mask_4: 1.6  loss_dice_4: 2.91  loss_bbox_4: 2.779  loss_giou_4: 1.755  loss_ce_dn_4: 2.951  loss_mask_dn_4: 1.563  loss_dice_dn_4: 2.628  loss_bbox_dn_4: 0.8712  loss_giou_dn_4: 0.7826  loss_ce_5: 2.269  loss_mask_5: 1.58  loss_dice_5: 2.928  loss_bbox_5: 2.773  loss_giou_5: 1.751  loss_ce_dn_5: 2.98  loss_mask_dn_5: 1.541  loss_dice_dn_5: 2.558  loss_bbox_dn_5: 0.8897  loss_giou_dn_5: 0.7789  loss_ce_6: 2.205  loss_mask_6: 1.563  loss_dice_6: 2.92  loss_bbox_6: 2.795  loss_giou_6: 1.727  loss_ce_dn_6: 2.943  loss_mask_dn_6: 1.546  loss_dice_dn_6: 2.58  loss_bbox_dn_6: 0.8984  loss_giou_dn_6: 0.7819  loss_ce_7: 2.328  loss_mask_7: 1.636  loss_dice_7: 2.851  loss_bbox_7: 2.789  loss_giou_7: 1.728  loss_ce_dn_7: 2.967  loss_mask_dn_7: 1.522  loss_dice_dn_7: 2.593  loss_bbox_dn_7: 0.8738  loss_giou_dn_7: 0.7752  loss_ce_8: 2.296  loss_mask_8: 1.626  loss_dice_8: 2.823  loss_bbox_8: 2.785  loss_giou_8: 1.719  loss_ce_dn_8: 3.103  loss_mask_dn_8: 1.533  loss_dice_dn_8: 2.604  loss_bbox_dn_8: 0.875  loss_giou_dn_8: 0.7813    time: 2.2749  last_time: 2.2501  data_time: 0.0110  last_data_time: 0.0069   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:19:16 d2.utils.events]: \u001b[0m eta: 8 days, 8:11:16  iter: 2199  total_loss: 192.4  loss_ce: 2.343  loss_mask: 1.532  loss_dice: 2.532  loss_bbox: 2.746  loss_giou: 1.538  loss_ce_dn: 3.021  loss_mask_dn: 1.444  loss_dice_dn: 2.32  loss_bbox_dn: 0.9248  loss_giou_dn: 0.7713  loss_ce_0: 12.17  loss_mask_0: 1.361  loss_dice_0: 2.77  loss_bbox_0: 3.721  loss_giou_0: 1.946  loss_ce_1: 2.459  loss_mask_1: 1.574  loss_dice_1: 2.668  loss_bbox_1: 3.043  loss_giou_1: 1.66  loss_ce_dn_1: 3.314  loss_mask_dn_1: 1.475  loss_dice_dn_1: 2.456  loss_bbox_dn_1: 1.034  loss_giou_dn_1: 0.8039  loss_ce_2: 2.368  loss_mask_2: 1.493  loss_dice_2: 2.608  loss_bbox_2: 2.771  loss_giou_2: 1.654  loss_ce_dn_2: 2.801  loss_mask_dn_2: 1.449  loss_dice_dn_2: 2.279  loss_bbox_dn_2: 0.9967  loss_giou_dn_2: 0.7882  loss_ce_3: 2.409  loss_mask_3: 1.496  loss_dice_3: 2.504  loss_bbox_3: 2.781  loss_giou_3: 1.61  loss_ce_dn_3: 2.658  loss_mask_dn_3: 1.442  loss_dice_dn_3: 2.248  loss_bbox_dn_3: 0.9824  loss_giou_dn_3: 0.7819  loss_ce_4: 2.287  loss_mask_4: 1.481  loss_dice_4: 2.529  loss_bbox_4: 2.794  loss_giou_4: 1.619  loss_ce_dn_4: 2.588  loss_mask_dn_4: 1.453  loss_dice_dn_4: 2.227  loss_bbox_dn_4: 0.9646  loss_giou_dn_4: 0.7809  loss_ce_5: 2.263  loss_mask_5: 1.502  loss_dice_5: 2.513  loss_bbox_5: 2.774  loss_giou_5: 1.597  loss_ce_dn_5: 2.726  loss_mask_dn_5: 1.321  loss_dice_dn_5: 2.26  loss_bbox_dn_5: 0.9636  loss_giou_dn_5: 0.7807  loss_ce_6: 2.425  loss_mask_6: 1.537  loss_dice_6: 2.477  loss_bbox_6: 2.729  loss_giou_6: 1.567  loss_ce_dn_6: 2.698  loss_mask_dn_6: 1.383  loss_dice_dn_6: 2.266  loss_bbox_dn_6: 0.9629  loss_giou_dn_6: 0.7835  loss_ce_7: 2.399  loss_mask_7: 1.487  loss_dice_7: 2.536  loss_bbox_7: 2.69  loss_giou_7: 1.534  loss_ce_dn_7: 2.726  loss_mask_dn_7: 1.408  loss_dice_dn_7: 2.265  loss_bbox_dn_7: 0.9366  loss_giou_dn_7: 0.7672  loss_ce_8: 2.27  loss_mask_8: 1.515  loss_dice_8: 2.552  loss_bbox_8: 2.747  loss_giou_8: 1.544  loss_ce_dn_8: 2.808  loss_mask_dn_8: 1.413  loss_dice_dn_8: 2.256  loss_bbox_dn_8: 0.9265  loss_giou_dn_8: 0.769    time: 2.2751  last_time: 2.2629  data_time: 0.0134  last_data_time: 0.0090   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:20:01 d2.utils.events]: \u001b[0m eta: 8 days, 8:11:22  iter: 2219  total_loss: 191.8  loss_ce: 2.049  loss_mask: 1.863  loss_dice: 2.448  loss_bbox: 2.702  loss_giou: 1.639  loss_ce_dn: 2.829  loss_mask_dn: 1.661  loss_dice_dn: 2.257  loss_bbox_dn: 0.9509  loss_giou_dn: 0.7602  loss_ce_0: 11.85  loss_mask_0: 1.721  loss_dice_0: 2.753  loss_bbox_0: 3.729  loss_giou_0: 1.892  loss_ce_1: 2.27  loss_mask_1: 1.825  loss_dice_1: 2.585  loss_bbox_1: 3.014  loss_giou_1: 1.577  loss_ce_dn_1: 3.22  loss_mask_dn_1: 1.623  loss_dice_dn_1: 2.499  loss_bbox_dn_1: 1.038  loss_giou_dn_1: 0.7994  loss_ce_2: 2.075  loss_mask_2: 1.763  loss_dice_2: 2.574  loss_bbox_2: 2.746  loss_giou_2: 1.566  loss_ce_dn_2: 2.785  loss_mask_dn_2: 1.651  loss_dice_dn_2: 2.352  loss_bbox_dn_2: 1  loss_giou_dn_2: 0.7705  loss_ce_3: 1.963  loss_mask_3: 1.839  loss_dice_3: 2.588  loss_bbox_3: 2.774  loss_giou_3: 1.543  loss_ce_dn_3: 2.615  loss_mask_dn_3: 1.636  loss_dice_dn_3: 2.313  loss_bbox_dn_3: 0.9724  loss_giou_dn_3: 0.7656  loss_ce_4: 1.961  loss_mask_4: 1.922  loss_dice_4: 2.6  loss_bbox_4: 2.736  loss_giou_4: 1.56  loss_ce_dn_4: 2.767  loss_mask_dn_4: 1.616  loss_dice_dn_4: 2.286  loss_bbox_dn_4: 0.9509  loss_giou_dn_4: 0.7569  loss_ce_5: 1.969  loss_mask_5: 1.858  loss_dice_5: 2.482  loss_bbox_5: 2.673  loss_giou_5: 1.61  loss_ce_dn_5: 2.741  loss_mask_dn_5: 1.613  loss_dice_dn_5: 2.279  loss_bbox_dn_5: 0.94  loss_giou_dn_5: 0.7539  loss_ce_6: 1.947  loss_mask_6: 1.827  loss_dice_6: 2.526  loss_bbox_6: 2.655  loss_giou_6: 1.639  loss_ce_dn_6: 2.753  loss_mask_dn_6: 1.595  loss_dice_dn_6: 2.231  loss_bbox_dn_6: 0.936  loss_giou_dn_6: 0.7564  loss_ce_7: 1.988  loss_mask_7: 1.885  loss_dice_7: 2.576  loss_bbox_7: 2.679  loss_giou_7: 1.637  loss_ce_dn_7: 2.739  loss_mask_dn_7: 1.638  loss_dice_dn_7: 2.227  loss_bbox_dn_7: 0.9326  loss_giou_dn_7: 0.7563  loss_ce_8: 1.968  loss_mask_8: 1.839  loss_dice_8: 2.52  loss_bbox_8: 2.704  loss_giou_8: 1.64  loss_ce_dn_8: 2.742  loss_mask_dn_8: 1.659  loss_dice_dn_8: 2.239  loss_bbox_dn_8: 0.9428  loss_giou_dn_8: 0.7585    time: 2.2752  last_time: 2.2769  data_time: 0.0128  last_data_time: 0.0263   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:20:47 d2.utils.events]: \u001b[0m eta: 8 days, 8:09:28  iter: 2239  total_loss: 185.2  loss_ce: 2.102  loss_mask: 1.559  loss_dice: 2.616  loss_bbox: 2.671  loss_giou: 1.479  loss_ce_dn: 2.584  loss_mask_dn: 1.344  loss_dice_dn: 2.362  loss_bbox_dn: 0.9356  loss_giou_dn: 0.743  loss_ce_0: 11.81  loss_mask_0: 1.482  loss_dice_0: 2.83  loss_bbox_0: 3.525  loss_giou_0: 1.91  loss_ce_1: 2.344  loss_mask_1: 1.484  loss_dice_1: 2.596  loss_bbox_1: 3.188  loss_giou_1: 1.676  loss_ce_dn_1: 3.254  loss_mask_dn_1: 1.381  loss_dice_dn_1: 2.609  loss_bbox_dn_1: 1.054  loss_giou_dn_1: 0.8041  loss_ce_2: 2.101  loss_mask_2: 1.539  loss_dice_2: 2.644  loss_bbox_2: 2.935  loss_giou_2: 1.586  loss_ce_dn_2: 2.631  loss_mask_dn_2: 1.32  loss_dice_dn_2: 2.489  loss_bbox_dn_2: 0.9702  loss_giou_dn_2: 0.7784  loss_ce_3: 2.036  loss_mask_3: 1.55  loss_dice_3: 2.581  loss_bbox_3: 2.867  loss_giou_3: 1.59  loss_ce_dn_3: 2.462  loss_mask_dn_3: 1.307  loss_dice_dn_3: 2.413  loss_bbox_dn_3: 0.9413  loss_giou_dn_3: 0.7606  loss_ce_4: 1.898  loss_mask_4: 1.531  loss_dice_4: 2.603  loss_bbox_4: 2.771  loss_giou_4: 1.585  loss_ce_dn_4: 2.308  loss_mask_dn_4: 1.314  loss_dice_dn_4: 2.402  loss_bbox_dn_4: 0.9205  loss_giou_dn_4: 0.7502  loss_ce_5: 1.969  loss_mask_5: 1.56  loss_dice_5: 2.597  loss_bbox_5: 2.715  loss_giou_5: 1.547  loss_ce_dn_5: 2.339  loss_mask_dn_5: 1.286  loss_dice_dn_5: 2.345  loss_bbox_dn_5: 0.9169  loss_giou_dn_5: 0.7508  loss_ce_6: 2.061  loss_mask_6: 1.581  loss_dice_6: 2.625  loss_bbox_6: 2.685  loss_giou_6: 1.52  loss_ce_dn_6: 2.353  loss_mask_dn_6: 1.312  loss_dice_dn_6: 2.388  loss_bbox_dn_6: 0.9188  loss_giou_dn_6: 0.7549  loss_ce_7: 2.137  loss_mask_7: 1.587  loss_dice_7: 2.633  loss_bbox_7: 2.665  loss_giou_7: 1.501  loss_ce_dn_7: 2.457  loss_mask_dn_7: 1.311  loss_dice_dn_7: 2.367  loss_bbox_dn_7: 0.9207  loss_giou_dn_7: 0.7448  loss_ce_8: 2.083  loss_mask_8: 1.565  loss_dice_8: 2.59  loss_bbox_8: 2.661  loss_giou_8: 1.484  loss_ce_dn_8: 2.466  loss_mask_dn_8: 1.322  loss_dice_dn_8: 2.37  loss_bbox_dn_8: 0.9272  loss_giou_dn_8: 0.7434    time: 2.2752  last_time: 2.3231  data_time: 0.0122  last_data_time: 0.0080   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:21:32 d2.utils.events]: \u001b[0m eta: 8 days, 8:07:29  iter: 2259  total_loss: 202  loss_ce: 2.064  loss_mask: 1.998  loss_dice: 2.482  loss_bbox: 3.042  loss_giou: 1.717  loss_ce_dn: 3.013  loss_mask_dn: 1.847  loss_dice_dn: 2.333  loss_bbox_dn: 1.021  loss_giou_dn: 0.7841  loss_ce_0: 11.57  loss_mask_0: 1.751  loss_dice_0: 2.895  loss_bbox_0: 4.008  loss_giou_0: 1.966  loss_ce_1: 2.326  loss_mask_1: 1.977  loss_dice_1: 2.542  loss_bbox_1: 3.578  loss_giou_1: 1.791  loss_ce_dn_1: 3.512  loss_mask_dn_1: 1.825  loss_dice_dn_1: 2.521  loss_bbox_dn_1: 1.087  loss_giou_dn_1: 0.8025  loss_ce_2: 2.029  loss_mask_2: 1.917  loss_dice_2: 2.458  loss_bbox_2: 3.185  loss_giou_2: 1.71  loss_ce_dn_2: 2.969  loss_mask_dn_2: 1.821  loss_dice_dn_2: 2.404  loss_bbox_dn_2: 1.048  loss_giou_dn_2: 0.7892  loss_ce_3: 1.975  loss_mask_3: 1.956  loss_dice_3: 2.454  loss_bbox_3: 3.122  loss_giou_3: 1.694  loss_ce_dn_3: 2.849  loss_mask_dn_3: 1.83  loss_dice_dn_3: 2.353  loss_bbox_dn_3: 1.022  loss_giou_dn_3: 0.7836  loss_ce_4: 1.938  loss_mask_4: 1.991  loss_dice_4: 2.465  loss_bbox_4: 3.163  loss_giou_4: 1.706  loss_ce_dn_4: 2.788  loss_mask_dn_4: 1.749  loss_dice_dn_4: 2.355  loss_bbox_dn_4: 1.017  loss_giou_dn_4: 0.779  loss_ce_5: 1.938  loss_mask_5: 1.936  loss_dice_5: 2.459  loss_bbox_5: 3.135  loss_giou_5: 1.703  loss_ce_dn_5: 2.758  loss_mask_dn_5: 1.761  loss_dice_dn_5: 2.355  loss_bbox_dn_5: 1.021  loss_giou_dn_5: 0.7828  loss_ce_6: 1.968  loss_mask_6: 1.978  loss_dice_6: 2.476  loss_bbox_6: 3.12  loss_giou_6: 1.712  loss_ce_dn_6: 2.732  loss_mask_dn_6: 1.801  loss_dice_dn_6: 2.359  loss_bbox_dn_6: 1.029  loss_giou_dn_6: 0.7886  loss_ce_7: 1.946  loss_mask_7: 2.005  loss_dice_7: 2.466  loss_bbox_7: 3.057  loss_giou_7: 1.699  loss_ce_dn_7: 2.734  loss_mask_dn_7: 1.775  loss_dice_dn_7: 2.349  loss_bbox_dn_7: 1.017  loss_giou_dn_7: 0.7728  loss_ce_8: 2.057  loss_mask_8: 2.045  loss_dice_8: 2.503  loss_bbox_8: 3.051  loss_giou_8: 1.701  loss_ce_dn_8: 2.82  loss_mask_dn_8: 1.827  loss_dice_dn_8: 2.334  loss_bbox_dn_8: 1.021  loss_giou_dn_8: 0.7788    time: 2.2751  last_time: 2.2930  data_time: 0.0125  last_data_time: 0.0210   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:22:18 d2.utils.events]: \u001b[0m eta: 8 days, 8:05:08  iter: 2279  total_loss: 196  loss_ce: 2.272  loss_mask: 1.527  loss_dice: 2.725  loss_bbox: 2.963  loss_giou: 1.761  loss_ce_dn: 2.582  loss_mask_dn: 1.325  loss_dice_dn: 2.287  loss_bbox_dn: 0.8549  loss_giou_dn: 0.7527  loss_ce_0: 11.87  loss_mask_0: 1.433  loss_dice_0: 2.946  loss_bbox_0: 3.784  loss_giou_0: 1.987  loss_ce_1: 2.536  loss_mask_1: 1.485  loss_dice_1: 2.753  loss_bbox_1: 3.349  loss_giou_1: 1.845  loss_ce_dn_1: 3.209  loss_mask_dn_1: 1.428  loss_dice_dn_1: 2.6  loss_bbox_dn_1: 1.007  loss_giou_dn_1: 0.805  loss_ce_2: 2.276  loss_mask_2: 1.434  loss_dice_2: 2.667  loss_bbox_2: 2.994  loss_giou_2: 1.782  loss_ce_dn_2: 2.476  loss_mask_dn_2: 1.443  loss_dice_dn_2: 2.374  loss_bbox_dn_2: 0.9423  loss_giou_dn_2: 0.7796  loss_ce_3: 2.139  loss_mask_3: 1.447  loss_dice_3: 2.647  loss_bbox_3: 2.995  loss_giou_3: 1.764  loss_ce_dn_3: 2.295  loss_mask_dn_3: 1.35  loss_dice_dn_3: 2.332  loss_bbox_dn_3: 0.893  loss_giou_dn_3: 0.7717  loss_ce_4: 2.188  loss_mask_4: 1.528  loss_dice_4: 2.663  loss_bbox_4: 3.117  loss_giou_4: 1.801  loss_ce_dn_4: 2.202  loss_mask_dn_4: 1.308  loss_dice_dn_4: 2.328  loss_bbox_dn_4: 0.8824  loss_giou_dn_4: 0.7597  loss_ce_5: 2.095  loss_mask_5: 1.538  loss_dice_5: 2.674  loss_bbox_5: 3.113  loss_giou_5: 1.804  loss_ce_dn_5: 2.316  loss_mask_dn_5: 1.289  loss_dice_dn_5: 2.345  loss_bbox_dn_5: 0.8785  loss_giou_dn_5: 0.7503  loss_ce_6: 2.249  loss_mask_6: 1.524  loss_dice_6: 2.657  loss_bbox_6: 3.026  loss_giou_6: 1.779  loss_ce_dn_6: 2.349  loss_mask_dn_6: 1.29  loss_dice_dn_6: 2.341  loss_bbox_dn_6: 0.8802  loss_giou_dn_6: 0.7597  loss_ce_7: 2.283  loss_mask_7: 1.506  loss_dice_7: 2.677  loss_bbox_7: 3.011  loss_giou_7: 1.751  loss_ce_dn_7: 2.399  loss_mask_dn_7: 1.302  loss_dice_dn_7: 2.306  loss_bbox_dn_7: 0.8551  loss_giou_dn_7: 0.7477  loss_ce_8: 2.282  loss_mask_8: 1.556  loss_dice_8: 2.637  loss_bbox_8: 2.989  loss_giou_8: 1.756  loss_ce_dn_8: 2.512  loss_mask_dn_8: 1.342  loss_dice_dn_8: 2.278  loss_bbox_dn_8: 0.8475  loss_giou_dn_8: 0.7472    time: 2.2751  last_time: 2.2211  data_time: 0.0120  last_data_time: 0.0048   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:23:03 d2.utils.events]: \u001b[0m eta: 8 days, 8:04:30  iter: 2299  total_loss: 191.1  loss_ce: 2.196  loss_mask: 1.392  loss_dice: 2.331  loss_bbox: 2.658  loss_giou: 1.676  loss_ce_dn: 2.642  loss_mask_dn: 1.266  loss_dice_dn: 2.258  loss_bbox_dn: 0.8513  loss_giou_dn: 0.7386  loss_ce_0: 11.55  loss_mask_0: 1.384  loss_dice_0: 2.81  loss_bbox_0: 3.681  loss_giou_0: 1.877  loss_ce_1: 2.344  loss_mask_1: 1.415  loss_dice_1: 2.524  loss_bbox_1: 3.042  loss_giou_1: 1.838  loss_ce_dn_1: 3.157  loss_mask_dn_1: 1.307  loss_dice_dn_1: 2.415  loss_bbox_dn_1: 1.032  loss_giou_dn_1: 0.8063  loss_ce_2: 2.277  loss_mask_2: 1.466  loss_dice_2: 2.386  loss_bbox_2: 2.681  loss_giou_2: 1.776  loss_ce_dn_2: 2.662  loss_mask_dn_2: 1.274  loss_dice_dn_2: 2.254  loss_bbox_dn_2: 0.9788  loss_giou_dn_2: 0.7873  loss_ce_3: 2.182  loss_mask_3: 1.427  loss_dice_3: 2.424  loss_bbox_3: 2.656  loss_giou_3: 1.729  loss_ce_dn_3: 2.547  loss_mask_dn_3: 1.226  loss_dice_dn_3: 2.24  loss_bbox_dn_3: 0.9314  loss_giou_dn_3: 0.7718  loss_ce_4: 2.121  loss_mask_4: 1.396  loss_dice_4: 2.365  loss_bbox_4: 2.643  loss_giou_4: 1.734  loss_ce_dn_4: 2.565  loss_mask_dn_4: 1.251  loss_dice_dn_4: 2.223  loss_bbox_dn_4: 0.9045  loss_giou_dn_4: 0.7641  loss_ce_5: 2.13  loss_mask_5: 1.393  loss_dice_5: 2.385  loss_bbox_5: 2.661  loss_giou_5: 1.735  loss_ce_dn_5: 2.486  loss_mask_dn_5: 1.228  loss_dice_dn_5: 2.218  loss_bbox_dn_5: 0.8837  loss_giou_dn_5: 0.7564  loss_ce_6: 2.087  loss_mask_6: 1.386  loss_dice_6: 2.342  loss_bbox_6: 2.666  loss_giou_6: 1.724  loss_ce_dn_6: 2.462  loss_mask_dn_6: 1.251  loss_dice_dn_6: 2.242  loss_bbox_dn_6: 0.8779  loss_giou_dn_6: 0.7532  loss_ce_7: 2.131  loss_mask_7: 1.368  loss_dice_7: 2.378  loss_bbox_7: 2.677  loss_giou_7: 1.673  loss_ce_dn_7: 2.468  loss_mask_dn_7: 1.279  loss_dice_dn_7: 2.264  loss_bbox_dn_7: 0.8545  loss_giou_dn_7: 0.7402  loss_ce_8: 2.205  loss_mask_8: 1.397  loss_dice_8: 2.334  loss_bbox_8: 2.664  loss_giou_8: 1.678  loss_ce_dn_8: 2.495  loss_mask_dn_8: 1.299  loss_dice_dn_8: 2.27  loss_bbox_dn_8: 0.8524  loss_giou_dn_8: 0.7398    time: 2.2751  last_time: 2.2563  data_time: 0.0105  last_data_time: 0.0020   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:23:49 d2.utils.events]: \u001b[0m eta: 8 days, 8:03:11  iter: 2319  total_loss: 202.1  loss_ce: 2.14  loss_mask: 1.467  loss_dice: 2.885  loss_bbox: 3.133  loss_giou: 1.576  loss_ce_dn: 2.923  loss_mask_dn: 1.538  loss_dice_dn: 2.54  loss_bbox_dn: 0.9391  loss_giou_dn: 0.7613  loss_ce_0: 11.59  loss_mask_0: 1.455  loss_dice_0: 3.015  loss_bbox_0: 3.93  loss_giou_0: 1.976  loss_ce_1: 2.509  loss_mask_1: 1.554  loss_dice_1: 2.846  loss_bbox_1: 3.421  loss_giou_1: 1.758  loss_ce_dn_1: 3.384  loss_mask_dn_1: 1.605  loss_dice_dn_1: 2.708  loss_bbox_dn_1: 1.088  loss_giou_dn_1: 0.7977  loss_ce_2: 2.081  loss_mask_2: 1.501  loss_dice_2: 2.835  loss_bbox_2: 3.347  loss_giou_2: 1.678  loss_ce_dn_2: 2.9  loss_mask_dn_2: 1.531  loss_dice_dn_2: 2.523  loss_bbox_dn_2: 1.017  loss_giou_dn_2: 0.7716  loss_ce_3: 2.016  loss_mask_3: 1.481  loss_dice_3: 2.84  loss_bbox_3: 3.157  loss_giou_3: 1.62  loss_ce_dn_3: 2.876  loss_mask_dn_3: 1.494  loss_dice_dn_3: 2.511  loss_bbox_dn_3: 0.9604  loss_giou_dn_3: 0.7549  loss_ce_4: 1.998  loss_mask_4: 1.471  loss_dice_4: 2.835  loss_bbox_4: 3.213  loss_giou_4: 1.622  loss_ce_dn_4: 2.772  loss_mask_dn_4: 1.527  loss_dice_dn_4: 2.522  loss_bbox_dn_4: 0.9342  loss_giou_dn_4: 0.7578  loss_ce_5: 1.935  loss_mask_5: 1.466  loss_dice_5: 2.849  loss_bbox_5: 3.179  loss_giou_5: 1.636  loss_ce_dn_5: 2.663  loss_mask_dn_5: 1.55  loss_dice_dn_5: 2.515  loss_bbox_dn_5: 0.9233  loss_giou_dn_5: 0.7573  loss_ce_6: 2.025  loss_mask_6: 1.414  loss_dice_6: 2.842  loss_bbox_6: 3.218  loss_giou_6: 1.577  loss_ce_dn_6: 2.722  loss_mask_dn_6: 1.535  loss_dice_dn_6: 2.498  loss_bbox_dn_6: 0.9221  loss_giou_dn_6: 0.7546  loss_ce_7: 2.041  loss_mask_7: 1.504  loss_dice_7: 2.861  loss_bbox_7: 3.154  loss_giou_7: 1.574  loss_ce_dn_7: 2.703  loss_mask_dn_7: 1.513  loss_dice_dn_7: 2.513  loss_bbox_dn_7: 0.9225  loss_giou_dn_7: 0.7429  loss_ce_8: 2.06  loss_mask_8: 1.466  loss_dice_8: 2.881  loss_bbox_8: 3.13  loss_giou_8: 1.573  loss_ce_dn_8: 2.802  loss_mask_dn_8: 1.506  loss_dice_dn_8: 2.515  loss_bbox_dn_8: 0.9335  loss_giou_dn_8: 0.748    time: 2.2750  last_time: 2.2234  data_time: 0.0099  last_data_time: 0.0150   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:24:34 d2.utils.events]: \u001b[0m eta: 8 days, 8:00:45  iter: 2339  total_loss: 189.5  loss_ce: 2.316  loss_mask: 1.46  loss_dice: 2.432  loss_bbox: 2.535  loss_giou: 1.587  loss_ce_dn: 2.359  loss_mask_dn: 1.27  loss_dice_dn: 2.201  loss_bbox_dn: 0.8863  loss_giou_dn: 0.7497  loss_ce_0: 11.25  loss_mask_0: 1.258  loss_dice_0: 2.751  loss_bbox_0: 3.871  loss_giou_0: 2.042  loss_ce_1: 2.832  loss_mask_1: 1.313  loss_dice_1: 2.452  loss_bbox_1: 2.879  loss_giou_1: 1.713  loss_ce_dn_1: 3.107  loss_mask_dn_1: 1.29  loss_dice_dn_1: 2.472  loss_bbox_dn_1: 1.043  loss_giou_dn_1: 0.8151  loss_ce_2: 2.421  loss_mask_2: 1.4  loss_dice_2: 2.49  loss_bbox_2: 2.651  loss_giou_2: 1.711  loss_ce_dn_2: 2.431  loss_mask_dn_2: 1.295  loss_dice_dn_2: 2.303  loss_bbox_dn_2: 0.984  loss_giou_dn_2: 0.7963  loss_ce_3: 2.335  loss_mask_3: 1.425  loss_dice_3: 2.42  loss_bbox_3: 2.579  loss_giou_3: 1.724  loss_ce_dn_3: 2.286  loss_mask_dn_3: 1.252  loss_dice_dn_3: 2.263  loss_bbox_dn_3: 0.9568  loss_giou_dn_3: 0.7756  loss_ce_4: 2.266  loss_mask_4: 1.401  loss_dice_4: 2.447  loss_bbox_4: 2.531  loss_giou_4: 1.69  loss_ce_dn_4: 2.177  loss_mask_dn_4: 1.263  loss_dice_dn_4: 2.322  loss_bbox_dn_4: 0.9236  loss_giou_dn_4: 0.7634  loss_ce_5: 2.2  loss_mask_5: 1.406  loss_dice_5: 2.453  loss_bbox_5: 2.521  loss_giou_5: 1.66  loss_ce_dn_5: 2.166  loss_mask_dn_5: 1.254  loss_dice_dn_5: 2.256  loss_bbox_dn_5: 0.8975  loss_giou_dn_5: 0.7542  loss_ce_6: 2.2  loss_mask_6: 1.424  loss_dice_6: 2.412  loss_bbox_6: 2.552  loss_giou_6: 1.618  loss_ce_dn_6: 2.186  loss_mask_dn_6: 1.256  loss_dice_dn_6: 2.233  loss_bbox_dn_6: 0.8896  loss_giou_dn_6: 0.7508  loss_ce_7: 2.247  loss_mask_7: 1.426  loss_dice_7: 2.426  loss_bbox_7: 2.525  loss_giou_7: 1.59  loss_ce_dn_7: 2.105  loss_mask_dn_7: 1.283  loss_dice_dn_7: 2.213  loss_bbox_dn_7: 0.871  loss_giou_dn_7: 0.7426  loss_ce_8: 2.303  loss_mask_8: 1.468  loss_dice_8: 2.446  loss_bbox_8: 2.535  loss_giou_8: 1.59  loss_ce_dn_8: 2.22  loss_mask_dn_8: 1.265  loss_dice_dn_8: 2.212  loss_bbox_dn_8: 0.8765  loss_giou_dn_8: 0.7459    time: 2.2749  last_time: 2.1999  data_time: 0.0109  last_data_time: 0.0057   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:25:19 d2.utils.events]: \u001b[0m eta: 8 days, 7:59:59  iter: 2359  total_loss: 196.9  loss_ce: 2.515  loss_mask: 1.625  loss_dice: 2.724  loss_bbox: 2.836  loss_giou: 1.58  loss_ce_dn: 2.671  loss_mask_dn: 1.391  loss_dice_dn: 2.433  loss_bbox_dn: 0.9381  loss_giou_dn: 0.7446  loss_ce_0: 11.13  loss_mask_0: 1.551  loss_dice_0: 2.87  loss_bbox_0: 3.594  loss_giou_0: 1.792  loss_ce_1: 2.876  loss_mask_1: 1.507  loss_dice_1: 2.755  loss_bbox_1: 2.993  loss_giou_1: 1.667  loss_ce_dn_1: 3.325  loss_mask_dn_1: 1.426  loss_dice_dn_1: 2.627  loss_bbox_dn_1: 1.082  loss_giou_dn_1: 0.8042  loss_ce_2: 2.585  loss_mask_2: 1.491  loss_dice_2: 2.675  loss_bbox_2: 2.809  loss_giou_2: 1.634  loss_ce_dn_2: 2.723  loss_mask_dn_2: 1.389  loss_dice_dn_2: 2.45  loss_bbox_dn_2: 1.008  loss_giou_dn_2: 0.7755  loss_ce_3: 2.58  loss_mask_3: 1.56  loss_dice_3: 2.67  loss_bbox_3: 2.755  loss_giou_3: 1.619  loss_ce_dn_3: 2.588  loss_mask_dn_3: 1.385  loss_dice_dn_3: 2.392  loss_bbox_dn_3: 0.9833  loss_giou_dn_3: 0.7649  loss_ce_4: 2.582  loss_mask_4: 1.563  loss_dice_4: 2.714  loss_bbox_4: 2.841  loss_giou_4: 1.629  loss_ce_dn_4: 2.467  loss_mask_dn_4: 1.431  loss_dice_dn_4: 2.381  loss_bbox_dn_4: 0.9575  loss_giou_dn_4: 0.7579  loss_ce_5: 2.491  loss_mask_5: 1.562  loss_dice_5: 2.65  loss_bbox_5: 2.906  loss_giou_5: 1.585  loss_ce_dn_5: 2.493  loss_mask_dn_5: 1.411  loss_dice_dn_5: 2.387  loss_bbox_dn_5: 0.955  loss_giou_dn_5: 0.7527  loss_ce_6: 2.46  loss_mask_6: 1.553  loss_dice_6: 2.695  loss_bbox_6: 2.886  loss_giou_6: 1.568  loss_ce_dn_6: 2.435  loss_mask_dn_6: 1.403  loss_dice_dn_6: 2.38  loss_bbox_dn_6: 0.952  loss_giou_dn_6: 0.751  loss_ce_7: 2.461  loss_mask_7: 1.589  loss_dice_7: 2.687  loss_bbox_7: 2.857  loss_giou_7: 1.546  loss_ce_dn_7: 2.46  loss_mask_dn_7: 1.43  loss_dice_dn_7: 2.366  loss_bbox_dn_7: 0.9366  loss_giou_dn_7: 0.7446  loss_ce_8: 2.456  loss_mask_8: 1.616  loss_dice_8: 2.703  loss_bbox_8: 2.859  loss_giou_8: 1.564  loss_ce_dn_8: 2.561  loss_mask_dn_8: 1.404  loss_dice_dn_8: 2.379  loss_bbox_dn_8: 0.9344  loss_giou_dn_8: 0.7426    time: 2.2749  last_time: 2.2722  data_time: 0.0109  last_data_time: 0.0079   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:26:05 d2.utils.events]: \u001b[0m eta: 8 days, 8:00:42  iter: 2379  total_loss: 196.5  loss_ce: 1.874  loss_mask: 1.906  loss_dice: 2.478  loss_bbox: 3.013  loss_giou: 1.682  loss_ce_dn: 2.766  loss_mask_dn: 1.602  loss_dice_dn: 2.108  loss_bbox_dn: 0.8946  loss_giou_dn: 0.7385  loss_ce_0: 11.35  loss_mask_0: 1.545  loss_dice_0: 2.729  loss_bbox_0: 3.697  loss_giou_0: 1.943  loss_ce_1: 2.353  loss_mask_1: 1.798  loss_dice_1: 2.319  loss_bbox_1: 2.996  loss_giou_1: 1.641  loss_ce_dn_1: 3.179  loss_mask_dn_1: 1.721  loss_dice_dn_1: 2.457  loss_bbox_dn_1: 1.094  loss_giou_dn_1: 0.7956  loss_ce_2: 1.934  loss_mask_2: 1.876  loss_dice_2: 2.368  loss_bbox_2: 2.921  loss_giou_2: 1.611  loss_ce_dn_2: 2.74  loss_mask_dn_2: 1.68  loss_dice_dn_2: 2.186  loss_bbox_dn_2: 1.027  loss_giou_dn_2: 0.7597  loss_ce_3: 1.852  loss_mask_3: 1.894  loss_dice_3: 2.4  loss_bbox_3: 2.897  loss_giou_3: 1.657  loss_ce_dn_3: 2.651  loss_mask_dn_3: 1.668  loss_dice_dn_3: 2.116  loss_bbox_dn_3: 0.9707  loss_giou_dn_3: 0.7495  loss_ce_4: 1.893  loss_mask_4: 1.924  loss_dice_4: 2.587  loss_bbox_4: 3.036  loss_giou_4: 1.637  loss_ce_dn_4: 2.564  loss_mask_dn_4: 1.696  loss_dice_dn_4: 2.11  loss_bbox_dn_4: 0.9365  loss_giou_dn_4: 0.7406  loss_ce_5: 1.833  loss_mask_5: 1.933  loss_dice_5: 2.511  loss_bbox_5: 2.968  loss_giou_5: 1.662  loss_ce_dn_5: 2.588  loss_mask_dn_5: 1.664  loss_dice_dn_5: 2.097  loss_bbox_dn_5: 0.9171  loss_giou_dn_5: 0.7372  loss_ce_6: 1.821  loss_mask_6: 1.93  loss_dice_6: 2.423  loss_bbox_6: 3.045  loss_giou_6: 1.671  loss_ce_dn_6: 2.544  loss_mask_dn_6: 1.588  loss_dice_dn_6: 2.065  loss_bbox_dn_6: 0.9112  loss_giou_dn_6: 0.7379  loss_ce_7: 1.748  loss_mask_7: 1.896  loss_dice_7: 2.457  loss_bbox_7: 3.076  loss_giou_7: 1.666  loss_ce_dn_7: 2.75  loss_mask_dn_7: 1.625  loss_dice_dn_7: 2.052  loss_bbox_dn_7: 0.8859  loss_giou_dn_7: 0.7287  loss_ce_8: 1.884  loss_mask_8: 1.919  loss_dice_8: 2.443  loss_bbox_8: 3.035  loss_giou_8: 1.672  loss_ce_dn_8: 2.798  loss_mask_dn_8: 1.62  loss_dice_dn_8: 2.078  loss_bbox_dn_8: 0.8865  loss_giou_dn_8: 0.7298    time: 2.2749  last_time: 2.2708  data_time: 0.0115  last_data_time: 0.0068   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:26:50 d2.utils.events]: \u001b[0m eta: 8 days, 8:00:30  iter: 2399  total_loss: 193.9  loss_ce: 1.958  loss_mask: 1.472  loss_dice: 2.38  loss_bbox: 2.531  loss_giou: 1.57  loss_ce_dn: 2.707  loss_mask_dn: 1.503  loss_dice_dn: 2.346  loss_bbox_dn: 0.9252  loss_giou_dn: 0.7301  loss_ce_0: 11.26  loss_mask_0: 1.465  loss_dice_0: 2.614  loss_bbox_0: 3.98  loss_giou_0: 1.988  loss_ce_1: 2.466  loss_mask_1: 1.414  loss_dice_1: 2.425  loss_bbox_1: 3.154  loss_giou_1: 1.709  loss_ce_dn_1: 3.267  loss_mask_dn_1: 1.535  loss_dice_dn_1: 2.546  loss_bbox_dn_1: 1.072  loss_giou_dn_1: 0.7989  loss_ce_2: 2.088  loss_mask_2: 1.43  loss_dice_2: 2.335  loss_bbox_2: 2.783  loss_giou_2: 1.647  loss_ce_dn_2: 2.618  loss_mask_dn_2: 1.472  loss_dice_dn_2: 2.403  loss_bbox_dn_2: 0.9827  loss_giou_dn_2: 0.7661  loss_ce_3: 1.936  loss_mask_3: 1.425  loss_dice_3: 2.371  loss_bbox_3: 2.744  loss_giou_3: 1.63  loss_ce_dn_3: 2.52  loss_mask_dn_3: 1.481  loss_dice_dn_3: 2.376  loss_bbox_dn_3: 0.9266  loss_giou_dn_3: 0.75  loss_ce_4: 1.797  loss_mask_4: 1.503  loss_dice_4: 2.359  loss_bbox_4: 2.608  loss_giou_4: 1.61  loss_ce_dn_4: 2.553  loss_mask_dn_4: 1.545  loss_dice_dn_4: 2.359  loss_bbox_dn_4: 0.9202  loss_giou_dn_4: 0.736  loss_ce_5: 1.847  loss_mask_5: 1.461  loss_dice_5: 2.373  loss_bbox_5: 2.6  loss_giou_5: 1.592  loss_ce_dn_5: 2.661  loss_mask_dn_5: 1.47  loss_dice_dn_5: 2.344  loss_bbox_dn_5: 0.9156  loss_giou_dn_5: 0.7271  loss_ce_6: 1.994  loss_mask_6: 1.491  loss_dice_6: 2.401  loss_bbox_6: 2.641  loss_giou_6: 1.586  loss_ce_dn_6: 2.76  loss_mask_dn_6: 1.508  loss_dice_dn_6: 2.327  loss_bbox_dn_6: 0.9181  loss_giou_dn_6: 0.7284  loss_ce_7: 1.932  loss_mask_7: 1.456  loss_dice_7: 2.394  loss_bbox_7: 2.6  loss_giou_7: 1.568  loss_ce_dn_7: 2.742  loss_mask_dn_7: 1.497  loss_dice_dn_7: 2.299  loss_bbox_dn_7: 0.9179  loss_giou_dn_7: 0.7225  loss_ce_8: 1.955  loss_mask_8: 1.487  loss_dice_8: 2.354  loss_bbox_8: 2.552  loss_giou_8: 1.554  loss_ce_dn_8: 2.717  loss_mask_dn_8: 1.501  loss_dice_dn_8: 2.322  loss_bbox_dn_8: 0.9177  loss_giou_dn_8: 0.7242    time: 2.2748  last_time: 2.2986  data_time: 0.0099  last_data_time: 0.0051   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:27:36 d2.utils.events]: \u001b[0m eta: 8 days, 7:59:13  iter: 2419  total_loss: 193.6  loss_ce: 2.305  loss_mask: 1.456  loss_dice: 2.66  loss_bbox: 2.775  loss_giou: 1.727  loss_ce_dn: 2.793  loss_mask_dn: 1.253  loss_dice_dn: 2.416  loss_bbox_dn: 0.8041  loss_giou_dn: 0.7676  loss_ce_0: 11.13  loss_mask_0: 1.354  loss_dice_0: 2.787  loss_bbox_0: 4.011  loss_giou_0: 1.912  loss_ce_1: 2.661  loss_mask_1: 1.44  loss_dice_1: 2.699  loss_bbox_1: 3.165  loss_giou_1: 1.856  loss_ce_dn_1: 3.264  loss_mask_dn_1: 1.215  loss_dice_dn_1: 2.651  loss_bbox_dn_1: 0.9556  loss_giou_dn_1: 0.8022  loss_ce_2: 2.238  loss_mask_2: 1.419  loss_dice_2: 2.702  loss_bbox_2: 2.961  loss_giou_2: 1.72  loss_ce_dn_2: 2.651  loss_mask_dn_2: 1.201  loss_dice_dn_2: 2.438  loss_bbox_dn_2: 0.9107  loss_giou_dn_2: 0.7812  loss_ce_3: 2.054  loss_mask_3: 1.474  loss_dice_3: 2.682  loss_bbox_3: 2.834  loss_giou_3: 1.707  loss_ce_dn_3: 2.613  loss_mask_dn_3: 1.165  loss_dice_dn_3: 2.403  loss_bbox_dn_3: 0.8854  loss_giou_dn_3: 0.777  loss_ce_4: 2.097  loss_mask_4: 1.479  loss_dice_4: 2.656  loss_bbox_4: 2.818  loss_giou_4: 1.716  loss_ce_dn_4: 2.581  loss_mask_dn_4: 1.195  loss_dice_dn_4: 2.413  loss_bbox_dn_4: 0.8802  loss_giou_dn_4: 0.7774  loss_ce_5: 2.099  loss_mask_5: 1.459  loss_dice_5: 2.637  loss_bbox_5: 2.778  loss_giou_5: 1.753  loss_ce_dn_5: 2.539  loss_mask_dn_5: 1.257  loss_dice_dn_5: 2.42  loss_bbox_dn_5: 0.8392  loss_giou_dn_5: 0.771  loss_ce_6: 2.096  loss_mask_6: 1.493  loss_dice_6: 2.625  loss_bbox_6: 2.757  loss_giou_6: 1.761  loss_ce_dn_6: 2.536  loss_mask_dn_6: 1.225  loss_dice_dn_6: 2.39  loss_bbox_dn_6: 0.8207  loss_giou_dn_6: 0.7688  loss_ce_7: 2.12  loss_mask_7: 1.484  loss_dice_7: 2.612  loss_bbox_7: 2.766  loss_giou_7: 1.763  loss_ce_dn_7: 2.568  loss_mask_dn_7: 1.223  loss_dice_dn_7: 2.403  loss_bbox_dn_7: 0.8041  loss_giou_dn_7: 0.7635  loss_ce_8: 2.167  loss_mask_8: 1.503  loss_dice_8: 2.604  loss_bbox_8: 2.77  loss_giou_8: 1.75  loss_ce_dn_8: 2.638  loss_mask_dn_8: 1.247  loss_dice_dn_8: 2.406  loss_bbox_dn_8: 0.8025  loss_giou_dn_8: 0.7646    time: 2.2748  last_time: 2.2328  data_time: 0.0124  last_data_time: 0.0057   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:28:21 d2.utils.events]: \u001b[0m eta: 8 days, 7:59:00  iter: 2439  total_loss: 207.4  loss_ce: 2.331  loss_mask: 1.518  loss_dice: 2.669  loss_bbox: 2.911  loss_giou: 1.643  loss_ce_dn: 3.123  loss_mask_dn: 1.406  loss_dice_dn: 2.498  loss_bbox_dn: 0.8929  loss_giou_dn: 0.7384  loss_ce_0: 11.29  loss_mask_0: 1.486  loss_dice_0: 2.807  loss_bbox_0: 3.918  loss_giou_0: 1.927  loss_ce_1: 2.294  loss_mask_1: 1.489  loss_dice_1: 2.703  loss_bbox_1: 3.359  loss_giou_1: 1.702  loss_ce_dn_1: 3.154  loss_mask_dn_1: 1.434  loss_dice_dn_1: 2.731  loss_bbox_dn_1: 1.093  loss_giou_dn_1: 0.7933  loss_ce_2: 2.184  loss_mask_2: 1.504  loss_dice_2: 2.696  loss_bbox_2: 3.006  loss_giou_2: 1.725  loss_ce_dn_2: 2.784  loss_mask_dn_2: 1.401  loss_dice_dn_2: 2.553  loss_bbox_dn_2: 1.019  loss_giou_dn_2: 0.7703  loss_ce_3: 2.121  loss_mask_3: 1.458  loss_dice_3: 2.62  loss_bbox_3: 2.91  loss_giou_3: 1.704  loss_ce_dn_3: 2.698  loss_mask_dn_3: 1.402  loss_dice_dn_3: 2.51  loss_bbox_dn_3: 0.9755  loss_giou_dn_3: 0.7501  loss_ce_4: 2.131  loss_mask_4: 1.508  loss_dice_4: 2.682  loss_bbox_4: 2.809  loss_giou_4: 1.697  loss_ce_dn_4: 2.682  loss_mask_dn_4: 1.426  loss_dice_dn_4: 2.513  loss_bbox_dn_4: 0.9521  loss_giou_dn_4: 0.7392  loss_ce_5: 2.206  loss_mask_5: 1.493  loss_dice_5: 2.636  loss_bbox_5: 2.846  loss_giou_5: 1.678  loss_ce_dn_5: 2.705  loss_mask_dn_5: 1.44  loss_dice_dn_5: 2.554  loss_bbox_dn_5: 0.9291  loss_giou_dn_5: 0.7372  loss_ce_6: 2.121  loss_mask_6: 1.481  loss_dice_6: 2.663  loss_bbox_6: 2.959  loss_giou_6: 1.671  loss_ce_dn_6: 2.808  loss_mask_dn_6: 1.429  loss_dice_dn_6: 2.538  loss_bbox_dn_6: 0.9214  loss_giou_dn_6: 0.7351  loss_ce_7: 2.23  loss_mask_7: 1.506  loss_dice_7: 2.69  loss_bbox_7: 2.914  loss_giou_7: 1.639  loss_ce_dn_7: 2.886  loss_mask_dn_7: 1.404  loss_dice_dn_7: 2.536  loss_bbox_dn_7: 0.8944  loss_giou_dn_7: 0.7304  loss_ce_8: 2.317  loss_mask_8: 1.404  loss_dice_8: 2.683  loss_bbox_8: 2.923  loss_giou_8: 1.637  loss_ce_dn_8: 3.025  loss_mask_dn_8: 1.394  loss_dice_dn_8: 2.544  loss_bbox_dn_8: 0.8931  loss_giou_dn_8: 0.7341    time: 2.2747  last_time: 2.2839  data_time: 0.0103  last_data_time: 0.0015   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:29:07 d2.utils.events]: \u001b[0m eta: 8 days, 7:59:03  iter: 2459  total_loss: 194.5  loss_ce: 2.214  loss_mask: 1.66  loss_dice: 2.529  loss_bbox: 2.795  loss_giou: 1.57  loss_ce_dn: 2.427  loss_mask_dn: 1.501  loss_dice_dn: 2.315  loss_bbox_dn: 0.8332  loss_giou_dn: 0.7096  loss_ce_0: 10.93  loss_mask_0: 1.539  loss_dice_0: 2.868  loss_bbox_0: 3.897  loss_giou_0: 1.848  loss_ce_1: 2.481  loss_mask_1: 1.686  loss_dice_1: 2.729  loss_bbox_1: 3.208  loss_giou_1: 1.766  loss_ce_dn_1: 3.131  loss_mask_dn_1: 1.587  loss_dice_dn_1: 2.564  loss_bbox_dn_1: 1.049  loss_giou_dn_1: 0.7889  loss_ce_2: 2.198  loss_mask_2: 1.778  loss_dice_2: 2.631  loss_bbox_2: 3.031  loss_giou_2: 1.69  loss_ce_dn_2: 2.516  loss_mask_dn_2: 1.502  loss_dice_dn_2: 2.391  loss_bbox_dn_2: 0.9815  loss_giou_dn_2: 0.7633  loss_ce_3: 2.016  loss_mask_3: 1.794  loss_dice_3: 2.596  loss_bbox_3: 2.993  loss_giou_3: 1.656  loss_ce_dn_3: 2.404  loss_mask_dn_3: 1.506  loss_dice_dn_3: 2.378  loss_bbox_dn_3: 0.9421  loss_giou_dn_3: 0.7435  loss_ce_4: 2.103  loss_mask_4: 1.832  loss_dice_4: 2.597  loss_bbox_4: 2.921  loss_giou_4: 1.653  loss_ce_dn_4: 2.304  loss_mask_dn_4: 1.476  loss_dice_dn_4: 2.382  loss_bbox_dn_4: 0.9031  loss_giou_dn_4: 0.7202  loss_ce_5: 1.94  loss_mask_5: 1.729  loss_dice_5: 2.559  loss_bbox_5: 2.925  loss_giou_5: 1.654  loss_ce_dn_5: 2.315  loss_mask_dn_5: 1.473  loss_dice_dn_5: 2.373  loss_bbox_dn_5: 0.8785  loss_giou_dn_5: 0.72  loss_ce_6: 1.998  loss_mask_6: 1.801  loss_dice_6: 2.575  loss_bbox_6: 2.872  loss_giou_6: 1.654  loss_ce_dn_6: 2.198  loss_mask_dn_6: 1.483  loss_dice_dn_6: 2.333  loss_bbox_dn_6: 0.8706  loss_giou_dn_6: 0.7184  loss_ce_7: 2.121  loss_mask_7: 1.732  loss_dice_7: 2.561  loss_bbox_7: 2.793  loss_giou_7: 1.608  loss_ce_dn_7: 2.24  loss_mask_dn_7: 1.466  loss_dice_dn_7: 2.349  loss_bbox_dn_7: 0.835  loss_giou_dn_7: 0.715  loss_ce_8: 2.173  loss_mask_8: 1.639  loss_dice_8: 2.574  loss_bbox_8: 2.802  loss_giou_8: 1.589  loss_ce_dn_8: 2.354  loss_mask_dn_8: 1.489  loss_dice_dn_8: 2.314  loss_bbox_dn_8: 0.8361  loss_giou_dn_8: 0.7127    time: 2.2748  last_time: 2.3267  data_time: 0.0143  last_data_time: 0.0218   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:29:52 d2.utils.events]: \u001b[0m eta: 8 days, 7:58:07  iter: 2479  total_loss: 198.5  loss_ce: 2.069  loss_mask: 1.663  loss_dice: 2.657  loss_bbox: 2.96  loss_giou: 1.616  loss_ce_dn: 3.279  loss_mask_dn: 1.462  loss_dice_dn: 2.544  loss_bbox_dn: 0.9508  loss_giou_dn: 0.7691  loss_ce_0: 11.36  loss_mask_0: 1.514  loss_dice_0: 2.876  loss_bbox_0: 3.817  loss_giou_0: 1.903  loss_ce_1: 2.734  loss_mask_1: 1.647  loss_dice_1: 2.746  loss_bbox_1: 3.391  loss_giou_1: 1.802  loss_ce_dn_1: 3.844  loss_mask_dn_1: 1.715  loss_dice_dn_1: 2.798  loss_bbox_dn_1: 1.061  loss_giou_dn_1: 0.7963  loss_ce_2: 2.321  loss_mask_2: 1.605  loss_dice_2: 2.674  loss_bbox_2: 3.051  loss_giou_2: 1.705  loss_ce_dn_2: 3.251  loss_mask_dn_2: 1.56  loss_dice_dn_2: 2.584  loss_bbox_dn_2: 1.028  loss_giou_dn_2: 0.7757  loss_ce_3: 2.191  loss_mask_3: 1.735  loss_dice_3: 2.729  loss_bbox_3: 3.121  loss_giou_3: 1.691  loss_ce_dn_3: 3.154  loss_mask_dn_3: 1.643  loss_dice_dn_3: 2.555  loss_bbox_dn_3: 0.9916  loss_giou_dn_3: 0.7723  loss_ce_4: 2.139  loss_mask_4: 1.698  loss_dice_4: 2.688  loss_bbox_4: 3.027  loss_giou_4: 1.644  loss_ce_dn_4: 3.115  loss_mask_dn_4: 1.606  loss_dice_dn_4: 2.571  loss_bbox_dn_4: 0.9689  loss_giou_dn_4: 0.766  loss_ce_5: 2.129  loss_mask_5: 1.738  loss_dice_5: 2.741  loss_bbox_5: 3.02  loss_giou_5: 1.626  loss_ce_dn_5: 3.071  loss_mask_dn_5: 1.609  loss_dice_dn_5: 2.588  loss_bbox_dn_5: 0.957  loss_giou_dn_5: 0.7662  loss_ce_6: 2.102  loss_mask_6: 1.658  loss_dice_6: 2.691  loss_bbox_6: 3.176  loss_giou_6: 1.643  loss_ce_dn_6: 3.008  loss_mask_dn_6: 1.568  loss_dice_dn_6: 2.545  loss_bbox_dn_6: 0.9546  loss_giou_dn_6: 0.7661  loss_ce_7: 2.165  loss_mask_7: 1.685  loss_dice_7: 2.707  loss_bbox_7: 3.158  loss_giou_7: 1.644  loss_ce_dn_7: 3.136  loss_mask_dn_7: 1.638  loss_dice_dn_7: 2.571  loss_bbox_dn_7: 0.9419  loss_giou_dn_7: 0.7618  loss_ce_8: 2.109  loss_mask_8: 1.629  loss_dice_8: 2.682  loss_bbox_8: 2.987  loss_giou_8: 1.588  loss_ce_dn_8: 3.228  loss_mask_dn_8: 1.591  loss_dice_dn_8: 2.561  loss_bbox_dn_8: 0.9456  loss_giou_dn_8: 0.7653    time: 2.2748  last_time: 2.3134  data_time: 0.0122  last_data_time: 0.0046   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:30:38 d2.utils.events]: \u001b[0m eta: 8 days, 7:56:23  iter: 2499  total_loss: 192.2  loss_ce: 2.24  loss_mask: 1.226  loss_dice: 2.553  loss_bbox: 2.566  loss_giou: 1.613  loss_ce_dn: 3.084  loss_mask_dn: 1.203  loss_dice_dn: 2.375  loss_bbox_dn: 0.8838  loss_giou_dn: 0.7344  loss_ce_0: 10.8  loss_mask_0: 1.398  loss_dice_0: 2.822  loss_bbox_0: 3.832  loss_giou_0: 2  loss_ce_1: 2.461  loss_mask_1: 1.319  loss_dice_1: 2.624  loss_bbox_1: 3.114  loss_giou_1: 1.799  loss_ce_dn_1: 3.377  loss_mask_dn_1: 1.306  loss_dice_dn_1: 2.62  loss_bbox_dn_1: 1.006  loss_giou_dn_1: 0.7976  loss_ce_2: 2.425  loss_mask_2: 1.367  loss_dice_2: 2.621  loss_bbox_2: 2.963  loss_giou_2: 1.757  loss_ce_dn_2: 2.951  loss_mask_dn_2: 1.26  loss_dice_dn_2: 2.489  loss_bbox_dn_2: 0.9433  loss_giou_dn_2: 0.774  loss_ce_3: 2.136  loss_mask_3: 1.236  loss_dice_3: 2.572  loss_bbox_3: 2.881  loss_giou_3: 1.722  loss_ce_dn_3: 2.685  loss_mask_dn_3: 1.233  loss_dice_dn_3: 2.414  loss_bbox_dn_3: 0.9161  loss_giou_dn_3: 0.7479  loss_ce_4: 2.399  loss_mask_4: 1.232  loss_dice_4: 2.58  loss_bbox_4: 2.763  loss_giou_4: 1.706  loss_ce_dn_4: 2.798  loss_mask_dn_4: 1.229  loss_dice_dn_4: 2.381  loss_bbox_dn_4: 0.8943  loss_giou_dn_4: 0.7394  loss_ce_5: 2.346  loss_mask_5: 1.207  loss_dice_5: 2.556  loss_bbox_5: 2.695  loss_giou_5: 1.653  loss_ce_dn_5: 2.708  loss_mask_dn_5: 1.219  loss_dice_dn_5: 2.356  loss_bbox_dn_5: 0.892  loss_giou_dn_5: 0.73  loss_ce_6: 2.238  loss_mask_6: 1.19  loss_dice_6: 2.565  loss_bbox_6: 2.648  loss_giou_6: 1.66  loss_ce_dn_6: 2.918  loss_mask_dn_6: 1.253  loss_dice_dn_6: 2.356  loss_bbox_dn_6: 0.8935  loss_giou_dn_6: 0.731  loss_ce_7: 2.239  loss_mask_7: 1.249  loss_dice_7: 2.553  loss_bbox_7: 2.567  loss_giou_7: 1.609  loss_ce_dn_7: 2.976  loss_mask_dn_7: 1.266  loss_dice_dn_7: 2.376  loss_bbox_dn_7: 0.8763  loss_giou_dn_7: 0.7327  loss_ce_8: 2.216  loss_mask_8: 1.234  loss_dice_8: 2.57  loss_bbox_8: 2.555  loss_giou_8: 1.614  loss_ce_dn_8: 2.966  loss_mask_dn_8: 1.215  loss_dice_dn_8: 2.348  loss_bbox_dn_8: 0.8808  loss_giou_dn_8: 0.734    time: 2.2748  last_time: 2.2666  data_time: 0.0128  last_data_time: 0.0227   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:31:24 d2.utils.events]: \u001b[0m eta: 8 days, 7:55:26  iter: 2519  total_loss: 202.6  loss_ce: 1.886  loss_mask: 1.795  loss_dice: 2.706  loss_bbox: 3.327  loss_giou: 1.633  loss_ce_dn: 2.465  loss_mask_dn: 1.495  loss_dice_dn: 2.419  loss_bbox_dn: 0.916  loss_giou_dn: 0.7713  loss_ce_0: 10.99  loss_mask_0: 1.466  loss_dice_0: 2.863  loss_bbox_0: 3.965  loss_giou_0: 1.973  loss_ce_1: 2.37  loss_mask_1: 1.571  loss_dice_1: 2.783  loss_bbox_1: 3.537  loss_giou_1: 1.712  loss_ce_dn_1: 3.112  loss_mask_dn_1: 1.548  loss_dice_dn_1: 2.593  loss_bbox_dn_1: 1.001  loss_giou_dn_1: 0.8031  loss_ce_2: 1.993  loss_mask_2: 1.698  loss_dice_2: 2.719  loss_bbox_2: 3.527  loss_giou_2: 1.712  loss_ce_dn_2: 2.562  loss_mask_dn_2: 1.408  loss_dice_dn_2: 2.433  loss_bbox_dn_2: 0.9622  loss_giou_dn_2: 0.7843  loss_ce_3: 2.038  loss_mask_3: 1.653  loss_dice_3: 2.628  loss_bbox_3: 3.449  loss_giou_3: 1.687  loss_ce_dn_3: 2.421  loss_mask_dn_3: 1.389  loss_dice_dn_3: 2.403  loss_bbox_dn_3: 0.939  loss_giou_dn_3: 0.7779  loss_ce_4: 1.948  loss_mask_4: 1.739  loss_dice_4: 2.609  loss_bbox_4: 3.426  loss_giou_4: 1.714  loss_ce_dn_4: 2.311  loss_mask_dn_4: 1.369  loss_dice_dn_4: 2.37  loss_bbox_dn_4: 0.9184  loss_giou_dn_4: 0.7708  loss_ce_5: 1.916  loss_mask_5: 1.741  loss_dice_5: 2.566  loss_bbox_5: 3.372  loss_giou_5: 1.705  loss_ce_dn_5: 2.193  loss_mask_dn_5: 1.373  loss_dice_dn_5: 2.379  loss_bbox_dn_5: 0.907  loss_giou_dn_5: 0.7656  loss_ce_6: 1.85  loss_mask_6: 1.806  loss_dice_6: 2.606  loss_bbox_6: 3.386  loss_giou_6: 1.672  loss_ce_dn_6: 2.254  loss_mask_dn_6: 1.396  loss_dice_dn_6: 2.388  loss_bbox_dn_6: 0.9131  loss_giou_dn_6: 0.7692  loss_ce_7: 1.841  loss_mask_7: 1.794  loss_dice_7: 2.674  loss_bbox_7: 3.359  loss_giou_7: 1.65  loss_ce_dn_7: 2.291  loss_mask_dn_7: 1.464  loss_dice_dn_7: 2.397  loss_bbox_dn_7: 0.914  loss_giou_dn_7: 0.7593  loss_ce_8: 1.807  loss_mask_8: 1.842  loss_dice_8: 2.674  loss_bbox_8: 3.357  loss_giou_8: 1.634  loss_ce_dn_8: 2.363  loss_mask_dn_8: 1.445  loss_dice_dn_8: 2.4  loss_bbox_dn_8: 0.9156  loss_giou_dn_8: 0.7647    time: 2.2749  last_time: 2.2233  data_time: 0.0116  last_data_time: 0.0045   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:32:09 d2.utils.events]: \u001b[0m eta: 8 days, 7:53:33  iter: 2539  total_loss: 189.1  loss_ce: 2.164  loss_mask: 1.769  loss_dice: 2.517  loss_bbox: 2.946  loss_giou: 1.695  loss_ce_dn: 2.637  loss_mask_dn: 1.452  loss_dice_dn: 2.323  loss_bbox_dn: 0.8954  loss_giou_dn: 0.7725  loss_ce_0: 10.96  loss_mask_0: 1.428  loss_dice_0: 2.754  loss_bbox_0: 3.75  loss_giou_0: 1.927  loss_ce_1: 2.39  loss_mask_1: 1.552  loss_dice_1: 2.611  loss_bbox_1: 3.202  loss_giou_1: 1.692  loss_ce_dn_1: 3.033  loss_mask_dn_1: 1.466  loss_dice_dn_1: 2.59  loss_bbox_dn_1: 0.9774  loss_giou_dn_1: 0.7957  loss_ce_2: 2.198  loss_mask_2: 1.614  loss_dice_2: 2.529  loss_bbox_2: 2.902  loss_giou_2: 1.641  loss_ce_dn_2: 2.438  loss_mask_dn_2: 1.542  loss_dice_dn_2: 2.424  loss_bbox_dn_2: 0.9316  loss_giou_dn_2: 0.7818  loss_ce_3: 2.139  loss_mask_3: 1.727  loss_dice_3: 2.528  loss_bbox_3: 2.987  loss_giou_3: 1.681  loss_ce_dn_3: 2.41  loss_mask_dn_3: 1.577  loss_dice_dn_3: 2.411  loss_bbox_dn_3: 0.9121  loss_giou_dn_3: 0.7708  loss_ce_4: 2.152  loss_mask_4: 1.707  loss_dice_4: 2.518  loss_bbox_4: 2.956  loss_giou_4: 1.708  loss_ce_dn_4: 2.345  loss_mask_dn_4: 1.578  loss_dice_dn_4: 2.351  loss_bbox_dn_4: 0.905  loss_giou_dn_4: 0.7687  loss_ce_5: 2.113  loss_mask_5: 1.632  loss_dice_5: 2.464  loss_bbox_5: 2.922  loss_giou_5: 1.719  loss_ce_dn_5: 2.415  loss_mask_dn_5: 1.49  loss_dice_dn_5: 2.328  loss_bbox_dn_5: 0.9023  loss_giou_dn_5: 0.7717  loss_ce_6: 2.024  loss_mask_6: 1.714  loss_dice_6: 2.495  loss_bbox_6: 3.047  loss_giou_6: 1.714  loss_ce_dn_6: 2.387  loss_mask_dn_6: 1.531  loss_dice_dn_6: 2.337  loss_bbox_dn_6: 0.8972  loss_giou_dn_6: 0.7746  loss_ce_7: 2.069  loss_mask_7: 1.646  loss_dice_7: 2.481  loss_bbox_7: 3.063  loss_giou_7: 1.7  loss_ce_dn_7: 2.436  loss_mask_dn_7: 1.479  loss_dice_dn_7: 2.312  loss_bbox_dn_7: 0.8941  loss_giou_dn_7: 0.7642  loss_ce_8: 2.092  loss_mask_8: 1.768  loss_dice_8: 2.48  loss_bbox_8: 3.012  loss_giou_8: 1.696  loss_ce_dn_8: 2.527  loss_mask_dn_8: 1.441  loss_dice_dn_8: 2.303  loss_bbox_dn_8: 0.8956  loss_giou_dn_8: 0.7671    time: 2.2748  last_time: 2.2511  data_time: 0.0136  last_data_time: 0.0055   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:32:55 d2.utils.events]: \u001b[0m eta: 8 days, 7:53:56  iter: 2559  total_loss: 203.5  loss_ce: 2.145  loss_mask: 1.663  loss_dice: 2.823  loss_bbox: 2.792  loss_giou: 1.65  loss_ce_dn: 2.52  loss_mask_dn: 1.482  loss_dice_dn: 2.504  loss_bbox_dn: 0.8857  loss_giou_dn: 0.7427  loss_ce_0: 10.78  loss_mask_0: 1.598  loss_dice_0: 2.998  loss_bbox_0: 3.671  loss_giou_0: 1.887  loss_ce_1: 2.392  loss_mask_1: 1.797  loss_dice_1: 2.812  loss_bbox_1: 3.239  loss_giou_1: 1.713  loss_ce_dn_1: 3.266  loss_mask_dn_1: 1.506  loss_dice_dn_1: 2.79  loss_bbox_dn_1: 1.098  loss_giou_dn_1: 0.799  loss_ce_2: 2.08  loss_mask_2: 1.843  loss_dice_2: 2.805  loss_bbox_2: 2.989  loss_giou_2: 1.771  loss_ce_dn_2: 2.654  loss_mask_dn_2: 1.407  loss_dice_dn_2: 2.634  loss_bbox_dn_2: 1.006  loss_giou_dn_2: 0.7715  loss_ce_3: 1.966  loss_mask_3: 1.759  loss_dice_3: 2.821  loss_bbox_3: 2.87  loss_giou_3: 1.706  loss_ce_dn_3: 2.527  loss_mask_dn_3: 1.467  loss_dice_dn_3: 2.566  loss_bbox_dn_3: 0.946  loss_giou_dn_3: 0.7527  loss_ce_4: 1.912  loss_mask_4: 1.738  loss_dice_4: 2.786  loss_bbox_4: 2.799  loss_giou_4: 1.668  loss_ce_dn_4: 2.431  loss_mask_dn_4: 1.456  loss_dice_dn_4: 2.59  loss_bbox_dn_4: 0.9209  loss_giou_dn_4: 0.7467  loss_ce_5: 2.022  loss_mask_5: 1.751  loss_dice_5: 2.812  loss_bbox_5: 2.732  loss_giou_5: 1.657  loss_ce_dn_5: 2.364  loss_mask_dn_5: 1.482  loss_dice_dn_5: 2.557  loss_bbox_dn_5: 0.8984  loss_giou_dn_5: 0.7366  loss_ce_6: 2.079  loss_mask_6: 1.754  loss_dice_6: 2.796  loss_bbox_6: 2.785  loss_giou_6: 1.639  loss_ce_dn_6: 2.323  loss_mask_dn_6: 1.423  loss_dice_dn_6: 2.52  loss_bbox_dn_6: 0.8906  loss_giou_dn_6: 0.7341  loss_ce_7: 2.053  loss_mask_7: 1.746  loss_dice_7: 2.839  loss_bbox_7: 2.727  loss_giou_7: 1.638  loss_ce_dn_7: 2.403  loss_mask_dn_7: 1.464  loss_dice_dn_7: 2.527  loss_bbox_dn_7: 0.877  loss_giou_dn_7: 0.7264  loss_ce_8: 2.12  loss_mask_8: 1.696  loss_dice_8: 2.827  loss_bbox_8: 2.791  loss_giou_8: 1.663  loss_ce_dn_8: 2.566  loss_mask_dn_8: 1.465  loss_dice_dn_8: 2.535  loss_bbox_dn_8: 0.8766  loss_giou_dn_8: 0.7321    time: 2.2748  last_time: 2.2815  data_time: 0.0138  last_data_time: 0.0178   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:33:40 d2.utils.events]: \u001b[0m eta: 8 days, 7:54:49  iter: 2579  total_loss: 192.3  loss_ce: 2.146  loss_mask: 1.486  loss_dice: 2.658  loss_bbox: 2.945  loss_giou: 1.771  loss_ce_dn: 2.595  loss_mask_dn: 1.337  loss_dice_dn: 2.529  loss_bbox_dn: 0.8106  loss_giou_dn: 0.7441  loss_ce_0: 10.55  loss_mask_0: 1.523  loss_dice_0: 3.008  loss_bbox_0: 3.832  loss_giou_0: 2.035  loss_ce_1: 2.649  loss_mask_1: 1.365  loss_dice_1: 2.759  loss_bbox_1: 3.378  loss_giou_1: 1.836  loss_ce_dn_1: 3.04  loss_mask_dn_1: 1.41  loss_dice_dn_1: 2.629  loss_bbox_dn_1: 0.9416  loss_giou_dn_1: 0.8062  loss_ce_2: 2.375  loss_mask_2: 1.404  loss_dice_2: 2.67  loss_bbox_2: 3.085  loss_giou_2: 1.833  loss_ce_dn_2: 2.512  loss_mask_dn_2: 1.341  loss_dice_dn_2: 2.506  loss_bbox_dn_2: 0.9309  loss_giou_dn_2: 0.7827  loss_ce_3: 2.111  loss_mask_3: 1.468  loss_dice_3: 2.677  loss_bbox_3: 3.07  loss_giou_3: 1.791  loss_ce_dn_3: 2.435  loss_mask_dn_3: 1.309  loss_dice_dn_3: 2.512  loss_bbox_dn_3: 0.8873  loss_giou_dn_3: 0.7664  loss_ce_4: 1.996  loss_mask_4: 1.513  loss_dice_4: 2.654  loss_bbox_4: 2.971  loss_giou_4: 1.773  loss_ce_dn_4: 2.362  loss_mask_dn_4: 1.333  loss_dice_dn_4: 2.466  loss_bbox_dn_4: 0.8556  loss_giou_dn_4: 0.7546  loss_ce_5: 1.984  loss_mask_5: 1.478  loss_dice_5: 2.661  loss_bbox_5: 2.997  loss_giou_5: 1.773  loss_ce_dn_5: 2.377  loss_mask_dn_5: 1.31  loss_dice_dn_5: 2.489  loss_bbox_dn_5: 0.8372  loss_giou_dn_5: 0.7472  loss_ce_6: 2.018  loss_mask_6: 1.495  loss_dice_6: 2.674  loss_bbox_6: 2.981  loss_giou_6: 1.774  loss_ce_dn_6: 2.336  loss_mask_dn_6: 1.329  loss_dice_dn_6: 2.512  loss_bbox_dn_6: 0.8364  loss_giou_dn_6: 0.745  loss_ce_7: 1.99  loss_mask_7: 1.474  loss_dice_7: 2.664  loss_bbox_7: 2.982  loss_giou_7: 1.783  loss_ce_dn_7: 2.412  loss_mask_dn_7: 1.343  loss_dice_dn_7: 2.508  loss_bbox_dn_7: 0.8114  loss_giou_dn_7: 0.743  loss_ce_8: 2.107  loss_mask_8: 1.479  loss_dice_8: 2.646  loss_bbox_8: 2.952  loss_giou_8: 1.783  loss_ce_dn_8: 2.411  loss_mask_dn_8: 1.33  loss_dice_dn_8: 2.508  loss_bbox_dn_8: 0.8113  loss_giou_dn_8: 0.7404    time: 2.2749  last_time: 2.2673  data_time: 0.0108  last_data_time: 0.0249   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:34:26 d2.utils.events]: \u001b[0m eta: 8 days, 7:54:16  iter: 2599  total_loss: 176.9  loss_ce: 1.819  loss_mask: 1.414  loss_dice: 2.598  loss_bbox: 2.538  loss_giou: 1.663  loss_ce_dn: 2.493  loss_mask_dn: 1.225  loss_dice_dn: 2.374  loss_bbox_dn: 0.9318  loss_giou_dn: 0.7638  loss_ce_0: 10.44  loss_mask_0: 1.28  loss_dice_0: 2.942  loss_bbox_0: 3.906  loss_giou_0: 1.984  loss_ce_1: 2.343  loss_mask_1: 1.29  loss_dice_1: 2.666  loss_bbox_1: 2.961  loss_giou_1: 1.778  loss_ce_dn_1: 3.248  loss_mask_dn_1: 1.326  loss_dice_dn_1: 2.609  loss_bbox_dn_1: 0.9787  loss_giou_dn_1: 0.7989  loss_ce_2: 1.908  loss_mask_2: 1.257  loss_dice_2: 2.509  loss_bbox_2: 2.909  loss_giou_2: 1.802  loss_ce_dn_2: 2.639  loss_mask_dn_2: 1.276  loss_dice_dn_2: 2.465  loss_bbox_dn_2: 0.9364  loss_giou_dn_2: 0.7906  loss_ce_3: 1.834  loss_mask_3: 1.292  loss_dice_3: 2.54  loss_bbox_3: 2.73  loss_giou_3: 1.781  loss_ce_dn_3: 2.507  loss_mask_dn_3: 1.217  loss_dice_dn_3: 2.413  loss_bbox_dn_3: 0.9285  loss_giou_dn_3: 0.7812  loss_ce_4: 1.835  loss_mask_4: 1.33  loss_dice_4: 2.571  loss_bbox_4: 2.699  loss_giou_4: 1.797  loss_ce_dn_4: 2.393  loss_mask_dn_4: 1.2  loss_dice_dn_4: 2.398  loss_bbox_dn_4: 0.9122  loss_giou_dn_4: 0.7738  loss_ce_5: 1.806  loss_mask_5: 1.323  loss_dice_5: 2.561  loss_bbox_5: 2.615  loss_giou_5: 1.77  loss_ce_dn_5: 2.341  loss_mask_dn_5: 1.245  loss_dice_dn_5: 2.398  loss_bbox_dn_5: 0.9164  loss_giou_dn_5: 0.7667  loss_ce_6: 1.834  loss_mask_6: 1.364  loss_dice_6: 2.569  loss_bbox_6: 2.564  loss_giou_6: 1.7  loss_ce_dn_6: 2.318  loss_mask_dn_6: 1.262  loss_dice_dn_6: 2.412  loss_bbox_dn_6: 0.9239  loss_giou_dn_6: 0.7673  loss_ce_7: 1.803  loss_mask_7: 1.413  loss_dice_7: 2.639  loss_bbox_7: 2.53  loss_giou_7: 1.656  loss_ce_dn_7: 2.377  loss_mask_dn_7: 1.238  loss_dice_dn_7: 2.439  loss_bbox_dn_7: 0.9187  loss_giou_dn_7: 0.7615  loss_ce_8: 1.867  loss_mask_8: 1.373  loss_dice_8: 2.618  loss_bbox_8: 2.537  loss_giou_8: 1.67  loss_ce_dn_8: 2.431  loss_mask_dn_8: 1.214  loss_dice_dn_8: 2.436  loss_bbox_dn_8: 0.921  loss_giou_dn_8: 0.7621    time: 2.2749  last_time: 2.2653  data_time: 0.0161  last_data_time: 0.0206   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:35:12 d2.utils.events]: \u001b[0m eta: 8 days, 7:52:49  iter: 2619  total_loss: 189.6  loss_ce: 2.426  loss_mask: 1.643  loss_dice: 2.568  loss_bbox: 3.034  loss_giou: 1.629  loss_ce_dn: 2.414  loss_mask_dn: 1.415  loss_dice_dn: 2.369  loss_bbox_dn: 0.9613  loss_giou_dn: 0.7474  loss_ce_0: 10.6  loss_mask_0: 1.587  loss_dice_0: 3.002  loss_bbox_0: 3.645  loss_giou_0: 1.969  loss_ce_1: 2.745  loss_mask_1: 1.634  loss_dice_1: 2.653  loss_bbox_1: 3.112  loss_giou_1: 1.652  loss_ce_dn_1: 3.169  loss_mask_dn_1: 1.472  loss_dice_dn_1: 2.517  loss_bbox_dn_1: 1.062  loss_giou_dn_1: 0.7963  loss_ce_2: 2.398  loss_mask_2: 1.561  loss_dice_2: 2.589  loss_bbox_2: 2.909  loss_giou_2: 1.638  loss_ce_dn_2: 2.706  loss_mask_dn_2: 1.42  loss_dice_dn_2: 2.403  loss_bbox_dn_2: 0.9928  loss_giou_dn_2: 0.7734  loss_ce_3: 2.241  loss_mask_3: 1.519  loss_dice_3: 2.592  loss_bbox_3: 2.941  loss_giou_3: 1.675  loss_ce_dn_3: 2.512  loss_mask_dn_3: 1.405  loss_dice_dn_3: 2.386  loss_bbox_dn_3: 0.9607  loss_giou_dn_3: 0.762  loss_ce_4: 2.357  loss_mask_4: 1.582  loss_dice_4: 2.584  loss_bbox_4: 2.998  loss_giou_4: 1.669  loss_ce_dn_4: 2.445  loss_mask_dn_4: 1.397  loss_dice_dn_4: 2.349  loss_bbox_dn_4: 0.9378  loss_giou_dn_4: 0.7535  loss_ce_5: 2.471  loss_mask_5: 1.573  loss_dice_5: 2.585  loss_bbox_5: 3.006  loss_giou_5: 1.661  loss_ce_dn_5: 2.436  loss_mask_dn_5: 1.394  loss_dice_dn_5: 2.307  loss_bbox_dn_5: 0.9344  loss_giou_dn_5: 0.7454  loss_ce_6: 2.405  loss_mask_6: 1.566  loss_dice_6: 2.579  loss_bbox_6: 3.027  loss_giou_6: 1.663  loss_ce_dn_6: 2.429  loss_mask_dn_6: 1.376  loss_dice_dn_6: 2.314  loss_bbox_dn_6: 0.9451  loss_giou_dn_6: 0.75  loss_ce_7: 2.24  loss_mask_7: 1.576  loss_dice_7: 2.553  loss_bbox_7: 3.039  loss_giou_7: 1.648  loss_ce_dn_7: 2.394  loss_mask_dn_7: 1.384  loss_dice_dn_7: 2.324  loss_bbox_dn_7: 0.9527  loss_giou_dn_7: 0.7448  loss_ce_8: 2.336  loss_mask_8: 1.626  loss_dice_8: 2.576  loss_bbox_8: 3.025  loss_giou_8: 1.647  loss_ce_dn_8: 2.343  loss_mask_dn_8: 1.4  loss_dice_dn_8: 2.381  loss_bbox_dn_8: 0.9554  loss_giou_dn_8: 0.747    time: 2.2749  last_time: 2.3772  data_time: 0.0137  last_data_time: 0.0049   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:35:57 d2.utils.events]: \u001b[0m eta: 8 days, 7:51:13  iter: 2639  total_loss: 190.4  loss_ce: 1.865  loss_mask: 1.468  loss_dice: 2.335  loss_bbox: 2.698  loss_giou: 1.593  loss_ce_dn: 2.412  loss_mask_dn: 1.501  loss_dice_dn: 2.221  loss_bbox_dn: 0.9335  loss_giou_dn: 0.7396  loss_ce_0: 10.54  loss_mask_0: 1.424  loss_dice_0: 2.714  loss_bbox_0: 4.071  loss_giou_0: 1.96  loss_ce_1: 2.181  loss_mask_1: 1.478  loss_dice_1: 2.333  loss_bbox_1: 3.095  loss_giou_1: 1.659  loss_ce_dn_1: 2.991  loss_mask_dn_1: 1.508  loss_dice_dn_1: 2.485  loss_bbox_dn_1: 1.068  loss_giou_dn_1: 0.783  loss_ce_2: 1.978  loss_mask_2: 1.451  loss_dice_2: 2.333  loss_bbox_2: 2.869  loss_giou_2: 1.614  loss_ce_dn_2: 2.575  loss_mask_dn_2: 1.518  loss_dice_dn_2: 2.3  loss_bbox_dn_2: 1.004  loss_giou_dn_2: 0.7586  loss_ce_3: 1.752  loss_mask_3: 1.503  loss_dice_3: 2.313  loss_bbox_3: 2.861  loss_giou_3: 1.567  loss_ce_dn_3: 2.515  loss_mask_dn_3: 1.398  loss_dice_dn_3: 2.28  loss_bbox_dn_3: 0.9746  loss_giou_dn_3: 0.7439  loss_ce_4: 1.737  loss_mask_4: 1.485  loss_dice_4: 2.329  loss_bbox_4: 2.86  loss_giou_4: 1.608  loss_ce_dn_4: 2.455  loss_mask_dn_4: 1.42  loss_dice_dn_4: 2.234  loss_bbox_dn_4: 0.9483  loss_giou_dn_4: 0.7372  loss_ce_5: 1.772  loss_mask_5: 1.527  loss_dice_5: 2.32  loss_bbox_5: 2.853  loss_giou_5: 1.6  loss_ce_dn_5: 2.443  loss_mask_dn_5: 1.422  loss_dice_dn_5: 2.235  loss_bbox_dn_5: 0.9351  loss_giou_dn_5: 0.7329  loss_ce_6: 1.695  loss_mask_6: 1.497  loss_dice_6: 2.352  loss_bbox_6: 2.801  loss_giou_6: 1.608  loss_ce_dn_6: 2.371  loss_mask_dn_6: 1.411  loss_dice_dn_6: 2.232  loss_bbox_dn_6: 0.9359  loss_giou_dn_6: 0.7371  loss_ce_7: 1.724  loss_mask_7: 1.469  loss_dice_7: 2.383  loss_bbox_7: 2.742  loss_giou_7: 1.573  loss_ce_dn_7: 2.453  loss_mask_dn_7: 1.46  loss_dice_dn_7: 2.258  loss_bbox_dn_7: 0.9227  loss_giou_dn_7: 0.73  loss_ce_8: 1.768  loss_mask_8: 1.505  loss_dice_8: 2.365  loss_bbox_8: 2.734  loss_giou_8: 1.588  loss_ce_dn_8: 2.411  loss_mask_dn_8: 1.461  loss_dice_dn_8: 2.212  loss_bbox_dn_8: 0.9216  loss_giou_dn_8: 0.7382    time: 2.2749  last_time: 2.2752  data_time: 0.0121  last_data_time: 0.0045   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:36:42 d2.utils.events]: \u001b[0m eta: 8 days, 7:51:19  iter: 2659  total_loss: 188.5  loss_ce: 1.869  loss_mask: 1.563  loss_dice: 2.466  loss_bbox: 2.787  loss_giou: 1.601  loss_ce_dn: 2.297  loss_mask_dn: 1.298  loss_dice_dn: 2.239  loss_bbox_dn: 0.9309  loss_giou_dn: 0.7609  loss_ce_0: 10.22  loss_mask_0: 1.387  loss_dice_0: 2.71  loss_bbox_0: 3.791  loss_giou_0: 1.858  loss_ce_1: 2.174  loss_mask_1: 1.474  loss_dice_1: 2.486  loss_bbox_1: 3.057  loss_giou_1: 1.692  loss_ce_dn_1: 3.023  loss_mask_dn_1: 1.404  loss_dice_dn_1: 2.503  loss_bbox_dn_1: 1.031  loss_giou_dn_1: 0.7982  loss_ce_2: 1.806  loss_mask_2: 1.564  loss_dice_2: 2.491  loss_bbox_2: 2.999  loss_giou_2: 1.695  loss_ce_dn_2: 2.479  loss_mask_dn_2: 1.363  loss_dice_dn_2: 2.334  loss_bbox_dn_2: 0.9751  loss_giou_dn_2: 0.7757  loss_ce_3: 1.674  loss_mask_3: 1.594  loss_dice_3: 2.495  loss_bbox_3: 2.866  loss_giou_3: 1.604  loss_ce_dn_3: 2.304  loss_mask_dn_3: 1.375  loss_dice_dn_3: 2.314  loss_bbox_dn_3: 0.9486  loss_giou_dn_3: 0.7664  loss_ce_4: 1.746  loss_mask_4: 1.567  loss_dice_4: 2.436  loss_bbox_4: 2.766  loss_giou_4: 1.578  loss_ce_dn_4: 2.134  loss_mask_dn_4: 1.379  loss_dice_dn_4: 2.295  loss_bbox_dn_4: 0.9417  loss_giou_dn_4: 0.7585  loss_ce_5: 1.711  loss_mask_5: 1.594  loss_dice_5: 2.396  loss_bbox_5: 2.762  loss_giou_5: 1.58  loss_ce_dn_5: 2.185  loss_mask_dn_5: 1.378  loss_dice_dn_5: 2.276  loss_bbox_dn_5: 0.9344  loss_giou_dn_5: 0.758  loss_ce_6: 1.806  loss_mask_6: 1.544  loss_dice_6: 2.43  loss_bbox_6: 2.796  loss_giou_6: 1.58  loss_ce_dn_6: 2.202  loss_mask_dn_6: 1.342  loss_dice_dn_6: 2.248  loss_bbox_dn_6: 0.9324  loss_giou_dn_6: 0.7624  loss_ce_7: 1.869  loss_mask_7: 1.566  loss_dice_7: 2.465  loss_bbox_7: 2.797  loss_giou_7: 1.592  loss_ce_dn_7: 2.224  loss_mask_dn_7: 1.324  loss_dice_dn_7: 2.227  loss_bbox_dn_7: 0.9142  loss_giou_dn_7: 0.7598  loss_ce_8: 1.856  loss_mask_8: 1.532  loss_dice_8: 2.446  loss_bbox_8: 2.793  loss_giou_8: 1.589  loss_ce_dn_8: 2.29  loss_mask_dn_8: 1.326  loss_dice_dn_8: 2.226  loss_bbox_dn_8: 0.9221  loss_giou_dn_8: 0.7628    time: 2.2748  last_time: 2.2434  data_time: 0.0111  last_data_time: 0.0201   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:37:28 d2.utils.events]: \u001b[0m eta: 8 days, 7:52:07  iter: 2679  total_loss: 200.2  loss_ce: 1.928  loss_mask: 1.742  loss_dice: 2.604  loss_bbox: 2.606  loss_giou: 1.536  loss_ce_dn: 3.218  loss_mask_dn: 1.636  loss_dice_dn: 2.343  loss_bbox_dn: 0.9136  loss_giou_dn: 0.7117  loss_ce_0: 10.58  loss_mask_0: 1.678  loss_dice_0: 2.765  loss_bbox_0: 3.992  loss_giou_0: 1.927  loss_ce_1: 2.726  loss_mask_1: 1.806  loss_dice_1: 2.732  loss_bbox_1: 3.104  loss_giou_1: 1.57  loss_ce_dn_1: 3.466  loss_mask_dn_1: 1.583  loss_dice_dn_1: 2.576  loss_bbox_dn_1: 1.071  loss_giou_dn_1: 0.7803  loss_ce_2: 2.326  loss_mask_2: 1.822  loss_dice_2: 2.685  loss_bbox_2: 2.84  loss_giou_2: 1.56  loss_ce_dn_2: 2.973  loss_mask_dn_2: 1.638  loss_dice_dn_2: 2.403  loss_bbox_dn_2: 1.024  loss_giou_dn_2: 0.7472  loss_ce_3: 2.196  loss_mask_3: 1.856  loss_dice_3: 2.643  loss_bbox_3: 2.776  loss_giou_3: 1.595  loss_ce_dn_3: 2.719  loss_mask_dn_3: 1.64  loss_dice_dn_3: 2.301  loss_bbox_dn_3: 0.9839  loss_giou_dn_3: 0.7343  loss_ce_4: 2.012  loss_mask_4: 1.845  loss_dice_4: 2.629  loss_bbox_4: 2.746  loss_giou_4: 1.58  loss_ce_dn_4: 2.677  loss_mask_dn_4: 1.535  loss_dice_dn_4: 2.29  loss_bbox_dn_4: 0.9541  loss_giou_dn_4: 0.7363  loss_ce_5: 2.05  loss_mask_5: 1.831  loss_dice_5: 2.625  loss_bbox_5: 2.814  loss_giou_5: 1.541  loss_ce_dn_5: 2.834  loss_mask_dn_5: 1.532  loss_dice_dn_5: 2.286  loss_bbox_dn_5: 0.9415  loss_giou_dn_5: 0.7286  loss_ce_6: 2.019  loss_mask_6: 1.797  loss_dice_6: 2.603  loss_bbox_6: 2.815  loss_giou_6: 1.52  loss_ce_dn_6: 3.017  loss_mask_dn_6: 1.551  loss_dice_dn_6: 2.285  loss_bbox_dn_6: 0.9313  loss_giou_dn_6: 0.7255  loss_ce_7: 2.007  loss_mask_7: 1.779  loss_dice_7: 2.634  loss_bbox_7: 2.691  loss_giou_7: 1.525  loss_ce_dn_7: 3.114  loss_mask_dn_7: 1.535  loss_dice_dn_7: 2.278  loss_bbox_dn_7: 0.9118  loss_giou_dn_7: 0.7158  loss_ce_8: 1.999  loss_mask_8: 1.769  loss_dice_8: 2.563  loss_bbox_8: 2.616  loss_giou_8: 1.518  loss_ce_dn_8: 3.078  loss_mask_dn_8: 1.599  loss_dice_dn_8: 2.314  loss_bbox_dn_8: 0.911  loss_giou_dn_8: 0.7155    time: 2.2749  last_time: 2.3416  data_time: 0.0137  last_data_time: 0.0318   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:38:13 d2.utils.events]: \u001b[0m eta: 8 days, 7:51:22  iter: 2699  total_loss: 184.6  loss_ce: 1.991  loss_mask: 1.593  loss_dice: 2.577  loss_bbox: 2.717  loss_giou: 1.537  loss_ce_dn: 2.552  loss_mask_dn: 1.405  loss_dice_dn: 2.213  loss_bbox_dn: 0.9105  loss_giou_dn: 0.7226  loss_ce_0: 10.31  loss_mask_0: 1.446  loss_dice_0: 2.882  loss_bbox_0: 4.083  loss_giou_0: 1.915  loss_ce_1: 2.183  loss_mask_1: 1.483  loss_dice_1: 2.573  loss_bbox_1: 3.243  loss_giou_1: 1.59  loss_ce_dn_1: 2.836  loss_mask_dn_1: 1.434  loss_dice_dn_1: 2.472  loss_bbox_dn_1: 1.087  loss_giou_dn_1: 0.7781  loss_ce_2: 2.057  loss_mask_2: 1.496  loss_dice_2: 2.541  loss_bbox_2: 2.8  loss_giou_2: 1.552  loss_ce_dn_2: 2.35  loss_mask_dn_2: 1.428  loss_dice_dn_2: 2.237  loss_bbox_dn_2: 1.023  loss_giou_dn_2: 0.756  loss_ce_3: 2.046  loss_mask_3: 1.515  loss_dice_3: 2.559  loss_bbox_3: 2.735  loss_giou_3: 1.512  loss_ce_dn_3: 2.163  loss_mask_dn_3: 1.42  loss_dice_dn_3: 2.211  loss_bbox_dn_3: 0.978  loss_giou_dn_3: 0.7403  loss_ce_4: 2.024  loss_mask_4: 1.557  loss_dice_4: 2.566  loss_bbox_4: 2.653  loss_giou_4: 1.545  loss_ce_dn_4: 2.181  loss_mask_dn_4: 1.381  loss_dice_dn_4: 2.202  loss_bbox_dn_4: 0.9559  loss_giou_dn_4: 0.7234  loss_ce_5: 1.884  loss_mask_5: 1.568  loss_dice_5: 2.628  loss_bbox_5: 2.61  loss_giou_5: 1.53  loss_ce_dn_5: 2.218  loss_mask_dn_5: 1.376  loss_dice_dn_5: 2.215  loss_bbox_dn_5: 0.9409  loss_giou_dn_5: 0.7204  loss_ce_6: 1.985  loss_mask_6: 1.544  loss_dice_6: 2.612  loss_bbox_6: 2.666  loss_giou_6: 1.537  loss_ce_dn_6: 2.308  loss_mask_dn_6: 1.355  loss_dice_dn_6: 2.207  loss_bbox_dn_6: 0.9367  loss_giou_dn_6: 0.7254  loss_ce_7: 2  loss_mask_7: 1.552  loss_dice_7: 2.612  loss_bbox_7: 2.681  loss_giou_7: 1.533  loss_ce_dn_7: 2.332  loss_mask_dn_7: 1.362  loss_dice_dn_7: 2.212  loss_bbox_dn_7: 0.9123  loss_giou_dn_7: 0.7175  loss_ce_8: 1.975  loss_mask_8: 1.581  loss_dice_8: 2.621  loss_bbox_8: 2.701  loss_giou_8: 1.531  loss_ce_dn_8: 2.413  loss_mask_dn_8: 1.38  loss_dice_dn_8: 2.238  loss_bbox_dn_8: 0.9107  loss_giou_dn_8: 0.7185    time: 2.2748  last_time: 2.2634  data_time: 0.0116  last_data_time: 0.0100   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:38:59 d2.utils.events]: \u001b[0m eta: 8 days, 7:49:55  iter: 2719  total_loss: 188.6  loss_ce: 2.297  loss_mask: 1.619  loss_dice: 2.502  loss_bbox: 2.827  loss_giou: 1.691  loss_ce_dn: 2.452  loss_mask_dn: 1.498  loss_dice_dn: 2.256  loss_bbox_dn: 0.8713  loss_giou_dn: 0.7334  loss_ce_0: 10.53  loss_mask_0: 1.419  loss_dice_0: 2.862  loss_bbox_0: 3.772  loss_giou_0: 1.863  loss_ce_1: 2.654  loss_mask_1: 1.481  loss_dice_1: 2.499  loss_bbox_1: 3.308  loss_giou_1: 1.777  loss_ce_dn_1: 3.447  loss_mask_dn_1: 1.579  loss_dice_dn_1: 2.444  loss_bbox_dn_1: 1.041  loss_giou_dn_1: 0.793  loss_ce_2: 2.232  loss_mask_2: 1.564  loss_dice_2: 2.431  loss_bbox_2: 2.919  loss_giou_2: 1.754  loss_ce_dn_2: 2.814  loss_mask_dn_2: 1.521  loss_dice_dn_2: 2.292  loss_bbox_dn_2: 0.9753  loss_giou_dn_2: 0.7639  loss_ce_3: 2.157  loss_mask_3: 1.66  loss_dice_3: 2.47  loss_bbox_3: 2.902  loss_giou_3: 1.718  loss_ce_dn_3: 2.59  loss_mask_dn_3: 1.532  loss_dice_dn_3: 2.27  loss_bbox_dn_3: 0.9378  loss_giou_dn_3: 0.7515  loss_ce_4: 2.032  loss_mask_4: 1.659  loss_dice_4: 2.411  loss_bbox_4: 2.909  loss_giou_4: 1.692  loss_ce_dn_4: 2.391  loss_mask_dn_4: 1.523  loss_dice_dn_4: 2.253  loss_bbox_dn_4: 0.9174  loss_giou_dn_4: 0.745  loss_ce_5: 2.102  loss_mask_5: 1.619  loss_dice_5: 2.434  loss_bbox_5: 2.898  loss_giou_5: 1.686  loss_ce_dn_5: 2.423  loss_mask_dn_5: 1.5  loss_dice_dn_5: 2.236  loss_bbox_dn_5: 0.8976  loss_giou_dn_5: 0.7395  loss_ce_6: 2.138  loss_mask_6: 1.667  loss_dice_6: 2.472  loss_bbox_6: 2.892  loss_giou_6: 1.677  loss_ce_dn_6: 2.376  loss_mask_dn_6: 1.516  loss_dice_dn_6: 2.24  loss_bbox_dn_6: 0.8922  loss_giou_dn_6: 0.7379  loss_ce_7: 2.153  loss_mask_7: 1.671  loss_dice_7: 2.468  loss_bbox_7: 2.861  loss_giou_7: 1.684  loss_ce_dn_7: 2.369  loss_mask_dn_7: 1.523  loss_dice_dn_7: 2.277  loss_bbox_dn_7: 0.8798  loss_giou_dn_7: 0.732  loss_ce_8: 2.207  loss_mask_8: 1.601  loss_dice_8: 2.445  loss_bbox_8: 2.838  loss_giou_8: 1.685  loss_ce_dn_8: 2.389  loss_mask_dn_8: 1.495  loss_dice_dn_8: 2.26  loss_bbox_dn_8: 0.8765  loss_giou_dn_8: 0.7327    time: 2.2748  last_time: 2.2519  data_time: 0.0099  last_data_time: 0.0078   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:39:44 d2.utils.events]: \u001b[0m eta: 8 days, 7:50:28  iter: 2739  total_loss: 187.3  loss_ce: 1.963  loss_mask: 1.632  loss_dice: 2.284  loss_bbox: 2.462  loss_giou: 1.44  loss_ce_dn: 2.743  loss_mask_dn: 1.477  loss_dice_dn: 2.366  loss_bbox_dn: 0.9768  loss_giou_dn: 0.7223  loss_ce_0: 10.21  loss_mask_0: 1.411  loss_dice_0: 2.695  loss_bbox_0: 3.766  loss_giou_0: 1.923  loss_ce_1: 2.176  loss_mask_1: 1.556  loss_dice_1: 2.521  loss_bbox_1: 2.9  loss_giou_1: 1.534  loss_ce_dn_1: 2.971  loss_mask_dn_1: 1.563  loss_dice_dn_1: 2.556  loss_bbox_dn_1: 1.055  loss_giou_dn_1: 0.7813  loss_ce_2: 1.861  loss_mask_2: 1.532  loss_dice_2: 2.344  loss_bbox_2: 2.657  loss_giou_2: 1.565  loss_ce_dn_2: 2.421  loss_mask_dn_2: 1.52  loss_dice_dn_2: 2.471  loss_bbox_dn_2: 1.018  loss_giou_dn_2: 0.7557  loss_ce_3: 1.907  loss_mask_3: 1.54  loss_dice_3: 2.345  loss_bbox_3: 2.643  loss_giou_3: 1.468  loss_ce_dn_3: 2.393  loss_mask_dn_3: 1.482  loss_dice_dn_3: 2.456  loss_bbox_dn_3: 0.9975  loss_giou_dn_3: 0.7419  loss_ce_4: 1.869  loss_mask_4: 1.514  loss_dice_4: 2.305  loss_bbox_4: 2.623  loss_giou_4: 1.43  loss_ce_dn_4: 2.408  loss_mask_dn_4: 1.42  loss_dice_dn_4: 2.391  loss_bbox_dn_4: 0.9716  loss_giou_dn_4: 0.7365  loss_ce_5: 1.912  loss_mask_5: 1.529  loss_dice_5: 2.301  loss_bbox_5: 2.58  loss_giou_5: 1.416  loss_ce_dn_5: 2.425  loss_mask_dn_5: 1.43  loss_dice_dn_5: 2.38  loss_bbox_dn_5: 0.9636  loss_giou_dn_5: 0.7296  loss_ce_6: 1.822  loss_mask_6: 1.597  loss_dice_6: 2.273  loss_bbox_6: 2.539  loss_giou_6: 1.448  loss_ce_dn_6: 2.393  loss_mask_dn_6: 1.428  loss_dice_dn_6: 2.373  loss_bbox_dn_6: 0.968  loss_giou_dn_6: 0.7276  loss_ce_7: 1.822  loss_mask_7: 1.621  loss_dice_7: 2.32  loss_bbox_7: 2.485  loss_giou_7: 1.423  loss_ce_dn_7: 2.45  loss_mask_dn_7: 1.434  loss_dice_dn_7: 2.416  loss_bbox_dn_7: 0.9507  loss_giou_dn_7: 0.7198  loss_ce_8: 1.907  loss_mask_8: 1.659  loss_dice_8: 2.273  loss_bbox_8: 2.476  loss_giou_8: 1.423  loss_ce_dn_8: 2.57  loss_mask_dn_8: 1.46  loss_dice_dn_8: 2.369  loss_bbox_dn_8: 0.9527  loss_giou_dn_8: 0.7214    time: 2.2748  last_time: 2.2441  data_time: 0.0117  last_data_time: 0.0080   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:40:30 d2.utils.events]: \u001b[0m eta: 8 days, 7:49:56  iter: 2759  total_loss: 189.1  loss_ce: 1.942  loss_mask: 1.648  loss_dice: 2.499  loss_bbox: 2.97  loss_giou: 1.529  loss_ce_dn: 2.885  loss_mask_dn: 1.502  loss_dice_dn: 2.386  loss_bbox_dn: 0.9311  loss_giou_dn: 0.7096  loss_ce_0: 10.32  loss_mask_0: 1.596  loss_dice_0: 2.727  loss_bbox_0: 3.892  loss_giou_0: 1.931  loss_ce_1: 2.327  loss_mask_1: 1.659  loss_dice_1: 2.579  loss_bbox_1: 3.271  loss_giou_1: 1.675  loss_ce_dn_1: 3.105  loss_mask_dn_1: 1.604  loss_dice_dn_1: 2.543  loss_bbox_dn_1: 1.059  loss_giou_dn_1: 0.7738  loss_ce_2: 2.09  loss_mask_2: 1.637  loss_dice_2: 2.516  loss_bbox_2: 3.224  loss_giou_2: 1.615  loss_ce_dn_2: 2.74  loss_mask_dn_2: 1.552  loss_dice_dn_2: 2.405  loss_bbox_dn_2: 0.9991  loss_giou_dn_2: 0.7491  loss_ce_3: 1.958  loss_mask_3: 1.622  loss_dice_3: 2.492  loss_bbox_3: 3.209  loss_giou_3: 1.614  loss_ce_dn_3: 2.566  loss_mask_dn_3: 1.535  loss_dice_dn_3: 2.373  loss_bbox_dn_3: 0.952  loss_giou_dn_3: 0.7326  loss_ce_4: 1.886  loss_mask_4: 1.655  loss_dice_4: 2.537  loss_bbox_4: 2.971  loss_giou_4: 1.575  loss_ce_dn_4: 2.634  loss_mask_dn_4: 1.56  loss_dice_dn_4: 2.39  loss_bbox_dn_4: 0.9428  loss_giou_dn_4: 0.7195  loss_ce_5: 1.842  loss_mask_5: 1.65  loss_dice_5: 2.517  loss_bbox_5: 2.966  loss_giou_5: 1.563  loss_ce_dn_5: 2.675  loss_mask_dn_5: 1.564  loss_dice_dn_5: 2.403  loss_bbox_dn_5: 0.9316  loss_giou_dn_5: 0.71  loss_ce_6: 1.866  loss_mask_6: 1.636  loss_dice_6: 2.515  loss_bbox_6: 2.963  loss_giou_6: 1.563  loss_ce_dn_6: 2.687  loss_mask_dn_6: 1.539  loss_dice_dn_6: 2.362  loss_bbox_dn_6: 0.921  loss_giou_dn_6: 0.709  loss_ce_7: 1.89  loss_mask_7: 1.616  loss_dice_7: 2.492  loss_bbox_7: 2.932  loss_giou_7: 1.542  loss_ce_dn_7: 2.702  loss_mask_dn_7: 1.542  loss_dice_dn_7: 2.38  loss_bbox_dn_7: 0.9158  loss_giou_dn_7: 0.7013  loss_ce_8: 1.929  loss_mask_8: 1.613  loss_dice_8: 2.499  loss_bbox_8: 2.933  loss_giou_8: 1.533  loss_ce_dn_8: 2.795  loss_mask_dn_8: 1.543  loss_dice_dn_8: 2.387  loss_bbox_dn_8: 0.9238  loss_giou_dn_8: 0.6987    time: 2.2747  last_time: 2.3242  data_time: 0.0134  last_data_time: 0.0069   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:41:15 d2.utils.events]: \u001b[0m eta: 8 days, 7:47:50  iter: 2779  total_loss: 184  loss_ce: 2.066  loss_mask: 1.412  loss_dice: 2.392  loss_bbox: 2.729  loss_giou: 1.482  loss_ce_dn: 2.212  loss_mask_dn: 1.241  loss_dice_dn: 2.261  loss_bbox_dn: 0.789  loss_giou_dn: 0.7068  loss_ce_0: 9.687  loss_mask_0: 1.324  loss_dice_0: 2.82  loss_bbox_0: 3.846  loss_giou_0: 2.03  loss_ce_1: 2.522  loss_mask_1: 1.455  loss_dice_1: 2.388  loss_bbox_1: 3.056  loss_giou_1: 1.652  loss_ce_dn_1: 2.835  loss_mask_dn_1: 1.367  loss_dice_dn_1: 2.404  loss_bbox_dn_1: 0.949  loss_giou_dn_1: 0.7929  loss_ce_2: 2.16  loss_mask_2: 1.42  loss_dice_2: 2.398  loss_bbox_2: 2.756  loss_giou_2: 1.587  loss_ce_dn_2: 2.278  loss_mask_dn_2: 1.326  loss_dice_dn_2: 2.233  loss_bbox_dn_2: 0.8916  loss_giou_dn_2: 0.7687  loss_ce_3: 2.085  loss_mask_3: 1.444  loss_dice_3: 2.419  loss_bbox_3: 2.835  loss_giou_3: 1.55  loss_ce_dn_3: 2.174  loss_mask_dn_3: 1.325  loss_dice_dn_3: 2.233  loss_bbox_dn_3: 0.8496  loss_giou_dn_3: 0.7509  loss_ce_4: 2.013  loss_mask_4: 1.45  loss_dice_4: 2.322  loss_bbox_4: 2.793  loss_giou_4: 1.515  loss_ce_dn_4: 2.073  loss_mask_dn_4: 1.282  loss_dice_dn_4: 2.206  loss_bbox_dn_4: 0.8291  loss_giou_dn_4: 0.7365  loss_ce_5: 1.937  loss_mask_5: 1.532  loss_dice_5: 2.382  loss_bbox_5: 2.699  loss_giou_5: 1.521  loss_ce_dn_5: 2.107  loss_mask_dn_5: 1.261  loss_dice_dn_5: 2.214  loss_bbox_dn_5: 0.8197  loss_giou_dn_5: 0.7202  loss_ce_6: 1.942  loss_mask_6: 1.533  loss_dice_6: 2.411  loss_bbox_6: 2.708  loss_giou_6: 1.52  loss_ce_dn_6: 2.087  loss_mask_dn_6: 1.276  loss_dice_dn_6: 2.223  loss_bbox_dn_6: 0.816  loss_giou_dn_6: 0.7166  loss_ce_7: 1.886  loss_mask_7: 1.434  loss_dice_7: 2.378  loss_bbox_7: 2.72  loss_giou_7: 1.493  loss_ce_dn_7: 2.161  loss_mask_dn_7: 1.239  loss_dice_dn_7: 2.235  loss_bbox_dn_7: 0.7931  loss_giou_dn_7: 0.7004  loss_ce_8: 2.099  loss_mask_8: 1.421  loss_dice_8: 2.384  loss_bbox_8: 2.718  loss_giou_8: 1.487  loss_ce_dn_8: 2.103  loss_mask_dn_8: 1.225  loss_dice_dn_8: 2.232  loss_bbox_dn_8: 0.7912  loss_giou_dn_8: 0.7014    time: 2.2747  last_time: 2.2393  data_time: 0.0137  last_data_time: 0.0046   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:42:01 d2.utils.events]: \u001b[0m eta: 8 days, 7:47:41  iter: 2799  total_loss: 177.8  loss_ce: 2.113  loss_mask: 1.377  loss_dice: 2.354  loss_bbox: 2.61  loss_giou: 1.647  loss_ce_dn: 2.302  loss_mask_dn: 1.326  loss_dice_dn: 2.329  loss_bbox_dn: 0.9439  loss_giou_dn: 0.7335  loss_ce_0: 10.06  loss_mask_0: 1.511  loss_dice_0: 2.805  loss_bbox_0: 3.723  loss_giou_0: 1.841  loss_ce_1: 2.72  loss_mask_1: 1.379  loss_dice_1: 2.474  loss_bbox_1: 2.841  loss_giou_1: 1.731  loss_ce_dn_1: 2.847  loss_mask_dn_1: 1.313  loss_dice_dn_1: 2.414  loss_bbox_dn_1: 1.067  loss_giou_dn_1: 0.7928  loss_ce_2: 2.404  loss_mask_2: 1.276  loss_dice_2: 2.384  loss_bbox_2: 2.632  loss_giou_2: 1.735  loss_ce_dn_2: 2.274  loss_mask_dn_2: 1.281  loss_dice_dn_2: 2.333  loss_bbox_dn_2: 1.008  loss_giou_dn_2: 0.7613  loss_ce_3: 2.192  loss_mask_3: 1.397  loss_dice_3: 2.353  loss_bbox_3: 2.539  loss_giou_3: 1.698  loss_ce_dn_3: 2.145  loss_mask_dn_3: 1.321  loss_dice_dn_3: 2.327  loss_bbox_dn_3: 0.9724  loss_giou_dn_3: 0.7474  loss_ce_4: 2.031  loss_mask_4: 1.354  loss_dice_4: 2.368  loss_bbox_4: 2.497  loss_giou_4: 1.69  loss_ce_dn_4: 2.092  loss_mask_dn_4: 1.377  loss_dice_dn_4: 2.33  loss_bbox_dn_4: 0.9499  loss_giou_dn_4: 0.7384  loss_ce_5: 1.948  loss_mask_5: 1.323  loss_dice_5: 2.384  loss_bbox_5: 2.566  loss_giou_5: 1.686  loss_ce_dn_5: 2.108  loss_mask_dn_5: 1.301  loss_dice_dn_5: 2.295  loss_bbox_dn_5: 0.9352  loss_giou_dn_5: 0.7333  loss_ce_6: 1.989  loss_mask_6: 1.349  loss_dice_6: 2.352  loss_bbox_6: 2.594  loss_giou_6: 1.686  loss_ce_dn_6: 2.126  loss_mask_dn_6: 1.27  loss_dice_dn_6: 2.298  loss_bbox_dn_6: 0.9384  loss_giou_dn_6: 0.7312  loss_ce_7: 2.122  loss_mask_7: 1.351  loss_dice_7: 2.344  loss_bbox_7: 2.606  loss_giou_7: 1.664  loss_ce_dn_7: 2.151  loss_mask_dn_7: 1.34  loss_dice_dn_7: 2.324  loss_bbox_dn_7: 0.9375  loss_giou_dn_7: 0.7238  loss_ce_8: 2.114  loss_mask_8: 1.361  loss_dice_8: 2.299  loss_bbox_8: 2.61  loss_giou_8: 1.628  loss_ce_dn_8: 2.198  loss_mask_dn_8: 1.316  loss_dice_dn_8: 2.321  loss_bbox_dn_8: 0.9432  loss_giou_dn_8: 0.7248    time: 2.2747  last_time: 2.2740  data_time: 0.0145  last_data_time: 0.0171   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:42:46 d2.utils.events]: \u001b[0m eta: 8 days, 7:46:56  iter: 2819  total_loss: 183.4  loss_ce: 1.796  loss_mask: 1.299  loss_dice: 2.513  loss_bbox: 2.805  loss_giou: 1.712  loss_ce_dn: 2.298  loss_mask_dn: 1.191  loss_dice_dn: 2.231  loss_bbox_dn: 0.8083  loss_giou_dn: 0.7434  loss_ce_0: 9.813  loss_mask_0: 1.307  loss_dice_0: 2.812  loss_bbox_0: 3.946  loss_giou_0: 2.008  loss_ce_1: 2.154  loss_mask_1: 1.336  loss_dice_1: 2.555  loss_bbox_1: 3.094  loss_giou_1: 1.786  loss_ce_dn_1: 2.62  loss_mask_dn_1: 1.271  loss_dice_dn_1: 2.509  loss_bbox_dn_1: 0.9577  loss_giou_dn_1: 0.799  loss_ce_2: 1.696  loss_mask_2: 1.354  loss_dice_2: 2.571  loss_bbox_2: 2.904  loss_giou_2: 1.723  loss_ce_dn_2: 2.21  loss_mask_dn_2: 1.198  loss_dice_dn_2: 2.371  loss_bbox_dn_2: 0.9367  loss_giou_dn_2: 0.7734  loss_ce_3: 1.749  loss_mask_3: 1.195  loss_dice_3: 2.549  loss_bbox_3: 2.836  loss_giou_3: 1.704  loss_ce_dn_3: 2.134  loss_mask_dn_3: 1.193  loss_dice_dn_3: 2.323  loss_bbox_dn_3: 0.907  loss_giou_dn_3: 0.7582  loss_ce_4: 1.655  loss_mask_4: 1.195  loss_dice_4: 2.549  loss_bbox_4: 2.822  loss_giou_4: 1.701  loss_ce_dn_4: 2.056  loss_mask_dn_4: 1.197  loss_dice_dn_4: 2.322  loss_bbox_dn_4: 0.8588  loss_giou_dn_4: 0.7529  loss_ce_5: 1.608  loss_mask_5: 1.176  loss_dice_5: 2.542  loss_bbox_5: 2.784  loss_giou_5: 1.699  loss_ce_dn_5: 2.036  loss_mask_dn_5: 1.2  loss_dice_dn_5: 2.323  loss_bbox_dn_5: 0.8255  loss_giou_dn_5: 0.7411  loss_ce_6: 1.595  loss_mask_6: 1.245  loss_dice_6: 2.581  loss_bbox_6: 2.806  loss_giou_6: 1.709  loss_ce_dn_6: 1.988  loss_mask_dn_6: 1.209  loss_dice_dn_6: 2.28  loss_bbox_dn_6: 0.8177  loss_giou_dn_6: 0.7379  loss_ce_7: 1.658  loss_mask_7: 1.243  loss_dice_7: 2.566  loss_bbox_7: 2.796  loss_giou_7: 1.697  loss_ce_dn_7: 2.105  loss_mask_dn_7: 1.214  loss_dice_dn_7: 2.302  loss_bbox_dn_7: 0.7975  loss_giou_dn_7: 0.7373  loss_ce_8: 1.681  loss_mask_8: 1.317  loss_dice_8: 2.516  loss_bbox_8: 2.798  loss_giou_8: 1.698  loss_ce_dn_8: 2.213  loss_mask_dn_8: 1.199  loss_dice_dn_8: 2.252  loss_bbox_dn_8: 0.8036  loss_giou_dn_8: 0.7414    time: 2.2748  last_time: 2.2616  data_time: 0.0145  last_data_time: 0.0075   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:43:32 d2.utils.events]: \u001b[0m eta: 8 days, 7:46:41  iter: 2839  total_loss: 190.7  loss_ce: 1.79  loss_mask: 1.528  loss_dice: 2.443  loss_bbox: 2.669  loss_giou: 1.609  loss_ce_dn: 2.617  loss_mask_dn: 1.594  loss_dice_dn: 2.346  loss_bbox_dn: 0.9036  loss_giou_dn: 0.7235  loss_ce_0: 9.916  loss_mask_0: 1.568  loss_dice_0: 2.749  loss_bbox_0: 3.777  loss_giou_0: 1.895  loss_ce_1: 2.318  loss_mask_1: 1.617  loss_dice_1: 2.467  loss_bbox_1: 3.093  loss_giou_1: 1.658  loss_ce_dn_1: 3.172  loss_mask_dn_1: 1.62  loss_dice_dn_1: 2.544  loss_bbox_dn_1: 1.014  loss_giou_dn_1: 0.7944  loss_ce_2: 1.962  loss_mask_2: 1.583  loss_dice_2: 2.423  loss_bbox_2: 2.886  loss_giou_2: 1.595  loss_ce_dn_2: 2.639  loss_mask_dn_2: 1.577  loss_dice_dn_2: 2.349  loss_bbox_dn_2: 0.9811  loss_giou_dn_2: 0.7728  loss_ce_3: 1.893  loss_mask_3: 1.582  loss_dice_3: 2.439  loss_bbox_3: 2.928  loss_giou_3: 1.642  loss_ce_dn_3: 2.578  loss_mask_dn_3: 1.582  loss_dice_dn_3: 2.316  loss_bbox_dn_3: 0.9443  loss_giou_dn_3: 0.7481  loss_ce_4: 1.779  loss_mask_4: 1.631  loss_dice_4: 2.359  loss_bbox_4: 2.834  loss_giou_4: 1.651  loss_ce_dn_4: 2.531  loss_mask_dn_4: 1.651  loss_dice_dn_4: 2.285  loss_bbox_dn_4: 0.9153  loss_giou_dn_4: 0.738  loss_ce_5: 1.772  loss_mask_5: 1.645  loss_dice_5: 2.417  loss_bbox_5: 2.777  loss_giou_5: 1.65  loss_ce_dn_5: 2.563  loss_mask_dn_5: 1.634  loss_dice_dn_5: 2.334  loss_bbox_dn_5: 0.9084  loss_giou_dn_5: 0.7292  loss_ce_6: 1.797  loss_mask_6: 1.596  loss_dice_6: 2.398  loss_bbox_6: 2.685  loss_giou_6: 1.658  loss_ce_dn_6: 2.535  loss_mask_dn_6: 1.592  loss_dice_dn_6: 2.325  loss_bbox_dn_6: 0.9084  loss_giou_dn_6: 0.7274  loss_ce_7: 1.778  loss_mask_7: 1.574  loss_dice_7: 2.407  loss_bbox_7: 2.667  loss_giou_7: 1.632  loss_ce_dn_7: 2.53  loss_mask_dn_7: 1.585  loss_dice_dn_7: 2.349  loss_bbox_dn_7: 0.8954  loss_giou_dn_7: 0.7188  loss_ce_8: 1.838  loss_mask_8: 1.554  loss_dice_8: 2.422  loss_bbox_8: 2.675  loss_giou_8: 1.622  loss_ce_dn_8: 2.538  loss_mask_dn_8: 1.598  loss_dice_dn_8: 2.377  loss_bbox_dn_8: 0.8972  loss_giou_dn_8: 0.7192    time: 2.2748  last_time: 2.3572  data_time: 0.0105  last_data_time: 0.0046   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:44:18 d2.utils.events]: \u001b[0m eta: 8 days, 7:46:54  iter: 2859  total_loss: 193.3  loss_ce: 1.992  loss_mask: 1.521  loss_dice: 2.7  loss_bbox: 2.774  loss_giou: 1.671  loss_ce_dn: 2.634  loss_mask_dn: 1.445  loss_dice_dn: 2.291  loss_bbox_dn: 0.8717  loss_giou_dn: 0.7218  loss_ce_0: 9.692  loss_mask_0: 1.486  loss_dice_0: 2.983  loss_bbox_0: 3.986  loss_giou_0: 2.052  loss_ce_1: 2.551  loss_mask_1: 1.572  loss_dice_1: 2.745  loss_bbox_1: 2.914  loss_giou_1: 1.709  loss_ce_dn_1: 3.038  loss_mask_dn_1: 1.456  loss_dice_dn_1: 2.404  loss_bbox_dn_1: 1.042  loss_giou_dn_1: 0.8  loss_ce_2: 2.284  loss_mask_2: 1.572  loss_dice_2: 2.761  loss_bbox_2: 2.824  loss_giou_2: 1.637  loss_ce_dn_2: 2.455  loss_mask_dn_2: 1.405  loss_dice_dn_2: 2.284  loss_bbox_dn_2: 0.9822  loss_giou_dn_2: 0.7752  loss_ce_3: 2.03  loss_mask_3: 1.557  loss_dice_3: 2.75  loss_bbox_3: 2.785  loss_giou_3: 1.671  loss_ce_dn_3: 2.321  loss_mask_dn_3: 1.359  loss_dice_dn_3: 2.288  loss_bbox_dn_3: 0.9175  loss_giou_dn_3: 0.758  loss_ce_4: 2.064  loss_mask_4: 1.514  loss_dice_4: 2.74  loss_bbox_4: 2.755  loss_giou_4: 1.665  loss_ce_dn_4: 2.257  loss_mask_dn_4: 1.423  loss_dice_dn_4: 2.259  loss_bbox_dn_4: 0.9025  loss_giou_dn_4: 0.7414  loss_ce_5: 2.038  loss_mask_5: 1.486  loss_dice_5: 2.71  loss_bbox_5: 2.736  loss_giou_5: 1.687  loss_ce_dn_5: 2.251  loss_mask_dn_5: 1.421  loss_dice_dn_5: 2.242  loss_bbox_dn_5: 0.895  loss_giou_dn_5: 0.7312  loss_ce_6: 1.923  loss_mask_6: 1.493  loss_dice_6: 2.749  loss_bbox_6: 2.73  loss_giou_6: 1.693  loss_ce_dn_6: 2.277  loss_mask_dn_6: 1.422  loss_dice_dn_6: 2.269  loss_bbox_dn_6: 0.894  loss_giou_dn_6: 0.7318  loss_ce_7: 1.881  loss_mask_7: 1.441  loss_dice_7: 2.745  loss_bbox_7: 2.755  loss_giou_7: 1.675  loss_ce_dn_7: 2.344  loss_mask_dn_7: 1.424  loss_dice_dn_7: 2.304  loss_bbox_dn_7: 0.8714  loss_giou_dn_7: 0.7215  loss_ce_8: 1.95  loss_mask_8: 1.454  loss_dice_8: 2.733  loss_bbox_8: 2.768  loss_giou_8: 1.666  loss_ce_dn_8: 2.487  loss_mask_dn_8: 1.426  loss_dice_dn_8: 2.297  loss_bbox_dn_8: 0.871  loss_giou_dn_8: 0.7215    time: 2.2748  last_time: 2.2900  data_time: 0.0128  last_data_time: 0.0064   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:45:03 d2.utils.events]: \u001b[0m eta: 8 days, 7:46:09  iter: 2879  total_loss: 176.5  loss_ce: 1.723  loss_mask: 1.316  loss_dice: 2.315  loss_bbox: 2.75  loss_giou: 1.556  loss_ce_dn: 2.066  loss_mask_dn: 1.248  loss_dice_dn: 2.154  loss_bbox_dn: 0.8951  loss_giou_dn: 0.724  loss_ce_0: 9.585  loss_mask_0: 1.288  loss_dice_0: 2.552  loss_bbox_0: 4.105  loss_giou_0: 1.956  loss_ce_1: 1.947  loss_mask_1: 1.375  loss_dice_1: 2.397  loss_bbox_1: 3.111  loss_giou_1: 1.672  loss_ce_dn_1: 2.519  loss_mask_dn_1: 1.285  loss_dice_dn_1: 2.413  loss_bbox_dn_1: 1.085  loss_giou_dn_1: 0.7891  loss_ce_2: 1.675  loss_mask_2: 1.336  loss_dice_2: 2.263  loss_bbox_2: 2.935  loss_giou_2: 1.598  loss_ce_dn_2: 2.04  loss_mask_dn_2: 1.206  loss_dice_dn_2: 2.26  loss_bbox_dn_2: 1.027  loss_giou_dn_2: 0.7723  loss_ce_3: 1.425  loss_mask_3: 1.362  loss_dice_3: 2.243  loss_bbox_3: 2.896  loss_giou_3: 1.568  loss_ce_dn_3: 2.042  loss_mask_dn_3: 1.217  loss_dice_dn_3: 2.2  loss_bbox_dn_3: 0.9595  loss_giou_dn_3: 0.7607  loss_ce_4: 1.438  loss_mask_4: 1.337  loss_dice_4: 2.322  loss_bbox_4: 2.866  loss_giou_4: 1.542  loss_ce_dn_4: 1.875  loss_mask_dn_4: 1.168  loss_dice_dn_4: 2.181  loss_bbox_dn_4: 0.9394  loss_giou_dn_4: 0.7531  loss_ce_5: 1.436  loss_mask_5: 1.33  loss_dice_5: 2.286  loss_bbox_5: 2.848  loss_giou_5: 1.535  loss_ce_dn_5: 1.993  loss_mask_dn_5: 1.186  loss_dice_dn_5: 2.076  loss_bbox_dn_5: 0.9238  loss_giou_dn_5: 0.7425  loss_ce_6: 1.523  loss_mask_6: 1.301  loss_dice_6: 2.281  loss_bbox_6: 2.821  loss_giou_6: 1.539  loss_ce_dn_6: 1.961  loss_mask_dn_6: 1.216  loss_dice_dn_6: 2.145  loss_bbox_dn_6: 0.9141  loss_giou_dn_6: 0.7353  loss_ce_7: 1.45  loss_mask_7: 1.278  loss_dice_7: 2.236  loss_bbox_7: 2.804  loss_giou_7: 1.527  loss_ce_dn_7: 1.994  loss_mask_dn_7: 1.216  loss_dice_dn_7: 2.149  loss_bbox_dn_7: 0.886  loss_giou_dn_7: 0.7235  loss_ce_8: 1.559  loss_mask_8: 1.296  loss_dice_8: 2.228  loss_bbox_8: 2.801  loss_giou_8: 1.538  loss_ce_dn_8: 2.004  loss_mask_dn_8: 1.21  loss_dice_dn_8: 2.137  loss_bbox_dn_8: 0.8863  loss_giou_dn_8: 0.7202    time: 2.2749  last_time: 2.2890  data_time: 0.0137  last_data_time: 0.0148   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:45:49 d2.utils.events]: \u001b[0m eta: 8 days, 7:44:58  iter: 2899  total_loss: 192.7  loss_ce: 2.299  loss_mask: 1.481  loss_dice: 2.562  loss_bbox: 2.924  loss_giou: 1.587  loss_ce_dn: 2.748  loss_mask_dn: 1.427  loss_dice_dn: 2.396  loss_bbox_dn: 0.9031  loss_giou_dn: 0.707  loss_ce_0: 9.622  loss_mask_0: 1.462  loss_dice_0: 2.815  loss_bbox_0: 3.937  loss_giou_0: 1.899  loss_ce_1: 2.616  loss_mask_1: 1.38  loss_dice_1: 2.506  loss_bbox_1: 3.197  loss_giou_1: 1.714  loss_ce_dn_1: 3.264  loss_mask_dn_1: 1.452  loss_dice_dn_1: 2.662  loss_bbox_dn_1: 1.045  loss_giou_dn_1: 0.7904  loss_ce_2: 2.221  loss_mask_2: 1.399  loss_dice_2: 2.498  loss_bbox_2: 3.071  loss_giou_2: 1.708  loss_ce_dn_2: 2.654  loss_mask_dn_2: 1.439  loss_dice_dn_2: 2.503  loss_bbox_dn_2: 0.9985  loss_giou_dn_2: 0.7641  loss_ce_3: 2.18  loss_mask_3: 1.442  loss_dice_3: 2.49  loss_bbox_3: 3.044  loss_giou_3: 1.667  loss_ce_dn_3: 2.468  loss_mask_dn_3: 1.491  loss_dice_dn_3: 2.485  loss_bbox_dn_3: 0.9774  loss_giou_dn_3: 0.7542  loss_ce_4: 2.132  loss_mask_4: 1.474  loss_dice_4: 2.52  loss_bbox_4: 3.063  loss_giou_4: 1.606  loss_ce_dn_4: 2.323  loss_mask_dn_4: 1.516  loss_dice_dn_4: 2.427  loss_bbox_dn_4: 0.9328  loss_giou_dn_4: 0.7337  loss_ce_5: 2.149  loss_mask_5: 1.463  loss_dice_5: 2.584  loss_bbox_5: 2.948  loss_giou_5: 1.586  loss_ce_dn_5: 2.439  loss_mask_dn_5: 1.481  loss_dice_dn_5: 2.454  loss_bbox_dn_5: 0.9069  loss_giou_dn_5: 0.7185  loss_ce_6: 2.064  loss_mask_6: 1.458  loss_dice_6: 2.553  loss_bbox_6: 3.063  loss_giou_6: 1.599  loss_ce_dn_6: 2.491  loss_mask_dn_6: 1.447  loss_dice_dn_6: 2.401  loss_bbox_dn_6: 0.902  loss_giou_dn_6: 0.7176  loss_ce_7: 2.185  loss_mask_7: 1.495  loss_dice_7: 2.573  loss_bbox_7: 2.944  loss_giou_7: 1.557  loss_ce_dn_7: 2.603  loss_mask_dn_7: 1.434  loss_dice_dn_7: 2.412  loss_bbox_dn_7: 0.894  loss_giou_dn_7: 0.7066  loss_ce_8: 2.187  loss_mask_8: 1.468  loss_dice_8: 2.551  loss_bbox_8: 2.958  loss_giou_8: 1.572  loss_ce_dn_8: 2.701  loss_mask_dn_8: 1.434  loss_dice_dn_8: 2.408  loss_bbox_dn_8: 0.8961  loss_giou_dn_8: 0.7062    time: 2.2748  last_time: 2.2299  data_time: 0.0144  last_data_time: 0.0152   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:46:34 d2.utils.events]: \u001b[0m eta: 8 days, 7:43:09  iter: 2919  total_loss: 171.9  loss_ce: 1.88  loss_mask: 1.256  loss_dice: 2.338  loss_bbox: 2.518  loss_giou: 1.58  loss_ce_dn: 2.013  loss_mask_dn: 1.165  loss_dice_dn: 2.203  loss_bbox_dn: 0.848  loss_giou_dn: 0.7465  loss_ce_0: 9.318  loss_mask_0: 1.214  loss_dice_0: 2.584  loss_bbox_0: 3.739  loss_giou_0: 2.089  loss_ce_1: 2.146  loss_mask_1: 1.28  loss_dice_1: 2.431  loss_bbox_1: 2.96  loss_giou_1: 1.771  loss_ce_dn_1: 2.784  loss_mask_dn_1: 1.298  loss_dice_dn_1: 2.396  loss_bbox_dn_1: 1.032  loss_giou_dn_1: 0.7835  loss_ce_2: 1.854  loss_mask_2: 1.284  loss_dice_2: 2.368  loss_bbox_2: 2.876  loss_giou_2: 1.741  loss_ce_dn_2: 2.239  loss_mask_dn_2: 1.206  loss_dice_dn_2: 2.188  loss_bbox_dn_2: 0.9464  loss_giou_dn_2: 0.76  loss_ce_3: 1.818  loss_mask_3: 1.318  loss_dice_3: 2.345  loss_bbox_3: 2.761  loss_giou_3: 1.704  loss_ce_dn_3: 2.042  loss_mask_dn_3: 1.179  loss_dice_dn_3: 2.173  loss_bbox_dn_3: 0.8894  loss_giou_dn_3: 0.7522  loss_ce_4: 1.837  loss_mask_4: 1.345  loss_dice_4: 2.354  loss_bbox_4: 2.677  loss_giou_4: 1.673  loss_ce_dn_4: 2.002  loss_mask_dn_4: 1.185  loss_dice_dn_4: 2.169  loss_bbox_dn_4: 0.8596  loss_giou_dn_4: 0.7447  loss_ce_5: 1.868  loss_mask_5: 1.337  loss_dice_5: 2.344  loss_bbox_5: 2.561  loss_giou_5: 1.607  loss_ce_dn_5: 1.977  loss_mask_dn_5: 1.193  loss_dice_dn_5: 2.177  loss_bbox_dn_5: 0.854  loss_giou_dn_5: 0.7442  loss_ce_6: 1.907  loss_mask_6: 1.294  loss_dice_6: 2.331  loss_bbox_6: 2.547  loss_giou_6: 1.599  loss_ce_dn_6: 1.935  loss_mask_dn_6: 1.182  loss_dice_dn_6: 2.149  loss_bbox_dn_6: 0.8539  loss_giou_dn_6: 0.7435  loss_ce_7: 1.835  loss_mask_7: 1.285  loss_dice_7: 2.326  loss_bbox_7: 2.536  loss_giou_7: 1.592  loss_ce_dn_7: 1.992  loss_mask_dn_7: 1.138  loss_dice_dn_7: 2.192  loss_bbox_dn_7: 0.8422  loss_giou_dn_7: 0.7385  loss_ce_8: 1.811  loss_mask_8: 1.268  loss_dice_8: 2.35  loss_bbox_8: 2.534  loss_giou_8: 1.582  loss_ce_dn_8: 2.042  loss_mask_dn_8: 1.171  loss_dice_dn_8: 2.203  loss_bbox_dn_8: 0.8439  loss_giou_dn_8: 0.739    time: 2.2748  last_time: 2.2935  data_time: 0.0125  last_data_time: 0.0047   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:47:20 d2.utils.events]: \u001b[0m eta: 8 days, 7:41:47  iter: 2939  total_loss: 186.4  loss_ce: 1.909  loss_mask: 1.507  loss_dice: 2.551  loss_bbox: 2.789  loss_giou: 1.52  loss_ce_dn: 2.485  loss_mask_dn: 1.313  loss_dice_dn: 2.404  loss_bbox_dn: 0.8805  loss_giou_dn: 0.6995  loss_ce_0: 9.278  loss_mask_0: 1.549  loss_dice_0: 2.768  loss_bbox_0: 4.176  loss_giou_0: 2.013  loss_ce_1: 2.145  loss_mask_1: 1.465  loss_dice_1: 2.588  loss_bbox_1: 3.202  loss_giou_1: 1.575  loss_ce_dn_1: 2.908  loss_mask_dn_1: 1.421  loss_dice_dn_1: 2.561  loss_bbox_dn_1: 1.029  loss_giou_dn_1: 0.7789  loss_ce_2: 1.86  loss_mask_2: 1.484  loss_dice_2: 2.528  loss_bbox_2: 3.055  loss_giou_2: 1.523  loss_ce_dn_2: 2.504  loss_mask_dn_2: 1.338  loss_dice_dn_2: 2.376  loss_bbox_dn_2: 0.9771  loss_giou_dn_2: 0.7556  loss_ce_3: 1.879  loss_mask_3: 1.503  loss_dice_3: 2.552  loss_bbox_3: 2.89  loss_giou_3: 1.532  loss_ce_dn_3: 2.427  loss_mask_dn_3: 1.336  loss_dice_dn_3: 2.343  loss_bbox_dn_3: 0.9438  loss_giou_dn_3: 0.7345  loss_ce_4: 1.934  loss_mask_4: 1.544  loss_dice_4: 2.53  loss_bbox_4: 2.8  loss_giou_4: 1.508  loss_ce_dn_4: 2.338  loss_mask_dn_4: 1.356  loss_dice_dn_4: 2.345  loss_bbox_dn_4: 0.9172  loss_giou_dn_4: 0.72  loss_ce_5: 1.925  loss_mask_5: 1.528  loss_dice_5: 2.534  loss_bbox_5: 2.84  loss_giou_5: 1.477  loss_ce_dn_5: 2.409  loss_mask_dn_5: 1.349  loss_dice_dn_5: 2.318  loss_bbox_dn_5: 0.9101  loss_giou_dn_5: 0.7133  loss_ce_6: 1.921  loss_mask_6: 1.545  loss_dice_6: 2.546  loss_bbox_6: 2.804  loss_giou_6: 1.47  loss_ce_dn_6: 2.38  loss_mask_dn_6: 1.356  loss_dice_dn_6: 2.343  loss_bbox_dn_6: 0.9086  loss_giou_dn_6: 0.7099  loss_ce_7: 1.9  loss_mask_7: 1.519  loss_dice_7: 2.565  loss_bbox_7: 2.772  loss_giou_7: 1.458  loss_ce_dn_7: 2.422  loss_mask_dn_7: 1.371  loss_dice_dn_7: 2.354  loss_bbox_dn_7: 0.8836  loss_giou_dn_7: 0.6995  loss_ce_8: 1.904  loss_mask_8: 1.493  loss_dice_8: 2.553  loss_bbox_8: 2.785  loss_giou_8: 1.475  loss_ce_dn_8: 2.428  loss_mask_dn_8: 1.341  loss_dice_dn_8: 2.365  loss_bbox_dn_8: 0.8771  loss_giou_dn_8: 0.7002    time: 2.2748  last_time: 2.2759  data_time: 0.0152  last_data_time: 0.0167   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:48:05 d2.utils.events]: \u001b[0m eta: 8 days, 7:41:02  iter: 2959  total_loss: 189.9  loss_ce: 2.263  loss_mask: 1.548  loss_dice: 2.356  loss_bbox: 2.832  loss_giou: 1.459  loss_ce_dn: 2.537  loss_mask_dn: 1.348  loss_dice_dn: 2.148  loss_bbox_dn: 0.8628  loss_giou_dn: 0.724  loss_ce_0: 10.04  loss_mask_0: 1.36  loss_dice_0: 2.604  loss_bbox_0: 3.951  loss_giou_0: 1.905  loss_ce_1: 2.309  loss_mask_1: 1.525  loss_dice_1: 2.417  loss_bbox_1: 3.156  loss_giou_1: 1.653  loss_ce_dn_1: 3.175  loss_mask_dn_1: 1.343  loss_dice_dn_1: 2.439  loss_bbox_dn_1: 0.9854  loss_giou_dn_1: 0.7864  loss_ce_2: 2.252  loss_mask_2: 1.517  loss_dice_2: 2.329  loss_bbox_2: 2.941  loss_giou_2: 1.517  loss_ce_dn_2: 2.521  loss_mask_dn_2: 1.289  loss_dice_dn_2: 2.213  loss_bbox_dn_2: 0.9303  loss_giou_dn_2: 0.752  loss_ce_3: 2.285  loss_mask_3: 1.57  loss_dice_3: 2.336  loss_bbox_3: 2.88  loss_giou_3: 1.504  loss_ce_dn_3: 2.315  loss_mask_dn_3: 1.277  loss_dice_dn_3: 2.146  loss_bbox_dn_3: 0.9099  loss_giou_dn_3: 0.7327  loss_ce_4: 2.319  loss_mask_4: 1.542  loss_dice_4: 2.313  loss_bbox_4: 2.831  loss_giou_4: 1.47  loss_ce_dn_4: 2.241  loss_mask_dn_4: 1.301  loss_dice_dn_4: 2.12  loss_bbox_dn_4: 0.8933  loss_giou_dn_4: 0.7258  loss_ce_5: 2.195  loss_mask_5: 1.536  loss_dice_5: 2.373  loss_bbox_5: 2.833  loss_giou_5: 1.476  loss_ce_dn_5: 2.311  loss_mask_dn_5: 1.306  loss_dice_dn_5: 2.134  loss_bbox_dn_5: 0.88  loss_giou_dn_5: 0.7248  loss_ce_6: 2.209  loss_mask_6: 1.539  loss_dice_6: 2.342  loss_bbox_6: 2.864  loss_giou_6: 1.462  loss_ce_dn_6: 2.348  loss_mask_dn_6: 1.281  loss_dice_dn_6: 2.111  loss_bbox_dn_6: 0.876  loss_giou_dn_6: 0.7203  loss_ce_7: 2.164  loss_mask_7: 1.545  loss_dice_7: 2.365  loss_bbox_7: 2.877  loss_giou_7: 1.449  loss_ce_dn_7: 2.358  loss_mask_dn_7: 1.311  loss_dice_dn_7: 2.106  loss_bbox_dn_7: 0.8536  loss_giou_dn_7: 0.7149  loss_ce_8: 2.283  loss_mask_8: 1.567  loss_dice_8: 2.429  loss_bbox_8: 2.848  loss_giou_8: 1.45  loss_ce_dn_8: 2.448  loss_mask_dn_8: 1.333  loss_dice_dn_8: 2.159  loss_bbox_dn_8: 0.8571  loss_giou_dn_8: 0.719    time: 2.2748  last_time: 2.2633  data_time: 0.0112  last_data_time: 0.0103   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:48:51 d2.utils.events]: \u001b[0m eta: 8 days, 7:40:05  iter: 2979  total_loss: 189.8  loss_ce: 1.921  loss_mask: 1.634  loss_dice: 2.439  loss_bbox: 2.867  loss_giou: 1.613  loss_ce_dn: 2.16  loss_mask_dn: 1.457  loss_dice_dn: 2.253  loss_bbox_dn: 0.9036  loss_giou_dn: 0.7461  loss_ce_0: 9.162  loss_mask_0: 1.571  loss_dice_0: 2.725  loss_bbox_0: 3.9  loss_giou_0: 1.913  loss_ce_1: 2.307  loss_mask_1: 1.544  loss_dice_1: 2.46  loss_bbox_1: 3.066  loss_giou_1: 1.764  loss_ce_dn_1: 2.589  loss_mask_dn_1: 1.552  loss_dice_dn_1: 2.414  loss_bbox_dn_1: 1.039  loss_giou_dn_1: 0.794  loss_ce_2: 1.981  loss_mask_2: 1.654  loss_dice_2: 2.395  loss_bbox_2: 2.81  loss_giou_2: 1.677  loss_ce_dn_2: 2.233  loss_mask_dn_2: 1.508  loss_dice_dn_2: 2.248  loss_bbox_dn_2: 0.9782  loss_giou_dn_2: 0.7773  loss_ce_3: 1.779  loss_mask_3: 1.611  loss_dice_3: 2.378  loss_bbox_3: 2.823  loss_giou_3: 1.602  loss_ce_dn_3: 2.08  loss_mask_dn_3: 1.516  loss_dice_dn_3: 2.223  loss_bbox_dn_3: 0.9356  loss_giou_dn_3: 0.7632  loss_ce_4: 1.897  loss_mask_4: 1.623  loss_dice_4: 2.379  loss_bbox_4: 2.801  loss_giou_4: 1.573  loss_ce_dn_4: 2.057  loss_mask_dn_4: 1.449  loss_dice_dn_4: 2.228  loss_bbox_dn_4: 0.909  loss_giou_dn_4: 0.7514  loss_ce_5: 1.944  loss_mask_5: 1.587  loss_dice_5: 2.421  loss_bbox_5: 2.863  loss_giou_5: 1.617  loss_ce_dn_5: 1.99  loss_mask_dn_5: 1.428  loss_dice_dn_5: 2.242  loss_bbox_dn_5: 0.8984  loss_giou_dn_5: 0.7537  loss_ce_6: 1.945  loss_mask_6: 1.654  loss_dice_6: 2.357  loss_bbox_6: 2.907  loss_giou_6: 1.628  loss_ce_dn_6: 2.015  loss_mask_dn_6: 1.436  loss_dice_dn_6: 2.234  loss_bbox_dn_6: 0.8967  loss_giou_dn_6: 0.7564  loss_ce_7: 1.887  loss_mask_7: 1.627  loss_dice_7: 2.45  loss_bbox_7: 2.901  loss_giou_7: 1.597  loss_ce_dn_7: 1.992  loss_mask_dn_7: 1.494  loss_dice_dn_7: 2.235  loss_bbox_dn_7: 0.8766  loss_giou_dn_7: 0.7394  loss_ce_8: 1.971  loss_mask_8: 1.584  loss_dice_8: 2.389  loss_bbox_8: 2.971  loss_giou_8: 1.601  loss_ce_dn_8: 2.015  loss_mask_dn_8: 1.461  loss_dice_dn_8: 2.239  loss_bbox_dn_8: 0.885  loss_giou_dn_8: 0.7382    time: 2.2748  last_time: 2.2903  data_time: 0.0138  last_data_time: 0.0077   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:49:36 d2.utils.events]: \u001b[0m eta: 8 days, 7:37:55  iter: 2999  total_loss: 187.9  loss_ce: 2.057  loss_mask: 1.388  loss_dice: 2.438  loss_bbox: 2.787  loss_giou: 1.537  loss_ce_dn: 2.866  loss_mask_dn: 1.285  loss_dice_dn: 2.198  loss_bbox_dn: 0.8745  loss_giou_dn: 0.7354  loss_ce_0: 9.735  loss_mask_0: 1.383  loss_dice_0: 2.711  loss_bbox_0: 3.485  loss_giou_0: 1.904  loss_ce_1: 2.554  loss_mask_1: 1.424  loss_dice_1: 2.456  loss_bbox_1: 3.053  loss_giou_1: 1.665  loss_ce_dn_1: 3.186  loss_mask_dn_1: 1.391  loss_dice_dn_1: 2.391  loss_bbox_dn_1: 1.017  loss_giou_dn_1: 0.7886  loss_ce_2: 2.144  loss_mask_2: 1.428  loss_dice_2: 2.383  loss_bbox_2: 2.853  loss_giou_2: 1.651  loss_ce_dn_2: 2.656  loss_mask_dn_2: 1.38  loss_dice_dn_2: 2.221  loss_bbox_dn_2: 0.9728  loss_giou_dn_2: 0.771  loss_ce_3: 2.028  loss_mask_3: 1.46  loss_dice_3: 2.435  loss_bbox_3: 2.827  loss_giou_3: 1.629  loss_ce_dn_3: 2.568  loss_mask_dn_3: 1.327  loss_dice_dn_3: 2.212  loss_bbox_dn_3: 0.9269  loss_giou_dn_3: 0.7585  loss_ce_4: 2.016  loss_mask_4: 1.462  loss_dice_4: 2.433  loss_bbox_4: 2.779  loss_giou_4: 1.612  loss_ce_dn_4: 2.499  loss_mask_dn_4: 1.258  loss_dice_dn_4: 2.183  loss_bbox_dn_4: 0.9046  loss_giou_dn_4: 0.7445  loss_ce_5: 1.911  loss_mask_5: 1.435  loss_dice_5: 2.427  loss_bbox_5: 2.767  loss_giou_5: 1.589  loss_ce_dn_5: 2.515  loss_mask_dn_5: 1.294  loss_dice_dn_5: 2.185  loss_bbox_dn_5: 0.8883  loss_giou_dn_5: 0.7372  loss_ce_6: 1.959  loss_mask_6: 1.409  loss_dice_6: 2.471  loss_bbox_6: 2.764  loss_giou_6: 1.559  loss_ce_dn_6: 2.55  loss_mask_dn_6: 1.287  loss_dice_dn_6: 2.172  loss_bbox_dn_6: 0.8771  loss_giou_dn_6: 0.7364  loss_ce_7: 1.993  loss_mask_7: 1.42  loss_dice_7: 2.415  loss_bbox_7: 2.774  loss_giou_7: 1.579  loss_ce_dn_7: 2.571  loss_mask_dn_7: 1.293  loss_dice_dn_7: 2.171  loss_bbox_dn_7: 0.871  loss_giou_dn_7: 0.73  loss_ce_8: 2.036  loss_mask_8: 1.392  loss_dice_8: 2.409  loss_bbox_8: 2.784  loss_giou_8: 1.574  loss_ce_dn_8: 2.771  loss_mask_dn_8: 1.242  loss_dice_dn_8: 2.173  loss_bbox_dn_8: 0.8719  loss_giou_dn_8: 0.7316    time: 2.2748  last_time: 2.2663  data_time: 0.0131  last_data_time: 0.0061   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:50:22 d2.utils.events]: \u001b[0m eta: 8 days, 7:37:10  iter: 3019  total_loss: 178.2  loss_ce: 1.817  loss_mask: 1.79  loss_dice: 2.146  loss_bbox: 2.418  loss_giou: 1.382  loss_ce_dn: 2.515  loss_mask_dn: 1.537  loss_dice_dn: 2.059  loss_bbox_dn: 0.9359  loss_giou_dn: 0.6897  loss_ce_0: 9.492  loss_mask_0: 1.733  loss_dice_0: 2.332  loss_bbox_0: 3.595  loss_giou_0: 1.751  loss_ce_1: 2.254  loss_mask_1: 1.757  loss_dice_1: 2.226  loss_bbox_1: 2.893  loss_giou_1: 1.505  loss_ce_dn_1: 2.831  loss_mask_dn_1: 1.664  loss_dice_dn_1: 2.264  loss_bbox_dn_1: 1.126  loss_giou_dn_1: 0.769  loss_ce_2: 1.976  loss_mask_2: 1.909  loss_dice_2: 2.126  loss_bbox_2: 2.591  loss_giou_2: 1.448  loss_ce_dn_2: 2.225  loss_mask_dn_2: 1.609  loss_dice_dn_2: 2.171  loss_bbox_dn_2: 1.029  loss_giou_dn_2: 0.7375  loss_ce_3: 1.824  loss_mask_3: 1.868  loss_dice_3: 2.186  loss_bbox_3: 2.539  loss_giou_3: 1.428  loss_ce_dn_3: 2.152  loss_mask_dn_3: 1.588  loss_dice_dn_3: 2.059  loss_bbox_dn_3: 0.9947  loss_giou_dn_3: 0.7164  loss_ce_4: 1.686  loss_mask_4: 1.898  loss_dice_4: 2.205  loss_bbox_4: 2.475  loss_giou_4: 1.409  loss_ce_dn_4: 2.241  loss_mask_dn_4: 1.591  loss_dice_dn_4: 2.052  loss_bbox_dn_4: 0.9739  loss_giou_dn_4: 0.7023  loss_ce_5: 1.675  loss_mask_5: 1.835  loss_dice_5: 2.135  loss_bbox_5: 2.452  loss_giou_5: 1.447  loss_ce_dn_5: 2.184  loss_mask_dn_5: 1.551  loss_dice_dn_5: 2.033  loss_bbox_dn_5: 0.9592  loss_giou_dn_5: 0.6985  loss_ce_6: 1.697  loss_mask_6: 1.805  loss_dice_6: 2.139  loss_bbox_6: 2.443  loss_giou_6: 1.425  loss_ce_dn_6: 2.293  loss_mask_dn_6: 1.559  loss_dice_dn_6: 2.059  loss_bbox_dn_6: 0.9565  loss_giou_dn_6: 0.6993  loss_ce_7: 1.726  loss_mask_7: 1.795  loss_dice_7: 2.163  loss_bbox_7: 2.396  loss_giou_7: 1.381  loss_ce_dn_7: 2.4  loss_mask_dn_7: 1.555  loss_dice_dn_7: 2.04  loss_bbox_dn_7: 0.9216  loss_giou_dn_7: 0.6865  loss_ce_8: 1.758  loss_mask_8: 1.807  loss_dice_8: 2.2  loss_bbox_8: 2.408  loss_giou_8: 1.38  loss_ce_dn_8: 2.431  loss_mask_dn_8: 1.575  loss_dice_dn_8: 2.073  loss_bbox_dn_8: 0.9235  loss_giou_dn_8: 0.6872    time: 2.2748  last_time: 2.2333  data_time: 0.0104  last_data_time: 0.0105   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:51:07 d2.utils.events]: \u001b[0m eta: 8 days, 7:36:24  iter: 3039  total_loss: 174.3  loss_ce: 1.42  loss_mask: 1.588  loss_dice: 2.355  loss_bbox: 2.63  loss_giou: 1.474  loss_ce_dn: 2.163  loss_mask_dn: 1.423  loss_dice_dn: 2.01  loss_bbox_dn: 0.8246  loss_giou_dn: 0.7041  loss_ce_0: 9.378  loss_mask_0: 1.434  loss_dice_0: 2.566  loss_bbox_0: 3.855  loss_giou_0: 1.937  loss_ce_1: 1.843  loss_mask_1: 1.518  loss_dice_1: 2.408  loss_bbox_1: 3.087  loss_giou_1: 1.665  loss_ce_dn_1: 2.41  loss_mask_dn_1: 1.56  loss_dice_dn_1: 2.206  loss_bbox_dn_1: 0.986  loss_giou_dn_1: 0.7822  loss_ce_2: 1.496  loss_mask_2: 1.577  loss_dice_2: 2.439  loss_bbox_2: 2.614  loss_giou_2: 1.609  loss_ce_dn_2: 1.934  loss_mask_dn_2: 1.473  loss_dice_dn_2: 2.07  loss_bbox_dn_2: 0.9214  loss_giou_dn_2: 0.7455  loss_ce_3: 1.401  loss_mask_3: 1.596  loss_dice_3: 2.406  loss_bbox_3: 2.567  loss_giou_3: 1.586  loss_ce_dn_3: 1.852  loss_mask_dn_3: 1.408  loss_dice_dn_3: 2.069  loss_bbox_dn_3: 0.8907  loss_giou_dn_3: 0.7247  loss_ce_4: 1.374  loss_mask_4: 1.546  loss_dice_4: 2.417  loss_bbox_4: 2.586  loss_giou_4: 1.578  loss_ce_dn_4: 1.77  loss_mask_dn_4: 1.39  loss_dice_dn_4: 2.032  loss_bbox_dn_4: 0.8619  loss_giou_dn_4: 0.7102  loss_ce_5: 1.394  loss_mask_5: 1.546  loss_dice_5: 2.385  loss_bbox_5: 2.582  loss_giou_5: 1.57  loss_ce_dn_5: 1.823  loss_mask_dn_5: 1.374  loss_dice_dn_5: 2.028  loss_bbox_dn_5: 0.8388  loss_giou_dn_5: 0.7047  loss_ce_6: 1.364  loss_mask_6: 1.565  loss_dice_6: 2.413  loss_bbox_6: 2.584  loss_giou_6: 1.538  loss_ce_dn_6: 1.947  loss_mask_dn_6: 1.454  loss_dice_dn_6: 2.035  loss_bbox_dn_6: 0.8332  loss_giou_dn_6: 0.7075  loss_ce_7: 1.429  loss_mask_7: 1.573  loss_dice_7: 2.388  loss_bbox_7: 2.61  loss_giou_7: 1.478  loss_ce_dn_7: 2.124  loss_mask_dn_7: 1.424  loss_dice_dn_7: 2.026  loss_bbox_dn_7: 0.8217  loss_giou_dn_7: 0.7022  loss_ce_8: 1.354  loss_mask_8: 1.602  loss_dice_8: 2.336  loss_bbox_8: 2.625  loss_giou_8: 1.473  loss_ce_dn_8: 2.002  loss_mask_dn_8: 1.41  loss_dice_dn_8: 2.007  loss_bbox_dn_8: 0.8219  loss_giou_dn_8: 0.7021    time: 2.2748  last_time: 2.2304  data_time: 0.0148  last_data_time: 0.0039   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:51:53 d2.utils.events]: \u001b[0m eta: 8 days, 7:35:07  iter: 3059  total_loss: 184.8  loss_ce: 1.914  loss_mask: 1.568  loss_dice: 2.619  loss_bbox: 2.903  loss_giou: 1.654  loss_ce_dn: 2.074  loss_mask_dn: 1.255  loss_dice_dn: 2.294  loss_bbox_dn: 0.8326  loss_giou_dn: 0.7223  loss_ce_0: 9.382  loss_mask_0: 1.658  loss_dice_0: 2.855  loss_bbox_0: 3.753  loss_giou_0: 2.018  loss_ce_1: 2.372  loss_mask_1: 1.469  loss_dice_1: 2.54  loss_bbox_1: 3.206  loss_giou_1: 1.789  loss_ce_dn_1: 2.835  loss_mask_dn_1: 1.367  loss_dice_dn_1: 2.494  loss_bbox_dn_1: 0.966  loss_giou_dn_1: 0.777  loss_ce_2: 2.067  loss_mask_2: 1.46  loss_dice_2: 2.455  loss_bbox_2: 3.008  loss_giou_2: 1.707  loss_ce_dn_2: 2.39  loss_mask_dn_2: 1.309  loss_dice_dn_2: 2.332  loss_bbox_dn_2: 0.9035  loss_giou_dn_2: 0.7484  loss_ce_3: 1.837  loss_mask_3: 1.415  loss_dice_3: 2.494  loss_bbox_3: 2.943  loss_giou_3: 1.693  loss_ce_dn_3: 2.27  loss_mask_dn_3: 1.292  loss_dice_dn_3: 2.259  loss_bbox_dn_3: 0.8575  loss_giou_dn_3: 0.7368  loss_ce_4: 1.88  loss_mask_4: 1.4  loss_dice_4: 2.514  loss_bbox_4: 2.896  loss_giou_4: 1.666  loss_ce_dn_4: 2.097  loss_mask_dn_4: 1.306  loss_dice_dn_4: 2.245  loss_bbox_dn_4: 0.8364  loss_giou_dn_4: 0.7265  loss_ce_5: 1.914  loss_mask_5: 1.465  loss_dice_5: 2.509  loss_bbox_5: 2.888  loss_giou_5: 1.664  loss_ce_dn_5: 2.043  loss_mask_dn_5: 1.298  loss_dice_dn_5: 2.275  loss_bbox_dn_5: 0.8277  loss_giou_dn_5: 0.7262  loss_ce_6: 1.89  loss_mask_6: 1.462  loss_dice_6: 2.548  loss_bbox_6: 2.862  loss_giou_6: 1.663  loss_ce_dn_6: 2.076  loss_mask_dn_6: 1.317  loss_dice_dn_6: 2.274  loss_bbox_dn_6: 0.8312  loss_giou_dn_6: 0.7261  loss_ce_7: 1.955  loss_mask_7: 1.499  loss_dice_7: 2.499  loss_bbox_7: 2.866  loss_giou_7: 1.643  loss_ce_dn_7: 2.114  loss_mask_dn_7: 1.29  loss_dice_dn_7: 2.266  loss_bbox_dn_7: 0.8232  loss_giou_dn_7: 0.7234  loss_ce_8: 1.979  loss_mask_8: 1.522  loss_dice_8: 2.559  loss_bbox_8: 2.862  loss_giou_8: 1.636  loss_ce_dn_8: 2.056  loss_mask_dn_8: 1.261  loss_dice_dn_8: 2.29  loss_bbox_dn_8: 0.8268  loss_giou_dn_8: 0.7205    time: 2.2747  last_time: 2.3742  data_time: 0.0119  last_data_time: 0.0017   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:52:38 d2.utils.events]: \u001b[0m eta: 8 days, 7:34:13  iter: 3079  total_loss: 178  loss_ce: 1.694  loss_mask: 1.432  loss_dice: 2.309  loss_bbox: 2.495  loss_giou: 1.604  loss_ce_dn: 2.263  loss_mask_dn: 1.388  loss_dice_dn: 2.317  loss_bbox_dn: 0.8205  loss_giou_dn: 0.7261  loss_ce_0: 8.871  loss_mask_0: 1.408  loss_dice_0: 2.713  loss_bbox_0: 3.926  loss_giou_0: 2  loss_ce_1: 1.966  loss_mask_1: 1.437  loss_dice_1: 2.354  loss_bbox_1: 2.884  loss_giou_1: 1.695  loss_ce_dn_1: 2.791  loss_mask_dn_1: 1.417  loss_dice_dn_1: 2.298  loss_bbox_dn_1: 0.9767  loss_giou_dn_1: 0.7881  loss_ce_2: 1.797  loss_mask_2: 1.405  loss_dice_2: 2.26  loss_bbox_2: 2.66  loss_giou_2: 1.646  loss_ce_dn_2: 2.293  loss_mask_dn_2: 1.376  loss_dice_dn_2: 2.316  loss_bbox_dn_2: 0.934  loss_giou_dn_2: 0.7698  loss_ce_3: 1.674  loss_mask_3: 1.416  loss_dice_3: 2.279  loss_bbox_3: 2.629  loss_giou_3: 1.628  loss_ce_dn_3: 2.165  loss_mask_dn_3: 1.344  loss_dice_dn_3: 2.28  loss_bbox_dn_3: 0.8929  loss_giou_dn_3: 0.7526  loss_ce_4: 1.598  loss_mask_4: 1.439  loss_dice_4: 2.347  loss_bbox_4: 2.555  loss_giou_4: 1.647  loss_ce_dn_4: 2.104  loss_mask_dn_4: 1.328  loss_dice_dn_4: 2.187  loss_bbox_dn_4: 0.8801  loss_giou_dn_4: 0.7417  loss_ce_5: 1.605  loss_mask_5: 1.516  loss_dice_5: 2.355  loss_bbox_5: 2.506  loss_giou_5: 1.617  loss_ce_dn_5: 2.102  loss_mask_dn_5: 1.326  loss_dice_dn_5: 2.256  loss_bbox_dn_5: 0.8616  loss_giou_dn_5: 0.7322  loss_ce_6: 1.532  loss_mask_6: 1.47  loss_dice_6: 2.335  loss_bbox_6: 2.498  loss_giou_6: 1.617  loss_ce_dn_6: 2.06  loss_mask_dn_6: 1.357  loss_dice_dn_6: 2.215  loss_bbox_dn_6: 0.85  loss_giou_dn_6: 0.7306  loss_ce_7: 1.554  loss_mask_7: 1.443  loss_dice_7: 2.352  loss_bbox_7: 2.483  loss_giou_7: 1.613  loss_ce_dn_7: 2.193  loss_mask_dn_7: 1.373  loss_dice_dn_7: 2.269  loss_bbox_dn_7: 0.8246  loss_giou_dn_7: 0.7281  loss_ce_8: 1.61  loss_mask_8: 1.479  loss_dice_8: 2.353  loss_bbox_8: 2.499  loss_giou_8: 1.605  loss_ce_dn_8: 2.21  loss_mask_dn_8: 1.363  loss_dice_dn_8: 2.271  loss_bbox_dn_8: 0.8223  loss_giou_dn_8: 0.7258    time: 2.2747  last_time: 2.2640  data_time: 0.0134  last_data_time: 0.0256   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:53:23 d2.utils.events]: \u001b[0m eta: 8 days, 7:32:59  iter: 3099  total_loss: 188.1  loss_ce: 2.108  loss_mask: 1.39  loss_dice: 2.487  loss_bbox: 2.662  loss_giou: 1.584  loss_ce_dn: 2.316  loss_mask_dn: 1.267  loss_dice_dn: 2.398  loss_bbox_dn: 0.8872  loss_giou_dn: 0.7348  loss_ce_0: 9.057  loss_mask_0: 1.386  loss_dice_0: 3.011  loss_bbox_0: 3.681  loss_giou_0: 1.88  loss_ce_1: 2.345  loss_mask_1: 1.511  loss_dice_1: 2.594  loss_bbox_1: 2.901  loss_giou_1: 1.684  loss_ce_dn_1: 2.884  loss_mask_dn_1: 1.402  loss_dice_dn_1: 2.646  loss_bbox_dn_1: 1.045  loss_giou_dn_1: 0.7821  loss_ce_2: 1.924  loss_mask_2: 1.383  loss_dice_2: 2.541  loss_bbox_2: 2.772  loss_giou_2: 1.643  loss_ce_dn_2: 2.272  loss_mask_dn_2: 1.34  loss_dice_dn_2: 2.519  loss_bbox_dn_2: 0.9673  loss_giou_dn_2: 0.7679  loss_ce_3: 1.965  loss_mask_3: 1.359  loss_dice_3: 2.437  loss_bbox_3: 2.712  loss_giou_3: 1.652  loss_ce_dn_3: 2.154  loss_mask_dn_3: 1.308  loss_dice_dn_3: 2.435  loss_bbox_dn_3: 0.94  loss_giou_dn_3: 0.7545  loss_ce_4: 1.91  loss_mask_4: 1.309  loss_dice_4: 2.469  loss_bbox_4: 2.672  loss_giou_4: 1.651  loss_ce_dn_4: 2.183  loss_mask_dn_4: 1.27  loss_dice_dn_4: 2.365  loss_bbox_dn_4: 0.9287  loss_giou_dn_4: 0.7465  loss_ce_5: 1.902  loss_mask_5: 1.303  loss_dice_5: 2.512  loss_bbox_5: 2.684  loss_giou_5: 1.626  loss_ce_dn_5: 2.228  loss_mask_dn_5: 1.268  loss_dice_dn_5: 2.405  loss_bbox_dn_5: 0.902  loss_giou_dn_5: 0.7404  loss_ce_6: 1.97  loss_mask_6: 1.33  loss_dice_6: 2.479  loss_bbox_6: 2.674  loss_giou_6: 1.616  loss_ce_dn_6: 2.22  loss_mask_dn_6: 1.267  loss_dice_dn_6: 2.367  loss_bbox_dn_6: 0.8884  loss_giou_dn_6: 0.7355  loss_ce_7: 2.005  loss_mask_7: 1.294  loss_dice_7: 2.49  loss_bbox_7: 2.628  loss_giou_7: 1.589  loss_ce_dn_7: 2.221  loss_mask_dn_7: 1.253  loss_dice_dn_7: 2.403  loss_bbox_dn_7: 0.8789  loss_giou_dn_7: 0.7268  loss_ce_8: 2.073  loss_mask_8: 1.385  loss_dice_8: 2.492  loss_bbox_8: 2.636  loss_giou_8: 1.591  loss_ce_dn_8: 2.221  loss_mask_dn_8: 1.261  loss_dice_dn_8: 2.374  loss_bbox_dn_8: 0.881  loss_giou_dn_8: 0.729    time: 2.2746  last_time: 2.2699  data_time: 0.0128  last_data_time: 0.0172   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:54:09 d2.utils.events]: \u001b[0m eta: 8 days, 7:31:31  iter: 3119  total_loss: 183  loss_ce: 1.89  loss_mask: 1.523  loss_dice: 2.386  loss_bbox: 2.672  loss_giou: 1.534  loss_ce_dn: 2.426  loss_mask_dn: 1.453  loss_dice_dn: 2.308  loss_bbox_dn: 0.9031  loss_giou_dn: 0.7207  loss_ce_0: 8.792  loss_mask_0: 1.413  loss_dice_0: 2.72  loss_bbox_0: 4.099  loss_giou_0: 1.895  loss_ce_1: 2.045  loss_mask_1: 1.531  loss_dice_1: 2.434  loss_bbox_1: 3.043  loss_giou_1: 1.583  loss_ce_dn_1: 2.994  loss_mask_dn_1: 1.511  loss_dice_dn_1: 2.549  loss_bbox_dn_1: 1.058  loss_giou_dn_1: 0.7771  loss_ce_2: 1.742  loss_mask_2: 1.506  loss_dice_2: 2.37  loss_bbox_2: 2.77  loss_giou_2: 1.568  loss_ce_dn_2: 2.515  loss_mask_dn_2: 1.493  loss_dice_dn_2: 2.387  loss_bbox_dn_2: 0.9766  loss_giou_dn_2: 0.7588  loss_ce_3: 1.703  loss_mask_3: 1.564  loss_dice_3: 2.37  loss_bbox_3: 2.684  loss_giou_3: 1.609  loss_ce_dn_3: 2.315  loss_mask_dn_3: 1.501  loss_dice_dn_3: 2.34  loss_bbox_dn_3: 0.941  loss_giou_dn_3: 0.743  loss_ce_4: 1.641  loss_mask_4: 1.539  loss_dice_4: 2.44  loss_bbox_4: 2.684  loss_giou_4: 1.592  loss_ce_dn_4: 2.213  loss_mask_dn_4: 1.456  loss_dice_dn_4: 2.314  loss_bbox_dn_4: 0.9303  loss_giou_dn_4: 0.7326  loss_ce_5: 1.728  loss_mask_5: 1.543  loss_dice_5: 2.438  loss_bbox_5: 2.651  loss_giou_5: 1.574  loss_ce_dn_5: 2.187  loss_mask_dn_5: 1.423  loss_dice_dn_5: 2.322  loss_bbox_dn_5: 0.9297  loss_giou_dn_5: 0.7257  loss_ce_6: 1.743  loss_mask_6: 1.529  loss_dice_6: 2.463  loss_bbox_6: 2.671  loss_giou_6: 1.567  loss_ce_dn_6: 2.152  loss_mask_dn_6: 1.447  loss_dice_dn_6: 2.323  loss_bbox_dn_6: 0.9271  loss_giou_dn_6: 0.7307  loss_ce_7: 1.784  loss_mask_7: 1.502  loss_dice_7: 2.441  loss_bbox_7: 2.734  loss_giou_7: 1.536  loss_ce_dn_7: 2.242  loss_mask_dn_7: 1.448  loss_dice_dn_7: 2.324  loss_bbox_dn_7: 0.9127  loss_giou_dn_7: 0.7157  loss_ce_8: 1.884  loss_mask_8: 1.532  loss_dice_8: 2.433  loss_bbox_8: 2.677  loss_giou_8: 1.53  loss_ce_dn_8: 2.309  loss_mask_dn_8: 1.476  loss_dice_dn_8: 2.305  loss_bbox_dn_8: 0.9103  loss_giou_dn_8: 0.7188    time: 2.2746  last_time: 2.2771  data_time: 0.0119  last_data_time: 0.0132   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:54:55 d2.utils.events]: \u001b[0m eta: 8 days, 7:30:13  iter: 3139  total_loss: 188.3  loss_ce: 1.847  loss_mask: 1.513  loss_dice: 2.525  loss_bbox: 2.622  loss_giou: 1.632  loss_ce_dn: 2.259  loss_mask_dn: 1.357  loss_dice_dn: 2.443  loss_bbox_dn: 0.9613  loss_giou_dn: 0.7721  loss_ce_0: 9.076  loss_mask_0: 1.363  loss_dice_0: 2.819  loss_bbox_0: 4.068  loss_giou_0: 1.963  loss_ce_1: 2.523  loss_mask_1: 1.512  loss_dice_1: 2.684  loss_bbox_1: 3.125  loss_giou_1: 1.758  loss_ce_dn_1: 3.097  loss_mask_dn_1: 1.388  loss_dice_dn_1: 2.552  loss_bbox_dn_1: 1.028  loss_giou_dn_1: 0.8053  loss_ce_2: 2.09  loss_mask_2: 1.534  loss_dice_2: 2.603  loss_bbox_2: 2.898  loss_giou_2: 1.722  loss_ce_dn_2: 2.403  loss_mask_dn_2: 1.374  loss_dice_dn_2: 2.407  loss_bbox_dn_2: 1.017  loss_giou_dn_2: 0.7822  loss_ce_3: 1.986  loss_mask_3: 1.509  loss_dice_3: 2.6  loss_bbox_3: 2.881  loss_giou_3: 1.714  loss_ce_dn_3: 2.216  loss_mask_dn_3: 1.344  loss_dice_dn_3: 2.382  loss_bbox_dn_3: 0.9672  loss_giou_dn_3: 0.7715  loss_ce_4: 1.903  loss_mask_4: 1.491  loss_dice_4: 2.579  loss_bbox_4: 2.67  loss_giou_4: 1.708  loss_ce_dn_4: 2.138  loss_mask_dn_4: 1.373  loss_dice_dn_4: 2.377  loss_bbox_dn_4: 0.9509  loss_giou_dn_4: 0.7765  loss_ce_5: 1.875  loss_mask_5: 1.527  loss_dice_5: 2.55  loss_bbox_5: 2.694  loss_giou_5: 1.693  loss_ce_dn_5: 2.087  loss_mask_dn_5: 1.351  loss_dice_dn_5: 2.364  loss_bbox_dn_5: 0.9569  loss_giou_dn_5: 0.7775  loss_ce_6: 1.817  loss_mask_6: 1.494  loss_dice_6: 2.529  loss_bbox_6: 2.625  loss_giou_6: 1.642  loss_ce_dn_6: 2.079  loss_mask_dn_6: 1.366  loss_dice_dn_6: 2.362  loss_bbox_dn_6: 0.962  loss_giou_dn_6: 0.7787  loss_ce_7: 1.821  loss_mask_7: 1.502  loss_dice_7: 2.494  loss_bbox_7: 2.621  loss_giou_7: 1.633  loss_ce_dn_7: 2.167  loss_mask_dn_7: 1.319  loss_dice_dn_7: 2.357  loss_bbox_dn_7: 0.9593  loss_giou_dn_7: 0.7665  loss_ce_8: 1.746  loss_mask_8: 1.489  loss_dice_8: 2.512  loss_bbox_8: 2.619  loss_giou_8: 1.625  loss_ce_dn_8: 2.174  loss_mask_dn_8: 1.332  loss_dice_dn_8: 2.374  loss_bbox_dn_8: 0.9586  loss_giou_dn_8: 0.7709    time: 2.2746  last_time: 2.2887  data_time: 0.0116  last_data_time: 0.0058   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:55:40 d2.utils.events]: \u001b[0m eta: 8 days, 7:29:28  iter: 3159  total_loss: 190.3  loss_ce: 1.853  loss_mask: 1.769  loss_dice: 2.45  loss_bbox: 2.662  loss_giou: 1.488  loss_ce_dn: 2.199  loss_mask_dn: 1.428  loss_dice_dn: 2.312  loss_bbox_dn: 0.9153  loss_giou_dn: 0.7004  loss_ce_0: 8.978  loss_mask_0: 1.572  loss_dice_0: 2.708  loss_bbox_0: 3.906  loss_giou_0: 1.839  loss_ce_1: 2.015  loss_mask_1: 1.621  loss_dice_1: 2.559  loss_bbox_1: 3.226  loss_giou_1: 1.636  loss_ce_dn_1: 2.561  loss_mask_dn_1: 1.494  loss_dice_dn_1: 2.45  loss_bbox_dn_1: 1.048  loss_giou_dn_1: 0.7719  loss_ce_2: 2.001  loss_mask_2: 1.603  loss_dice_2: 2.52  loss_bbox_2: 2.99  loss_giou_2: 1.659  loss_ce_dn_2: 2.07  loss_mask_dn_2: 1.369  loss_dice_dn_2: 2.393  loss_bbox_dn_2: 0.9874  loss_giou_dn_2: 0.7483  loss_ce_3: 2.053  loss_mask_3: 1.642  loss_dice_3: 2.476  loss_bbox_3: 2.813  loss_giou_3: 1.594  loss_ce_dn_3: 2.041  loss_mask_dn_3: 1.319  loss_dice_dn_3: 2.388  loss_bbox_dn_3: 0.9262  loss_giou_dn_3: 0.7283  loss_ce_4: 2.066  loss_mask_4: 1.757  loss_dice_4: 2.463  loss_bbox_4: 2.796  loss_giou_4: 1.598  loss_ce_dn_4: 2.03  loss_mask_dn_4: 1.428  loss_dice_dn_4: 2.353  loss_bbox_dn_4: 0.9084  loss_giou_dn_4: 0.7129  loss_ce_5: 1.791  loss_mask_5: 1.722  loss_dice_5: 2.479  loss_bbox_5: 2.9  loss_giou_5: 1.573  loss_ce_dn_5: 2.097  loss_mask_dn_5: 1.369  loss_dice_dn_5: 2.36  loss_bbox_dn_5: 0.9165  loss_giou_dn_5: 0.7017  loss_ce_6: 1.805  loss_mask_6: 1.704  loss_dice_6: 2.457  loss_bbox_6: 2.872  loss_giou_6: 1.554  loss_ce_dn_6: 2.055  loss_mask_dn_6: 1.378  loss_dice_dn_6: 2.341  loss_bbox_dn_6: 0.9247  loss_giou_dn_6: 0.7082  loss_ce_7: 1.889  loss_mask_7: 1.693  loss_dice_7: 2.377  loss_bbox_7: 2.794  loss_giou_7: 1.513  loss_ce_dn_7: 2.188  loss_mask_dn_7: 1.384  loss_dice_dn_7: 2.324  loss_bbox_dn_7: 0.9192  loss_giou_dn_7: 0.6978  loss_ce_8: 1.822  loss_mask_8: 1.707  loss_dice_8: 2.502  loss_bbox_8: 2.789  loss_giou_8: 1.512  loss_ce_dn_8: 2.157  loss_mask_dn_8: 1.464  loss_dice_dn_8: 2.303  loss_bbox_dn_8: 0.9188  loss_giou_dn_8: 0.697    time: 2.2746  last_time: 2.2525  data_time: 0.0134  last_data_time: 0.0066   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:56:26 d2.utils.events]: \u001b[0m eta: 8 days, 7:29:01  iter: 3179  total_loss: 180.3  loss_ce: 1.489  loss_mask: 1.694  loss_dice: 2.534  loss_bbox: 2.608  loss_giou: 1.589  loss_ce_dn: 2.091  loss_mask_dn: 1.395  loss_dice_dn: 2.376  loss_bbox_dn: 0.9106  loss_giou_dn: 0.7016  loss_ce_0: 8.688  loss_mask_0: 1.431  loss_dice_0: 2.691  loss_bbox_0: 3.88  loss_giou_0: 1.953  loss_ce_1: 1.885  loss_mask_1: 1.477  loss_dice_1: 2.588  loss_bbox_1: 3.038  loss_giou_1: 1.651  loss_ce_dn_1: 2.833  loss_mask_dn_1: 1.361  loss_dice_dn_1: 2.468  loss_bbox_dn_1: 1.065  loss_giou_dn_1: 0.7639  loss_ce_2: 1.556  loss_mask_2: 1.659  loss_dice_2: 2.495  loss_bbox_2: 2.828  loss_giou_2: 1.636  loss_ce_dn_2: 2.373  loss_mask_dn_2: 1.328  loss_dice_dn_2: 2.346  loss_bbox_dn_2: 1.001  loss_giou_dn_2: 0.738  loss_ce_3: 1.434  loss_mask_3: 1.498  loss_dice_3: 2.488  loss_bbox_3: 2.822  loss_giou_3: 1.658  loss_ce_dn_3: 2.173  loss_mask_dn_3: 1.337  loss_dice_dn_3: 2.373  loss_bbox_dn_3: 0.9644  loss_giou_dn_3: 0.7298  loss_ce_4: 1.389  loss_mask_4: 1.582  loss_dice_4: 2.496  loss_bbox_4: 2.687  loss_giou_4: 1.633  loss_ce_dn_4: 2.031  loss_mask_dn_4: 1.344  loss_dice_dn_4: 2.366  loss_bbox_dn_4: 0.9424  loss_giou_dn_4: 0.721  loss_ce_5: 1.418  loss_mask_5: 1.633  loss_dice_5: 2.489  loss_bbox_5: 2.639  loss_giou_5: 1.625  loss_ce_dn_5: 1.998  loss_mask_dn_5: 1.371  loss_dice_dn_5: 2.353  loss_bbox_dn_5: 0.9271  loss_giou_dn_5: 0.7177  loss_ce_6: 1.372  loss_mask_6: 1.591  loss_dice_6: 2.492  loss_bbox_6: 2.64  loss_giou_6: 1.573  loss_ce_dn_6: 1.96  loss_mask_dn_6: 1.37  loss_dice_dn_6: 2.36  loss_bbox_dn_6: 0.9167  loss_giou_dn_6: 0.7151  loss_ce_7: 1.358  loss_mask_7: 1.597  loss_dice_7: 2.53  loss_bbox_7: 2.599  loss_giou_7: 1.591  loss_ce_dn_7: 1.991  loss_mask_dn_7: 1.377  loss_dice_dn_7: 2.361  loss_bbox_dn_7: 0.8972  loss_giou_dn_7: 0.7045  loss_ce_8: 1.417  loss_mask_8: 1.699  loss_dice_8: 2.534  loss_bbox_8: 2.601  loss_giou_8: 1.585  loss_ce_dn_8: 2.009  loss_mask_dn_8: 1.385  loss_dice_dn_8: 2.362  loss_bbox_dn_8: 0.8981  loss_giou_dn_8: 0.7014    time: 2.2746  last_time: 2.2634  data_time: 0.0135  last_data_time: 0.0207   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:57:11 d2.utils.events]: \u001b[0m eta: 8 days, 7:27:38  iter: 3199  total_loss: 180.9  loss_ce: 1.845  loss_mask: 1.532  loss_dice: 2.418  loss_bbox: 2.647  loss_giou: 1.606  loss_ce_dn: 2.287  loss_mask_dn: 1.35  loss_dice_dn: 2.316  loss_bbox_dn: 0.8428  loss_giou_dn: 0.7148  loss_ce_0: 8.713  loss_mask_0: 1.486  loss_dice_0: 2.848  loss_bbox_0: 4.093  loss_giou_0: 2.056  loss_ce_1: 2.134  loss_mask_1: 1.534  loss_dice_1: 2.62  loss_bbox_1: 3.026  loss_giou_1: 1.667  loss_ce_dn_1: 2.716  loss_mask_dn_1: 1.505  loss_dice_dn_1: 2.445  loss_bbox_dn_1: 1.02  loss_giou_dn_1: 0.7848  loss_ce_2: 1.85  loss_mask_2: 1.505  loss_dice_2: 2.519  loss_bbox_2: 2.84  loss_giou_2: 1.733  loss_ce_dn_2: 2.324  loss_mask_dn_2: 1.37  loss_dice_dn_2: 2.336  loss_bbox_dn_2: 0.9666  loss_giou_dn_2: 0.7603  loss_ce_3: 1.729  loss_mask_3: 1.533  loss_dice_3: 2.439  loss_bbox_3: 2.746  loss_giou_3: 1.659  loss_ce_dn_3: 2.247  loss_mask_dn_3: 1.336  loss_dice_dn_3: 2.282  loss_bbox_dn_3: 0.9313  loss_giou_dn_3: 0.744  loss_ce_4: 1.785  loss_mask_4: 1.525  loss_dice_4: 2.396  loss_bbox_4: 2.66  loss_giou_4: 1.635  loss_ce_dn_4: 2.177  loss_mask_dn_4: 1.348  loss_dice_dn_4: 2.27  loss_bbox_dn_4: 0.9104  loss_giou_dn_4: 0.7409  loss_ce_5: 1.716  loss_mask_5: 1.522  loss_dice_5: 2.384  loss_bbox_5: 2.641  loss_giou_5: 1.629  loss_ce_dn_5: 2.237  loss_mask_dn_5: 1.343  loss_dice_dn_5: 2.266  loss_bbox_dn_5: 0.9062  loss_giou_dn_5: 0.7319  loss_ce_6: 1.703  loss_mask_6: 1.517  loss_dice_6: 2.399  loss_bbox_6: 2.672  loss_giou_6: 1.623  loss_ce_dn_6: 2.172  loss_mask_dn_6: 1.338  loss_dice_dn_6: 2.287  loss_bbox_dn_6: 0.8928  loss_giou_dn_6: 0.729  loss_ce_7: 1.798  loss_mask_7: 1.542  loss_dice_7: 2.391  loss_bbox_7: 2.643  loss_giou_7: 1.583  loss_ce_dn_7: 2.264  loss_mask_dn_7: 1.372  loss_dice_dn_7: 2.281  loss_bbox_dn_7: 0.8446  loss_giou_dn_7: 0.7172  loss_ce_8: 1.826  loss_mask_8: 1.523  loss_dice_8: 2.375  loss_bbox_8: 2.634  loss_giou_8: 1.6  loss_ce_dn_8: 2.222  loss_mask_dn_8: 1.366  loss_dice_dn_8: 2.257  loss_bbox_dn_8: 0.846  loss_giou_dn_8: 0.7167    time: 2.2746  last_time: 2.3089  data_time: 0.0135  last_data_time: 0.0134   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:57:56 d2.utils.events]: \u001b[0m eta: 8 days, 7:24:27  iter: 3219  total_loss: 182.1  loss_ce: 2.105  loss_mask: 1.467  loss_dice: 2.419  loss_bbox: 2.673  loss_giou: 1.536  loss_ce_dn: 2.272  loss_mask_dn: 1.353  loss_dice_dn: 2.024  loss_bbox_dn: 0.9077  loss_giou_dn: 0.743  loss_ce_0: 9.11  loss_mask_0: 1.324  loss_dice_0: 2.664  loss_bbox_0: 3.821  loss_giou_0: 1.941  loss_ce_1: 2.236  loss_mask_1: 1.501  loss_dice_1: 2.419  loss_bbox_1: 2.931  loss_giou_1: 1.737  loss_ce_dn_1: 3.067  loss_mask_dn_1: 1.287  loss_dice_dn_1: 2.267  loss_bbox_dn_1: 1.063  loss_giou_dn_1: 0.7905  loss_ce_2: 2.089  loss_mask_2: 1.439  loss_dice_2: 2.353  loss_bbox_2: 2.797  loss_giou_2: 1.665  loss_ce_dn_2: 2.479  loss_mask_dn_2: 1.285  loss_dice_dn_2: 2.15  loss_bbox_dn_2: 1.005  loss_giou_dn_2: 0.7639  loss_ce_3: 1.884  loss_mask_3: 1.393  loss_dice_3: 2.369  loss_bbox_3: 2.731  loss_giou_3: 1.591  loss_ce_dn_3: 2.246  loss_mask_dn_3: 1.236  loss_dice_dn_3: 2.069  loss_bbox_dn_3: 0.9576  loss_giou_dn_3: 0.7464  loss_ce_4: 1.899  loss_mask_4: 1.447  loss_dice_4: 2.427  loss_bbox_4: 2.671  loss_giou_4: 1.566  loss_ce_dn_4: 2.217  loss_mask_dn_4: 1.24  loss_dice_dn_4: 2.064  loss_bbox_dn_4: 0.9342  loss_giou_dn_4: 0.7364  loss_ce_5: 1.895  loss_mask_5: 1.461  loss_dice_5: 2.38  loss_bbox_5: 2.621  loss_giou_5: 1.581  loss_ce_dn_5: 2.237  loss_mask_dn_5: 1.245  loss_dice_dn_5: 2.073  loss_bbox_dn_5: 0.9241  loss_giou_dn_5: 0.736  loss_ce_6: 1.987  loss_mask_6: 1.485  loss_dice_6: 2.392  loss_bbox_6: 2.608  loss_giou_6: 1.582  loss_ce_dn_6: 2.241  loss_mask_dn_6: 1.244  loss_dice_dn_6: 2.045  loss_bbox_dn_6: 0.9213  loss_giou_dn_6: 0.7414  loss_ce_7: 2.017  loss_mask_7: 1.463  loss_dice_7: 2.409  loss_bbox_7: 2.635  loss_giou_7: 1.564  loss_ce_dn_7: 2.281  loss_mask_dn_7: 1.287  loss_dice_dn_7: 2.098  loss_bbox_dn_7: 0.9037  loss_giou_dn_7: 0.7308  loss_ce_8: 2.054  loss_mask_8: 1.485  loss_dice_8: 2.405  loss_bbox_8: 2.66  loss_giou_8: 1.544  loss_ce_dn_8: 2.28  loss_mask_dn_8: 1.299  loss_dice_dn_8: 2.025  loss_bbox_dn_8: 0.9045  loss_giou_dn_8: 0.7343    time: 2.2746  last_time: 2.2618  data_time: 0.0126  last_data_time: 0.0075   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:58:42 d2.utils.events]: \u001b[0m eta: 8 days, 7:21:38  iter: 3239  total_loss: 186.6  loss_ce: 1.719  loss_mask: 1.552  loss_dice: 2.683  loss_bbox: 2.873  loss_giou: 1.784  loss_ce_dn: 2.498  loss_mask_dn: 1.405  loss_dice_dn: 2.335  loss_bbox_dn: 0.8373  loss_giou_dn: 0.7336  loss_ce_0: 8.927  loss_mask_0: 1.45  loss_dice_0: 2.698  loss_bbox_0: 4.064  loss_giou_0: 2.014  loss_ce_1: 2.128  loss_mask_1: 1.587  loss_dice_1: 2.691  loss_bbox_1: 3.386  loss_giou_1: 1.849  loss_ce_dn_1: 2.945  loss_mask_dn_1: 1.381  loss_dice_dn_1: 2.525  loss_bbox_dn_1: 0.9757  loss_giou_dn_1: 0.7935  loss_ce_2: 1.76  loss_mask_2: 1.491  loss_dice_2: 2.612  loss_bbox_2: 3.094  loss_giou_2: 1.808  loss_ce_dn_2: 2.416  loss_mask_dn_2: 1.402  loss_dice_dn_2: 2.358  loss_bbox_dn_2: 0.9287  loss_giou_dn_2: 0.7767  loss_ce_3: 1.663  loss_mask_3: 1.531  loss_dice_3: 2.616  loss_bbox_3: 2.988  loss_giou_3: 1.825  loss_ce_dn_3: 2.321  loss_mask_dn_3: 1.404  loss_dice_dn_3: 2.319  loss_bbox_dn_3: 0.9114  loss_giou_dn_3: 0.7572  loss_ce_4: 1.62  loss_mask_4: 1.527  loss_dice_4: 2.622  loss_bbox_4: 2.892  loss_giou_4: 1.847  loss_ce_dn_4: 2.281  loss_mask_dn_4: 1.411  loss_dice_dn_4: 2.314  loss_bbox_dn_4: 0.8788  loss_giou_dn_4: 0.7462  loss_ce_5: 1.637  loss_mask_5: 1.542  loss_dice_5: 2.611  loss_bbox_5: 2.908  loss_giou_5: 1.828  loss_ce_dn_5: 2.3  loss_mask_dn_5: 1.366  loss_dice_dn_5: 2.292  loss_bbox_dn_5: 0.8547  loss_giou_dn_5: 0.7426  loss_ce_6: 1.704  loss_mask_6: 1.541  loss_dice_6: 2.673  loss_bbox_6: 2.891  loss_giou_6: 1.81  loss_ce_dn_6: 2.335  loss_mask_dn_6: 1.387  loss_dice_dn_6: 2.305  loss_bbox_dn_6: 0.8494  loss_giou_dn_6: 0.7393  loss_ce_7: 1.685  loss_mask_7: 1.519  loss_dice_7: 2.657  loss_bbox_7: 2.846  loss_giou_7: 1.779  loss_ce_dn_7: 2.354  loss_mask_dn_7: 1.385  loss_dice_dn_7: 2.292  loss_bbox_dn_7: 0.8373  loss_giou_dn_7: 0.7308  loss_ce_8: 1.668  loss_mask_8: 1.563  loss_dice_8: 2.651  loss_bbox_8: 2.876  loss_giou_8: 1.788  loss_ce_dn_8: 2.43  loss_mask_dn_8: 1.397  loss_dice_dn_8: 2.334  loss_bbox_dn_8: 0.8388  loss_giou_dn_8: 0.7339    time: 2.2745  last_time: 2.2688  data_time: 0.0119  last_data_time: 0.0077   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 16:59:28 d2.utils.events]: \u001b[0m eta: 8 days, 7:22:56  iter: 3259  total_loss: 181.1  loss_ce: 1.873  loss_mask: 1.499  loss_dice: 2.633  loss_bbox: 2.581  loss_giou: 1.614  loss_ce_dn: 2.491  loss_mask_dn: 1.447  loss_dice_dn: 2.413  loss_bbox_dn: 0.8952  loss_giou_dn: 0.7073  loss_ce_0: 8.496  loss_mask_0: 1.495  loss_dice_0: 2.733  loss_bbox_0: 4.076  loss_giou_0: 1.89  loss_ce_1: 2.061  loss_mask_1: 1.47  loss_dice_1: 2.582  loss_bbox_1: 2.834  loss_giou_1: 1.629  loss_ce_dn_1: 2.829  loss_mask_dn_1: 1.472  loss_dice_dn_1: 2.51  loss_bbox_dn_1: 1.004  loss_giou_dn_1: 0.7875  loss_ce_2: 1.918  loss_mask_2: 1.483  loss_dice_2: 2.507  loss_bbox_2: 2.695  loss_giou_2: 1.603  loss_ce_dn_2: 2.445  loss_mask_dn_2: 1.425  loss_dice_dn_2: 2.374  loss_bbox_dn_2: 0.9445  loss_giou_dn_2: 0.7678  loss_ce_3: 1.758  loss_mask_3: 1.445  loss_dice_3: 2.524  loss_bbox_3: 2.589  loss_giou_3: 1.649  loss_ce_dn_3: 2.319  loss_mask_dn_3: 1.38  loss_dice_dn_3: 2.346  loss_bbox_dn_3: 0.9067  loss_giou_dn_3: 0.742  loss_ce_4: 1.873  loss_mask_4: 1.443  loss_dice_4: 2.521  loss_bbox_4: 2.574  loss_giou_4: 1.633  loss_ce_dn_4: 2.287  loss_mask_dn_4: 1.404  loss_dice_dn_4: 2.327  loss_bbox_dn_4: 0.891  loss_giou_dn_4: 0.7292  loss_ce_5: 1.929  loss_mask_5: 1.425  loss_dice_5: 2.508  loss_bbox_5: 2.556  loss_giou_5: 1.622  loss_ce_dn_5: 2.256  loss_mask_dn_5: 1.41  loss_dice_dn_5: 2.377  loss_bbox_dn_5: 0.8826  loss_giou_dn_5: 0.7194  loss_ce_6: 1.89  loss_mask_6: 1.452  loss_dice_6: 2.507  loss_bbox_6: 2.582  loss_giou_6: 1.596  loss_ce_dn_6: 2.316  loss_mask_dn_6: 1.42  loss_dice_dn_6: 2.36  loss_bbox_dn_6: 0.8825  loss_giou_dn_6: 0.7162  loss_ce_7: 1.857  loss_mask_7: 1.456  loss_dice_7: 2.567  loss_bbox_7: 2.567  loss_giou_7: 1.616  loss_ce_dn_7: 2.38  loss_mask_dn_7: 1.423  loss_dice_dn_7: 2.376  loss_bbox_dn_7: 0.8824  loss_giou_dn_7: 0.7071  loss_ce_8: 1.842  loss_mask_8: 1.471  loss_dice_8: 2.57  loss_bbox_8: 2.572  loss_giou_8: 1.615  loss_ce_dn_8: 2.42  loss_mask_dn_8: 1.457  loss_dice_dn_8: 2.368  loss_bbox_dn_8: 0.8881  loss_giou_dn_8: 0.7039    time: 2.2746  last_time: 2.2554  data_time: 0.0139  last_data_time: 0.0076   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:00:13 d2.utils.events]: \u001b[0m eta: 8 days, 7:22:41  iter: 3279  total_loss: 188.7  loss_ce: 1.835  loss_mask: 1.713  loss_dice: 2.446  loss_bbox: 2.981  loss_giou: 1.571  loss_ce_dn: 2.578  loss_mask_dn: 1.414  loss_dice_dn: 2.223  loss_bbox_dn: 0.9065  loss_giou_dn: 0.7069  loss_ce_0: 9.078  loss_mask_0: 1.563  loss_dice_0: 2.664  loss_bbox_0: 4.122  loss_giou_0: 1.953  loss_ce_1: 2.211  loss_mask_1: 1.796  loss_dice_1: 2.541  loss_bbox_1: 3.275  loss_giou_1: 1.713  loss_ce_dn_1: 3.106  loss_mask_dn_1: 1.502  loss_dice_dn_1: 2.443  loss_bbox_dn_1: 1.028  loss_giou_dn_1: 0.7676  loss_ce_2: 2.045  loss_mask_2: 1.703  loss_dice_2: 2.415  loss_bbox_2: 3.076  loss_giou_2: 1.674  loss_ce_dn_2: 2.624  loss_mask_dn_2: 1.38  loss_dice_dn_2: 2.259  loss_bbox_dn_2: 0.9825  loss_giou_dn_2: 0.7419  loss_ce_3: 1.927  loss_mask_3: 1.653  loss_dice_3: 2.418  loss_bbox_3: 2.955  loss_giou_3: 1.643  loss_ce_dn_3: 2.457  loss_mask_dn_3: 1.359  loss_dice_dn_3: 2.138  loss_bbox_dn_3: 0.9294  loss_giou_dn_3: 0.7209  loss_ce_4: 1.922  loss_mask_4: 1.757  loss_dice_4: 2.406  loss_bbox_4: 2.955  loss_giou_4: 1.628  loss_ce_dn_4: 2.404  loss_mask_dn_4: 1.389  loss_dice_dn_4: 2.159  loss_bbox_dn_4: 0.9168  loss_giou_dn_4: 0.7093  loss_ce_5: 1.903  loss_mask_5: 1.729  loss_dice_5: 2.406  loss_bbox_5: 2.953  loss_giou_5: 1.603  loss_ce_dn_5: 2.396  loss_mask_dn_5: 1.392  loss_dice_dn_5: 2.18  loss_bbox_dn_5: 0.907  loss_giou_dn_5: 0.6993  loss_ce_6: 1.844  loss_mask_6: 1.703  loss_dice_6: 2.426  loss_bbox_6: 2.965  loss_giou_6: 1.594  loss_ce_dn_6: 2.398  loss_mask_dn_6: 1.381  loss_dice_dn_6: 2.206  loss_bbox_dn_6: 0.9036  loss_giou_dn_6: 0.6981  loss_ce_7: 1.827  loss_mask_7: 1.679  loss_dice_7: 2.433  loss_bbox_7: 2.96  loss_giou_7: 1.57  loss_ce_dn_7: 2.456  loss_mask_dn_7: 1.419  loss_dice_dn_7: 2.219  loss_bbox_dn_7: 0.9021  loss_giou_dn_7: 0.6988  loss_ce_8: 1.858  loss_mask_8: 1.698  loss_dice_8: 2.437  loss_bbox_8: 2.958  loss_giou_8: 1.571  loss_ce_dn_8: 2.556  loss_mask_dn_8: 1.404  loss_dice_dn_8: 2.218  loss_bbox_dn_8: 0.9056  loss_giou_dn_8: 0.6968    time: 2.2746  last_time: 2.3605  data_time: 0.0135  last_data_time: 0.0053   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:00:58 d2.utils.events]: \u001b[0m eta: 8 days, 7:19:22  iter: 3299  total_loss: 183.6  loss_ce: 2.046  loss_mask: 1.62  loss_dice: 2.46  loss_bbox: 2.583  loss_giou: 1.563  loss_ce_dn: 2.677  loss_mask_dn: 1.49  loss_dice_dn: 2.181  loss_bbox_dn: 0.8472  loss_giou_dn: 0.7071  loss_ce_0: 8.976  loss_mask_0: 1.517  loss_dice_0: 2.817  loss_bbox_0: 3.668  loss_giou_0: 1.908  loss_ce_1: 2.096  loss_mask_1: 1.624  loss_dice_1: 2.487  loss_bbox_1: 2.969  loss_giou_1: 1.775  loss_ce_dn_1: 2.835  loss_mask_dn_1: 1.487  loss_dice_dn_1: 2.27  loss_bbox_dn_1: 1.037  loss_giou_dn_1: 0.7814  loss_ce_2: 1.982  loss_mask_2: 1.659  loss_dice_2: 2.454  loss_bbox_2: 2.739  loss_giou_2: 1.7  loss_ce_dn_2: 2.452  loss_mask_dn_2: 1.53  loss_dice_dn_2: 2.152  loss_bbox_dn_2: 0.9962  loss_giou_dn_2: 0.7643  loss_ce_3: 1.828  loss_mask_3: 1.646  loss_dice_3: 2.484  loss_bbox_3: 2.737  loss_giou_3: 1.653  loss_ce_dn_3: 2.363  loss_mask_dn_3: 1.553  loss_dice_dn_3: 2.137  loss_bbox_dn_3: 0.9631  loss_giou_dn_3: 0.7427  loss_ce_4: 1.878  loss_mask_4: 1.707  loss_dice_4: 2.45  loss_bbox_4: 2.728  loss_giou_4: 1.642  loss_ce_dn_4: 2.312  loss_mask_dn_4: 1.488  loss_dice_dn_4: 2.155  loss_bbox_dn_4: 0.9227  loss_giou_dn_4: 0.7353  loss_ce_5: 1.83  loss_mask_5: 1.705  loss_dice_5: 2.41  loss_bbox_5: 2.723  loss_giou_5: 1.634  loss_ce_dn_5: 2.331  loss_mask_dn_5: 1.462  loss_dice_dn_5: 2.145  loss_bbox_dn_5: 0.9033  loss_giou_dn_5: 0.7259  loss_ce_6: 1.841  loss_mask_6: 1.731  loss_dice_6: 2.453  loss_bbox_6: 2.678  loss_giou_6: 1.589  loss_ce_dn_6: 2.294  loss_mask_dn_6: 1.452  loss_dice_dn_6: 2.193  loss_bbox_dn_6: 0.8981  loss_giou_dn_6: 0.7219  loss_ce_7: 1.949  loss_mask_7: 1.693  loss_dice_7: 2.497  loss_bbox_7: 2.643  loss_giou_7: 1.569  loss_ce_dn_7: 2.462  loss_mask_dn_7: 1.481  loss_dice_dn_7: 2.207  loss_bbox_dn_7: 0.8501  loss_giou_dn_7: 0.7106  loss_ce_8: 1.915  loss_mask_8: 1.602  loss_dice_8: 2.431  loss_bbox_8: 2.559  loss_giou_8: 1.57  loss_ce_dn_8: 2.562  loss_mask_dn_8: 1.533  loss_dice_dn_8: 2.193  loss_bbox_dn_8: 0.8513  loss_giou_dn_8: 0.7081    time: 2.2745  last_time: 2.2473  data_time: 0.0107  last_data_time: 0.0107   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:01:44 d2.utils.events]: \u001b[0m eta: 8 days, 7:20:13  iter: 3319  total_loss: 171  loss_ce: 1.581  loss_mask: 1.379  loss_dice: 2.322  loss_bbox: 2.642  loss_giou: 1.53  loss_ce_dn: 2.399  loss_mask_dn: 1.348  loss_dice_dn: 2.141  loss_bbox_dn: 0.8168  loss_giou_dn: 0.7114  loss_ce_0: 8.388  loss_mask_0: 1.341  loss_dice_0: 2.592  loss_bbox_0: 3.694  loss_giou_0: 1.962  loss_ce_1: 2.017  loss_mask_1: 1.395  loss_dice_1: 2.321  loss_bbox_1: 2.82  loss_giou_1: 1.66  loss_ce_dn_1: 2.905  loss_mask_dn_1: 1.353  loss_dice_dn_1: 2.375  loss_bbox_dn_1: 0.9908  loss_giou_dn_1: 0.7669  loss_ce_2: 1.658  loss_mask_2: 1.426  loss_dice_2: 2.325  loss_bbox_2: 2.664  loss_giou_2: 1.561  loss_ce_dn_2: 2.374  loss_mask_dn_2: 1.326  loss_dice_dn_2: 2.181  loss_bbox_dn_2: 0.9328  loss_giou_dn_2: 0.7367  loss_ce_3: 1.713  loss_mask_3: 1.381  loss_dice_3: 2.284  loss_bbox_3: 2.676  loss_giou_3: 1.53  loss_ce_dn_3: 2.262  loss_mask_dn_3: 1.291  loss_dice_dn_3: 2.118  loss_bbox_dn_3: 0.9268  loss_giou_dn_3: 0.7205  loss_ce_4: 1.702  loss_mask_4: 1.371  loss_dice_4: 2.32  loss_bbox_4: 2.677  loss_giou_4: 1.568  loss_ce_dn_4: 2.279  loss_mask_dn_4: 1.308  loss_dice_dn_4: 2.154  loss_bbox_dn_4: 0.8907  loss_giou_dn_4: 0.7142  loss_ce_5: 1.62  loss_mask_5: 1.387  loss_dice_5: 2.321  loss_bbox_5: 2.661  loss_giou_5: 1.553  loss_ce_dn_5: 2.321  loss_mask_dn_5: 1.32  loss_dice_dn_5: 2.098  loss_bbox_dn_5: 0.8577  loss_giou_dn_5: 0.7137  loss_ce_6: 1.588  loss_mask_6: 1.338  loss_dice_6: 2.285  loss_bbox_6: 2.655  loss_giou_6: 1.54  loss_ce_dn_6: 2.299  loss_mask_dn_6: 1.31  loss_dice_dn_6: 2.077  loss_bbox_dn_6: 0.851  loss_giou_dn_6: 0.7124  loss_ce_7: 1.55  loss_mask_7: 1.338  loss_dice_7: 2.299  loss_bbox_7: 2.616  loss_giou_7: 1.537  loss_ce_dn_7: 2.38  loss_mask_dn_7: 1.323  loss_dice_dn_7: 2.092  loss_bbox_dn_7: 0.8233  loss_giou_dn_7: 0.7034  loss_ce_8: 1.616  loss_mask_8: 1.383  loss_dice_8: 2.302  loss_bbox_8: 2.622  loss_giou_8: 1.537  loss_ce_dn_8: 2.374  loss_mask_dn_8: 1.363  loss_dice_dn_8: 2.084  loss_bbox_dn_8: 0.821  loss_giou_dn_8: 0.7068    time: 2.2745  last_time: 2.3041  data_time: 0.0134  last_data_time: 0.0269   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:02:29 d2.utils.events]: \u001b[0m eta: 8 days, 7:19:28  iter: 3339  total_loss: 176.2  loss_ce: 1.464  loss_mask: 1.536  loss_dice: 2.252  loss_bbox: 2.717  loss_giou: 1.588  loss_ce_dn: 1.922  loss_mask_dn: 1.371  loss_dice_dn: 1.969  loss_bbox_dn: 0.8593  loss_giou_dn: 0.7217  loss_ce_0: 8.751  loss_mask_0: 1.313  loss_dice_0: 2.52  loss_bbox_0: 3.932  loss_giou_0: 1.908  loss_ce_1: 1.845  loss_mask_1: 1.485  loss_dice_1: 2.305  loss_bbox_1: 3.148  loss_giou_1: 1.648  loss_ce_dn_1: 2.369  loss_mask_dn_1: 1.388  loss_dice_dn_1: 2.213  loss_bbox_dn_1: 1.019  loss_giou_dn_1: 0.7674  loss_ce_2: 1.593  loss_mask_2: 1.472  loss_dice_2: 2.22  loss_bbox_2: 2.982  loss_giou_2: 1.636  loss_ce_dn_2: 1.977  loss_mask_dn_2: 1.346  loss_dice_dn_2: 2.007  loss_bbox_dn_2: 0.963  loss_giou_dn_2: 0.7467  loss_ce_3: 1.469  loss_mask_3: 1.519  loss_dice_3: 2.27  loss_bbox_3: 2.855  loss_giou_3: 1.613  loss_ce_dn_3: 1.834  loss_mask_dn_3: 1.301  loss_dice_dn_3: 2.006  loss_bbox_dn_3: 0.9114  loss_giou_dn_3: 0.7317  loss_ce_4: 1.449  loss_mask_4: 1.49  loss_dice_4: 2.275  loss_bbox_4: 2.821  loss_giou_4: 1.599  loss_ce_dn_4: 1.767  loss_mask_dn_4: 1.245  loss_dice_dn_4: 1.967  loss_bbox_dn_4: 0.8731  loss_giou_dn_4: 0.7344  loss_ce_5: 1.465  loss_mask_5: 1.538  loss_dice_5: 2.224  loss_bbox_5: 2.804  loss_giou_5: 1.604  loss_ce_dn_5: 1.782  loss_mask_dn_5: 1.277  loss_dice_dn_5: 1.939  loss_bbox_dn_5: 0.8662  loss_giou_dn_5: 0.7267  loss_ce_6: 1.452  loss_mask_6: 1.49  loss_dice_6: 2.278  loss_bbox_6: 2.774  loss_giou_6: 1.609  loss_ce_dn_6: 1.796  loss_mask_dn_6: 1.283  loss_dice_dn_6: 1.949  loss_bbox_dn_6: 0.8745  loss_giou_dn_6: 0.7261  loss_ce_7: 1.416  loss_mask_7: 1.503  loss_dice_7: 2.269  loss_bbox_7: 2.747  loss_giou_7: 1.584  loss_ce_dn_7: 1.81  loss_mask_dn_7: 1.312  loss_dice_dn_7: 1.91  loss_bbox_dn_7: 0.8558  loss_giou_dn_7: 0.7173  loss_ce_8: 1.457  loss_mask_8: 1.548  loss_dice_8: 2.29  loss_bbox_8: 2.738  loss_giou_8: 1.587  loss_ce_dn_8: 1.742  loss_mask_dn_8: 1.352  loss_dice_dn_8: 1.937  loss_bbox_dn_8: 0.8618  loss_giou_dn_8: 0.7186    time: 2.2744  last_time: 2.2405  data_time: 0.0115  last_data_time: 0.0163   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:03:15 d2.utils.events]: \u001b[0m eta: 8 days, 7:15:08  iter: 3359  total_loss: 191.9  loss_ce: 2.173  loss_mask: 1.619  loss_dice: 2.668  loss_bbox: 2.902  loss_giou: 1.711  loss_ce_dn: 2.59  loss_mask_dn: 1.44  loss_dice_dn: 2.487  loss_bbox_dn: 0.8942  loss_giou_dn: 0.7306  loss_ce_0: 8.599  loss_mask_0: 1.597  loss_dice_0: 2.869  loss_bbox_0: 4.156  loss_giou_0: 2.01  loss_ce_1: 2.147  loss_mask_1: 1.619  loss_dice_1: 2.573  loss_bbox_1: 3.274  loss_giou_1: 1.757  loss_ce_dn_1: 2.812  loss_mask_dn_1: 1.49  loss_dice_dn_1: 2.521  loss_bbox_dn_1: 1.036  loss_giou_dn_1: 0.7845  loss_ce_2: 1.879  loss_mask_2: 1.667  loss_dice_2: 2.518  loss_bbox_2: 3.1  loss_giou_2: 1.749  loss_ce_dn_2: 2.344  loss_mask_dn_2: 1.419  loss_dice_dn_2: 2.338  loss_bbox_dn_2: 0.9851  loss_giou_dn_2: 0.7691  loss_ce_3: 1.931  loss_mask_3: 1.686  loss_dice_3: 2.607  loss_bbox_3: 3.09  loss_giou_3: 1.742  loss_ce_dn_3: 2.186  loss_mask_dn_3: 1.408  loss_dice_dn_3: 2.283  loss_bbox_dn_3: 0.9505  loss_giou_dn_3: 0.7565  loss_ce_4: 1.913  loss_mask_4: 1.733  loss_dice_4: 2.595  loss_bbox_4: 3.025  loss_giou_4: 1.744  loss_ce_dn_4: 2.124  loss_mask_dn_4: 1.39  loss_dice_dn_4: 2.32  loss_bbox_dn_4: 0.9305  loss_giou_dn_4: 0.743  loss_ce_5: 1.945  loss_mask_5: 1.657  loss_dice_5: 2.563  loss_bbox_5: 3.05  loss_giou_5: 1.734  loss_ce_dn_5: 2.163  loss_mask_dn_5: 1.383  loss_dice_dn_5: 2.313  loss_bbox_dn_5: 0.9221  loss_giou_dn_5: 0.7306  loss_ce_6: 2.089  loss_mask_6: 1.671  loss_dice_6: 2.653  loss_bbox_6: 2.898  loss_giou_6: 1.73  loss_ce_dn_6: 2.202  loss_mask_dn_6: 1.383  loss_dice_dn_6: 2.348  loss_bbox_dn_6: 0.9106  loss_giou_dn_6: 0.7285  loss_ce_7: 2.083  loss_mask_7: 1.654  loss_dice_7: 2.713  loss_bbox_7: 2.885  loss_giou_7: 1.695  loss_ce_dn_7: 2.309  loss_mask_dn_7: 1.449  loss_dice_dn_7: 2.416  loss_bbox_dn_7: 0.8916  loss_giou_dn_7: 0.7226  loss_ce_8: 2.197  loss_mask_8: 1.596  loss_dice_8: 2.685  loss_bbox_8: 2.894  loss_giou_8: 1.706  loss_ce_dn_8: 2.371  loss_mask_dn_8: 1.444  loss_dice_dn_8: 2.408  loss_bbox_dn_8: 0.8915  loss_giou_dn_8: 0.7246    time: 2.2744  last_time: 2.2684  data_time: 0.0137  last_data_time: 0.0244   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:04:00 d2.utils.events]: \u001b[0m eta: 8 days, 7:14:35  iter: 3379  total_loss: 183.2  loss_ce: 1.888  loss_mask: 1.384  loss_dice: 2.633  loss_bbox: 2.802  loss_giou: 1.601  loss_ce_dn: 2.342  loss_mask_dn: 1.29  loss_dice_dn: 2.328  loss_bbox_dn: 0.8178  loss_giou_dn: 0.7266  loss_ce_0: 8.271  loss_mask_0: 1.462  loss_dice_0: 2.741  loss_bbox_0: 4.076  loss_giou_0: 2.016  loss_ce_1: 1.87  loss_mask_1: 1.45  loss_dice_1: 2.59  loss_bbox_1: 3.242  loss_giou_1: 1.694  loss_ce_dn_1: 2.842  loss_mask_dn_1: 1.387  loss_dice_dn_1: 2.49  loss_bbox_dn_1: 0.987  loss_giou_dn_1: 0.784  loss_ce_2: 1.547  loss_mask_2: 1.453  loss_dice_2: 2.591  loss_bbox_2: 2.862  loss_giou_2: 1.691  loss_ce_dn_2: 2.277  loss_mask_dn_2: 1.332  loss_dice_dn_2: 2.287  loss_bbox_dn_2: 0.9245  loss_giou_dn_2: 0.7655  loss_ce_3: 1.706  loss_mask_3: 1.48  loss_dice_3: 2.614  loss_bbox_3: 2.84  loss_giou_3: 1.671  loss_ce_dn_3: 2.112  loss_mask_dn_3: 1.313  loss_dice_dn_3: 2.239  loss_bbox_dn_3: 0.8862  loss_giou_dn_3: 0.7375  loss_ce_4: 1.674  loss_mask_4: 1.484  loss_dice_4: 2.654  loss_bbox_4: 2.847  loss_giou_4: 1.64  loss_ce_dn_4: 2.098  loss_mask_dn_4: 1.274  loss_dice_dn_4: 2.231  loss_bbox_dn_4: 0.8694  loss_giou_dn_4: 0.7431  loss_ce_5: 1.74  loss_mask_5: 1.431  loss_dice_5: 2.59  loss_bbox_5: 2.884  loss_giou_5: 1.639  loss_ce_dn_5: 2.07  loss_mask_dn_5: 1.247  loss_dice_dn_5: 2.237  loss_bbox_dn_5: 0.8574  loss_giou_dn_5: 0.7371  loss_ce_6: 1.74  loss_mask_6: 1.401  loss_dice_6: 2.62  loss_bbox_6: 2.87  loss_giou_6: 1.629  loss_ce_dn_6: 2.033  loss_mask_dn_6: 1.232  loss_dice_dn_6: 2.26  loss_bbox_dn_6: 0.855  loss_giou_dn_6: 0.7333  loss_ce_7: 1.771  loss_mask_7: 1.372  loss_dice_7: 2.592  loss_bbox_7: 2.843  loss_giou_7: 1.614  loss_ce_dn_7: 2.079  loss_mask_dn_7: 1.294  loss_dice_dn_7: 2.259  loss_bbox_dn_7: 0.8248  loss_giou_dn_7: 0.7169  loss_ce_8: 1.916  loss_mask_8: 1.395  loss_dice_8: 2.617  loss_bbox_8: 2.813  loss_giou_8: 1.61  loss_ce_dn_8: 2.21  loss_mask_dn_8: 1.258  loss_dice_dn_8: 2.316  loss_bbox_dn_8: 0.8195  loss_giou_dn_8: 0.7179    time: 2.2744  last_time: 2.2465  data_time: 0.0125  last_data_time: 0.0193   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:04:45 d2.utils.events]: \u001b[0m eta: 8 days, 7:13:01  iter: 3399  total_loss: 178.2  loss_ce: 1.802  loss_mask: 1.454  loss_dice: 2.382  loss_bbox: 2.522  loss_giou: 1.442  loss_ce_dn: 2.262  loss_mask_dn: 1.235  loss_dice_dn: 2.002  loss_bbox_dn: 0.8655  loss_giou_dn: 0.6984  loss_ce_0: 8.45  loss_mask_0: 1.429  loss_dice_0: 2.613  loss_bbox_0: 3.855  loss_giou_0: 1.866  loss_ce_1: 2.232  loss_mask_1: 1.469  loss_dice_1: 2.408  loss_bbox_1: 2.94  loss_giou_1: 1.605  loss_ce_dn_1: 2.714  loss_mask_dn_1: 1.343  loss_dice_dn_1: 2.283  loss_bbox_dn_1: 1.03  loss_giou_dn_1: 0.7621  loss_ce_2: 2.079  loss_mask_2: 1.441  loss_dice_2: 2.375  loss_bbox_2: 2.713  loss_giou_2: 1.535  loss_ce_dn_2: 2.156  loss_mask_dn_2: 1.426  loss_dice_dn_2: 2.048  loss_bbox_dn_2: 0.9812  loss_giou_dn_2: 0.7424  loss_ce_3: 2.019  loss_mask_3: 1.386  loss_dice_3: 2.424  loss_bbox_3: 2.617  loss_giou_3: 1.47  loss_ce_dn_3: 2.048  loss_mask_dn_3: 1.386  loss_dice_dn_3: 2.064  loss_bbox_dn_3: 0.9042  loss_giou_dn_3: 0.7274  loss_ce_4: 1.913  loss_mask_4: 1.38  loss_dice_4: 2.426  loss_bbox_4: 2.56  loss_giou_4: 1.415  loss_ce_dn_4: 2.038  loss_mask_dn_4: 1.338  loss_dice_dn_4: 2.078  loss_bbox_dn_4: 0.8838  loss_giou_dn_4: 0.7132  loss_ce_5: 1.911  loss_mask_5: 1.425  loss_dice_5: 2.425  loss_bbox_5: 2.57  loss_giou_5: 1.445  loss_ce_dn_5: 2.076  loss_mask_dn_5: 1.266  loss_dice_dn_5: 2.031  loss_bbox_dn_5: 0.8751  loss_giou_dn_5: 0.6992  loss_ce_6: 1.706  loss_mask_6: 1.416  loss_dice_6: 2.404  loss_bbox_6: 2.577  loss_giou_6: 1.425  loss_ce_dn_6: 2.124  loss_mask_dn_6: 1.231  loss_dice_dn_6: 2.017  loss_bbox_dn_6: 0.8735  loss_giou_dn_6: 0.702  loss_ce_7: 1.719  loss_mask_7: 1.409  loss_dice_7: 2.352  loss_bbox_7: 2.538  loss_giou_7: 1.431  loss_ce_dn_7: 2.199  loss_mask_dn_7: 1.293  loss_dice_dn_7: 1.998  loss_bbox_dn_7: 0.8587  loss_giou_dn_7: 0.694  loss_ce_8: 1.842  loss_mask_8: 1.425  loss_dice_8: 2.395  loss_bbox_8: 2.539  loss_giou_8: 1.439  loss_ce_dn_8: 2.17  loss_mask_dn_8: 1.274  loss_dice_dn_8: 1.977  loss_bbox_dn_8: 0.8614  loss_giou_dn_8: 0.696    time: 2.2743  last_time: 2.2489  data_time: 0.0117  last_data_time: 0.0068   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:05:31 d2.utils.events]: \u001b[0m eta: 8 days, 7:12:41  iter: 3419  total_loss: 185.6  loss_ce: 1.766  loss_mask: 1.561  loss_dice: 2.593  loss_bbox: 2.802  loss_giou: 1.547  loss_ce_dn: 2.506  loss_mask_dn: 1.515  loss_dice_dn: 2.292  loss_bbox_dn: 0.8984  loss_giou_dn: 0.7082  loss_ce_0: 8.535  loss_mask_0: 1.769  loss_dice_0: 2.759  loss_bbox_0: 3.746  loss_giou_0: 1.875  loss_ce_1: 2.29  loss_mask_1: 1.646  loss_dice_1: 2.535  loss_bbox_1: 3.151  loss_giou_1: 1.743  loss_ce_dn_1: 3.039  loss_mask_dn_1: 1.536  loss_dice_dn_1: 2.4  loss_bbox_dn_1: 1.028  loss_giou_dn_1: 0.7751  loss_ce_2: 2.06  loss_mask_2: 1.598  loss_dice_2: 2.508  loss_bbox_2: 3.019  loss_giou_2: 1.671  loss_ce_dn_2: 2.566  loss_mask_dn_2: 1.509  loss_dice_dn_2: 2.271  loss_bbox_dn_2: 0.9836  loss_giou_dn_2: 0.7546  loss_ce_3: 1.85  loss_mask_3: 1.578  loss_dice_3: 2.52  loss_bbox_3: 2.9  loss_giou_3: 1.619  loss_ce_dn_3: 2.528  loss_mask_dn_3: 1.519  loss_dice_dn_3: 2.257  loss_bbox_dn_3: 0.9556  loss_giou_dn_3: 0.7448  loss_ce_4: 1.763  loss_mask_4: 1.586  loss_dice_4: 2.529  loss_bbox_4: 2.852  loss_giou_4: 1.613  loss_ce_dn_4: 2.484  loss_mask_dn_4: 1.547  loss_dice_dn_4: 2.22  loss_bbox_dn_4: 0.9405  loss_giou_dn_4: 0.7382  loss_ce_5: 1.763  loss_mask_5: 1.653  loss_dice_5: 2.51  loss_bbox_5: 2.824  loss_giou_5: 1.585  loss_ce_dn_5: 2.381  loss_mask_dn_5: 1.483  loss_dice_dn_5: 2.236  loss_bbox_dn_5: 0.9216  loss_giou_dn_5: 0.7251  loss_ce_6: 1.775  loss_mask_6: 1.582  loss_dice_6: 2.53  loss_bbox_6: 2.81  loss_giou_6: 1.584  loss_ce_dn_6: 2.413  loss_mask_dn_6: 1.474  loss_dice_dn_6: 2.26  loss_bbox_dn_6: 0.9063  loss_giou_dn_6: 0.7229  loss_ce_7: 1.751  loss_mask_7: 1.601  loss_dice_7: 2.583  loss_bbox_7: 2.805  loss_giou_7: 1.553  loss_ce_dn_7: 2.42  loss_mask_dn_7: 1.536  loss_dice_dn_7: 2.259  loss_bbox_dn_7: 0.9071  loss_giou_dn_7: 0.706  loss_ce_8: 1.811  loss_mask_8: 1.499  loss_dice_8: 2.57  loss_bbox_8: 2.773  loss_giou_8: 1.543  loss_ce_dn_8: 2.489  loss_mask_dn_8: 1.521  loss_dice_dn_8: 2.266  loss_bbox_dn_8: 0.9003  loss_giou_dn_8: 0.7068    time: 2.2743  last_time: 2.3378  data_time: 0.0122  last_data_time: 0.0195   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:06:16 d2.utils.events]: \u001b[0m eta: 8 days, 7:11:55  iter: 3439  total_loss: 182.4  loss_ce: 1.941  loss_mask: 1.565  loss_dice: 2.238  loss_bbox: 2.712  loss_giou: 1.648  loss_ce_dn: 2.409  loss_mask_dn: 1.268  loss_dice_dn: 2.218  loss_bbox_dn: 0.8548  loss_giou_dn: 0.7147  loss_ce_0: 7.966  loss_mask_0: 1.266  loss_dice_0: 2.61  loss_bbox_0: 4.189  loss_giou_0: 1.965  loss_ce_1: 1.931  loss_mask_1: 1.563  loss_dice_1: 2.283  loss_bbox_1: 3.182  loss_giou_1: 1.671  loss_ce_dn_1: 2.717  loss_mask_dn_1: 1.301  loss_dice_dn_1: 2.409  loss_bbox_dn_1: 0.9844  loss_giou_dn_1: 0.7714  loss_ce_2: 1.892  loss_mask_2: 1.619  loss_dice_2: 2.214  loss_bbox_2: 2.927  loss_giou_2: 1.718  loss_ce_dn_2: 2.279  loss_mask_dn_2: 1.372  loss_dice_dn_2: 2.265  loss_bbox_dn_2: 0.9474  loss_giou_dn_2: 0.7471  loss_ce_3: 1.851  loss_mask_3: 1.605  loss_dice_3: 2.213  loss_bbox_3: 2.756  loss_giou_3: 1.711  loss_ce_dn_3: 2.184  loss_mask_dn_3: 1.335  loss_dice_dn_3: 2.232  loss_bbox_dn_3: 0.9327  loss_giou_dn_3: 0.729  loss_ce_4: 1.801  loss_mask_4: 1.613  loss_dice_4: 2.258  loss_bbox_4: 2.684  loss_giou_4: 1.68  loss_ce_dn_4: 2.123  loss_mask_dn_4: 1.319  loss_dice_dn_4: 2.199  loss_bbox_dn_4: 0.8975  loss_giou_dn_4: 0.7178  loss_ce_5: 1.865  loss_mask_5: 1.605  loss_dice_5: 2.223  loss_bbox_5: 2.663  loss_giou_5: 1.651  loss_ce_dn_5: 2.18  loss_mask_dn_5: 1.319  loss_dice_dn_5: 2.162  loss_bbox_dn_5: 0.8766  loss_giou_dn_5: 0.72  loss_ce_6: 1.868  loss_mask_6: 1.59  loss_dice_6: 2.187  loss_bbox_6: 2.672  loss_giou_6: 1.652  loss_ce_dn_6: 2.157  loss_mask_dn_6: 1.311  loss_dice_dn_6: 2.114  loss_bbox_dn_6: 0.8715  loss_giou_dn_6: 0.7216  loss_ce_7: 1.826  loss_mask_7: 1.575  loss_dice_7: 2.247  loss_bbox_7: 2.709  loss_giou_7: 1.643  loss_ce_dn_7: 2.186  loss_mask_dn_7: 1.3  loss_dice_dn_7: 2.142  loss_bbox_dn_7: 0.8569  loss_giou_dn_7: 0.7148  loss_ce_8: 1.896  loss_mask_8: 1.557  loss_dice_8: 2.213  loss_bbox_8: 2.715  loss_giou_8: 1.649  loss_ce_dn_8: 2.258  loss_mask_dn_8: 1.297  loss_dice_dn_8: 2.166  loss_bbox_dn_8: 0.8557  loss_giou_dn_8: 0.7113    time: 2.2743  last_time: 2.2495  data_time: 0.0134  last_data_time: 0.0169   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:07:02 d2.utils.events]: \u001b[0m eta: 8 days, 7:11:10  iter: 3459  total_loss: 184.4  loss_ce: 1.632  loss_mask: 1.804  loss_dice: 2.443  loss_bbox: 3.049  loss_giou: 1.486  loss_ce_dn: 2.067  loss_mask_dn: 1.422  loss_dice_dn: 2.12  loss_bbox_dn: 0.879  loss_giou_dn: 0.677  loss_ce_0: 8.273  loss_mask_0: 1.551  loss_dice_0: 2.668  loss_bbox_0: 3.797  loss_giou_0: 1.904  loss_ce_1: 1.869  loss_mask_1: 1.841  loss_dice_1: 2.489  loss_bbox_1: 3.091  loss_giou_1: 1.611  loss_ce_dn_1: 2.783  loss_mask_dn_1: 1.497  loss_dice_dn_1: 2.266  loss_bbox_dn_1: 1.058  loss_giou_dn_1: 0.7597  loss_ce_2: 1.718  loss_mask_2: 1.802  loss_dice_2: 2.441  loss_bbox_2: 3.084  loss_giou_2: 1.594  loss_ce_dn_2: 2.324  loss_mask_dn_2: 1.386  loss_dice_dn_2: 2.106  loss_bbox_dn_2: 0.9959  loss_giou_dn_2: 0.7371  loss_ce_3: 1.615  loss_mask_3: 1.796  loss_dice_3: 2.402  loss_bbox_3: 3.082  loss_giou_3: 1.558  loss_ce_dn_3: 2.23  loss_mask_dn_3: 1.318  loss_dice_dn_3: 2.103  loss_bbox_dn_3: 0.9441  loss_giou_dn_3: 0.7207  loss_ce_4: 1.603  loss_mask_4: 1.772  loss_dice_4: 2.38  loss_bbox_4: 3.139  loss_giou_4: 1.538  loss_ce_dn_4: 2.226  loss_mask_dn_4: 1.36  loss_dice_dn_4: 2.093  loss_bbox_dn_4: 0.9152  loss_giou_dn_4: 0.7085  loss_ce_5: 1.614  loss_mask_5: 1.763  loss_dice_5: 2.379  loss_bbox_5: 3.133  loss_giou_5: 1.496  loss_ce_dn_5: 2.192  loss_mask_dn_5: 1.376  loss_dice_dn_5: 2.078  loss_bbox_dn_5: 0.8981  loss_giou_dn_5: 0.6989  loss_ce_6: 1.598  loss_mask_6: 1.828  loss_dice_6: 2.399  loss_bbox_6: 3.098  loss_giou_6: 1.498  loss_ce_dn_6: 2.126  loss_mask_dn_6: 1.427  loss_dice_dn_6: 2.103  loss_bbox_dn_6: 0.8949  loss_giou_dn_6: 0.6913  loss_ce_7: 1.643  loss_mask_7: 1.846  loss_dice_7: 2.41  loss_bbox_7: 3.058  loss_giou_7: 1.477  loss_ce_dn_7: 2.117  loss_mask_dn_7: 1.451  loss_dice_dn_7: 2.112  loss_bbox_dn_7: 0.8762  loss_giou_dn_7: 0.6778  loss_ce_8: 1.598  loss_mask_8: 1.861  loss_dice_8: 2.412  loss_bbox_8: 3.053  loss_giou_8: 1.476  loss_ce_dn_8: 2.064  loss_mask_dn_8: 1.414  loss_dice_dn_8: 2.11  loss_bbox_dn_8: 0.8767  loss_giou_dn_8: 0.6755    time: 2.2744  last_time: 2.2934  data_time: 0.0116  last_data_time: 0.0100   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:07:47 d2.utils.events]: \u001b[0m eta: 8 days, 7:09:31  iter: 3479  total_loss: 172.7  loss_ce: 1.715  loss_mask: 1.53  loss_dice: 2.545  loss_bbox: 2.659  loss_giou: 1.597  loss_ce_dn: 2.072  loss_mask_dn: 1.297  loss_dice_dn: 2.346  loss_bbox_dn: 0.8402  loss_giou_dn: 0.7332  loss_ce_0: 8.398  loss_mask_0: 1.376  loss_dice_0: 2.771  loss_bbox_0: 3.898  loss_giou_0: 1.933  loss_ce_1: 2.295  loss_mask_1: 1.52  loss_dice_1: 2.652  loss_bbox_1: 2.914  loss_giou_1: 1.767  loss_ce_dn_1: 2.608  loss_mask_dn_1: 1.314  loss_dice_dn_1: 2.434  loss_bbox_dn_1: 0.9469  loss_giou_dn_1: 0.7822  loss_ce_2: 2.177  loss_mask_2: 1.517  loss_dice_2: 2.485  loss_bbox_2: 2.744  loss_giou_2: 1.686  loss_ce_dn_2: 2.198  loss_mask_dn_2: 1.351  loss_dice_dn_2: 2.293  loss_bbox_dn_2: 0.9196  loss_giou_dn_2: 0.754  loss_ce_3: 1.886  loss_mask_3: 1.522  loss_dice_3: 2.493  loss_bbox_3: 2.681  loss_giou_3: 1.645  loss_ce_dn_3: 1.985  loss_mask_dn_3: 1.268  loss_dice_dn_3: 2.289  loss_bbox_dn_3: 0.905  loss_giou_dn_3: 0.7367  loss_ce_4: 1.798  loss_mask_4: 1.532  loss_dice_4: 2.513  loss_bbox_4: 2.621  loss_giou_4: 1.633  loss_ce_dn_4: 1.905  loss_mask_dn_4: 1.281  loss_dice_dn_4: 2.284  loss_bbox_dn_4: 0.9017  loss_giou_dn_4: 0.7251  loss_ce_5: 1.749  loss_mask_5: 1.535  loss_dice_5: 2.515  loss_bbox_5: 2.594  loss_giou_5: 1.606  loss_ce_dn_5: 1.899  loss_mask_dn_5: 1.293  loss_dice_dn_5: 2.295  loss_bbox_dn_5: 0.8726  loss_giou_dn_5: 0.7227  loss_ce_6: 1.787  loss_mask_6: 1.579  loss_dice_6: 2.525  loss_bbox_6: 2.627  loss_giou_6: 1.62  loss_ce_dn_6: 1.943  loss_mask_dn_6: 1.307  loss_dice_dn_6: 2.284  loss_bbox_dn_6: 0.8608  loss_giou_dn_6: 0.7224  loss_ce_7: 1.817  loss_mask_7: 1.55  loss_dice_7: 2.552  loss_bbox_7: 2.641  loss_giou_7: 1.607  loss_ce_dn_7: 2.07  loss_mask_dn_7: 1.33  loss_dice_dn_7: 2.294  loss_bbox_dn_7: 0.8316  loss_giou_dn_7: 0.7216  loss_ce_8: 1.774  loss_mask_8: 1.518  loss_dice_8: 2.545  loss_bbox_8: 2.644  loss_giou_8: 1.595  loss_ce_dn_8: 2.089  loss_mask_dn_8: 1.306  loss_dice_dn_8: 2.316  loss_bbox_dn_8: 0.831  loss_giou_dn_8: 0.7239    time: 2.2743  last_time: 2.2767  data_time: 0.0116  last_data_time: 0.0076   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:08:33 d2.utils.events]: \u001b[0m eta: 8 days, 7:08:26  iter: 3499  total_loss: 163.8  loss_ce: 1.778  loss_mask: 1.053  loss_dice: 2.371  loss_bbox: 2.467  loss_giou: 1.579  loss_ce_dn: 2.149  loss_mask_dn: 0.9751  loss_dice_dn: 2.156  loss_bbox_dn: 0.6841  loss_giou_dn: 0.7034  loss_ce_0: 7.945  loss_mask_0: 1.072  loss_dice_0: 2.624  loss_bbox_0: 3.991  loss_giou_0: 1.949  loss_ce_1: 2.193  loss_mask_1: 1.039  loss_dice_1: 2.447  loss_bbox_1: 2.773  loss_giou_1: 1.676  loss_ce_dn_1: 2.583  loss_mask_dn_1: 0.9967  loss_dice_dn_1: 2.428  loss_bbox_dn_1: 0.8991  loss_giou_dn_1: 0.7791  loss_ce_2: 1.732  loss_mask_2: 1.067  loss_dice_2: 2.39  loss_bbox_2: 2.616  loss_giou_2: 1.657  loss_ce_dn_2: 2.275  loss_mask_dn_2: 0.9755  loss_dice_dn_2: 2.215  loss_bbox_dn_2: 0.8442  loss_giou_dn_2: 0.7511  loss_ce_3: 1.566  loss_mask_3: 1.02  loss_dice_3: 2.373  loss_bbox_3: 2.495  loss_giou_3: 1.576  loss_ce_dn_3: 2.168  loss_mask_dn_3: 0.9691  loss_dice_dn_3: 2.191  loss_bbox_dn_3: 0.7629  loss_giou_dn_3: 0.7257  loss_ce_4: 1.534  loss_mask_4: 1.091  loss_dice_4: 2.324  loss_bbox_4: 2.433  loss_giou_4: 1.588  loss_ce_dn_4: 2.127  loss_mask_dn_4: 0.9928  loss_dice_dn_4: 2.133  loss_bbox_dn_4: 0.7216  loss_giou_dn_4: 0.7104  loss_ce_5: 1.618  loss_mask_5: 1.066  loss_dice_5: 2.345  loss_bbox_5: 2.453  loss_giou_5: 1.591  loss_ce_dn_5: 2.12  loss_mask_dn_5: 0.9727  loss_dice_dn_5: 2.124  loss_bbox_dn_5: 0.6935  loss_giou_dn_5: 0.707  loss_ce_6: 1.707  loss_mask_6: 1.097  loss_dice_6: 2.378  loss_bbox_6: 2.472  loss_giou_6: 1.599  loss_ce_dn_6: 2.085  loss_mask_dn_6: 0.9829  loss_dice_dn_6: 2.095  loss_bbox_dn_6: 0.6929  loss_giou_dn_6: 0.7071  loss_ce_7: 1.763  loss_mask_7: 1.112  loss_dice_7: 2.341  loss_bbox_7: 2.453  loss_giou_7: 1.611  loss_ce_dn_7: 2.069  loss_mask_dn_7: 0.962  loss_dice_dn_7: 2.122  loss_bbox_dn_7: 0.6802  loss_giou_dn_7: 0.6998  loss_ce_8: 1.743  loss_mask_8: 1.086  loss_dice_8: 2.406  loss_bbox_8: 2.459  loss_giou_8: 1.599  loss_ce_dn_8: 2.097  loss_mask_dn_8: 0.9815  loss_dice_dn_8: 2.136  loss_bbox_dn_8: 0.6833  loss_giou_dn_8: 0.7018    time: 2.2743  last_time: 2.2988  data_time: 0.0128  last_data_time: 0.0289   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:09:18 d2.utils.events]: \u001b[0m eta: 8 days, 7:07:41  iter: 3519  total_loss: 183.1  loss_ce: 1.477  loss_mask: 1.456  loss_dice: 2.592  loss_bbox: 2.763  loss_giou: 1.577  loss_ce_dn: 2.134  loss_mask_dn: 1.405  loss_dice_dn: 2.184  loss_bbox_dn: 0.8611  loss_giou_dn: 0.7255  loss_ce_0: 8.032  loss_mask_0: 1.457  loss_dice_0: 2.607  loss_bbox_0: 4.001  loss_giou_0: 1.909  loss_ce_1: 1.879  loss_mask_1: 1.456  loss_dice_1: 2.599  loss_bbox_1: 3.28  loss_giou_1: 1.761  loss_ce_dn_1: 2.713  loss_mask_dn_1: 1.46  loss_dice_dn_1: 2.443  loss_bbox_dn_1: 0.9914  loss_giou_dn_1: 0.7949  loss_ce_2: 1.624  loss_mask_2: 1.417  loss_dice_2: 2.545  loss_bbox_2: 3.023  loss_giou_2: 1.67  loss_ce_dn_2: 2.199  loss_mask_dn_2: 1.47  loss_dice_dn_2: 2.249  loss_bbox_dn_2: 0.9577  loss_giou_dn_2: 0.7725  loss_ce_3: 1.496  loss_mask_3: 1.43  loss_dice_3: 2.566  loss_bbox_3: 2.865  loss_giou_3: 1.64  loss_ce_dn_3: 2.088  loss_mask_dn_3: 1.425  loss_dice_dn_3: 2.217  loss_bbox_dn_3: 0.9163  loss_giou_dn_3: 0.755  loss_ce_4: 1.475  loss_mask_4: 1.474  loss_dice_4: 2.544  loss_bbox_4: 2.81  loss_giou_4: 1.617  loss_ce_dn_4: 2.031  loss_mask_dn_4: 1.361  loss_dice_dn_4: 2.173  loss_bbox_dn_4: 0.9043  loss_giou_dn_4: 0.7497  loss_ce_5: 1.468  loss_mask_5: 1.491  loss_dice_5: 2.574  loss_bbox_5: 2.831  loss_giou_5: 1.596  loss_ce_dn_5: 2.002  loss_mask_dn_5: 1.364  loss_dice_dn_5: 2.201  loss_bbox_dn_5: 0.8945  loss_giou_dn_5: 0.7441  loss_ce_6: 1.46  loss_mask_6: 1.457  loss_dice_6: 2.521  loss_bbox_6: 2.835  loss_giou_6: 1.594  loss_ce_dn_6: 1.948  loss_mask_dn_6: 1.381  loss_dice_dn_6: 2.181  loss_bbox_dn_6: 0.8889  loss_giou_dn_6: 0.7421  loss_ce_7: 1.509  loss_mask_7: 1.429  loss_dice_7: 2.517  loss_bbox_7: 2.8  loss_giou_7: 1.576  loss_ce_dn_7: 1.971  loss_mask_dn_7: 1.392  loss_dice_dn_7: 2.15  loss_bbox_dn_7: 0.8718  loss_giou_dn_7: 0.7294  loss_ce_8: 1.55  loss_mask_8: 1.454  loss_dice_8: 2.542  loss_bbox_8: 2.776  loss_giou_8: 1.599  loss_ce_dn_8: 2.096  loss_mask_dn_8: 1.363  loss_dice_dn_8: 2.16  loss_bbox_dn_8: 0.8665  loss_giou_dn_8: 0.7262    time: 2.2742  last_time: 2.2682  data_time: 0.0115  last_data_time: 0.0215   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:10:04 d2.utils.events]: \u001b[0m eta: 8 days, 7:07:19  iter: 3539  total_loss: 181.9  loss_ce: 1.738  loss_mask: 1.553  loss_dice: 2.334  loss_bbox: 2.762  loss_giou: 1.522  loss_ce_dn: 2.713  loss_mask_dn: 1.447  loss_dice_dn: 2.247  loss_bbox_dn: 0.7959  loss_giou_dn: 0.697  loss_ce_0: 7.95  loss_mask_0: 1.498  loss_dice_0: 2.734  loss_bbox_0: 4.225  loss_giou_0: 1.905  loss_ce_1: 2.258  loss_mask_1: 1.455  loss_dice_1: 2.356  loss_bbox_1: 3.087  loss_giou_1: 1.641  loss_ce_dn_1: 3.013  loss_mask_dn_1: 1.456  loss_dice_dn_1: 2.405  loss_bbox_dn_1: 0.9685  loss_giou_dn_1: 0.7597  loss_ce_2: 1.96  loss_mask_2: 1.521  loss_dice_2: 2.328  loss_bbox_2: 2.987  loss_giou_2: 1.607  loss_ce_dn_2: 2.562  loss_mask_dn_2: 1.433  loss_dice_dn_2: 2.397  loss_bbox_dn_2: 0.9297  loss_giou_dn_2: 0.7379  loss_ce_3: 1.846  loss_mask_3: 1.521  loss_dice_3: 2.301  loss_bbox_3: 2.822  loss_giou_3: 1.606  loss_ce_dn_3: 2.433  loss_mask_dn_3: 1.418  loss_dice_dn_3: 2.32  loss_bbox_dn_3: 0.8912  loss_giou_dn_3: 0.7198  loss_ce_4: 1.756  loss_mask_4: 1.583  loss_dice_4: 2.333  loss_bbox_4: 2.828  loss_giou_4: 1.621  loss_ce_dn_4: 2.424  loss_mask_dn_4: 1.39  loss_dice_dn_4: 2.324  loss_bbox_dn_4: 0.8491  loss_giou_dn_4: 0.7051  loss_ce_5: 1.758  loss_mask_5: 1.556  loss_dice_5: 2.374  loss_bbox_5: 2.784  loss_giou_5: 1.591  loss_ce_dn_5: 2.375  loss_mask_dn_5: 1.379  loss_dice_dn_5: 2.325  loss_bbox_dn_5: 0.8306  loss_giou_dn_5: 0.6965  loss_ce_6: 1.754  loss_mask_6: 1.539  loss_dice_6: 2.418  loss_bbox_6: 2.781  loss_giou_6: 1.58  loss_ce_dn_6: 2.43  loss_mask_dn_6: 1.355  loss_dice_dn_6: 2.28  loss_bbox_dn_6: 0.8279  loss_giou_dn_6: 0.6939  loss_ce_7: 1.775  loss_mask_7: 1.582  loss_dice_7: 2.427  loss_bbox_7: 2.743  loss_giou_7: 1.54  loss_ce_dn_7: 2.43  loss_mask_dn_7: 1.396  loss_dice_dn_7: 2.262  loss_bbox_dn_7: 0.793  loss_giou_dn_7: 0.6898  loss_ce_8: 1.78  loss_mask_8: 1.521  loss_dice_8: 2.351  loss_bbox_8: 2.76  loss_giou_8: 1.55  loss_ce_dn_8: 2.604  loss_mask_dn_8: 1.435  loss_dice_dn_8: 2.259  loss_bbox_dn_8: 0.7957  loss_giou_dn_8: 0.6929    time: 2.2742  last_time: 2.2485  data_time: 0.0119  last_data_time: 0.0066   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:10:49 d2.utils.events]: \u001b[0m eta: 8 days, 7:06:01  iter: 3559  total_loss: 185.3  loss_ce: 1.607  loss_mask: 1.538  loss_dice: 2.505  loss_bbox: 2.598  loss_giou: 1.431  loss_ce_dn: 2.464  loss_mask_dn: 1.342  loss_dice_dn: 2.273  loss_bbox_dn: 0.8741  loss_giou_dn: 0.7049  loss_ce_0: 7.761  loss_mask_0: 1.508  loss_dice_0: 2.668  loss_bbox_0: 4.142  loss_giou_0: 1.953  loss_ce_1: 2.049  loss_mask_1: 1.572  loss_dice_1: 2.463  loss_bbox_1: 2.979  loss_giou_1: 1.637  loss_ce_dn_1: 2.894  loss_mask_dn_1: 1.446  loss_dice_dn_1: 2.379  loss_bbox_dn_1: 1.062  loss_giou_dn_1: 0.777  loss_ce_2: 1.891  loss_mask_2: 1.544  loss_dice_2: 2.401  loss_bbox_2: 2.848  loss_giou_2: 1.63  loss_ce_dn_2: 2.538  loss_mask_dn_2: 1.378  loss_dice_dn_2: 2.236  loss_bbox_dn_2: 1.003  loss_giou_dn_2: 0.7522  loss_ce_3: 1.605  loss_mask_3: 1.542  loss_dice_3: 2.453  loss_bbox_3: 2.759  loss_giou_3: 1.594  loss_ce_dn_3: 2.421  loss_mask_dn_3: 1.377  loss_dice_dn_3: 2.268  loss_bbox_dn_3: 0.931  loss_giou_dn_3: 0.7332  loss_ce_4: 1.572  loss_mask_4: 1.555  loss_dice_4: 2.453  loss_bbox_4: 2.66  loss_giou_4: 1.52  loss_ce_dn_4: 2.418  loss_mask_dn_4: 1.356  loss_dice_dn_4: 2.244  loss_bbox_dn_4: 0.8822  loss_giou_dn_4: 0.7127  loss_ce_5: 1.591  loss_mask_5: 1.554  loss_dice_5: 2.423  loss_bbox_5: 2.604  loss_giou_5: 1.506  loss_ce_dn_5: 2.376  loss_mask_dn_5: 1.351  loss_dice_dn_5: 2.236  loss_bbox_dn_5: 0.8711  loss_giou_dn_5: 0.716  loss_ce_6: 1.678  loss_mask_6: 1.536  loss_dice_6: 2.465  loss_bbox_6: 2.62  loss_giou_6: 1.476  loss_ce_dn_6: 2.344  loss_mask_dn_6: 1.328  loss_dice_dn_6: 2.25  loss_bbox_dn_6: 0.8668  loss_giou_dn_6: 0.7169  loss_ce_7: 1.634  loss_mask_7: 1.547  loss_dice_7: 2.461  loss_bbox_7: 2.585  loss_giou_7: 1.453  loss_ce_dn_7: 2.436  loss_mask_dn_7: 1.347  loss_dice_dn_7: 2.218  loss_bbox_dn_7: 0.8669  loss_giou_dn_7: 0.6971  loss_ce_8: 1.599  loss_mask_8: 1.569  loss_dice_8: 2.474  loss_bbox_8: 2.593  loss_giou_8: 1.431  loss_ce_dn_8: 2.378  loss_mask_dn_8: 1.338  loss_dice_dn_8: 2.231  loss_bbox_dn_8: 0.8712  loss_giou_dn_8: 0.7015    time: 2.2741  last_time: 2.2298  data_time: 0.0122  last_data_time: 0.0192   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:11:34 d2.utils.events]: \u001b[0m eta: 8 days, 7:03:51  iter: 3579  total_loss: 180.5  loss_ce: 1.924  loss_mask: 1.246  loss_dice: 2.624  loss_bbox: 2.424  loss_giou: 1.627  loss_ce_dn: 2.666  loss_mask_dn: 0.9969  loss_dice_dn: 2.233  loss_bbox_dn: 0.7791  loss_giou_dn: 0.7062  loss_ce_0: 8.043  loss_mask_0: 1.201  loss_dice_0: 2.781  loss_bbox_0: 3.811  loss_giou_0: 1.967  loss_ce_1: 2.375  loss_mask_1: 1.378  loss_dice_1: 2.669  loss_bbox_1: 2.646  loss_giou_1: 1.676  loss_ce_dn_1: 3.168  loss_mask_dn_1: 1.17  loss_dice_dn_1: 2.458  loss_bbox_dn_1: 0.9137  loss_giou_dn_1: 0.7827  loss_ce_2: 2.108  loss_mask_2: 1.194  loss_dice_2: 2.62  loss_bbox_2: 2.536  loss_giou_2: 1.67  loss_ce_dn_2: 2.721  loss_mask_dn_2: 1.082  loss_dice_dn_2: 2.318  loss_bbox_dn_2: 0.8773  loss_giou_dn_2: 0.7585  loss_ce_3: 1.967  loss_mask_3: 1.236  loss_dice_3: 2.562  loss_bbox_3: 2.574  loss_giou_3: 1.635  loss_ce_dn_3: 2.599  loss_mask_dn_3: 1.079  loss_dice_dn_3: 2.265  loss_bbox_dn_3: 0.8297  loss_giou_dn_3: 0.7411  loss_ce_4: 1.933  loss_mask_4: 1.25  loss_dice_4: 2.556  loss_bbox_4: 2.488  loss_giou_4: 1.612  loss_ce_dn_4: 2.546  loss_mask_dn_4: 1.028  loss_dice_dn_4: 2.231  loss_bbox_dn_4: 0.8095  loss_giou_dn_4: 0.7262  loss_ce_5: 1.926  loss_mask_5: 1.27  loss_dice_5: 2.586  loss_bbox_5: 2.458  loss_giou_5: 1.623  loss_ce_dn_5: 2.548  loss_mask_dn_5: 0.9969  loss_dice_dn_5: 2.214  loss_bbox_dn_5: 0.8021  loss_giou_dn_5: 0.7169  loss_ce_6: 1.94  loss_mask_6: 1.276  loss_dice_6: 2.564  loss_bbox_6: 2.461  loss_giou_6: 1.628  loss_ce_dn_6: 2.57  loss_mask_dn_6: 0.99  loss_dice_dn_6: 2.167  loss_bbox_dn_6: 0.7965  loss_giou_dn_6: 0.7139  loss_ce_7: 1.887  loss_mask_7: 1.263  loss_dice_7: 2.564  loss_bbox_7: 2.441  loss_giou_7: 1.627  loss_ce_dn_7: 2.584  loss_mask_dn_7: 0.9811  loss_dice_dn_7: 2.188  loss_bbox_dn_7: 0.7791  loss_giou_dn_7: 0.7004  loss_ce_8: 1.882  loss_mask_8: 1.285  loss_dice_8: 2.636  loss_bbox_8: 2.421  loss_giou_8: 1.623  loss_ce_dn_8: 2.564  loss_mask_dn_8: 0.9939  loss_dice_dn_8: 2.196  loss_bbox_dn_8: 0.7781  loss_giou_dn_8: 0.7019    time: 2.2742  last_time: 2.3532  data_time: 0.0108  last_data_time: 0.0041   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:12:20 d2.utils.events]: \u001b[0m eta: 8 days, 6:59:42  iter: 3599  total_loss: 183.9  loss_ce: 1.622  loss_mask: 1.544  loss_dice: 2.191  loss_bbox: 2.665  loss_giou: 1.465  loss_ce_dn: 2.389  loss_mask_dn: 1.438  loss_dice_dn: 2.101  loss_bbox_dn: 0.9108  loss_giou_dn: 0.7167  loss_ce_0: 8.093  loss_mask_0: 1.439  loss_dice_0: 2.541  loss_bbox_0: 4.096  loss_giou_0: 1.969  loss_ce_1: 1.919  loss_mask_1: 1.51  loss_dice_1: 2.376  loss_bbox_1: 2.997  loss_giou_1: 1.609  loss_ce_dn_1: 2.599  loss_mask_dn_1: 1.374  loss_dice_dn_1: 2.367  loss_bbox_dn_1: 1.027  loss_giou_dn_1: 0.7712  loss_ce_2: 1.973  loss_mask_2: 1.492  loss_dice_2: 2.279  loss_bbox_2: 2.715  loss_giou_2: 1.563  loss_ce_dn_2: 2.239  loss_mask_dn_2: 1.385  loss_dice_dn_2: 2.269  loss_bbox_dn_2: 0.9683  loss_giou_dn_2: 0.7468  loss_ce_3: 1.766  loss_mask_3: 1.501  loss_dice_3: 2.301  loss_bbox_3: 2.967  loss_giou_3: 1.555  loss_ce_dn_3: 2.185  loss_mask_dn_3: 1.383  loss_dice_dn_3: 2.169  loss_bbox_dn_3: 0.9265  loss_giou_dn_3: 0.725  loss_ce_4: 1.624  loss_mask_4: 1.539  loss_dice_4: 2.269  loss_bbox_4: 2.917  loss_giou_4: 1.511  loss_ce_dn_4: 2.226  loss_mask_dn_4: 1.365  loss_dice_dn_4: 2.141  loss_bbox_dn_4: 0.9141  loss_giou_dn_4: 0.7177  loss_ce_5: 1.61  loss_mask_5: 1.517  loss_dice_5: 2.233  loss_bbox_5: 2.798  loss_giou_5: 1.494  loss_ce_dn_5: 2.183  loss_mask_dn_5: 1.377  loss_dice_dn_5: 2.081  loss_bbox_dn_5: 0.913  loss_giou_dn_5: 0.7115  loss_ce_6: 1.57  loss_mask_6: 1.577  loss_dice_6: 2.251  loss_bbox_6: 2.667  loss_giou_6: 1.486  loss_ce_dn_6: 2.195  loss_mask_dn_6: 1.447  loss_dice_dn_6: 2.1  loss_bbox_dn_6: 0.9192  loss_giou_dn_6: 0.71  loss_ce_7: 1.589  loss_mask_7: 1.544  loss_dice_7: 2.231  loss_bbox_7: 2.683  loss_giou_7: 1.47  loss_ce_dn_7: 2.216  loss_mask_dn_7: 1.442  loss_dice_dn_7: 2.115  loss_bbox_dn_7: 0.8911  loss_giou_dn_7: 0.7016  loss_ce_8: 1.583  loss_mask_8: 1.534  loss_dice_8: 2.267  loss_bbox_8: 2.686  loss_giou_8: 1.471  loss_ce_dn_8: 2.348  loss_mask_dn_8: 1.422  loss_dice_dn_8: 2.095  loss_bbox_dn_8: 0.9  loss_giou_dn_8: 0.7084    time: 2.2741  last_time: 2.2544  data_time: 0.0148  last_data_time: 0.0276   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:13:05 d2.utils.events]: \u001b[0m eta: 8 days, 6:58:25  iter: 3619  total_loss: 182.4  loss_ce: 1.9  loss_mask: 1.58  loss_dice: 2.416  loss_bbox: 3.086  loss_giou: 1.642  loss_ce_dn: 2.395  loss_mask_dn: 1.471  loss_dice_dn: 2.201  loss_bbox_dn: 0.8872  loss_giou_dn: 0.7514  loss_ce_0: 8.158  loss_mask_0: 1.431  loss_dice_0: 2.713  loss_bbox_0: 3.841  loss_giou_0: 1.935  loss_ce_1: 2.424  loss_mask_1: 1.501  loss_dice_1: 2.453  loss_bbox_1: 3.186  loss_giou_1: 1.784  loss_ce_dn_1: 2.715  loss_mask_dn_1: 1.427  loss_dice_dn_1: 2.366  loss_bbox_dn_1: 0.9885  loss_giou_dn_1: 0.7878  loss_ce_2: 2.162  loss_mask_2: 1.514  loss_dice_2: 2.398  loss_bbox_2: 3.051  loss_giou_2: 1.736  loss_ce_dn_2: 2.197  loss_mask_dn_2: 1.441  loss_dice_dn_2: 2.222  loss_bbox_dn_2: 0.9466  loss_giou_dn_2: 0.7707  loss_ce_3: 1.907  loss_mask_3: 1.532  loss_dice_3: 2.403  loss_bbox_3: 3.025  loss_giou_3: 1.708  loss_ce_dn_3: 2.161  loss_mask_dn_3: 1.405  loss_dice_dn_3: 2.236  loss_bbox_dn_3: 0.9379  loss_giou_dn_3: 0.759  loss_ce_4: 1.812  loss_mask_4: 1.558  loss_dice_4: 2.424  loss_bbox_4: 3.049  loss_giou_4: 1.7  loss_ce_dn_4: 2.164  loss_mask_dn_4: 1.434  loss_dice_dn_4: 2.209  loss_bbox_dn_4: 0.9299  loss_giou_dn_4: 0.7458  loss_ce_5: 1.804  loss_mask_5: 1.573  loss_dice_5: 2.423  loss_bbox_5: 3.031  loss_giou_5: 1.681  loss_ce_dn_5: 2.058  loss_mask_dn_5: 1.465  loss_dice_dn_5: 2.223  loss_bbox_dn_5: 0.9083  loss_giou_dn_5: 0.7431  loss_ce_6: 1.805  loss_mask_6: 1.587  loss_dice_6: 2.419  loss_bbox_6: 3.022  loss_giou_6: 1.682  loss_ce_dn_6: 2.019  loss_mask_dn_6: 1.442  loss_dice_dn_6: 2.189  loss_bbox_dn_6: 0.8978  loss_giou_dn_6: 0.7412  loss_ce_7: 1.792  loss_mask_7: 1.559  loss_dice_7: 2.425  loss_bbox_7: 3.073  loss_giou_7: 1.676  loss_ce_dn_7: 2.131  loss_mask_dn_7: 1.47  loss_dice_dn_7: 2.198  loss_bbox_dn_7: 0.8808  loss_giou_dn_7: 0.7443  loss_ce_8: 1.829  loss_mask_8: 1.537  loss_dice_8: 2.418  loss_bbox_8: 3.082  loss_giou_8: 1.678  loss_ce_dn_8: 2.261  loss_mask_dn_8: 1.496  loss_dice_dn_8: 2.185  loss_bbox_dn_8: 0.8854  loss_giou_dn_8: 0.7461    time: 2.2740  last_time: 2.2284  data_time: 0.0123  last_data_time: 0.0052   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:13:51 d2.utils.events]: \u001b[0m eta: 8 days, 6:57:53  iter: 3639  total_loss: 185.6  loss_ce: 1.893  loss_mask: 1.755  loss_dice: 2.621  loss_bbox: 2.841  loss_giou: 1.682  loss_ce_dn: 2.338  loss_mask_dn: 1.616  loss_dice_dn: 2.409  loss_bbox_dn: 0.8295  loss_giou_dn: 0.7284  loss_ce_0: 7.672  loss_mask_0: 1.694  loss_dice_0: 2.761  loss_bbox_0: 4.058  loss_giou_0: 1.968  loss_ce_1: 2.138  loss_mask_1: 1.775  loss_dice_1: 2.606  loss_bbox_1: 3.229  loss_giou_1: 1.732  loss_ce_dn_1: 2.601  loss_mask_dn_1: 1.586  loss_dice_dn_1: 2.451  loss_bbox_dn_1: 0.995  loss_giou_dn_1: 0.7735  loss_ce_2: 1.797  loss_mask_2: 1.824  loss_dice_2: 2.544  loss_bbox_2: 2.933  loss_giou_2: 1.782  loss_ce_dn_2: 2.267  loss_mask_dn_2: 1.59  loss_dice_dn_2: 2.315  loss_bbox_dn_2: 0.9577  loss_giou_dn_2: 0.7503  loss_ce_3: 1.826  loss_mask_3: 1.717  loss_dice_3: 2.559  loss_bbox_3: 2.901  loss_giou_3: 1.774  loss_ce_dn_3: 2.149  loss_mask_dn_3: 1.573  loss_dice_dn_3: 2.387  loss_bbox_dn_3: 0.9071  loss_giou_dn_3: 0.7397  loss_ce_4: 1.843  loss_mask_4: 1.757  loss_dice_4: 2.547  loss_bbox_4: 2.891  loss_giou_4: 1.74  loss_ce_dn_4: 2.081  loss_mask_dn_4: 1.567  loss_dice_dn_4: 2.383  loss_bbox_dn_4: 0.8747  loss_giou_dn_4: 0.733  loss_ce_5: 1.85  loss_mask_5: 1.748  loss_dice_5: 2.539  loss_bbox_5: 2.936  loss_giou_5: 1.748  loss_ce_dn_5: 2.158  loss_mask_dn_5: 1.543  loss_dice_dn_5: 2.371  loss_bbox_dn_5: 0.86  loss_giou_dn_5: 0.7244  loss_ce_6: 1.857  loss_mask_6: 1.755  loss_dice_6: 2.521  loss_bbox_6: 2.876  loss_giou_6: 1.704  loss_ce_dn_6: 2.238  loss_mask_dn_6: 1.564  loss_dice_dn_6: 2.369  loss_bbox_dn_6: 0.8533  loss_giou_dn_6: 0.727  loss_ce_7: 1.894  loss_mask_7: 1.783  loss_dice_7: 2.561  loss_bbox_7: 2.833  loss_giou_7: 1.69  loss_ce_dn_7: 2.254  loss_mask_dn_7: 1.574  loss_dice_dn_7: 2.379  loss_bbox_dn_7: 0.8284  loss_giou_dn_7: 0.7269  loss_ce_8: 1.867  loss_mask_8: 1.752  loss_dice_8: 2.552  loss_bbox_8: 2.831  loss_giou_8: 1.686  loss_ce_dn_8: 2.337  loss_mask_dn_8: 1.586  loss_dice_dn_8: 2.382  loss_bbox_dn_8: 0.8304  loss_giou_dn_8: 0.7217    time: 2.2741  last_time: 2.2410  data_time: 0.0114  last_data_time: 0.0080   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:14:36 d2.utils.events]: \u001b[0m eta: 8 days, 6:56:54  iter: 3659  total_loss: 175.3  loss_ce: 1.658  loss_mask: 1.619  loss_dice: 2.287  loss_bbox: 2.85  loss_giou: 1.561  loss_ce_dn: 1.816  loss_mask_dn: 1.491  loss_dice_dn: 2.008  loss_bbox_dn: 0.8568  loss_giou_dn: 0.7114  loss_ce_0: 7.841  loss_mask_0: 1.669  loss_dice_0: 2.546  loss_bbox_0: 3.74  loss_giou_0: 1.907  loss_ce_1: 2.134  loss_mask_1: 1.53  loss_dice_1: 2.314  loss_bbox_1: 3.026  loss_giou_1: 1.669  loss_ce_dn_1: 2.581  loss_mask_dn_1: 1.458  loss_dice_dn_1: 2.235  loss_bbox_dn_1: 1.002  loss_giou_dn_1: 0.7687  loss_ce_2: 1.866  loss_mask_2: 1.581  loss_dice_2: 2.256  loss_bbox_2: 2.922  loss_giou_2: 1.589  loss_ce_dn_2: 2.013  loss_mask_dn_2: 1.498  loss_dice_dn_2: 2.094  loss_bbox_dn_2: 0.9516  loss_giou_dn_2: 0.7449  loss_ce_3: 1.895  loss_mask_3: 1.614  loss_dice_3: 2.262  loss_bbox_3: 2.893  loss_giou_3: 1.561  loss_ce_dn_3: 1.846  loss_mask_dn_3: 1.496  loss_dice_dn_3: 2.056  loss_bbox_dn_3: 0.9101  loss_giou_dn_3: 0.7324  loss_ce_4: 1.767  loss_mask_4: 1.651  loss_dice_4: 2.28  loss_bbox_4: 2.792  loss_giou_4: 1.562  loss_ce_dn_4: 1.759  loss_mask_dn_4: 1.454  loss_dice_dn_4: 2.041  loss_bbox_dn_4: 0.8733  loss_giou_dn_4: 0.7164  loss_ce_5: 1.645  loss_mask_5: 1.579  loss_dice_5: 2.276  loss_bbox_5: 2.817  loss_giou_5: 1.588  loss_ce_dn_5: 1.736  loss_mask_dn_5: 1.438  loss_dice_dn_5: 2.034  loss_bbox_dn_5: 0.8694  loss_giou_dn_5: 0.7117  loss_ce_6: 1.591  loss_mask_6: 1.59  loss_dice_6: 2.303  loss_bbox_6: 2.828  loss_giou_6: 1.595  loss_ce_dn_6: 1.75  loss_mask_dn_6: 1.462  loss_dice_dn_6: 2.023  loss_bbox_dn_6: 0.8686  loss_giou_dn_6: 0.7116  loss_ce_7: 1.627  loss_mask_7: 1.611  loss_dice_7: 2.296  loss_bbox_7: 2.828  loss_giou_7: 1.56  loss_ce_dn_7: 1.79  loss_mask_dn_7: 1.477  loss_dice_dn_7: 1.99  loss_bbox_dn_7: 0.856  loss_giou_dn_7: 0.7082  loss_ce_8: 1.681  loss_mask_8: 1.585  loss_dice_8: 2.312  loss_bbox_8: 2.84  loss_giou_8: 1.56  loss_ce_dn_8: 1.814  loss_mask_dn_8: 1.459  loss_dice_dn_8: 2.006  loss_bbox_dn_8: 0.8528  loss_giou_dn_8: 0.7097    time: 2.2740  last_time: 2.3211  data_time: 0.0127  last_data_time: 0.0161   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:15:22 d2.utils.events]: \u001b[0m eta: 8 days, 6:55:49  iter: 3679  total_loss: 170.8  loss_ce: 1.794  loss_mask: 1.446  loss_dice: 2.431  loss_bbox: 2.493  loss_giou: 1.595  loss_ce_dn: 2.237  loss_mask_dn: 1.238  loss_dice_dn: 2.024  loss_bbox_dn: 0.8031  loss_giou_dn: 0.7216  loss_ce_0: 7.708  loss_mask_0: 1.336  loss_dice_0: 2.6  loss_bbox_0: 3.929  loss_giou_0: 1.995  loss_ce_1: 2.401  loss_mask_1: 1.37  loss_dice_1: 2.495  loss_bbox_1: 3.145  loss_giou_1: 1.642  loss_ce_dn_1: 2.768  loss_mask_dn_1: 1.282  loss_dice_dn_1: 2.293  loss_bbox_dn_1: 0.9463  loss_giou_dn_1: 0.7784  loss_ce_2: 2.023  loss_mask_2: 1.445  loss_dice_2: 2.426  loss_bbox_2: 2.759  loss_giou_2: 1.653  loss_ce_dn_2: 2.273  loss_mask_dn_2: 1.262  loss_dice_dn_2: 2.071  loss_bbox_dn_2: 0.8929  loss_giou_dn_2: 0.7531  loss_ce_3: 1.859  loss_mask_3: 1.4  loss_dice_3: 2.424  loss_bbox_3: 2.668  loss_giou_3: 1.635  loss_ce_dn_3: 2.103  loss_mask_dn_3: 1.199  loss_dice_dn_3: 2.051  loss_bbox_dn_3: 0.8515  loss_giou_dn_3: 0.7337  loss_ce_4: 1.841  loss_mask_4: 1.398  loss_dice_4: 2.357  loss_bbox_4: 2.572  loss_giou_4: 1.642  loss_ce_dn_4: 2.042  loss_mask_dn_4: 1.165  loss_dice_dn_4: 1.999  loss_bbox_dn_4: 0.8147  loss_giou_dn_4: 0.7268  loss_ce_5: 1.792  loss_mask_5: 1.449  loss_dice_5: 2.351  loss_bbox_5: 2.553  loss_giou_5: 1.626  loss_ce_dn_5: 2.006  loss_mask_dn_5: 1.197  loss_dice_dn_5: 1.966  loss_bbox_dn_5: 0.8042  loss_giou_dn_5: 0.7251  loss_ce_6: 1.784  loss_mask_6: 1.487  loss_dice_6: 2.394  loss_bbox_6: 2.545  loss_giou_6: 1.616  loss_ce_dn_6: 2.061  loss_mask_dn_6: 1.204  loss_dice_dn_6: 1.964  loss_bbox_dn_6: 0.8086  loss_giou_dn_6: 0.7204  loss_ce_7: 1.827  loss_mask_7: 1.422  loss_dice_7: 2.391  loss_bbox_7: 2.51  loss_giou_7: 1.609  loss_ce_dn_7: 2.09  loss_mask_dn_7: 1.24  loss_dice_dn_7: 1.993  loss_bbox_dn_7: 0.8001  loss_giou_dn_7: 0.7176  loss_ce_8: 1.817  loss_mask_8: 1.434  loss_dice_8: 2.416  loss_bbox_8: 2.506  loss_giou_8: 1.601  loss_ce_dn_8: 2.151  loss_mask_dn_8: 1.202  loss_dice_dn_8: 1.989  loss_bbox_dn_8: 0.8044  loss_giou_dn_8: 0.7187    time: 2.2740  last_time: 2.2478  data_time: 0.0151  last_data_time: 0.0175   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:16:07 d2.utils.events]: \u001b[0m eta: 8 days, 6:53:15  iter: 3699  total_loss: 178.8  loss_ce: 1.662  loss_mask: 1.583  loss_dice: 2.422  loss_bbox: 2.684  loss_giou: 1.581  loss_ce_dn: 2.392  loss_mask_dn: 1.358  loss_dice_dn: 2.245  loss_bbox_dn: 0.8451  loss_giou_dn: 0.7085  loss_ce_0: 7.622  loss_mask_0: 1.597  loss_dice_0: 2.744  loss_bbox_0: 3.709  loss_giou_0: 1.971  loss_ce_1: 2.127  loss_mask_1: 1.56  loss_dice_1: 2.603  loss_bbox_1: 2.876  loss_giou_1: 1.676  loss_ce_dn_1: 2.756  loss_mask_dn_1: 1.469  loss_dice_dn_1: 2.41  loss_bbox_dn_1: 1.001  loss_giou_dn_1: 0.7772  loss_ce_2: 1.739  loss_mask_2: 1.594  loss_dice_2: 2.47  loss_bbox_2: 2.956  loss_giou_2: 1.621  loss_ce_dn_2: 2.28  loss_mask_dn_2: 1.408  loss_dice_dn_2: 2.186  loss_bbox_dn_2: 0.9437  loss_giou_dn_2: 0.7551  loss_ce_3: 1.715  loss_mask_3: 1.561  loss_dice_3: 2.442  loss_bbox_3: 2.988  loss_giou_3: 1.596  loss_ce_dn_3: 2.13  loss_mask_dn_3: 1.434  loss_dice_dn_3: 2.187  loss_bbox_dn_3: 0.899  loss_giou_dn_3: 0.7397  loss_ce_4: 1.677  loss_mask_4: 1.492  loss_dice_4: 2.445  loss_bbox_4: 2.919  loss_giou_4: 1.611  loss_ce_dn_4: 2.117  loss_mask_dn_4: 1.418  loss_dice_dn_4: 2.192  loss_bbox_dn_4: 0.878  loss_giou_dn_4: 0.7267  loss_ce_5: 1.646  loss_mask_5: 1.5  loss_dice_5: 2.457  loss_bbox_5: 2.792  loss_giou_5: 1.601  loss_ce_dn_5: 2.068  loss_mask_dn_5: 1.363  loss_dice_dn_5: 2.325  loss_bbox_dn_5: 0.8657  loss_giou_dn_5: 0.7098  loss_ce_6: 1.689  loss_mask_6: 1.541  loss_dice_6: 2.434  loss_bbox_6: 2.765  loss_giou_6: 1.6  loss_ce_dn_6: 2.165  loss_mask_dn_6: 1.366  loss_dice_dn_6: 2.242  loss_bbox_dn_6: 0.8594  loss_giou_dn_6: 0.7079  loss_ce_7: 1.676  loss_mask_7: 1.535  loss_dice_7: 2.467  loss_bbox_7: 2.734  loss_giou_7: 1.594  loss_ce_dn_7: 2.108  loss_mask_dn_7: 1.363  loss_dice_dn_7: 2.289  loss_bbox_dn_7: 0.848  loss_giou_dn_7: 0.7047  loss_ce_8: 1.608  loss_mask_8: 1.565  loss_dice_8: 2.446  loss_bbox_8: 2.739  loss_giou_8: 1.583  loss_ce_dn_8: 2.122  loss_mask_dn_8: 1.378  loss_dice_dn_8: 2.249  loss_bbox_dn_8: 0.846  loss_giou_dn_8: 0.7067    time: 2.2740  last_time: 2.2392  data_time: 0.0123  last_data_time: 0.0071   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:16:52 d2.utils.events]: \u001b[0m eta: 8 days, 6:52:49  iter: 3719  total_loss: 167.6  loss_ce: 1.47  loss_mask: 1.371  loss_dice: 2.264  loss_bbox: 2.156  loss_giou: 1.311  loss_ce_dn: 2.003  loss_mask_dn: 1.276  loss_dice_dn: 2.052  loss_bbox_dn: 0.7852  loss_giou_dn: 0.6604  loss_ce_0: 7.217  loss_mask_0: 1.365  loss_dice_0: 2.431  loss_bbox_0: 3.855  loss_giou_0: 1.877  loss_ce_1: 2.056  loss_mask_1: 1.358  loss_dice_1: 2.263  loss_bbox_1: 2.695  loss_giou_1: 1.526  loss_ce_dn_1: 2.302  loss_mask_dn_1: 1.412  loss_dice_dn_1: 2.285  loss_bbox_dn_1: 0.9974  loss_giou_dn_1: 0.7542  loss_ce_2: 1.609  loss_mask_2: 1.388  loss_dice_2: 2.243  loss_bbox_2: 2.342  loss_giou_2: 1.425  loss_ce_dn_2: 1.909  loss_mask_dn_2: 1.362  loss_dice_dn_2: 2.166  loss_bbox_dn_2: 0.9076  loss_giou_dn_2: 0.7114  loss_ce_3: 1.532  loss_mask_3: 1.381  loss_dice_3: 2.249  loss_bbox_3: 2.241  loss_giou_3: 1.364  loss_ce_dn_3: 1.805  loss_mask_dn_3: 1.333  loss_dice_dn_3: 2.1  loss_bbox_dn_3: 0.8486  loss_giou_dn_3: 0.6876  loss_ce_4: 1.575  loss_mask_4: 1.406  loss_dice_4: 2.302  loss_bbox_4: 2.168  loss_giou_4: 1.346  loss_ce_dn_4: 1.771  loss_mask_dn_4: 1.321  loss_dice_dn_4: 2.07  loss_bbox_dn_4: 0.8052  loss_giou_dn_4: 0.675  loss_ce_5: 1.534  loss_mask_5: 1.36  loss_dice_5: 2.283  loss_bbox_5: 2.145  loss_giou_5: 1.336  loss_ce_dn_5: 1.711  loss_mask_dn_5: 1.294  loss_dice_dn_5: 2.071  loss_bbox_dn_5: 0.7958  loss_giou_dn_5: 0.6727  loss_ce_6: 1.478  loss_mask_6: 1.368  loss_dice_6: 2.289  loss_bbox_6: 2.137  loss_giou_6: 1.34  loss_ce_dn_6: 1.877  loss_mask_dn_6: 1.269  loss_dice_dn_6: 2.02  loss_bbox_dn_6: 0.7936  loss_giou_dn_6: 0.6718  loss_ce_7: 1.471  loss_mask_7: 1.357  loss_dice_7: 2.287  loss_bbox_7: 2.159  loss_giou_7: 1.315  loss_ce_dn_7: 1.893  loss_mask_dn_7: 1.278  loss_dice_dn_7: 2.017  loss_bbox_dn_7: 0.7766  loss_giou_dn_7: 0.6622  loss_ce_8: 1.405  loss_mask_8: 1.331  loss_dice_8: 2.273  loss_bbox_8: 2.155  loss_giou_8: 1.307  loss_ce_dn_8: 1.917  loss_mask_dn_8: 1.258  loss_dice_dn_8: 2.042  loss_bbox_dn_8: 0.7807  loss_giou_dn_8: 0.6617    time: 2.2740  last_time: 2.3082  data_time: 0.0123  last_data_time: 0.0361   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:17:38 d2.utils.events]: \u001b[0m eta: 8 days, 6:50:32  iter: 3739  total_loss: 164.2  loss_ce: 1.373  loss_mask: 1.578  loss_dice: 2.208  loss_bbox: 2.556  loss_giou: 1.408  loss_ce_dn: 2.533  loss_mask_dn: 1.399  loss_dice_dn: 1.993  loss_bbox_dn: 0.8291  loss_giou_dn: 0.6485  loss_ce_0: 7.389  loss_mask_0: 1.309  loss_dice_0: 2.448  loss_bbox_0: 4.02  loss_giou_0: 1.847  loss_ce_1: 1.69  loss_mask_1: 1.6  loss_dice_1: 2.249  loss_bbox_1: 2.861  loss_giou_1: 1.487  loss_ce_dn_1: 2.589  loss_mask_dn_1: 1.438  loss_dice_dn_1: 2.175  loss_bbox_dn_1: 0.9829  loss_giou_dn_1: 0.7469  loss_ce_2: 1.457  loss_mask_2: 1.606  loss_dice_2: 2.257  loss_bbox_2: 2.654  loss_giou_2: 1.468  loss_ce_dn_2: 2.176  loss_mask_dn_2: 1.445  loss_dice_dn_2: 2.08  loss_bbox_dn_2: 0.9336  loss_giou_dn_2: 0.699  loss_ce_3: 1.386  loss_mask_3: 1.558  loss_dice_3: 2.27  loss_bbox_3: 2.536  loss_giou_3: 1.489  loss_ce_dn_3: 2.143  loss_mask_dn_3: 1.424  loss_dice_dn_3: 2.051  loss_bbox_dn_3: 0.8578  loss_giou_dn_3: 0.6789  loss_ce_4: 1.348  loss_mask_4: 1.627  loss_dice_4: 2.277  loss_bbox_4: 2.533  loss_giou_4: 1.44  loss_ce_dn_4: 2.121  loss_mask_dn_4: 1.441  loss_dice_dn_4: 2.002  loss_bbox_dn_4: 0.8399  loss_giou_dn_4: 0.6728  loss_ce_5: 1.362  loss_mask_5: 1.516  loss_dice_5: 2.287  loss_bbox_5: 2.595  loss_giou_5: 1.436  loss_ce_dn_5: 2.264  loss_mask_dn_5: 1.363  loss_dice_dn_5: 2.001  loss_bbox_dn_5: 0.8306  loss_giou_dn_5: 0.6688  loss_ce_6: 1.405  loss_mask_6: 1.527  loss_dice_6: 2.18  loss_bbox_6: 2.592  loss_giou_6: 1.422  loss_ce_dn_6: 2.23  loss_mask_dn_6: 1.362  loss_dice_dn_6: 1.974  loss_bbox_dn_6: 0.8322  loss_giou_dn_6: 0.6711  loss_ce_7: 1.399  loss_mask_7: 1.559  loss_dice_7: 2.232  loss_bbox_7: 2.567  loss_giou_7: 1.401  loss_ce_dn_7: 2.368  loss_mask_dn_7: 1.393  loss_dice_dn_7: 2.005  loss_bbox_dn_7: 0.8074  loss_giou_dn_7: 0.6463  loss_ce_8: 1.392  loss_mask_8: 1.602  loss_dice_8: 2.207  loss_bbox_8: 2.565  loss_giou_8: 1.406  loss_ce_dn_8: 2.469  loss_mask_dn_8: 1.41  loss_dice_dn_8: 1.962  loss_bbox_dn_8: 0.8182  loss_giou_dn_8: 0.6452    time: 2.2739  last_time: 2.2264  data_time: 0.0108  last_data_time: 0.0013   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:18:24 d2.utils.events]: \u001b[0m eta: 8 days, 6:49:47  iter: 3759  total_loss: 163.9  loss_ce: 1.565  loss_mask: 1.276  loss_dice: 2.242  loss_bbox: 2.607  loss_giou: 1.453  loss_ce_dn: 1.805  loss_mask_dn: 1.192  loss_dice_dn: 2.07  loss_bbox_dn: 0.8296  loss_giou_dn: 0.6989  loss_ce_0: 7.182  loss_mask_0: 1.313  loss_dice_0: 2.681  loss_bbox_0: 3.966  loss_giou_0: 1.937  loss_ce_1: 1.928  loss_mask_1: 1.336  loss_dice_1: 2.458  loss_bbox_1: 3.032  loss_giou_1: 1.599  loss_ce_dn_1: 2.43  loss_mask_dn_1: 1.35  loss_dice_dn_1: 2.251  loss_bbox_dn_1: 0.9753  loss_giou_dn_1: 0.7786  loss_ce_2: 1.61  loss_mask_2: 1.296  loss_dice_2: 2.279  loss_bbox_2: 2.837  loss_giou_2: 1.57  loss_ce_dn_2: 1.962  loss_mask_dn_2: 1.254  loss_dice_dn_2: 2.086  loss_bbox_dn_2: 0.9377  loss_giou_dn_2: 0.7581  loss_ce_3: 1.545  loss_mask_3: 1.328  loss_dice_3: 2.253  loss_bbox_3: 2.732  loss_giou_3: 1.553  loss_ce_dn_3: 1.877  loss_mask_dn_3: 1.238  loss_dice_dn_3: 2.05  loss_bbox_dn_3: 0.8956  loss_giou_dn_3: 0.7374  loss_ce_4: 1.522  loss_mask_4: 1.324  loss_dice_4: 2.255  loss_bbox_4: 2.695  loss_giou_4: 1.527  loss_ce_dn_4: 1.793  loss_mask_dn_4: 1.285  loss_dice_dn_4: 2.021  loss_bbox_dn_4: 0.8617  loss_giou_dn_4: 0.7165  loss_ce_5: 1.55  loss_mask_5: 1.301  loss_dice_5: 2.274  loss_bbox_5: 2.64  loss_giou_5: 1.496  loss_ce_dn_5: 1.736  loss_mask_dn_5: 1.221  loss_dice_dn_5: 2.061  loss_bbox_dn_5: 0.8531  loss_giou_dn_5: 0.708  loss_ce_6: 1.553  loss_mask_6: 1.31  loss_dice_6: 2.294  loss_bbox_6: 2.626  loss_giou_6: 1.49  loss_ce_dn_6: 1.774  loss_mask_dn_6: 1.205  loss_dice_dn_6: 2.081  loss_bbox_dn_6: 0.8481  loss_giou_dn_6: 0.7078  loss_ce_7: 1.577  loss_mask_7: 1.291  loss_dice_7: 2.24  loss_bbox_7: 2.62  loss_giou_7: 1.451  loss_ce_dn_7: 1.771  loss_mask_dn_7: 1.209  loss_dice_dn_7: 2.046  loss_bbox_dn_7: 0.825  loss_giou_dn_7: 0.6967  loss_ce_8: 1.594  loss_mask_8: 1.318  loss_dice_8: 2.227  loss_bbox_8: 2.611  loss_giou_8: 1.45  loss_ce_dn_8: 1.76  loss_mask_dn_8: 1.228  loss_dice_dn_8: 2.074  loss_bbox_dn_8: 0.8273  loss_giou_dn_8: 0.6964    time: 2.2740  last_time: 2.2618  data_time: 0.0145  last_data_time: 0.0321   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:19:09 d2.utils.events]: \u001b[0m eta: 8 days, 6:49:30  iter: 3779  total_loss: 174.2  loss_ce: 1.647  loss_mask: 1.463  loss_dice: 2.348  loss_bbox: 2.716  loss_giou: 1.584  loss_ce_dn: 2.193  loss_mask_dn: 1.247  loss_dice_dn: 2.021  loss_bbox_dn: 0.7798  loss_giou_dn: 0.6821  loss_ce_0: 7.064  loss_mask_0: 1.446  loss_dice_0: 2.59  loss_bbox_0: 4.424  loss_giou_0: 1.994  loss_ce_1: 1.829  loss_mask_1: 1.446  loss_dice_1: 2.374  loss_bbox_1: 3.239  loss_giou_1: 1.643  loss_ce_dn_1: 2.52  loss_mask_dn_1: 1.38  loss_dice_dn_1: 2.281  loss_bbox_dn_1: 0.9352  loss_giou_dn_1: 0.7758  loss_ce_2: 1.705  loss_mask_2: 1.518  loss_dice_2: 2.342  loss_bbox_2: 2.857  loss_giou_2: 1.686  loss_ce_dn_2: 2.128  loss_mask_dn_2: 1.308  loss_dice_dn_2: 2.09  loss_bbox_dn_2: 0.8737  loss_giou_dn_2: 0.7478  loss_ce_3: 1.516  loss_mask_3: 1.466  loss_dice_3: 2.32  loss_bbox_3: 2.711  loss_giou_3: 1.686  loss_ce_dn_3: 2.047  loss_mask_dn_3: 1.288  loss_dice_dn_3: 2.047  loss_bbox_dn_3: 0.8431  loss_giou_dn_3: 0.7317  loss_ce_4: 1.519  loss_mask_4: 1.478  loss_dice_4: 2.326  loss_bbox_4: 2.672  loss_giou_4: 1.601  loss_ce_dn_4: 1.915  loss_mask_dn_4: 1.28  loss_dice_dn_4: 2.045  loss_bbox_dn_4: 0.8238  loss_giou_dn_4: 0.713  loss_ce_5: 1.475  loss_mask_5: 1.457  loss_dice_5: 2.319  loss_bbox_5: 2.724  loss_giou_5: 1.565  loss_ce_dn_5: 1.967  loss_mask_dn_5: 1.257  loss_dice_dn_5: 2.044  loss_bbox_dn_5: 0.8056  loss_giou_dn_5: 0.6988  loss_ce_6: 1.514  loss_mask_6: 1.425  loss_dice_6: 2.337  loss_bbox_6: 2.709  loss_giou_6: 1.545  loss_ce_dn_6: 1.932  loss_mask_dn_6: 1.255  loss_dice_dn_6: 2.046  loss_bbox_dn_6: 0.7992  loss_giou_dn_6: 0.6924  loss_ce_7: 1.587  loss_mask_7: 1.414  loss_dice_7: 2.377  loss_bbox_7: 2.717  loss_giou_7: 1.538  loss_ce_dn_7: 2.028  loss_mask_dn_7: 1.238  loss_dice_dn_7: 2.077  loss_bbox_dn_7: 0.7658  loss_giou_dn_7: 0.6816  loss_ce_8: 1.595  loss_mask_8: 1.429  loss_dice_8: 2.339  loss_bbox_8: 2.715  loss_giou_8: 1.592  loss_ce_dn_8: 1.989  loss_mask_dn_8: 1.282  loss_dice_dn_8: 2.061  loss_bbox_dn_8: 0.7722  loss_giou_dn_8: 0.6804    time: 2.2740  last_time: 2.2542  data_time: 0.0120  last_data_time: 0.0044   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:19:55 d2.utils.events]: \u001b[0m eta: 8 days, 6:48:44  iter: 3799  total_loss: 175.9  loss_ce: 1.564  loss_mask: 1.588  loss_dice: 2.665  loss_bbox: 2.688  loss_giou: 1.599  loss_ce_dn: 1.965  loss_mask_dn: 1.451  loss_dice_dn: 2.435  loss_bbox_dn: 0.871  loss_giou_dn: 0.7176  loss_ce_0: 7.113  loss_mask_0: 1.532  loss_dice_0: 2.721  loss_bbox_0: 4.037  loss_giou_0: 1.971  loss_ce_1: 1.936  loss_mask_1: 1.583  loss_dice_1: 2.691  loss_bbox_1: 2.867  loss_giou_1: 1.694  loss_ce_dn_1: 2.63  loss_mask_dn_1: 1.476  loss_dice_dn_1: 2.609  loss_bbox_dn_1: 0.9787  loss_giou_dn_1: 0.7623  loss_ce_2: 1.733  loss_mask_2: 1.574  loss_dice_2: 2.772  loss_bbox_2: 2.729  loss_giou_2: 1.675  loss_ce_dn_2: 2.211  loss_mask_dn_2: 1.494  loss_dice_dn_2: 2.452  loss_bbox_dn_2: 0.9438  loss_giou_dn_2: 0.7359  loss_ce_3: 1.642  loss_mask_3: 1.62  loss_dice_3: 2.729  loss_bbox_3: 2.65  loss_giou_3: 1.699  loss_ce_dn_3: 2.005  loss_mask_dn_3: 1.487  loss_dice_dn_3: 2.442  loss_bbox_dn_3: 0.9062  loss_giou_dn_3: 0.7244  loss_ce_4: 1.544  loss_mask_4: 1.612  loss_dice_4: 2.725  loss_bbox_4: 2.681  loss_giou_4: 1.673  loss_ce_dn_4: 1.962  loss_mask_dn_4: 1.471  loss_dice_dn_4: 2.446  loss_bbox_dn_4: 0.8864  loss_giou_dn_4: 0.7145  loss_ce_5: 1.602  loss_mask_5: 1.583  loss_dice_5: 2.6  loss_bbox_5: 2.754  loss_giou_5: 1.7  loss_ce_dn_5: 1.928  loss_mask_dn_5: 1.45  loss_dice_dn_5: 2.411  loss_bbox_dn_5: 0.8711  loss_giou_dn_5: 0.7152  loss_ce_6: 1.562  loss_mask_6: 1.622  loss_dice_6: 2.651  loss_bbox_6: 2.68  loss_giou_6: 1.651  loss_ce_dn_6: 1.874  loss_mask_dn_6: 1.463  loss_dice_dn_6: 2.398  loss_bbox_dn_6: 0.8817  loss_giou_dn_6: 0.7158  loss_ce_7: 1.648  loss_mask_7: 1.642  loss_dice_7: 2.587  loss_bbox_7: 2.669  loss_giou_7: 1.606  loss_ce_dn_7: 1.884  loss_mask_dn_7: 1.463  loss_dice_dn_7: 2.42  loss_bbox_dn_7: 0.8647  loss_giou_dn_7: 0.7106  loss_ce_8: 1.575  loss_mask_8: 1.602  loss_dice_8: 2.61  loss_bbox_8: 2.676  loss_giou_8: 1.608  loss_ce_dn_8: 1.937  loss_mask_dn_8: 1.444  loss_dice_dn_8: 2.422  loss_bbox_dn_8: 0.871  loss_giou_dn_8: 0.7126    time: 2.2740  last_time: 2.3595  data_time: 0.0123  last_data_time: 0.0196   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:20:40 d2.utils.events]: \u001b[0m eta: 8 days, 6:47:21  iter: 3819  total_loss: 187.1  loss_ce: 1.76  loss_mask: 1.468  loss_dice: 2.387  loss_bbox: 2.428  loss_giou: 1.35  loss_ce_dn: 2.452  loss_mask_dn: 1.506  loss_dice_dn: 2.156  loss_bbox_dn: 0.7677  loss_giou_dn: 0.6645  loss_ce_0: 7.249  loss_mask_0: 1.422  loss_dice_0: 2.614  loss_bbox_0: 4.085  loss_giou_0: 1.911  loss_ce_1: 1.936  loss_mask_1: 1.439  loss_dice_1: 2.441  loss_bbox_1: 2.893  loss_giou_1: 1.583  loss_ce_dn_1: 2.976  loss_mask_dn_1: 1.612  loss_dice_dn_1: 2.328  loss_bbox_dn_1: 0.9527  loss_giou_dn_1: 0.7287  loss_ce_2: 1.736  loss_mask_2: 1.401  loss_dice_2: 2.318  loss_bbox_2: 2.752  loss_giou_2: 1.519  loss_ce_dn_2: 2.427  loss_mask_dn_2: 1.574  loss_dice_dn_2: 2.241  loss_bbox_dn_2: 0.8808  loss_giou_dn_2: 0.7056  loss_ce_3: 1.768  loss_mask_3: 1.36  loss_dice_3: 2.324  loss_bbox_3: 2.702  loss_giou_3: 1.438  loss_ce_dn_3: 2.236  loss_mask_dn_3: 1.489  loss_dice_dn_3: 2.24  loss_bbox_dn_3: 0.8068  loss_giou_dn_3: 0.6873  loss_ce_4: 1.736  loss_mask_4: 1.476  loss_dice_4: 2.314  loss_bbox_4: 2.581  loss_giou_4: 1.397  loss_ce_dn_4: 2.347  loss_mask_dn_4: 1.482  loss_dice_dn_4: 2.256  loss_bbox_dn_4: 0.7619  loss_giou_dn_4: 0.6732  loss_ce_5: 1.718  loss_mask_5: 1.473  loss_dice_5: 2.323  loss_bbox_5: 2.455  loss_giou_5: 1.353  loss_ce_dn_5: 2.425  loss_mask_dn_5: 1.486  loss_dice_dn_5: 2.222  loss_bbox_dn_5: 0.7635  loss_giou_dn_5: 0.6684  loss_ce_6: 1.668  loss_mask_6: 1.444  loss_dice_6: 2.301  loss_bbox_6: 2.526  loss_giou_6: 1.35  loss_ce_dn_6: 2.499  loss_mask_dn_6: 1.459  loss_dice_dn_6: 2.19  loss_bbox_dn_6: 0.7661  loss_giou_dn_6: 0.6691  loss_ce_7: 1.721  loss_mask_7: 1.474  loss_dice_7: 2.384  loss_bbox_7: 2.468  loss_giou_7: 1.334  loss_ce_dn_7: 2.532  loss_mask_dn_7: 1.5  loss_dice_dn_7: 2.152  loss_bbox_dn_7: 0.7635  loss_giou_dn_7: 0.6586  loss_ce_8: 1.666  loss_mask_8: 1.429  loss_dice_8: 2.339  loss_bbox_8: 2.472  loss_giou_8: 1.344  loss_ce_dn_8: 2.465  loss_mask_dn_8: 1.5  loss_dice_dn_8: 2.171  loss_bbox_dn_8: 0.765  loss_giou_dn_8: 0.6602    time: 2.2739  last_time: 2.2284  data_time: 0.0129  last_data_time: 0.0016   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:21:25 d2.utils.events]: \u001b[0m eta: 8 days, 6:46:13  iter: 3839  total_loss: 169  loss_ce: 2.134  loss_mask: 1.319  loss_dice: 2.345  loss_bbox: 2.525  loss_giou: 1.543  loss_ce_dn: 2.066  loss_mask_dn: 1.253  loss_dice_dn: 2.116  loss_bbox_dn: 0.7802  loss_giou_dn: 0.7185  loss_ce_0: 7.441  loss_mask_0: 1.331  loss_dice_0: 2.602  loss_bbox_0: 3.73  loss_giou_0: 2.015  loss_ce_1: 2.425  loss_mask_1: 1.373  loss_dice_1: 2.435  loss_bbox_1: 2.726  loss_giou_1: 1.693  loss_ce_dn_1: 2.476  loss_mask_dn_1: 1.336  loss_dice_dn_1: 2.365  loss_bbox_dn_1: 0.9205  loss_giou_dn_1: 0.7774  loss_ce_2: 2.166  loss_mask_2: 1.31  loss_dice_2: 2.341  loss_bbox_2: 2.611  loss_giou_2: 1.672  loss_ce_dn_2: 1.907  loss_mask_dn_2: 1.274  loss_dice_dn_2: 2.165  loss_bbox_dn_2: 0.8512  loss_giou_dn_2: 0.7457  loss_ce_3: 2.162  loss_mask_3: 1.391  loss_dice_3: 2.335  loss_bbox_3: 2.563  loss_giou_3: 1.645  loss_ce_dn_3: 1.812  loss_mask_dn_3: 1.271  loss_dice_dn_3: 2.086  loss_bbox_dn_3: 0.8226  loss_giou_dn_3: 0.7433  loss_ce_4: 2.15  loss_mask_4: 1.343  loss_dice_4: 2.327  loss_bbox_4: 2.51  loss_giou_4: 1.576  loss_ce_dn_4: 1.744  loss_mask_dn_4: 1.297  loss_dice_dn_4: 2.095  loss_bbox_dn_4: 0.7973  loss_giou_dn_4: 0.7397  loss_ce_5: 2.167  loss_mask_5: 1.344  loss_dice_5: 2.321  loss_bbox_5: 2.513  loss_giou_5: 1.555  loss_ce_dn_5: 1.768  loss_mask_dn_5: 1.274  loss_dice_dn_5: 2.063  loss_bbox_dn_5: 0.7911  loss_giou_dn_5: 0.7316  loss_ce_6: 2.186  loss_mask_6: 1.353  loss_dice_6: 2.362  loss_bbox_6: 2.505  loss_giou_6: 1.53  loss_ce_dn_6: 1.803  loss_mask_dn_6: 1.266  loss_dice_dn_6: 2.042  loss_bbox_dn_6: 0.7887  loss_giou_dn_6: 0.7282  loss_ce_7: 2.236  loss_mask_7: 1.327  loss_dice_7: 2.376  loss_bbox_7: 2.496  loss_giou_7: 1.551  loss_ce_dn_7: 1.83  loss_mask_dn_7: 1.269  loss_dice_dn_7: 2.075  loss_bbox_dn_7: 0.7785  loss_giou_dn_7: 0.7192  loss_ce_8: 2.12  loss_mask_8: 1.307  loss_dice_8: 2.35  loss_bbox_8: 2.523  loss_giou_8: 1.541  loss_ce_dn_8: 1.981  loss_mask_dn_8: 1.275  loss_dice_dn_8: 2.103  loss_bbox_dn_8: 0.7792  loss_giou_dn_8: 0.719    time: 2.2739  last_time: 2.2072  data_time: 0.0141  last_data_time: 0.0045   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:22:11 d2.utils.events]: \u001b[0m eta: 8 days, 6:43:39  iter: 3859  total_loss: 186.1  loss_ce: 1.654  loss_mask: 1.589  loss_dice: 2.223  loss_bbox: 2.45  loss_giou: 1.462  loss_ce_dn: 2.52  loss_mask_dn: 1.523  loss_dice_dn: 2.02  loss_bbox_dn: 0.898  loss_giou_dn: 0.669  loss_ce_0: 7.197  loss_mask_0: 1.377  loss_dice_0: 2.312  loss_bbox_0: 3.966  loss_giou_0: 1.792  loss_ce_1: 2.159  loss_mask_1: 1.617  loss_dice_1: 2.29  loss_bbox_1: 2.718  loss_giou_1: 1.635  loss_ce_dn_1: 2.97  loss_mask_dn_1: 1.499  loss_dice_dn_1: 2.06  loss_bbox_dn_1: 0.9873  loss_giou_dn_1: 0.7467  loss_ce_2: 1.847  loss_mask_2: 1.58  loss_dice_2: 2.232  loss_bbox_2: 2.681  loss_giou_2: 1.518  loss_ce_dn_2: 2.467  loss_mask_dn_2: 1.57  loss_dice_dn_2: 1.961  loss_bbox_dn_2: 0.9672  loss_giou_dn_2: 0.7186  loss_ce_3: 1.712  loss_mask_3: 1.591  loss_dice_3: 2.225  loss_bbox_3: 2.624  loss_giou_3: 1.499  loss_ce_dn_3: 2.383  loss_mask_dn_3: 1.545  loss_dice_dn_3: 1.964  loss_bbox_dn_3: 0.9528  loss_giou_dn_3: 0.7025  loss_ce_4: 1.704  loss_mask_4: 1.588  loss_dice_4: 2.183  loss_bbox_4: 2.502  loss_giou_4: 1.467  loss_ce_dn_4: 2.449  loss_mask_dn_4: 1.512  loss_dice_dn_4: 1.996  loss_bbox_dn_4: 0.9456  loss_giou_dn_4: 0.6945  loss_ce_5: 1.74  loss_mask_5: 1.592  loss_dice_5: 2.18  loss_bbox_5: 2.505  loss_giou_5: 1.46  loss_ce_dn_5: 2.475  loss_mask_dn_5: 1.458  loss_dice_dn_5: 1.988  loss_bbox_dn_5: 0.924  loss_giou_dn_5: 0.6813  loss_ce_6: 1.705  loss_mask_6: 1.623  loss_dice_6: 2.178  loss_bbox_6: 2.445  loss_giou_6: 1.465  loss_ce_dn_6: 2.352  loss_mask_dn_6: 1.446  loss_dice_dn_6: 2.02  loss_bbox_dn_6: 0.9124  loss_giou_dn_6: 0.6807  loss_ce_7: 1.701  loss_mask_7: 1.619  loss_dice_7: 2.176  loss_bbox_7: 2.455  loss_giou_7: 1.464  loss_ce_dn_7: 2.298  loss_mask_dn_7: 1.465  loss_dice_dn_7: 2.041  loss_bbox_dn_7: 0.8959  loss_giou_dn_7: 0.6641  loss_ce_8: 1.596  loss_mask_8: 1.591  loss_dice_8: 2.196  loss_bbox_8: 2.455  loss_giou_8: 1.463  loss_ce_dn_8: 2.422  loss_mask_dn_8: 1.508  loss_dice_dn_8: 1.962  loss_bbox_dn_8: 0.8922  loss_giou_dn_8: 0.6653    time: 2.2739  last_time: 2.2703  data_time: 0.0134  last_data_time: 0.0228   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:22:56 d2.utils.events]: \u001b[0m eta: 8 days, 6:40:49  iter: 3879  total_loss: 174.7  loss_ce: 1.583  loss_mask: 1.762  loss_dice: 2.314  loss_bbox: 2.654  loss_giou: 1.44  loss_ce_dn: 2.343  loss_mask_dn: 1.517  loss_dice_dn: 2.048  loss_bbox_dn: 0.8754  loss_giou_dn: 0.7046  loss_ce_0: 7.464  loss_mask_0: 1.73  loss_dice_0: 2.672  loss_bbox_0: 4.068  loss_giou_0: 1.901  loss_ce_1: 1.966  loss_mask_1: 1.715  loss_dice_1: 2.5  loss_bbox_1: 2.985  loss_giou_1: 1.591  loss_ce_dn_1: 2.767  loss_mask_dn_1: 1.571  loss_dice_dn_1: 2.317  loss_bbox_dn_1: 1.005  loss_giou_dn_1: 0.7594  loss_ce_2: 1.74  loss_mask_2: 1.716  loss_dice_2: 2.377  loss_bbox_2: 2.66  loss_giou_2: 1.572  loss_ce_dn_2: 2.364  loss_mask_dn_2: 1.547  loss_dice_dn_2: 2.086  loss_bbox_dn_2: 0.9537  loss_giou_dn_2: 0.7363  loss_ce_3: 1.656  loss_mask_3: 1.885  loss_dice_3: 2.328  loss_bbox_3: 2.641  loss_giou_3: 1.557  loss_ce_dn_3: 2.142  loss_mask_dn_3: 1.543  loss_dice_dn_3: 2.065  loss_bbox_dn_3: 0.9205  loss_giou_dn_3: 0.7182  loss_ce_4: 1.631  loss_mask_4: 1.853  loss_dice_4: 2.377  loss_bbox_4: 2.707  loss_giou_4: 1.513  loss_ce_dn_4: 2.19  loss_mask_dn_4: 1.499  loss_dice_dn_4: 1.999  loss_bbox_dn_4: 0.9357  loss_giou_dn_4: 0.7103  loss_ce_5: 1.508  loss_mask_5: 1.903  loss_dice_5: 2.372  loss_bbox_5: 2.678  loss_giou_5: 1.461  loss_ce_dn_5: 2.139  loss_mask_dn_5: 1.525  loss_dice_dn_5: 2.02  loss_bbox_dn_5: 0.9361  loss_giou_dn_5: 0.701  loss_ce_6: 1.472  loss_mask_6: 1.807  loss_dice_6: 2.345  loss_bbox_6: 2.702  loss_giou_6: 1.438  loss_ce_dn_6: 2.097  loss_mask_dn_6: 1.516  loss_dice_dn_6: 2.002  loss_bbox_dn_6: 0.9139  loss_giou_dn_6: 0.696  loss_ce_7: 1.5  loss_mask_7: 1.779  loss_dice_7: 2.346  loss_bbox_7: 2.657  loss_giou_7: 1.438  loss_ce_dn_7: 2.217  loss_mask_dn_7: 1.498  loss_dice_dn_7: 2.017  loss_bbox_dn_7: 0.8673  loss_giou_dn_7: 0.6881  loss_ce_8: 1.553  loss_mask_8: 1.743  loss_dice_8: 2.341  loss_bbox_8: 2.648  loss_giou_8: 1.431  loss_ce_dn_8: 2.329  loss_mask_dn_8: 1.482  loss_dice_dn_8: 1.976  loss_bbox_dn_8: 0.8697  loss_giou_dn_8: 0.6938    time: 2.2738  last_time: 2.2411  data_time: 0.0139  last_data_time: 0.0166   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:23:41 d2.utils.events]: \u001b[0m eta: 8 days, 6:39:57  iter: 3899  total_loss: 173.6  loss_ce: 1.998  loss_mask: 1.439  loss_dice: 2.499  loss_bbox: 2.688  loss_giou: 1.544  loss_ce_dn: 2.531  loss_mask_dn: 1.212  loss_dice_dn: 2.148  loss_bbox_dn: 0.7929  loss_giou_dn: 0.7078  loss_ce_0: 7.025  loss_mask_0: 1.338  loss_dice_0: 2.778  loss_bbox_0: 4.063  loss_giou_0: 2.004  loss_ce_1: 2.252  loss_mask_1: 1.409  loss_dice_1: 2.45  loss_bbox_1: 3.023  loss_giou_1: 1.7  loss_ce_dn_1: 2.828  loss_mask_dn_1: 1.273  loss_dice_dn_1: 2.366  loss_bbox_dn_1: 0.9213  loss_giou_dn_1: 0.7685  loss_ce_2: 2.045  loss_mask_2: 1.457  loss_dice_2: 2.467  loss_bbox_2: 2.924  loss_giou_2: 1.641  loss_ce_dn_2: 2.35  loss_mask_dn_2: 1.24  loss_dice_dn_2: 2.253  loss_bbox_dn_2: 0.8665  loss_giou_dn_2: 0.7445  loss_ce_3: 2.121  loss_mask_3: 1.46  loss_dice_3: 2.429  loss_bbox_3: 2.879  loss_giou_3: 1.605  loss_ce_dn_3: 2.251  loss_mask_dn_3: 1.211  loss_dice_dn_3: 2.18  loss_bbox_dn_3: 0.8405  loss_giou_dn_3: 0.7266  loss_ce_4: 2.063  loss_mask_4: 1.452  loss_dice_4: 2.409  loss_bbox_4: 2.819  loss_giou_4: 1.583  loss_ce_dn_4: 2.213  loss_mask_dn_4: 1.197  loss_dice_dn_4: 2.161  loss_bbox_dn_4: 0.8237  loss_giou_dn_4: 0.7191  loss_ce_5: 1.956  loss_mask_5: 1.454  loss_dice_5: 2.446  loss_bbox_5: 2.742  loss_giou_5: 1.559  loss_ce_dn_5: 2.219  loss_mask_dn_5: 1.209  loss_dice_dn_5: 2.144  loss_bbox_dn_5: 0.8098  loss_giou_dn_5: 0.712  loss_ce_6: 1.853  loss_mask_6: 1.476  loss_dice_6: 2.45  loss_bbox_6: 2.725  loss_giou_6: 1.573  loss_ce_dn_6: 2.26  loss_mask_dn_6: 1.166  loss_dice_dn_6: 2.148  loss_bbox_dn_6: 0.8059  loss_giou_dn_6: 0.7133  loss_ce_7: 1.903  loss_mask_7: 1.418  loss_dice_7: 2.457  loss_bbox_7: 2.729  loss_giou_7: 1.576  loss_ce_dn_7: 2.383  loss_mask_dn_7: 1.198  loss_dice_dn_7: 2.162  loss_bbox_dn_7: 0.7954  loss_giou_dn_7: 0.7062  loss_ce_8: 1.977  loss_mask_8: 1.456  loss_dice_8: 2.46  loss_bbox_8: 2.691  loss_giou_8: 1.559  loss_ce_dn_8: 2.425  loss_mask_dn_8: 1.216  loss_dice_dn_8: 2.138  loss_bbox_dn_8: 0.7929  loss_giou_dn_8: 0.7045    time: 2.2737  last_time: 2.2515  data_time: 0.0149  last_data_time: 0.0087   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:24:27 d2.utils.events]: \u001b[0m eta: 8 days, 6:36:56  iter: 3919  total_loss: 177.4  loss_ce: 1.966  loss_mask: 1.229  loss_dice: 2.273  loss_bbox: 3.112  loss_giou: 1.601  loss_ce_dn: 2.532  loss_mask_dn: 1.192  loss_dice_dn: 2.117  loss_bbox_dn: 0.8459  loss_giou_dn: 0.6951  loss_ce_0: 7.06  loss_mask_0: 1.219  loss_dice_0: 2.416  loss_bbox_0: 3.832  loss_giou_0: 1.977  loss_ce_1: 2.041  loss_mask_1: 1.218  loss_dice_1: 2.208  loss_bbox_1: 3.167  loss_giou_1: 1.712  loss_ce_dn_1: 2.725  loss_mask_dn_1: 1.27  loss_dice_dn_1: 2.126  loss_bbox_dn_1: 0.9913  loss_giou_dn_1: 0.7694  loss_ce_2: 1.684  loss_mask_2: 1.249  loss_dice_2: 2.222  loss_bbox_2: 3.157  loss_giou_2: 1.723  loss_ce_dn_2: 2.339  loss_mask_dn_2: 1.194  loss_dice_dn_2: 2.044  loss_bbox_dn_2: 0.9563  loss_giou_dn_2: 0.7507  loss_ce_3: 1.732  loss_mask_3: 1.251  loss_dice_3: 2.239  loss_bbox_3: 3.122  loss_giou_3: 1.699  loss_ce_dn_3: 2.203  loss_mask_dn_3: 1.196  loss_dice_dn_3: 2.054  loss_bbox_dn_3: 0.9277  loss_giou_dn_3: 0.7345  loss_ce_4: 1.685  loss_mask_4: 1.242  loss_dice_4: 2.232  loss_bbox_4: 3.098  loss_giou_4: 1.654  loss_ce_dn_4: 2.122  loss_mask_dn_4: 1.194  loss_dice_dn_4: 2.054  loss_bbox_dn_4: 0.8928  loss_giou_dn_4: 0.7198  loss_ce_5: 1.7  loss_mask_5: 1.21  loss_dice_5: 2.236  loss_bbox_5: 3.103  loss_giou_5: 1.615  loss_ce_dn_5: 2.172  loss_mask_dn_5: 1.134  loss_dice_dn_5: 2.055  loss_bbox_dn_5: 0.8849  loss_giou_dn_5: 0.7124  loss_ce_6: 1.827  loss_mask_6: 1.236  loss_dice_6: 2.27  loss_bbox_6: 3.104  loss_giou_6: 1.603  loss_ce_dn_6: 2.202  loss_mask_dn_6: 1.13  loss_dice_dn_6: 2.032  loss_bbox_dn_6: 0.8824  loss_giou_dn_6: 0.7061  loss_ce_7: 1.827  loss_mask_7: 1.222  loss_dice_7: 2.296  loss_bbox_7: 3.108  loss_giou_7: 1.599  loss_ce_dn_7: 2.174  loss_mask_dn_7: 1.121  loss_dice_dn_7: 2.083  loss_bbox_dn_7: 0.8566  loss_giou_dn_7: 0.6926  loss_ce_8: 1.928  loss_mask_8: 1.205  loss_dice_8: 2.278  loss_bbox_8: 3.113  loss_giou_8: 1.593  loss_ce_dn_8: 2.437  loss_mask_dn_8: 1.13  loss_dice_dn_8: 2.086  loss_bbox_dn_8: 0.8521  loss_giou_dn_8: 0.6931    time: 2.2737  last_time: 2.2680  data_time: 0.0127  last_data_time: 0.0133   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:25:12 d2.utils.events]: \u001b[0m eta: 8 days, 6:38:06  iter: 3939  total_loss: 180.3  loss_ce: 1.46  loss_mask: 1.531  loss_dice: 2.403  loss_bbox: 2.479  loss_giou: 1.324  loss_ce_dn: 2.152  loss_mask_dn: 1.538  loss_dice_dn: 2.208  loss_bbox_dn: 0.8306  loss_giou_dn: 0.6567  loss_ce_0: 6.627  loss_mask_0: 1.457  loss_dice_0: 2.503  loss_bbox_0: 4.088  loss_giou_0: 1.925  loss_ce_1: 1.549  loss_mask_1: 1.569  loss_dice_1: 2.414  loss_bbox_1: 2.881  loss_giou_1: 1.486  loss_ce_dn_1: 2.301  loss_mask_dn_1: 1.487  loss_dice_dn_1: 2.317  loss_bbox_dn_1: 1.02  loss_giou_dn_1: 0.7588  loss_ce_2: 1.264  loss_mask_2: 1.649  loss_dice_2: 2.386  loss_bbox_2: 2.628  loss_giou_2: 1.423  loss_ce_dn_2: 2.082  loss_mask_dn_2: 1.439  loss_dice_dn_2: 2.204  loss_bbox_dn_2: 0.9602  loss_giou_dn_2: 0.7256  loss_ce_3: 1.186  loss_mask_3: 1.658  loss_dice_3: 2.319  loss_bbox_3: 2.584  loss_giou_3: 1.428  loss_ce_dn_3: 1.994  loss_mask_dn_3: 1.407  loss_dice_dn_3: 2.213  loss_bbox_dn_3: 0.9079  loss_giou_dn_3: 0.7007  loss_ce_4: 1.181  loss_mask_4: 1.651  loss_dice_4: 2.313  loss_bbox_4: 2.523  loss_giou_4: 1.41  loss_ce_dn_4: 1.93  loss_mask_dn_4: 1.435  loss_dice_dn_4: 2.164  loss_bbox_dn_4: 0.8676  loss_giou_dn_4: 0.6878  loss_ce_5: 1.27  loss_mask_5: 1.593  loss_dice_5: 2.337  loss_bbox_5: 2.52  loss_giou_5: 1.363  loss_ce_dn_5: 1.905  loss_mask_dn_5: 1.453  loss_dice_dn_5: 2.175  loss_bbox_dn_5: 0.8442  loss_giou_dn_5: 0.6717  loss_ce_6: 1.221  loss_mask_6: 1.575  loss_dice_6: 2.405  loss_bbox_6: 2.513  loss_giou_6: 1.352  loss_ce_dn_6: 1.969  loss_mask_dn_6: 1.416  loss_dice_dn_6: 2.165  loss_bbox_dn_6: 0.8404  loss_giou_dn_6: 0.6708  loss_ce_7: 1.275  loss_mask_7: 1.54  loss_dice_7: 2.38  loss_bbox_7: 2.507  loss_giou_7: 1.329  loss_ce_dn_7: 2.015  loss_mask_dn_7: 1.441  loss_dice_dn_7: 2.172  loss_bbox_dn_7: 0.8243  loss_giou_dn_7: 0.6535  loss_ce_8: 1.344  loss_mask_8: 1.586  loss_dice_8: 2.401  loss_bbox_8: 2.483  loss_giou_8: 1.324  loss_ce_dn_8: 2.017  loss_mask_dn_8: 1.514  loss_dice_dn_8: 2.204  loss_bbox_dn_8: 0.8279  loss_giou_dn_8: 0.6542    time: 2.2737  last_time: 2.2767  data_time: 0.0150  last_data_time: 0.0109   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:25:57 d2.utils.events]: \u001b[0m eta: 8 days, 6:34:19  iter: 3959  total_loss: 186  loss_ce: 1.917  loss_mask: 1.592  loss_dice: 2.768  loss_bbox: 2.871  loss_giou: 1.744  loss_ce_dn: 2.683  loss_mask_dn: 1.378  loss_dice_dn: 2.323  loss_bbox_dn: 0.8849  loss_giou_dn: 0.7398  loss_ce_0: 7.052  loss_mask_0: 1.644  loss_dice_0: 2.916  loss_bbox_0: 3.694  loss_giou_0: 1.921  loss_ce_1: 2.121  loss_mask_1: 1.568  loss_dice_1: 2.834  loss_bbox_1: 3.102  loss_giou_1: 1.789  loss_ce_dn_1: 3.131  loss_mask_dn_1: 1.493  loss_dice_dn_1: 2.447  loss_bbox_dn_1: 1.013  loss_giou_dn_1: 0.7778  loss_ce_2: 1.927  loss_mask_2: 1.559  loss_dice_2: 2.732  loss_bbox_2: 3.048  loss_giou_2: 1.768  loss_ce_dn_2: 2.63  loss_mask_dn_2: 1.442  loss_dice_dn_2: 2.338  loss_bbox_dn_2: 0.9613  loss_giou_dn_2: 0.7668  loss_ce_3: 1.942  loss_mask_3: 1.565  loss_dice_3: 2.766  loss_bbox_3: 2.982  loss_giou_3: 1.736  loss_ce_dn_3: 2.575  loss_mask_dn_3: 1.387  loss_dice_dn_3: 2.356  loss_bbox_dn_3: 0.903  loss_giou_dn_3: 0.7512  loss_ce_4: 1.893  loss_mask_4: 1.577  loss_dice_4: 2.774  loss_bbox_4: 2.999  loss_giou_4: 1.759  loss_ce_dn_4: 2.5  loss_mask_dn_4: 1.371  loss_dice_dn_4: 2.373  loss_bbox_dn_4: 0.8883  loss_giou_dn_4: 0.7396  loss_ce_5: 1.912  loss_mask_5: 1.592  loss_dice_5: 2.734  loss_bbox_5: 2.943  loss_giou_5: 1.753  loss_ce_dn_5: 2.394  loss_mask_dn_5: 1.376  loss_dice_dn_5: 2.39  loss_bbox_dn_5: 0.8857  loss_giou_dn_5: 0.7364  loss_ce_6: 1.83  loss_mask_6: 1.586  loss_dice_6: 2.792  loss_bbox_6: 2.932  loss_giou_6: 1.754  loss_ce_dn_6: 2.37  loss_mask_dn_6: 1.343  loss_dice_dn_6: 2.342  loss_bbox_dn_6: 0.8821  loss_giou_dn_6: 0.7347  loss_ce_7: 1.81  loss_mask_7: 1.589  loss_dice_7: 2.77  loss_bbox_7: 2.903  loss_giou_7: 1.723  loss_ce_dn_7: 2.467  loss_mask_dn_7: 1.365  loss_dice_dn_7: 2.358  loss_bbox_dn_7: 0.8775  loss_giou_dn_7: 0.7308  loss_ce_8: 1.778  loss_mask_8: 1.59  loss_dice_8: 2.764  loss_bbox_8: 2.873  loss_giou_8: 1.742  loss_ce_dn_8: 2.55  loss_mask_dn_8: 1.389  loss_dice_dn_8: 2.346  loss_bbox_dn_8: 0.8793  loss_giou_dn_8: 0.7354    time: 2.2736  last_time: 2.2521  data_time: 0.0128  last_data_time: 0.0050   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:26:43 d2.utils.events]: \u001b[0m eta: 8 days, 6:33:21  iter: 3979  total_loss: 169.6  loss_ce: 1.445  loss_mask: 1.537  loss_dice: 2.364  loss_bbox: 2.495  loss_giou: 1.362  loss_ce_dn: 2.293  loss_mask_dn: 1.3  loss_dice_dn: 2.075  loss_bbox_dn: 0.7987  loss_giou_dn: 0.6629  loss_ce_0: 6.961  loss_mask_0: 1.448  loss_dice_0: 2.51  loss_bbox_0: 3.907  loss_giou_0: 1.867  loss_ce_1: 1.85  loss_mask_1: 1.439  loss_dice_1: 2.346  loss_bbox_1: 2.694  loss_giou_1: 1.52  loss_ce_dn_1: 2.747  loss_mask_dn_1: 1.313  loss_dice_dn_1: 2.225  loss_bbox_dn_1: 1.034  loss_giou_dn_1: 0.764  loss_ce_2: 1.556  loss_mask_2: 1.52  loss_dice_2: 2.363  loss_bbox_2: 2.642  loss_giou_2: 1.519  loss_ce_dn_2: 2.33  loss_mask_dn_2: 1.228  loss_dice_dn_2: 2.118  loss_bbox_dn_2: 0.9568  loss_giou_dn_2: 0.7353  loss_ce_3: 1.496  loss_mask_3: 1.476  loss_dice_3: 2.365  loss_bbox_3: 2.567  loss_giou_3: 1.46  loss_ce_dn_3: 2.279  loss_mask_dn_3: 1.222  loss_dice_dn_3: 2.088  loss_bbox_dn_3: 0.8717  loss_giou_dn_3: 0.6997  loss_ce_4: 1.516  loss_mask_4: 1.46  loss_dice_4: 2.38  loss_bbox_4: 2.535  loss_giou_4: 1.435  loss_ce_dn_4: 2.212  loss_mask_dn_4: 1.236  loss_dice_dn_4: 2.051  loss_bbox_dn_4: 0.8344  loss_giou_dn_4: 0.6786  loss_ce_5: 1.5  loss_mask_5: 1.497  loss_dice_5: 2.3  loss_bbox_5: 2.501  loss_giou_5: 1.397  loss_ce_dn_5: 2.262  loss_mask_dn_5: 1.257  loss_dice_dn_5: 2.092  loss_bbox_dn_5: 0.818  loss_giou_dn_5: 0.6706  loss_ce_6: 1.438  loss_mask_6: 1.528  loss_dice_6: 2.413  loss_bbox_6: 2.517  loss_giou_6: 1.376  loss_ce_dn_6: 2.224  loss_mask_dn_6: 1.263  loss_dice_dn_6: 2.115  loss_bbox_dn_6: 0.8134  loss_giou_dn_6: 0.6716  loss_ce_7: 1.385  loss_mask_7: 1.512  loss_dice_7: 2.416  loss_bbox_7: 2.493  loss_giou_7: 1.353  loss_ce_dn_7: 2.231  loss_mask_dn_7: 1.306  loss_dice_dn_7: 2.103  loss_bbox_dn_7: 0.7987  loss_giou_dn_7: 0.6616  loss_ce_8: 1.402  loss_mask_8: 1.55  loss_dice_8: 2.402  loss_bbox_8: 2.492  loss_giou_8: 1.358  loss_ce_dn_8: 2.293  loss_mask_dn_8: 1.289  loss_dice_dn_8: 2.088  loss_bbox_dn_8: 0.7979  loss_giou_dn_8: 0.6623    time: 2.2736  last_time: 2.2623  data_time: 0.0132  last_data_time: 0.0166   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:27:28 d2.utils.events]: \u001b[0m eta: 8 days, 6:32:17  iter: 3999  total_loss: 163.9  loss_ce: 1.491  loss_mask: 1.268  loss_dice: 2.26  loss_bbox: 2.577  loss_giou: 1.529  loss_ce_dn: 1.835  loss_mask_dn: 1.155  loss_dice_dn: 2.176  loss_bbox_dn: 0.8251  loss_giou_dn: 0.6826  loss_ce_0: 6.531  loss_mask_0: 1.268  loss_dice_0: 2.434  loss_bbox_0: 4.004  loss_giou_0: 1.958  loss_ce_1: 1.839  loss_mask_1: 1.279  loss_dice_1: 2.349  loss_bbox_1: 2.81  loss_giou_1: 1.666  loss_ce_dn_1: 2.376  loss_mask_dn_1: 1.224  loss_dice_dn_1: 2.265  loss_bbox_dn_1: 0.9922  loss_giou_dn_1: 0.7628  loss_ce_2: 1.572  loss_mask_2: 1.329  loss_dice_2: 2.296  loss_bbox_2: 2.632  loss_giou_2: 1.588  loss_ce_dn_2: 1.958  loss_mask_dn_2: 1.2  loss_dice_dn_2: 2.062  loss_bbox_dn_2: 0.9136  loss_giou_dn_2: 0.7424  loss_ce_3: 1.55  loss_mask_3: 1.299  loss_dice_3: 2.303  loss_bbox_3: 2.593  loss_giou_3: 1.567  loss_ce_dn_3: 1.828  loss_mask_dn_3: 1.166  loss_dice_dn_3: 2.076  loss_bbox_dn_3: 0.8711  loss_giou_dn_3: 0.7056  loss_ce_4: 1.563  loss_mask_4: 1.262  loss_dice_4: 2.285  loss_bbox_4: 2.545  loss_giou_4: 1.544  loss_ce_dn_4: 1.768  loss_mask_dn_4: 1.191  loss_dice_dn_4: 2.049  loss_bbox_dn_4: 0.8517  loss_giou_dn_4: 0.6984  loss_ce_5: 1.573  loss_mask_5: 1.269  loss_dice_5: 2.27  loss_bbox_5: 2.578  loss_giou_5: 1.55  loss_ce_dn_5: 1.692  loss_mask_dn_5: 1.179  loss_dice_dn_5: 2.062  loss_bbox_dn_5: 0.8419  loss_giou_dn_5: 0.6921  loss_ce_6: 1.552  loss_mask_6: 1.278  loss_dice_6: 2.235  loss_bbox_6: 2.583  loss_giou_6: 1.555  loss_ce_dn_6: 1.655  loss_mask_dn_6: 1.168  loss_dice_dn_6: 2.083  loss_bbox_dn_6: 0.8409  loss_giou_dn_6: 0.6869  loss_ce_7: 1.502  loss_mask_7: 1.284  loss_dice_7: 2.235  loss_bbox_7: 2.591  loss_giou_7: 1.526  loss_ce_dn_7: 1.666  loss_mask_dn_7: 1.198  loss_dice_dn_7: 2.083  loss_bbox_dn_7: 0.8244  loss_giou_dn_7: 0.6781  loss_ce_8: 1.492  loss_mask_8: 1.288  loss_dice_8: 2.227  loss_bbox_8: 2.575  loss_giou_8: 1.526  loss_ce_dn_8: 1.729  loss_mask_dn_8: 1.187  loss_dice_dn_8: 2.156  loss_bbox_dn_8: 0.8266  loss_giou_dn_8: 0.6756    time: 2.2736  last_time: 2.2330  data_time: 0.0131  last_data_time: 0.0126   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:28:14 d2.utils.events]: \u001b[0m eta: 8 days, 6:31:29  iter: 4019  total_loss: 171.5  loss_ce: 1.587  loss_mask: 1.73  loss_dice: 2.306  loss_bbox: 2.545  loss_giou: 1.505  loss_ce_dn: 2.535  loss_mask_dn: 1.515  loss_dice_dn: 1.962  loss_bbox_dn: 0.7934  loss_giou_dn: 0.6837  loss_ce_0: 6.934  loss_mask_0: 1.69  loss_dice_0: 2.453  loss_bbox_0: 4.256  loss_giou_0: 2.026  loss_ce_1: 1.896  loss_mask_1: 1.63  loss_dice_1: 2.346  loss_bbox_1: 3.178  loss_giou_1: 1.623  loss_ce_dn_1: 2.801  loss_mask_dn_1: 1.468  loss_dice_dn_1: 2.228  loss_bbox_dn_1: 0.9565  loss_giou_dn_1: 0.7554  loss_ce_2: 1.841  loss_mask_2: 1.658  loss_dice_2: 2.317  loss_bbox_2: 2.885  loss_giou_2: 1.568  loss_ce_dn_2: 2.372  loss_mask_dn_2: 1.499  loss_dice_dn_2: 2.057  loss_bbox_dn_2: 0.8938  loss_giou_dn_2: 0.732  loss_ce_3: 1.678  loss_mask_3: 1.726  loss_dice_3: 2.284  loss_bbox_3: 2.707  loss_giou_3: 1.532  loss_ce_dn_3: 2.268  loss_mask_dn_3: 1.492  loss_dice_dn_3: 1.999  loss_bbox_dn_3: 0.86  loss_giou_dn_3: 0.7166  loss_ce_4: 1.711  loss_mask_4: 1.663  loss_dice_4: 2.298  loss_bbox_4: 2.675  loss_giou_4: 1.538  loss_ce_dn_4: 2.311  loss_mask_dn_4: 1.483  loss_dice_dn_4: 1.967  loss_bbox_dn_4: 0.8506  loss_giou_dn_4: 0.7012  loss_ce_5: 1.578  loss_mask_5: 1.678  loss_dice_5: 2.294  loss_bbox_5: 2.561  loss_giou_5: 1.522  loss_ce_dn_5: 2.347  loss_mask_dn_5: 1.473  loss_dice_dn_5: 2.016  loss_bbox_dn_5: 0.8254  loss_giou_dn_5: 0.7  loss_ce_6: 1.596  loss_mask_6: 1.691  loss_dice_6: 2.283  loss_bbox_6: 2.569  loss_giou_6: 1.53  loss_ce_dn_6: 2.332  loss_mask_dn_6: 1.521  loss_dice_dn_6: 1.976  loss_bbox_dn_6: 0.8241  loss_giou_dn_6: 0.6955  loss_ce_7: 1.633  loss_mask_7: 1.721  loss_dice_7: 2.308  loss_bbox_7: 2.591  loss_giou_7: 1.525  loss_ce_dn_7: 2.328  loss_mask_dn_7: 1.522  loss_dice_dn_7: 1.937  loss_bbox_dn_7: 0.8022  loss_giou_dn_7: 0.6877  loss_ce_8: 1.607  loss_mask_8: 1.681  loss_dice_8: 2.288  loss_bbox_8: 2.587  loss_giou_8: 1.506  loss_ce_dn_8: 2.419  loss_mask_dn_8: 1.494  loss_dice_dn_8: 1.966  loss_bbox_dn_8: 0.8006  loss_giou_dn_8: 0.6849    time: 2.2736  last_time: 2.3700  data_time: 0.0132  last_data_time: 0.0156   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:28:59 d2.utils.events]: \u001b[0m eta: 8 days, 6:30:34  iter: 4039  total_loss: 171.7  loss_ce: 1.744  loss_mask: 1.624  loss_dice: 2.338  loss_bbox: 2.459  loss_giou: 1.373  loss_ce_dn: 2.146  loss_mask_dn: 1.495  loss_dice_dn: 2.036  loss_bbox_dn: 0.8608  loss_giou_dn: 0.6839  loss_ce_0: 6.632  loss_mask_0: 1.53  loss_dice_0: 2.553  loss_bbox_0: 4.006  loss_giou_0: 1.898  loss_ce_1: 1.856  loss_mask_1: 1.621  loss_dice_1: 2.318  loss_bbox_1: 2.793  loss_giou_1: 1.622  loss_ce_dn_1: 2.523  loss_mask_dn_1: 1.459  loss_dice_dn_1: 2.187  loss_bbox_dn_1: 0.9975  loss_giou_dn_1: 0.7646  loss_ce_2: 1.514  loss_mask_2: 1.626  loss_dice_2: 2.33  loss_bbox_2: 2.567  loss_giou_2: 1.538  loss_ce_dn_2: 2.171  loss_mask_dn_2: 1.428  loss_dice_dn_2: 2.07  loss_bbox_dn_2: 0.9638  loss_giou_dn_2: 0.7325  loss_ce_3: 1.574  loss_mask_3: 1.636  loss_dice_3: 2.263  loss_bbox_3: 2.488  loss_giou_3: 1.506  loss_ce_dn_3: 2.128  loss_mask_dn_3: 1.461  loss_dice_dn_3: 2.011  loss_bbox_dn_3: 0.9322  loss_giou_dn_3: 0.7251  loss_ce_4: 1.498  loss_mask_4: 1.677  loss_dice_4: 2.302  loss_bbox_4: 2.475  loss_giou_4: 1.466  loss_ce_dn_4: 2.003  loss_mask_dn_4: 1.467  loss_dice_dn_4: 2.005  loss_bbox_dn_4: 0.9053  loss_giou_dn_4: 0.7059  loss_ce_5: 1.497  loss_mask_5: 1.692  loss_dice_5: 2.259  loss_bbox_5: 2.487  loss_giou_5: 1.436  loss_ce_dn_5: 1.911  loss_mask_dn_5: 1.503  loss_dice_dn_5: 2.034  loss_bbox_dn_5: 0.8893  loss_giou_dn_5: 0.6891  loss_ce_6: 1.555  loss_mask_6: 1.571  loss_dice_6: 2.271  loss_bbox_6: 2.495  loss_giou_6: 1.41  loss_ce_dn_6: 1.909  loss_mask_dn_6: 1.507  loss_dice_dn_6: 1.994  loss_bbox_dn_6: 0.8836  loss_giou_dn_6: 0.6889  loss_ce_7: 1.525  loss_mask_7: 1.658  loss_dice_7: 2.263  loss_bbox_7: 2.458  loss_giou_7: 1.4  loss_ce_dn_7: 1.873  loss_mask_dn_7: 1.498  loss_dice_dn_7: 2.037  loss_bbox_dn_7: 0.861  loss_giou_dn_7: 0.6808  loss_ce_8: 1.554  loss_mask_8: 1.603  loss_dice_8: 2.324  loss_bbox_8: 2.437  loss_giou_8: 1.359  loss_ce_dn_8: 1.929  loss_mask_dn_8: 1.491  loss_dice_dn_8: 2.023  loss_bbox_dn_8: 0.8602  loss_giou_dn_8: 0.6821    time: 2.2735  last_time: 2.2269  data_time: 0.0132  last_data_time: 0.0077   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:29:45 d2.utils.events]: \u001b[0m eta: 8 days, 6:30:20  iter: 4059  total_loss: 176.9  loss_ce: 1.658  loss_mask: 1.649  loss_dice: 2.329  loss_bbox: 2.83  loss_giou: 1.556  loss_ce_dn: 2.398  loss_mask_dn: 1.28  loss_dice_dn: 2.13  loss_bbox_dn: 0.7156  loss_giou_dn: 0.6793  loss_ce_0: 6.878  loss_mask_0: 1.358  loss_dice_0: 2.591  loss_bbox_0: 3.888  loss_giou_0: 1.978  loss_ce_1: 1.846  loss_mask_1: 1.434  loss_dice_1: 2.387  loss_bbox_1: 2.963  loss_giou_1: 1.657  loss_ce_dn_1: 2.803  loss_mask_dn_1: 1.302  loss_dice_dn_1: 2.348  loss_bbox_dn_1: 0.9292  loss_giou_dn_1: 0.756  loss_ce_2: 1.627  loss_mask_2: 1.56  loss_dice_2: 2.378  loss_bbox_2: 2.931  loss_giou_2: 1.606  loss_ce_dn_2: 2.407  loss_mask_dn_2: 1.223  loss_dice_dn_2: 2.183  loss_bbox_dn_2: 0.8627  loss_giou_dn_2: 0.7232  loss_ce_3: 1.571  loss_mask_3: 1.579  loss_dice_3: 2.288  loss_bbox_3: 2.875  loss_giou_3: 1.581  loss_ce_dn_3: 2.249  loss_mask_dn_3: 1.257  loss_dice_dn_3: 2.125  loss_bbox_dn_3: 0.7914  loss_giou_dn_3: 0.7046  loss_ce_4: 1.514  loss_mask_4: 1.574  loss_dice_4: 2.288  loss_bbox_4: 2.809  loss_giou_4: 1.579  loss_ce_dn_4: 2.246  loss_mask_dn_4: 1.258  loss_dice_dn_4: 2.086  loss_bbox_dn_4: 0.7703  loss_giou_dn_4: 0.6959  loss_ce_5: 1.549  loss_mask_5: 1.567  loss_dice_5: 2.249  loss_bbox_5: 2.797  loss_giou_5: 1.567  loss_ce_dn_5: 2.198  loss_mask_dn_5: 1.237  loss_dice_dn_5: 2.124  loss_bbox_dn_5: 0.7413  loss_giou_dn_5: 0.6888  loss_ce_6: 1.585  loss_mask_6: 1.566  loss_dice_6: 2.248  loss_bbox_6: 2.794  loss_giou_6: 1.557  loss_ce_dn_6: 2.186  loss_mask_dn_6: 1.249  loss_dice_dn_6: 2.12  loss_bbox_dn_6: 0.7457  loss_giou_dn_6: 0.6883  loss_ce_7: 1.629  loss_mask_7: 1.598  loss_dice_7: 2.318  loss_bbox_7: 2.807  loss_giou_7: 1.547  loss_ce_dn_7: 2.296  loss_mask_dn_7: 1.24  loss_dice_dn_7: 2.089  loss_bbox_dn_7: 0.7147  loss_giou_dn_7: 0.6729  loss_ce_8: 1.606  loss_mask_8: 1.642  loss_dice_8: 2.299  loss_bbox_8: 2.82  loss_giou_8: 1.549  loss_ce_dn_8: 2.306  loss_mask_dn_8: 1.268  loss_dice_dn_8: 2.133  loss_bbox_dn_8: 0.7111  loss_giou_dn_8: 0.6753    time: 2.2736  last_time: 2.2597  data_time: 0.0125  last_data_time: 0.0077   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:30:30 d2.utils.events]: \u001b[0m eta: 8 days, 6:29:34  iter: 4079  total_loss: 172.4  loss_ce: 1.612  loss_mask: 1.531  loss_dice: 2.378  loss_bbox: 2.489  loss_giou: 1.606  loss_ce_dn: 1.997  loss_mask_dn: 1.378  loss_dice_dn: 2.101  loss_bbox_dn: 0.8633  loss_giou_dn: 0.7376  loss_ce_0: 6.575  loss_mask_0: 1.347  loss_dice_0: 2.49  loss_bbox_0: 4.051  loss_giou_0: 1.941  loss_ce_1: 1.969  loss_mask_1: 1.699  loss_dice_1: 2.43  loss_bbox_1: 3.094  loss_giou_1: 1.769  loss_ce_dn_1: 2.591  loss_mask_dn_1: 1.442  loss_dice_dn_1: 2.276  loss_bbox_dn_1: 0.9918  loss_giou_dn_1: 0.7742  loss_ce_2: 1.799  loss_mask_2: 1.628  loss_dice_2: 2.37  loss_bbox_2: 2.926  loss_giou_2: 1.756  loss_ce_dn_2: 2.04  loss_mask_dn_2: 1.31  loss_dice_dn_2: 2.134  loss_bbox_dn_2: 0.9439  loss_giou_dn_2: 0.7533  loss_ce_3: 1.55  loss_mask_3: 1.631  loss_dice_3: 2.419  loss_bbox_3: 2.831  loss_giou_3: 1.744  loss_ce_dn_3: 1.969  loss_mask_dn_3: 1.322  loss_dice_dn_3: 2.08  loss_bbox_dn_3: 0.9152  loss_giou_dn_3: 0.7355  loss_ce_4: 1.454  loss_mask_4: 1.593  loss_dice_4: 2.427  loss_bbox_4: 2.747  loss_giou_4: 1.705  loss_ce_dn_4: 1.853  loss_mask_dn_4: 1.3  loss_dice_dn_4: 2.046  loss_bbox_dn_4: 0.8808  loss_giou_dn_4: 0.7334  loss_ce_5: 1.509  loss_mask_5: 1.621  loss_dice_5: 2.401  loss_bbox_5: 2.628  loss_giou_5: 1.669  loss_ce_dn_5: 1.827  loss_mask_dn_5: 1.284  loss_dice_dn_5: 2.083  loss_bbox_dn_5: 0.8707  loss_giou_dn_5: 0.734  loss_ce_6: 1.524  loss_mask_6: 1.634  loss_dice_6: 2.418  loss_bbox_6: 2.572  loss_giou_6: 1.625  loss_ce_dn_6: 1.834  loss_mask_dn_6: 1.337  loss_dice_dn_6: 2.089  loss_bbox_dn_6: 0.8709  loss_giou_dn_6: 0.731  loss_ce_7: 1.489  loss_mask_7: 1.516  loss_dice_7: 2.374  loss_bbox_7: 2.464  loss_giou_7: 1.638  loss_ce_dn_7: 1.89  loss_mask_dn_7: 1.35  loss_dice_dn_7: 2.101  loss_bbox_dn_7: 0.8605  loss_giou_dn_7: 0.7302  loss_ce_8: 1.546  loss_mask_8: 1.548  loss_dice_8: 2.35  loss_bbox_8: 2.483  loss_giou_8: 1.605  loss_ce_dn_8: 1.935  loss_mask_dn_8: 1.391  loss_dice_dn_8: 2.1  loss_bbox_dn_8: 0.8545  loss_giou_dn_8: 0.7342    time: 2.2736  last_time: 2.2168  data_time: 0.0126  last_data_time: 0.0076   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:31:16 d2.utils.events]: \u001b[0m eta: 8 days, 6:28:28  iter: 4099  total_loss: 180.3  loss_ce: 1.896  loss_mask: 1.376  loss_dice: 2.568  loss_bbox: 2.67  loss_giou: 1.705  loss_ce_dn: 2.146  loss_mask_dn: 1.194  loss_dice_dn: 2.332  loss_bbox_dn: 0.8507  loss_giou_dn: 0.7126  loss_ce_0: 6.818  loss_mask_0: 1.475  loss_dice_0: 2.838  loss_bbox_0: 3.826  loss_giou_0: 1.977  loss_ce_1: 2.185  loss_mask_1: 1.468  loss_dice_1: 2.646  loss_bbox_1: 2.98  loss_giou_1: 1.797  loss_ce_dn_1: 2.631  loss_mask_dn_1: 1.25  loss_dice_dn_1: 2.512  loss_bbox_dn_1: 0.9279  loss_giou_dn_1: 0.759  loss_ce_2: 1.953  loss_mask_2: 1.392  loss_dice_2: 2.579  loss_bbox_2: 2.886  loss_giou_2: 1.749  loss_ce_dn_2: 2.125  loss_mask_dn_2: 1.201  loss_dice_dn_2: 2.352  loss_bbox_dn_2: 0.9002  loss_giou_dn_2: 0.7432  loss_ce_3: 1.863  loss_mask_3: 1.408  loss_dice_3: 2.566  loss_bbox_3: 2.772  loss_giou_3: 1.759  loss_ce_dn_3: 2.055  loss_mask_dn_3: 1.195  loss_dice_dn_3: 2.299  loss_bbox_dn_3: 0.8887  loss_giou_dn_3: 0.7296  loss_ce_4: 1.826  loss_mask_4: 1.437  loss_dice_4: 2.546  loss_bbox_4: 2.711  loss_giou_4: 1.728  loss_ce_dn_4: 1.958  loss_mask_dn_4: 1.224  loss_dice_dn_4: 2.254  loss_bbox_dn_4: 0.8688  loss_giou_dn_4: 0.7172  loss_ce_5: 1.866  loss_mask_5: 1.372  loss_dice_5: 2.521  loss_bbox_5: 2.709  loss_giou_5: 1.691  loss_ce_dn_5: 1.958  loss_mask_dn_5: 1.203  loss_dice_dn_5: 2.296  loss_bbox_dn_5: 0.8581  loss_giou_dn_5: 0.7186  loss_ce_6: 1.896  loss_mask_6: 1.399  loss_dice_6: 2.547  loss_bbox_6: 2.676  loss_giou_6: 1.705  loss_ce_dn_6: 1.958  loss_mask_dn_6: 1.234  loss_dice_dn_6: 2.314  loss_bbox_dn_6: 0.8537  loss_giou_dn_6: 0.7163  loss_ce_7: 1.948  loss_mask_7: 1.375  loss_dice_7: 2.543  loss_bbox_7: 2.716  loss_giou_7: 1.708  loss_ce_dn_7: 2.08  loss_mask_dn_7: 1.25  loss_dice_dn_7: 2.329  loss_bbox_dn_7: 0.847  loss_giou_dn_7: 0.7126  loss_ce_8: 1.908  loss_mask_8: 1.359  loss_dice_8: 2.546  loss_bbox_8: 2.668  loss_giou_8: 1.705  loss_ce_dn_8: 2.101  loss_mask_dn_8: 1.221  loss_dice_dn_8: 2.317  loss_bbox_dn_8: 0.8455  loss_giou_dn_8: 0.7117    time: 2.2736  last_time: 2.3025  data_time: 0.0115  last_data_time: 0.0076   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:32:01 d2.utils.events]: \u001b[0m eta: 8 days, 6:27:34  iter: 4119  total_loss: 178.1  loss_ce: 1.816  loss_mask: 1.373  loss_dice: 2.286  loss_bbox: 2.968  loss_giou: 1.666  loss_ce_dn: 2.363  loss_mask_dn: 1.2  loss_dice_dn: 2.039  loss_bbox_dn: 0.8979  loss_giou_dn: 0.7267  loss_ce_0: 6.731  loss_mask_0: 1.384  loss_dice_0: 2.659  loss_bbox_0: 4.012  loss_giou_0: 1.981  loss_ce_1: 2.015  loss_mask_1: 1.3  loss_dice_1: 2.324  loss_bbox_1: 3.059  loss_giou_1: 1.741  loss_ce_dn_1: 2.683  loss_mask_dn_1: 1.355  loss_dice_dn_1: 2.284  loss_bbox_dn_1: 0.9846  loss_giou_dn_1: 0.7757  loss_ce_2: 1.747  loss_mask_2: 1.314  loss_dice_2: 2.212  loss_bbox_2: 3.02  loss_giou_2: 1.692  loss_ce_dn_2: 2.237  loss_mask_dn_2: 1.241  loss_dice_dn_2: 2.046  loss_bbox_dn_2: 0.9605  loss_giou_dn_2: 0.7636  loss_ce_3: 1.649  loss_mask_3: 1.373  loss_dice_3: 2.301  loss_bbox_3: 2.985  loss_giou_3: 1.663  loss_ce_dn_3: 2.202  loss_mask_dn_3: 1.214  loss_dice_dn_3: 2.04  loss_bbox_dn_3: 0.9348  loss_giou_dn_3: 0.7541  loss_ce_4: 1.701  loss_mask_4: 1.374  loss_dice_4: 2.341  loss_bbox_4: 2.982  loss_giou_4: 1.669  loss_ce_dn_4: 2.155  loss_mask_dn_4: 1.183  loss_dice_dn_4: 2.013  loss_bbox_dn_4: 0.9194  loss_giou_dn_4: 0.7353  loss_ce_5: 1.755  loss_mask_5: 1.323  loss_dice_5: 2.34  loss_bbox_5: 2.971  loss_giou_5: 1.643  loss_ce_dn_5: 2.19  loss_mask_dn_5: 1.14  loss_dice_dn_5: 2.052  loss_bbox_dn_5: 0.915  loss_giou_dn_5: 0.7317  loss_ce_6: 1.758  loss_mask_6: 1.347  loss_dice_6: 2.305  loss_bbox_6: 2.972  loss_giou_6: 1.653  loss_ce_dn_6: 2.244  loss_mask_dn_6: 1.167  loss_dice_dn_6: 2.03  loss_bbox_dn_6: 0.9124  loss_giou_dn_6: 0.7314  loss_ce_7: 1.754  loss_mask_7: 1.337  loss_dice_7: 2.358  loss_bbox_7: 2.957  loss_giou_7: 1.653  loss_ce_dn_7: 2.29  loss_mask_dn_7: 1.159  loss_dice_dn_7: 2.044  loss_bbox_dn_7: 0.9041  loss_giou_dn_7: 0.7224  loss_ce_8: 1.757  loss_mask_8: 1.367  loss_dice_8: 2.31  loss_bbox_8: 2.965  loss_giou_8: 1.665  loss_ce_dn_8: 2.345  loss_mask_dn_8: 1.204  loss_dice_dn_8: 2.057  loss_bbox_dn_8: 0.9024  loss_giou_dn_8: 0.7245    time: 2.2735  last_time: 2.1945  data_time: 0.0136  last_data_time: 0.0049   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:32:47 d2.utils.events]: \u001b[0m eta: 8 days, 6:26:57  iter: 4139  total_loss: 164.2  loss_ce: 1.626  loss_mask: 1.591  loss_dice: 2.28  loss_bbox: 2.543  loss_giou: 1.424  loss_ce_dn: 2.041  loss_mask_dn: 1.329  loss_dice_dn: 2.048  loss_bbox_dn: 0.823  loss_giou_dn: 0.6696  loss_ce_0: 6.909  loss_mask_0: 1.503  loss_dice_0: 2.532  loss_bbox_0: 3.667  loss_giou_0: 1.868  loss_ce_1: 2.047  loss_mask_1: 1.622  loss_dice_1: 2.278  loss_bbox_1: 2.874  loss_giou_1: 1.601  loss_ce_dn_1: 2.53  loss_mask_dn_1: 1.407  loss_dice_dn_1: 2.238  loss_bbox_dn_1: 0.9743  loss_giou_dn_1: 0.7408  loss_ce_2: 1.844  loss_mask_2: 1.583  loss_dice_2: 2.262  loss_bbox_2: 2.774  loss_giou_2: 1.547  loss_ce_dn_2: 2.166  loss_mask_dn_2: 1.391  loss_dice_dn_2: 2.082  loss_bbox_dn_2: 0.9221  loss_giou_dn_2: 0.7151  loss_ce_3: 1.645  loss_mask_3: 1.539  loss_dice_3: 2.237  loss_bbox_3: 2.664  loss_giou_3: 1.521  loss_ce_dn_3: 2.024  loss_mask_dn_3: 1.36  loss_dice_dn_3: 2.05  loss_bbox_dn_3: 0.8842  loss_giou_dn_3: 0.6947  loss_ce_4: 1.598  loss_mask_4: 1.544  loss_dice_4: 2.255  loss_bbox_4: 2.648  loss_giou_4: 1.493  loss_ce_dn_4: 1.984  loss_mask_dn_4: 1.375  loss_dice_dn_4: 2.026  loss_bbox_dn_4: 0.8607  loss_giou_dn_4: 0.6824  loss_ce_5: 1.651  loss_mask_5: 1.587  loss_dice_5: 2.252  loss_bbox_5: 2.605  loss_giou_5: 1.468  loss_ce_dn_5: 2.032  loss_mask_dn_5: 1.351  loss_dice_dn_5: 2.017  loss_bbox_dn_5: 0.8358  loss_giou_dn_5: 0.6754  loss_ce_6: 1.725  loss_mask_6: 1.567  loss_dice_6: 2.267  loss_bbox_6: 2.55  loss_giou_6: 1.459  loss_ce_dn_6: 2.053  loss_mask_dn_6: 1.336  loss_dice_dn_6: 2.021  loss_bbox_dn_6: 0.832  loss_giou_dn_6: 0.6733  loss_ce_7: 1.739  loss_mask_7: 1.587  loss_dice_7: 2.277  loss_bbox_7: 2.532  loss_giou_7: 1.434  loss_ce_dn_7: 2.049  loss_mask_dn_7: 1.345  loss_dice_dn_7: 2.067  loss_bbox_dn_7: 0.8171  loss_giou_dn_7: 0.6637  loss_ce_8: 1.748  loss_mask_8: 1.573  loss_dice_8: 2.273  loss_bbox_8: 2.525  loss_giou_8: 1.43  loss_ce_dn_8: 2.007  loss_mask_dn_8: 1.344  loss_dice_dn_8: 2.058  loss_bbox_dn_8: 0.8204  loss_giou_dn_8: 0.6689    time: 2.2736  last_time: 2.2316  data_time: 0.0110  last_data_time: 0.0102   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:33:32 d2.utils.events]: \u001b[0m eta: 8 days, 6:25:02  iter: 4159  total_loss: 157.7  loss_ce: 1.864  loss_mask: 1.259  loss_dice: 2.038  loss_bbox: 2.533  loss_giou: 1.557  loss_ce_dn: 2.259  loss_mask_dn: 1.166  loss_dice_dn: 2.004  loss_bbox_dn: 0.8208  loss_giou_dn: 0.6751  loss_ce_0: 6.568  loss_mask_0: 1.228  loss_dice_0: 2.371  loss_bbox_0: 3.914  loss_giou_0: 1.927  loss_ce_1: 1.799  loss_mask_1: 1.295  loss_dice_1: 2.228  loss_bbox_1: 2.723  loss_giou_1: 1.615  loss_ce_dn_1: 2.548  loss_mask_dn_1: 1.264  loss_dice_dn_1: 2.18  loss_bbox_dn_1: 0.9518  loss_giou_dn_1: 0.7564  loss_ce_2: 1.65  loss_mask_2: 1.26  loss_dice_2: 2.135  loss_bbox_2: 2.557  loss_giou_2: 1.629  loss_ce_dn_2: 2.15  loss_mask_dn_2: 1.189  loss_dice_dn_2: 2.049  loss_bbox_dn_2: 0.9001  loss_giou_dn_2: 0.7289  loss_ce_3: 1.705  loss_mask_3: 1.235  loss_dice_3: 2.153  loss_bbox_3: 2.518  loss_giou_3: 1.665  loss_ce_dn_3: 1.977  loss_mask_dn_3: 1.162  loss_dice_dn_3: 2.014  loss_bbox_dn_3: 0.8702  loss_giou_dn_3: 0.7054  loss_ce_4: 1.7  loss_mask_4: 1.267  loss_dice_4: 2.143  loss_bbox_4: 2.51  loss_giou_4: 1.618  loss_ce_dn_4: 2.026  loss_mask_dn_4: 1.194  loss_dice_dn_4: 2.006  loss_bbox_dn_4: 0.8581  loss_giou_dn_4: 0.6944  loss_ce_5: 1.695  loss_mask_5: 1.264  loss_dice_5: 2.123  loss_bbox_5: 2.515  loss_giou_5: 1.586  loss_ce_dn_5: 1.979  loss_mask_dn_5: 1.148  loss_dice_dn_5: 2.022  loss_bbox_dn_5: 0.8386  loss_giou_dn_5: 0.6823  loss_ce_6: 1.726  loss_mask_6: 1.303  loss_dice_6: 2.142  loss_bbox_6: 2.522  loss_giou_6: 1.58  loss_ce_dn_6: 2.037  loss_mask_dn_6: 1.133  loss_dice_dn_6: 1.982  loss_bbox_dn_6: 0.8317  loss_giou_dn_6: 0.6799  loss_ce_7: 1.912  loss_mask_7: 1.255  loss_dice_7: 2.154  loss_bbox_7: 2.517  loss_giou_7: 1.567  loss_ce_dn_7: 2.117  loss_mask_dn_7: 1.115  loss_dice_dn_7: 2.003  loss_bbox_dn_7: 0.8222  loss_giou_dn_7: 0.6725  loss_ce_8: 1.851  loss_mask_8: 1.276  loss_dice_8: 2.1  loss_bbox_8: 2.525  loss_giou_8: 1.56  loss_ce_dn_8: 2.188  loss_mask_dn_8: 1.134  loss_dice_dn_8: 2.014  loss_bbox_dn_8: 0.8195  loss_giou_dn_8: 0.6705    time: 2.2735  last_time: 2.3115  data_time: 0.0132  last_data_time: 0.0257   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:34:17 d2.utils.events]: \u001b[0m eta: 8 days, 6:24:21  iter: 4179  total_loss: 174.4  loss_ce: 1.824  loss_mask: 1.322  loss_dice: 2.507  loss_bbox: 2.66  loss_giou: 1.54  loss_ce_dn: 2.437  loss_mask_dn: 1.131  loss_dice_dn: 2.349  loss_bbox_dn: 0.7332  loss_giou_dn: 0.6978  loss_ce_0: 6.521  loss_mask_0: 1.253  loss_dice_0: 2.755  loss_bbox_0: 4.022  loss_giou_0: 1.968  loss_ce_1: 2.126  loss_mask_1: 1.357  loss_dice_1: 2.631  loss_bbox_1: 2.991  loss_giou_1: 1.723  loss_ce_dn_1: 2.552  loss_mask_dn_1: 1.175  loss_dice_dn_1: 2.515  loss_bbox_dn_1: 0.9103  loss_giou_dn_1: 0.7646  loss_ce_2: 1.852  loss_mask_2: 1.34  loss_dice_2: 2.504  loss_bbox_2: 2.872  loss_giou_2: 1.586  loss_ce_dn_2: 2.186  loss_mask_dn_2: 1.151  loss_dice_dn_2: 2.444  loss_bbox_dn_2: 0.8507  loss_giou_dn_2: 0.7463  loss_ce_3: 1.773  loss_mask_3: 1.365  loss_dice_3: 2.514  loss_bbox_3: 2.737  loss_giou_3: 1.543  loss_ce_dn_3: 2.163  loss_mask_dn_3: 1.173  loss_dice_dn_3: 2.421  loss_bbox_dn_3: 0.7946  loss_giou_dn_3: 0.7239  loss_ce_4: 1.755  loss_mask_4: 1.33  loss_dice_4: 2.494  loss_bbox_4: 2.663  loss_giou_4: 1.531  loss_ce_dn_4: 2.123  loss_mask_dn_4: 1.206  loss_dice_dn_4: 2.387  loss_bbox_dn_4: 0.7706  loss_giou_dn_4: 0.7021  loss_ce_5: 1.807  loss_mask_5: 1.287  loss_dice_5: 2.516  loss_bbox_5: 2.708  loss_giou_5: 1.524  loss_ce_dn_5: 2.246  loss_mask_dn_5: 1.16  loss_dice_dn_5: 2.369  loss_bbox_dn_5: 0.7444  loss_giou_dn_5: 0.6893  loss_ce_6: 1.87  loss_mask_6: 1.3  loss_dice_6: 2.457  loss_bbox_6: 2.696  loss_giou_6: 1.534  loss_ce_dn_6: 2.23  loss_mask_dn_6: 1.15  loss_dice_dn_6: 2.353  loss_bbox_dn_6: 0.7419  loss_giou_dn_6: 0.6875  loss_ce_7: 1.784  loss_mask_7: 1.306  loss_dice_7: 2.49  loss_bbox_7: 2.646  loss_giou_7: 1.529  loss_ce_dn_7: 2.332  loss_mask_dn_7: 1.129  loss_dice_dn_7: 2.355  loss_bbox_dn_7: 0.7321  loss_giou_dn_7: 0.6871  loss_ce_8: 1.786  loss_mask_8: 1.307  loss_dice_8: 2.481  loss_bbox_8: 2.668  loss_giou_8: 1.53  loss_ce_dn_8: 2.326  loss_mask_dn_8: 1.116  loss_dice_dn_8: 2.348  loss_bbox_dn_8: 0.7289  loss_giou_dn_8: 0.6906    time: 2.2735  last_time: 2.2653  data_time: 0.0137  last_data_time: 0.0088   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:35:03 d2.utils.events]: \u001b[0m eta: 8 days, 6:24:33  iter: 4199  total_loss: 176.4  loss_ce: 1.536  loss_mask: 1.595  loss_dice: 2.196  loss_bbox: 2.685  loss_giou: 1.533  loss_ce_dn: 1.741  loss_mask_dn: 1.402  loss_dice_dn: 2.176  loss_bbox_dn: 0.8296  loss_giou_dn: 0.6839  loss_ce_0: 6.477  loss_mask_0: 1.554  loss_dice_0: 2.422  loss_bbox_0: 3.969  loss_giou_0: 1.897  loss_ce_1: 1.97  loss_mask_1: 1.602  loss_dice_1: 2.203  loss_bbox_1: 2.897  loss_giou_1: 1.646  loss_ce_dn_1: 2.306  loss_mask_dn_1: 1.405  loss_dice_dn_1: 2.187  loss_bbox_dn_1: 0.9715  loss_giou_dn_1: 0.7579  loss_ce_2: 1.708  loss_mask_2: 1.607  loss_dice_2: 2.066  loss_bbox_2: 2.72  loss_giou_2: 1.577  loss_ce_dn_2: 1.875  loss_mask_dn_2: 1.339  loss_dice_dn_2: 2.128  loss_bbox_dn_2: 0.9243  loss_giou_dn_2: 0.7374  loss_ce_3: 1.574  loss_mask_3: 1.564  loss_dice_3: 2.119  loss_bbox_3: 2.681  loss_giou_3: 1.541  loss_ce_dn_3: 1.795  loss_mask_dn_3: 1.336  loss_dice_dn_3: 2.184  loss_bbox_dn_3: 0.8774  loss_giou_dn_3: 0.7165  loss_ce_4: 1.521  loss_mask_4: 1.55  loss_dice_4: 2.129  loss_bbox_4: 2.684  loss_giou_4: 1.542  loss_ce_dn_4: 1.707  loss_mask_dn_4: 1.353  loss_dice_dn_4: 2.169  loss_bbox_dn_4: 0.8556  loss_giou_dn_4: 0.7048  loss_ce_5: 1.498  loss_mask_5: 1.579  loss_dice_5: 2.179  loss_bbox_5: 2.712  loss_giou_5: 1.548  loss_ce_dn_5: 1.667  loss_mask_dn_5: 1.363  loss_dice_dn_5: 2.163  loss_bbox_dn_5: 0.8454  loss_giou_dn_5: 0.6991  loss_ce_6: 1.463  loss_mask_6: 1.567  loss_dice_6: 2.252  loss_bbox_6: 2.667  loss_giou_6: 1.565  loss_ce_dn_6: 1.628  loss_mask_dn_6: 1.385  loss_dice_dn_6: 2.148  loss_bbox_dn_6: 0.8396  loss_giou_dn_6: 0.6969  loss_ce_7: 1.513  loss_mask_7: 1.579  loss_dice_7: 2.159  loss_bbox_7: 2.664  loss_giou_7: 1.554  loss_ce_dn_7: 1.661  loss_mask_dn_7: 1.381  loss_dice_dn_7: 2.169  loss_bbox_dn_7: 0.827  loss_giou_dn_7: 0.6823  loss_ce_8: 1.557  loss_mask_8: 1.587  loss_dice_8: 2.176  loss_bbox_8: 2.67  loss_giou_8: 1.544  loss_ce_dn_8: 1.674  loss_mask_dn_8: 1.387  loss_dice_dn_8: 2.176  loss_bbox_dn_8: 0.8272  loss_giou_dn_8: 0.6841    time: 2.2735  last_time: 2.2513  data_time: 0.0107  last_data_time: 0.0183   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:35:48 d2.utils.events]: \u001b[0m eta: 8 days, 6:24:38  iter: 4219  total_loss: 180.6  loss_ce: 1.648  loss_mask: 1.522  loss_dice: 2.442  loss_bbox: 2.955  loss_giou: 1.617  loss_ce_dn: 2.448  loss_mask_dn: 1.3  loss_dice_dn: 2.217  loss_bbox_dn: 0.7895  loss_giou_dn: 0.6822  loss_ce_0: 6.517  loss_mask_0: 1.426  loss_dice_0: 2.615  loss_bbox_0: 4.372  loss_giou_0: 2.02  loss_ce_1: 1.931  loss_mask_1: 1.461  loss_dice_1: 2.516  loss_bbox_1: 3.158  loss_giou_1: 1.713  loss_ce_dn_1: 2.753  loss_mask_dn_1: 1.403  loss_dice_dn_1: 2.368  loss_bbox_dn_1: 0.9198  loss_giou_dn_1: 0.7718  loss_ce_2: 1.74  loss_mask_2: 1.453  loss_dice_2: 2.454  loss_bbox_2: 3.081  loss_giou_2: 1.713  loss_ce_dn_2: 2.423  loss_mask_dn_2: 1.346  loss_dice_dn_2: 2.216  loss_bbox_dn_2: 0.8746  loss_giou_dn_2: 0.752  loss_ce_3: 1.658  loss_mask_3: 1.439  loss_dice_3: 2.43  loss_bbox_3: 3.044  loss_giou_3: 1.677  loss_ce_dn_3: 2.309  loss_mask_dn_3: 1.26  loss_dice_dn_3: 2.167  loss_bbox_dn_3: 0.8276  loss_giou_dn_3: 0.7285  loss_ce_4: 1.644  loss_mask_4: 1.468  loss_dice_4: 2.463  loss_bbox_4: 2.979  loss_giou_4: 1.687  loss_ce_dn_4: 2.342  loss_mask_dn_4: 1.282  loss_dice_dn_4: 2.21  loss_bbox_dn_4: 0.7974  loss_giou_dn_4: 0.7072  loss_ce_5: 1.652  loss_mask_5: 1.494  loss_dice_5: 2.44  loss_bbox_5: 2.962  loss_giou_5: 1.672  loss_ce_dn_5: 2.329  loss_mask_dn_5: 1.306  loss_dice_dn_5: 2.217  loss_bbox_dn_5: 0.7853  loss_giou_dn_5: 0.6954  loss_ce_6: 1.638  loss_mask_6: 1.517  loss_dice_6: 2.437  loss_bbox_6: 2.938  loss_giou_6: 1.657  loss_ce_dn_6: 2.309  loss_mask_dn_6: 1.338  loss_dice_dn_6: 2.172  loss_bbox_dn_6: 0.7859  loss_giou_dn_6: 0.6923  loss_ce_7: 1.664  loss_mask_7: 1.531  loss_dice_7: 2.448  loss_bbox_7: 2.919  loss_giou_7: 1.631  loss_ce_dn_7: 2.372  loss_mask_dn_7: 1.31  loss_dice_dn_7: 2.183  loss_bbox_dn_7: 0.7786  loss_giou_dn_7: 0.6761  loss_ce_8: 1.596  loss_mask_8: 1.512  loss_dice_8: 2.436  loss_bbox_8: 2.942  loss_giou_8: 1.621  loss_ce_dn_8: 2.437  loss_mask_dn_8: 1.332  loss_dice_dn_8: 2.212  loss_bbox_dn_8: 0.7826  loss_giou_dn_8: 0.6787    time: 2.2735  last_time: 2.2689  data_time: 0.0119  last_data_time: 0.0179   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:36:34 d2.utils.events]: \u001b[0m eta: 8 days, 6:24:14  iter: 4239  total_loss: 148.4  loss_ce: 1.452  loss_mask: 1.507  loss_dice: 2.136  loss_bbox: 2.484  loss_giou: 1.463  loss_ce_dn: 1.747  loss_mask_dn: 1.203  loss_dice_dn: 1.882  loss_bbox_dn: 0.7933  loss_giou_dn: 0.6878  loss_ce_0: 6.506  loss_mask_0: 1.206  loss_dice_0: 2.46  loss_bbox_0: 4.049  loss_giou_0: 1.977  loss_ce_1: 1.706  loss_mask_1: 1.397  loss_dice_1: 2.165  loss_bbox_1: 3.051  loss_giou_1: 1.638  loss_ce_dn_1: 2.329  loss_mask_dn_1: 1.298  loss_dice_dn_1: 2.084  loss_bbox_dn_1: 0.9591  loss_giou_dn_1: 0.7517  loss_ce_2: 1.509  loss_mask_2: 1.337  loss_dice_2: 2.016  loss_bbox_2: 2.831  loss_giou_2: 1.573  loss_ce_dn_2: 1.748  loss_mask_dn_2: 1.156  loss_dice_dn_2: 1.933  loss_bbox_dn_2: 0.9049  loss_giou_dn_2: 0.7346  loss_ce_3: 1.434  loss_mask_3: 1.426  loss_dice_3: 2.065  loss_bbox_3: 2.719  loss_giou_3: 1.544  loss_ce_dn_3: 1.621  loss_mask_dn_3: 1.152  loss_dice_dn_3: 1.944  loss_bbox_dn_3: 0.8568  loss_giou_dn_3: 0.7137  loss_ce_4: 1.468  loss_mask_4: 1.411  loss_dice_4: 2.031  loss_bbox_4: 2.658  loss_giou_4: 1.512  loss_ce_dn_4: 1.547  loss_mask_dn_4: 1.174  loss_dice_dn_4: 1.92  loss_bbox_dn_4: 0.835  loss_giou_dn_4: 0.707  loss_ce_5: 1.52  loss_mask_5: 1.445  loss_dice_5: 2.102  loss_bbox_5: 2.574  loss_giou_5: 1.499  loss_ce_dn_5: 1.609  loss_mask_dn_5: 1.125  loss_dice_dn_5: 1.911  loss_bbox_dn_5: 0.8222  loss_giou_dn_5: 0.6991  loss_ce_6: 1.476  loss_mask_6: 1.479  loss_dice_6: 2.155  loss_bbox_6: 2.541  loss_giou_6: 1.485  loss_ce_dn_6: 1.576  loss_mask_dn_6: 1.144  loss_dice_dn_6: 1.914  loss_bbox_dn_6: 0.817  loss_giou_dn_6: 0.6969  loss_ce_7: 1.506  loss_mask_7: 1.471  loss_dice_7: 2.119  loss_bbox_7: 2.515  loss_giou_7: 1.466  loss_ce_dn_7: 1.685  loss_mask_dn_7: 1.151  loss_dice_dn_7: 1.901  loss_bbox_dn_7: 0.7977  loss_giou_dn_7: 0.6864  loss_ce_8: 1.479  loss_mask_8: 1.483  loss_dice_8: 2.132  loss_bbox_8: 2.498  loss_giou_8: 1.467  loss_ce_dn_8: 1.727  loss_mask_dn_8: 1.203  loss_dice_dn_8: 1.944  loss_bbox_dn_8: 0.7946  loss_giou_dn_8: 0.6868    time: 2.2735  last_time: 2.3214  data_time: 0.0128  last_data_time: 0.0073   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:37:19 d2.utils.events]: \u001b[0m eta: 8 days, 6:23:10  iter: 4259  total_loss: 169.9  loss_ce: 1.603  loss_mask: 1.638  loss_dice: 2.286  loss_bbox: 2.539  loss_giou: 1.526  loss_ce_dn: 2.035  loss_mask_dn: 1.238  loss_dice_dn: 2.041  loss_bbox_dn: 0.7751  loss_giou_dn: 0.6975  loss_ce_0: 6.976  loss_mask_0: 1.305  loss_dice_0: 2.571  loss_bbox_0: 3.913  loss_giou_0: 1.967  loss_ce_1: 1.718  loss_mask_1: 1.478  loss_dice_1: 2.286  loss_bbox_1: 2.792  loss_giou_1: 1.691  loss_ce_dn_1: 2.385  loss_mask_dn_1: 1.329  loss_dice_dn_1: 2.187  loss_bbox_dn_1: 0.9627  loss_giou_dn_1: 0.7752  loss_ce_2: 1.58  loss_mask_2: 1.496  loss_dice_2: 2.277  loss_bbox_2: 2.63  loss_giou_2: 1.648  loss_ce_dn_2: 2.065  loss_mask_dn_2: 1.208  loss_dice_dn_2: 2.06  loss_bbox_dn_2: 0.8945  loss_giou_dn_2: 0.749  loss_ce_3: 1.488  loss_mask_3: 1.537  loss_dice_3: 2.295  loss_bbox_3: 2.599  loss_giou_3: 1.601  loss_ce_dn_3: 2.016  loss_mask_dn_3: 1.231  loss_dice_dn_3: 2.033  loss_bbox_dn_3: 0.8174  loss_giou_dn_3: 0.7173  loss_ce_4: 1.5  loss_mask_4: 1.529  loss_dice_4: 2.282  loss_bbox_4: 2.505  loss_giou_4: 1.605  loss_ce_dn_4: 1.928  loss_mask_dn_4: 1.22  loss_dice_dn_4: 1.995  loss_bbox_dn_4: 0.785  loss_giou_dn_4: 0.7092  loss_ce_5: 1.58  loss_mask_5: 1.514  loss_dice_5: 2.267  loss_bbox_5: 2.469  loss_giou_5: 1.582  loss_ce_dn_5: 1.903  loss_mask_dn_5: 1.28  loss_dice_dn_5: 1.995  loss_bbox_dn_5: 0.7676  loss_giou_dn_5: 0.7032  loss_ce_6: 1.595  loss_mask_6: 1.57  loss_dice_6: 2.278  loss_bbox_6: 2.539  loss_giou_6: 1.552  loss_ce_dn_6: 1.878  loss_mask_dn_6: 1.276  loss_dice_dn_6: 1.954  loss_bbox_dn_6: 0.7606  loss_giou_dn_6: 0.7  loss_ce_7: 1.613  loss_mask_7: 1.581  loss_dice_7: 2.244  loss_bbox_7: 2.437  loss_giou_7: 1.529  loss_ce_dn_7: 1.88  loss_mask_dn_7: 1.268  loss_dice_dn_7: 2.015  loss_bbox_dn_7: 0.755  loss_giou_dn_7: 0.7012  loss_ce_8: 1.63  loss_mask_8: 1.613  loss_dice_8: 2.248  loss_bbox_8: 2.429  loss_giou_8: 1.524  loss_ce_dn_8: 1.924  loss_mask_dn_8: 1.253  loss_dice_dn_8: 2.007  loss_bbox_dn_8: 0.761  loss_giou_dn_8: 0.6964    time: 2.2734  last_time: 2.2559  data_time: 0.0122  last_data_time: 0.0103   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:38:05 d2.utils.events]: \u001b[0m eta: 8 days, 6:22:22  iter: 4279  total_loss: 164.9  loss_ce: 1.413  loss_mask: 1.68  loss_dice: 2.468  loss_bbox: 2.314  loss_giou: 1.415  loss_ce_dn: 1.953  loss_mask_dn: 1.309  loss_dice_dn: 2.286  loss_bbox_dn: 0.7838  loss_giou_dn: 0.6858  loss_ce_0: 6.285  loss_mask_0: 1.21  loss_dice_0: 2.529  loss_bbox_0: 4.024  loss_giou_0: 1.891  loss_ce_1: 1.772  loss_mask_1: 1.456  loss_dice_1: 2.491  loss_bbox_1: 2.887  loss_giou_1: 1.536  loss_ce_dn_1: 2.371  loss_mask_dn_1: 1.268  loss_dice_dn_1: 2.402  loss_bbox_dn_1: 0.9733  loss_giou_dn_1: 0.7575  loss_ce_2: 1.536  loss_mask_2: 1.618  loss_dice_2: 2.453  loss_bbox_2: 2.681  loss_giou_2: 1.509  loss_ce_dn_2: 1.873  loss_mask_dn_2: 1.239  loss_dice_dn_2: 2.376  loss_bbox_dn_2: 0.9063  loss_giou_dn_2: 0.7312  loss_ce_3: 1.398  loss_mask_3: 1.655  loss_dice_3: 2.426  loss_bbox_3: 2.576  loss_giou_3: 1.49  loss_ce_dn_3: 1.871  loss_mask_dn_3: 1.252  loss_dice_dn_3: 2.325  loss_bbox_dn_3: 0.8357  loss_giou_dn_3: 0.7053  loss_ce_4: 1.38  loss_mask_4: 1.635  loss_dice_4: 2.455  loss_bbox_4: 2.422  loss_giou_4: 1.479  loss_ce_dn_4: 1.749  loss_mask_dn_4: 1.207  loss_dice_dn_4: 2.3  loss_bbox_dn_4: 0.8203  loss_giou_dn_4: 0.6964  loss_ce_5: 1.308  loss_mask_5: 1.642  loss_dice_5: 2.455  loss_bbox_5: 2.368  loss_giou_5: 1.435  loss_ce_dn_5: 1.803  loss_mask_dn_5: 1.22  loss_dice_dn_5: 2.286  loss_bbox_dn_5: 0.8083  loss_giou_dn_5: 0.685  loss_ce_6: 1.345  loss_mask_6: 1.592  loss_dice_6: 2.476  loss_bbox_6: 2.353  loss_giou_6: 1.424  loss_ce_dn_6: 1.762  loss_mask_dn_6: 1.2  loss_dice_dn_6: 2.254  loss_bbox_dn_6: 0.8072  loss_giou_dn_6: 0.6866  loss_ce_7: 1.35  loss_mask_7: 1.633  loss_dice_7: 2.463  loss_bbox_7: 2.327  loss_giou_7: 1.406  loss_ce_dn_7: 1.756  loss_mask_dn_7: 1.222  loss_dice_dn_7: 2.246  loss_bbox_dn_7: 0.7872  loss_giou_dn_7: 0.6796  loss_ce_8: 1.357  loss_mask_8: 1.623  loss_dice_8: 2.437  loss_bbox_8: 2.321  loss_giou_8: 1.4  loss_ce_dn_8: 1.846  loss_mask_dn_8: 1.273  loss_dice_dn_8: 2.273  loss_bbox_dn_8: 0.7864  loss_giou_dn_8: 0.6833    time: 2.2734  last_time: 2.2740  data_time: 0.0118  last_data_time: 0.0043   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:38:50 d2.utils.events]: \u001b[0m eta: 8 days, 6:21:59  iter: 4299  total_loss: 172.8  loss_ce: 1.721  loss_mask: 1.565  loss_dice: 2.436  loss_bbox: 2.604  loss_giou: 1.447  loss_ce_dn: 2.216  loss_mask_dn: 1.362  loss_dice_dn: 2.235  loss_bbox_dn: 0.8454  loss_giou_dn: 0.6954  loss_ce_0: 6.421  loss_mask_0: 1.464  loss_dice_0: 2.638  loss_bbox_0: 3.576  loss_giou_0: 1.904  loss_ce_1: 1.934  loss_mask_1: 1.564  loss_dice_1: 2.441  loss_bbox_1: 2.93  loss_giou_1: 1.608  loss_ce_dn_1: 2.727  loss_mask_dn_1: 1.452  loss_dice_dn_1: 2.399  loss_bbox_dn_1: 0.972  loss_giou_dn_1: 0.7753  loss_ce_2: 1.767  loss_mask_2: 1.573  loss_dice_2: 2.448  loss_bbox_2: 2.762  loss_giou_2: 1.584  loss_ce_dn_2: 2.17  loss_mask_dn_2: 1.422  loss_dice_dn_2: 2.338  loss_bbox_dn_2: 0.9289  loss_giou_dn_2: 0.745  loss_ce_3: 1.69  loss_mask_3: 1.571  loss_dice_3: 2.44  loss_bbox_3: 2.602  loss_giou_3: 1.543  loss_ce_dn_3: 2.094  loss_mask_dn_3: 1.376  loss_dice_dn_3: 2.297  loss_bbox_dn_3: 0.8738  loss_giou_dn_3: 0.7213  loss_ce_4: 1.648  loss_mask_4: 1.533  loss_dice_4: 2.429  loss_bbox_4: 2.562  loss_giou_4: 1.536  loss_ce_dn_4: 1.968  loss_mask_dn_4: 1.353  loss_dice_dn_4: 2.297  loss_bbox_dn_4: 0.8517  loss_giou_dn_4: 0.7115  loss_ce_5: 1.61  loss_mask_5: 1.526  loss_dice_5: 2.419  loss_bbox_5: 2.559  loss_giou_5: 1.52  loss_ce_dn_5: 2.065  loss_mask_dn_5: 1.305  loss_dice_dn_5: 2.314  loss_bbox_dn_5: 0.8427  loss_giou_dn_5: 0.7038  loss_ce_6: 1.609  loss_mask_6: 1.53  loss_dice_6: 2.42  loss_bbox_6: 2.57  loss_giou_6: 1.492  loss_ce_dn_6: 2.059  loss_mask_dn_6: 1.328  loss_dice_dn_6: 2.281  loss_bbox_dn_6: 0.8461  loss_giou_dn_6: 0.7005  loss_ce_7: 1.624  loss_mask_7: 1.503  loss_dice_7: 2.439  loss_bbox_7: 2.598  loss_giou_7: 1.451  loss_ce_dn_7: 2.131  loss_mask_dn_7: 1.358  loss_dice_dn_7: 2.269  loss_bbox_dn_7: 0.8479  loss_giou_dn_7: 0.6942  loss_ce_8: 1.712  loss_mask_8: 1.505  loss_dice_8: 2.454  loss_bbox_8: 2.592  loss_giou_8: 1.453  loss_ce_dn_8: 2.165  loss_mask_dn_8: 1.35  loss_dice_dn_8: 2.234  loss_bbox_dn_8: 0.8462  loss_giou_dn_8: 0.6947    time: 2.2734  last_time: 2.3068  data_time: 0.0126  last_data_time: 0.0046   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:39:35 d2.utils.events]: \u001b[0m eta: 8 days, 6:21:44  iter: 4319  total_loss: 174.4  loss_ce: 1.571  loss_mask: 1.315  loss_dice: 2.572  loss_bbox: 2.711  loss_giou: 1.483  loss_ce_dn: 2.571  loss_mask_dn: 1.179  loss_dice_dn: 2.381  loss_bbox_dn: 0.8331  loss_giou_dn: 0.6873  loss_ce_0: 6.204  loss_mask_0: 1.279  loss_dice_0: 2.745  loss_bbox_0: 4.004  loss_giou_0: 1.864  loss_ce_1: 2.306  loss_mask_1: 1.215  loss_dice_1: 2.64  loss_bbox_1: 2.932  loss_giou_1: 1.536  loss_ce_dn_1: 2.683  loss_mask_dn_1: 1.244  loss_dice_dn_1: 2.38  loss_bbox_dn_1: 0.9904  loss_giou_dn_1: 0.7692  loss_ce_2: 1.875  loss_mask_2: 1.325  loss_dice_2: 2.542  loss_bbox_2: 2.862  loss_giou_2: 1.546  loss_ce_dn_2: 2.339  loss_mask_dn_2: 1.227  loss_dice_dn_2: 2.295  loss_bbox_dn_2: 0.9596  loss_giou_dn_2: 0.7556  loss_ce_3: 1.753  loss_mask_3: 1.348  loss_dice_3: 2.533  loss_bbox_3: 2.785  loss_giou_3: 1.574  loss_ce_dn_3: 2.293  loss_mask_dn_3: 1.212  loss_dice_dn_3: 2.362  loss_bbox_dn_3: 0.9013  loss_giou_dn_3: 0.7315  loss_ce_4: 1.672  loss_mask_4: 1.314  loss_dice_4: 2.549  loss_bbox_4: 2.766  loss_giou_4: 1.522  loss_ce_dn_4: 2.343  loss_mask_dn_4: 1.187  loss_dice_dn_4: 2.336  loss_bbox_dn_4: 0.8671  loss_giou_dn_4: 0.7101  loss_ce_5: 1.641  loss_mask_5: 1.383  loss_dice_5: 2.542  loss_bbox_5: 2.718  loss_giou_5: 1.543  loss_ce_dn_5: 2.328  loss_mask_dn_5: 1.19  loss_dice_dn_5: 2.332  loss_bbox_dn_5: 0.851  loss_giou_dn_5: 0.7021  loss_ce_6: 1.664  loss_mask_6: 1.313  loss_dice_6: 2.586  loss_bbox_6: 2.729  loss_giou_6: 1.513  loss_ce_dn_6: 2.489  loss_mask_dn_6: 1.197  loss_dice_dn_6: 2.369  loss_bbox_dn_6: 0.8472  loss_giou_dn_6: 0.6997  loss_ce_7: 1.535  loss_mask_7: 1.33  loss_dice_7: 2.576  loss_bbox_7: 2.718  loss_giou_7: 1.486  loss_ce_dn_7: 2.559  loss_mask_dn_7: 1.229  loss_dice_dn_7: 2.379  loss_bbox_dn_7: 0.825  loss_giou_dn_7: 0.6831  loss_ce_8: 1.558  loss_mask_8: 1.334  loss_dice_8: 2.59  loss_bbox_8: 2.717  loss_giou_8: 1.484  loss_ce_dn_8: 2.668  loss_mask_dn_8: 1.212  loss_dice_dn_8: 2.391  loss_bbox_dn_8: 0.8215  loss_giou_dn_8: 0.6849    time: 2.2734  last_time: 2.2755  data_time: 0.0129  last_data_time: 0.0196   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:40:21 d2.utils.events]: \u001b[0m eta: 8 days, 6:23:02  iter: 4339  total_loss: 162.8  loss_ce: 1.56  loss_mask: 1.194  loss_dice: 2.233  loss_bbox: 2.666  loss_giou: 1.596  loss_ce_dn: 2.013  loss_mask_dn: 1.19  loss_dice_dn: 2.111  loss_bbox_dn: 0.7823  loss_giou_dn: 0.7183  loss_ce_0: 6.118  loss_mask_0: 1.124  loss_dice_0: 2.399  loss_bbox_0: 4.034  loss_giou_0: 1.957  loss_ce_1: 1.82  loss_mask_1: 1.116  loss_dice_1: 2.235  loss_bbox_1: 2.959  loss_giou_1: 1.747  loss_ce_dn_1: 2.412  loss_mask_dn_1: 1.295  loss_dice_dn_1: 2.237  loss_bbox_dn_1: 0.9119  loss_giou_dn_1: 0.7738  loss_ce_2: 1.737  loss_mask_2: 1.3  loss_dice_2: 2.218  loss_bbox_2: 2.882  loss_giou_2: 1.689  loss_ce_dn_2: 1.919  loss_mask_dn_2: 1.296  loss_dice_dn_2: 2.072  loss_bbox_dn_2: 0.871  loss_giou_dn_2: 0.7534  loss_ce_3: 1.771  loss_mask_3: 1.266  loss_dice_3: 2.25  loss_bbox_3: 2.822  loss_giou_3: 1.66  loss_ce_dn_3: 1.865  loss_mask_dn_3: 1.235  loss_dice_dn_3: 2.082  loss_bbox_dn_3: 0.8396  loss_giou_dn_3: 0.7366  loss_ce_4: 1.702  loss_mask_4: 1.238  loss_dice_4: 2.186  loss_bbox_4: 2.795  loss_giou_4: 1.642  loss_ce_dn_4: 1.826  loss_mask_dn_4: 1.194  loss_dice_dn_4: 2.098  loss_bbox_dn_4: 0.8171  loss_giou_dn_4: 0.7301  loss_ce_5: 1.54  loss_mask_5: 1.211  loss_dice_5: 2.22  loss_bbox_5: 2.753  loss_giou_5: 1.619  loss_ce_dn_5: 1.883  loss_mask_dn_5: 1.207  loss_dice_dn_5: 2.106  loss_bbox_dn_5: 0.7895  loss_giou_dn_5: 0.7211  loss_ce_6: 1.591  loss_mask_6: 1.179  loss_dice_6: 2.232  loss_bbox_6: 2.698  loss_giou_6: 1.605  loss_ce_dn_6: 1.878  loss_mask_dn_6: 1.163  loss_dice_dn_6: 2.11  loss_bbox_dn_6: 0.7833  loss_giou_dn_6: 0.7183  loss_ce_7: 1.62  loss_mask_7: 1.176  loss_dice_7: 2.271  loss_bbox_7: 2.675  loss_giou_7: 1.599  loss_ce_dn_7: 1.895  loss_mask_dn_7: 1.166  loss_dice_dn_7: 2.092  loss_bbox_dn_7: 0.7761  loss_giou_dn_7: 0.7139  loss_ce_8: 1.609  loss_mask_8: 1.178  loss_dice_8: 2.265  loss_bbox_8: 2.681  loss_giou_8: 1.597  loss_ce_dn_8: 1.966  loss_mask_dn_8: 1.196  loss_dice_dn_8: 2.08  loss_bbox_dn_8: 0.7743  loss_giou_dn_8: 0.7155    time: 2.2733  last_time: 2.2443  data_time: 0.0143  last_data_time: 0.0087   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:41:06 d2.utils.events]: \u001b[0m eta: 8 days, 6:24:08  iter: 4359  total_loss: 170.3  loss_ce: 1.847  loss_mask: 1.157  loss_dice: 2.529  loss_bbox: 2.814  loss_giou: 1.763  loss_ce_dn: 2.079  loss_mask_dn: 1.136  loss_dice_dn: 2.196  loss_bbox_dn: 0.8253  loss_giou_dn: 0.7247  loss_ce_0: 6.049  loss_mask_0: 1.21  loss_dice_0: 2.731  loss_bbox_0: 4.011  loss_giou_0: 2.068  loss_ce_1: 1.982  loss_mask_1: 1.164  loss_dice_1: 2.568  loss_bbox_1: 2.884  loss_giou_1: 1.758  loss_ce_dn_1: 2.477  loss_mask_dn_1: 1.234  loss_dice_dn_1: 2.48  loss_bbox_dn_1: 0.9275  loss_giou_dn_1: 0.7748  loss_ce_2: 1.792  loss_mask_2: 1.172  loss_dice_2: 2.439  loss_bbox_2: 2.849  loss_giou_2: 1.764  loss_ce_dn_2: 1.99  loss_mask_dn_2: 1.169  loss_dice_dn_2: 2.294  loss_bbox_dn_2: 0.8852  loss_giou_dn_2: 0.7609  loss_ce_3: 1.84  loss_mask_3: 1.149  loss_dice_3: 2.492  loss_bbox_3: 2.794  loss_giou_3: 1.769  loss_ce_dn_3: 1.896  loss_mask_dn_3: 1.165  loss_dice_dn_3: 2.271  loss_bbox_dn_3: 0.8629  loss_giou_dn_3: 0.7442  loss_ce_4: 1.854  loss_mask_4: 1.148  loss_dice_4: 2.523  loss_bbox_4: 2.815  loss_giou_4: 1.763  loss_ce_dn_4: 1.853  loss_mask_dn_4: 1.127  loss_dice_dn_4: 2.26  loss_bbox_dn_4: 0.8382  loss_giou_dn_4: 0.7352  loss_ce_5: 1.865  loss_mask_5: 1.15  loss_dice_5: 2.536  loss_bbox_5: 2.832  loss_giou_5: 1.768  loss_ce_dn_5: 1.851  loss_mask_dn_5: 1.122  loss_dice_dn_5: 2.249  loss_bbox_dn_5: 0.8291  loss_giou_dn_5: 0.7281  loss_ce_6: 1.838  loss_mask_6: 1.154  loss_dice_6: 2.528  loss_bbox_6: 2.851  loss_giou_6: 1.771  loss_ce_dn_6: 1.909  loss_mask_dn_6: 1.144  loss_dice_dn_6: 2.226  loss_bbox_dn_6: 0.827  loss_giou_dn_6: 0.7274  loss_ce_7: 1.821  loss_mask_7: 1.146  loss_dice_7: 2.563  loss_bbox_7: 2.825  loss_giou_7: 1.759  loss_ce_dn_7: 1.926  loss_mask_dn_7: 1.158  loss_dice_dn_7: 2.208  loss_bbox_dn_7: 0.8189  loss_giou_dn_7: 0.719  loss_ce_8: 1.843  loss_mask_8: 1.148  loss_dice_8: 2.566  loss_bbox_8: 2.821  loss_giou_8: 1.767  loss_ce_dn_8: 1.996  loss_mask_dn_8: 1.151  loss_dice_dn_8: 2.21  loss_bbox_dn_8: 0.8207  loss_giou_dn_8: 0.7218    time: 2.2733  last_time: 2.2942  data_time: 0.0140  last_data_time: 0.0259   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:41:52 d2.utils.events]: \u001b[0m eta: 8 days, 6:22:15  iter: 4379  total_loss: 164.6  loss_ce: 1.666  loss_mask: 1.607  loss_dice: 2.268  loss_bbox: 2.622  loss_giou: 1.602  loss_ce_dn: 2.045  loss_mask_dn: 1.412  loss_dice_dn: 1.955  loss_bbox_dn: 0.86  loss_giou_dn: 0.6899  loss_ce_0: 6.249  loss_mask_0: 1.546  loss_dice_0: 2.605  loss_bbox_0: 3.848  loss_giou_0: 1.888  loss_ce_1: 1.768  loss_mask_1: 1.583  loss_dice_1: 2.424  loss_bbox_1: 3.045  loss_giou_1: 1.775  loss_ce_dn_1: 2.578  loss_mask_dn_1: 1.497  loss_dice_dn_1: 2.185  loss_bbox_dn_1: 0.9671  loss_giou_dn_1: 0.7618  loss_ce_2: 1.509  loss_mask_2: 1.538  loss_dice_2: 2.322  loss_bbox_2: 2.901  loss_giou_2: 1.711  loss_ce_dn_2: 2.189  loss_mask_dn_2: 1.498  loss_dice_dn_2: 2.069  loss_bbox_dn_2: 0.9382  loss_giou_dn_2: 0.7408  loss_ce_3: 1.461  loss_mask_3: 1.586  loss_dice_3: 2.287  loss_bbox_3: 2.725  loss_giou_3: 1.666  loss_ce_dn_3: 1.996  loss_mask_dn_3: 1.46  loss_dice_dn_3: 2.006  loss_bbox_dn_3: 0.9026  loss_giou_dn_3: 0.7336  loss_ce_4: 1.5  loss_mask_4: 1.604  loss_dice_4: 2.326  loss_bbox_4: 2.632  loss_giou_4: 1.645  loss_ce_dn_4: 1.879  loss_mask_dn_4: 1.411  loss_dice_dn_4: 1.965  loss_bbox_dn_4: 0.8851  loss_giou_dn_4: 0.7154  loss_ce_5: 1.575  loss_mask_5: 1.616  loss_dice_5: 2.286  loss_bbox_5: 2.622  loss_giou_5: 1.614  loss_ce_dn_5: 1.874  loss_mask_dn_5: 1.383  loss_dice_dn_5: 1.982  loss_bbox_dn_5: 0.8791  loss_giou_dn_5: 0.7029  loss_ce_6: 1.566  loss_mask_6: 1.616  loss_dice_6: 2.294  loss_bbox_6: 2.608  loss_giou_6: 1.611  loss_ce_dn_6: 1.914  loss_mask_dn_6: 1.374  loss_dice_dn_6: 1.964  loss_bbox_dn_6: 0.8762  loss_giou_dn_6: 0.7016  loss_ce_7: 1.577  loss_mask_7: 1.595  loss_dice_7: 2.281  loss_bbox_7: 2.612  loss_giou_7: 1.591  loss_ce_dn_7: 1.935  loss_mask_dn_7: 1.381  loss_dice_dn_7: 1.958  loss_bbox_dn_7: 0.8537  loss_giou_dn_7: 0.6873  loss_ce_8: 1.679  loss_mask_8: 1.59  loss_dice_8: 2.307  loss_bbox_8: 2.62  loss_giou_8: 1.597  loss_ce_dn_8: 1.937  loss_mask_dn_8: 1.402  loss_dice_dn_8: 1.946  loss_bbox_dn_8: 0.8579  loss_giou_dn_8: 0.688    time: 2.2734  last_time: 2.3440  data_time: 0.0122  last_data_time: 0.0153   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:42:38 d2.utils.events]: \u001b[0m eta: 8 days, 6:22:00  iter: 4399  total_loss: 163.5  loss_ce: 1.524  loss_mask: 1.461  loss_dice: 2.405  loss_bbox: 2.353  loss_giou: 1.511  loss_ce_dn: 1.822  loss_mask_dn: 1.363  loss_dice_dn: 2.228  loss_bbox_dn: 0.8216  loss_giou_dn: 0.682  loss_ce_0: 5.769  loss_mask_0: 1.291  loss_dice_0: 2.559  loss_bbox_0: 3.832  loss_giou_0: 1.929  loss_ce_1: 1.705  loss_mask_1: 1.388  loss_dice_1: 2.528  loss_bbox_1: 2.815  loss_giou_1: 1.726  loss_ce_dn_1: 2.341  loss_mask_dn_1: 1.362  loss_dice_dn_1: 2.388  loss_bbox_dn_1: 0.9537  loss_giou_dn_1: 0.7733  loss_ce_2: 1.508  loss_mask_2: 1.394  loss_dice_2: 2.368  loss_bbox_2: 2.57  loss_giou_2: 1.711  loss_ce_dn_2: 1.978  loss_mask_dn_2: 1.365  loss_dice_dn_2: 2.266  loss_bbox_dn_2: 0.9078  loss_giou_dn_2: 0.7416  loss_ce_3: 1.507  loss_mask_3: 1.37  loss_dice_3: 2.384  loss_bbox_3: 2.459  loss_giou_3: 1.665  loss_ce_dn_3: 1.847  loss_mask_dn_3: 1.32  loss_dice_dn_3: 2.211  loss_bbox_dn_3: 0.8727  loss_giou_dn_3: 0.7253  loss_ce_4: 1.418  loss_mask_4: 1.327  loss_dice_4: 2.374  loss_bbox_4: 2.415  loss_giou_4: 1.609  loss_ce_dn_4: 1.74  loss_mask_dn_4: 1.328  loss_dice_dn_4: 2.215  loss_bbox_dn_4: 0.8445  loss_giou_dn_4: 0.708  loss_ce_5: 1.431  loss_mask_5: 1.379  loss_dice_5: 2.413  loss_bbox_5: 2.391  loss_giou_5: 1.567  loss_ce_dn_5: 1.723  loss_mask_dn_5: 1.326  loss_dice_dn_5: 2.247  loss_bbox_dn_5: 0.8287  loss_giou_dn_5: 0.6985  loss_ce_6: 1.404  loss_mask_6: 1.404  loss_dice_6: 2.38  loss_bbox_6: 2.399  loss_giou_6: 1.556  loss_ce_dn_6: 1.722  loss_mask_dn_6: 1.325  loss_dice_dn_6: 2.241  loss_bbox_dn_6: 0.8259  loss_giou_dn_6: 0.6926  loss_ce_7: 1.441  loss_mask_7: 1.402  loss_dice_7: 2.376  loss_bbox_7: 2.366  loss_giou_7: 1.537  loss_ce_dn_7: 1.807  loss_mask_dn_7: 1.32  loss_dice_dn_7: 2.249  loss_bbox_dn_7: 0.8172  loss_giou_dn_7: 0.6849  loss_ce_8: 1.462  loss_mask_8: 1.406  loss_dice_8: 2.397  loss_bbox_8: 2.367  loss_giou_8: 1.527  loss_ce_dn_8: 1.831  loss_mask_dn_8: 1.325  loss_dice_dn_8: 2.232  loss_bbox_dn_8: 0.8173  loss_giou_dn_8: 0.681    time: 2.2735  last_time: 2.2535  data_time: 0.0113  last_data_time: 0.0102   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:43:23 d2.utils.events]: \u001b[0m eta: 8 days, 6:21:15  iter: 4419  total_loss: 168.2  loss_ce: 1.991  loss_mask: 1.224  loss_dice: 2.393  loss_bbox: 2.256  loss_giou: 1.502  loss_ce_dn: 2.613  loss_mask_dn: 1.16  loss_dice_dn: 2.28  loss_bbox_dn: 0.7534  loss_giou_dn: 0.7015  loss_ce_0: 6.769  loss_mask_0: 1.077  loss_dice_0: 2.631  loss_bbox_0: 4.113  loss_giou_0: 2.045  loss_ce_1: 2.358  loss_mask_1: 1.189  loss_dice_1: 2.42  loss_bbox_1: 2.49  loss_giou_1: 1.706  loss_ce_dn_1: 2.871  loss_mask_dn_1: 1.192  loss_dice_dn_1: 2.388  loss_bbox_dn_1: 0.877  loss_giou_dn_1: 0.77  loss_ce_2: 2.05  loss_mask_2: 1.163  loss_dice_2: 2.385  loss_bbox_2: 2.314  loss_giou_2: 1.638  loss_ce_dn_2: 2.388  loss_mask_dn_2: 1.17  loss_dice_dn_2: 2.258  loss_bbox_dn_2: 0.8409  loss_giou_dn_2: 0.7508  loss_ce_3: 1.985  loss_mask_3: 1.155  loss_dice_3: 2.369  loss_bbox_3: 2.26  loss_giou_3: 1.587  loss_ce_dn_3: 2.265  loss_mask_dn_3: 1.138  loss_dice_dn_3: 2.243  loss_bbox_dn_3: 0.7984  loss_giou_dn_3: 0.7261  loss_ce_4: 1.972  loss_mask_4: 1.159  loss_dice_4: 2.396  loss_bbox_4: 2.297  loss_giou_4: 1.6  loss_ce_dn_4: 2.255  loss_mask_dn_4: 1.118  loss_dice_dn_4: 2.251  loss_bbox_dn_4: 0.7778  loss_giou_dn_4: 0.7128  loss_ce_5: 1.979  loss_mask_5: 1.162  loss_dice_5: 2.394  loss_bbox_5: 2.253  loss_giou_5: 1.572  loss_ce_dn_5: 2.247  loss_mask_dn_5: 1.139  loss_dice_dn_5: 2.276  loss_bbox_dn_5: 0.7556  loss_giou_dn_5: 0.705  loss_ce_6: 2.04  loss_mask_6: 1.197  loss_dice_6: 2.39  loss_bbox_6: 2.242  loss_giou_6: 1.57  loss_ce_dn_6: 2.266  loss_mask_dn_6: 1.145  loss_dice_dn_6: 2.287  loss_bbox_dn_6: 0.7568  loss_giou_dn_6: 0.7055  loss_ce_7: 1.984  loss_mask_7: 1.247  loss_dice_7: 2.391  loss_bbox_7: 2.24  loss_giou_7: 1.519  loss_ce_dn_7: 2.458  loss_mask_dn_7: 1.156  loss_dice_dn_7: 2.292  loss_bbox_dn_7: 0.7397  loss_giou_dn_7: 0.6945  loss_ce_8: 2.008  loss_mask_8: 1.217  loss_dice_8: 2.373  loss_bbox_8: 2.253  loss_giou_8: 1.512  loss_ce_dn_8: 2.571  loss_mask_dn_8: 1.154  loss_dice_dn_8: 2.264  loss_bbox_dn_8: 0.7473  loss_giou_dn_8: 0.6996    time: 2.2734  last_time: 2.2646  data_time: 0.0097  last_data_time: 0.0053   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:44:09 d2.utils.events]: \u001b[0m eta: 8 days, 6:20:57  iter: 4439  total_loss: 168.8  loss_ce: 1.739  loss_mask: 1.286  loss_dice: 2.644  loss_bbox: 2.629  loss_giou: 1.546  loss_ce_dn: 2.079  loss_mask_dn: 1.123  loss_dice_dn: 2.267  loss_bbox_dn: 0.7625  loss_giou_dn: 0.7099  loss_ce_0: 6.052  loss_mask_0: 1.379  loss_dice_0: 2.776  loss_bbox_0: 4.257  loss_giou_0: 1.964  loss_ce_1: 1.799  loss_mask_1: 1.423  loss_dice_1: 2.685  loss_bbox_1: 3.152  loss_giou_1: 1.686  loss_ce_dn_1: 2.252  loss_mask_dn_1: 1.177  loss_dice_dn_1: 2.299  loss_bbox_dn_1: 0.8771  loss_giou_dn_1: 0.7801  loss_ce_2: 1.658  loss_mask_2: 1.37  loss_dice_2: 2.596  loss_bbox_2: 2.851  loss_giou_2: 1.607  loss_ce_dn_2: 1.837  loss_mask_dn_2: 1.142  loss_dice_dn_2: 2.261  loss_bbox_dn_2: 0.8252  loss_giou_dn_2: 0.753  loss_ce_3: 1.558  loss_mask_3: 1.356  loss_dice_3: 2.575  loss_bbox_3: 2.768  loss_giou_3: 1.581  loss_ce_dn_3: 1.742  loss_mask_dn_3: 1.115  loss_dice_dn_3: 2.27  loss_bbox_dn_3: 0.7984  loss_giou_dn_3: 0.7353  loss_ce_4: 1.526  loss_mask_4: 1.337  loss_dice_4: 2.561  loss_bbox_4: 2.697  loss_giou_4: 1.596  loss_ce_dn_4: 1.754  loss_mask_dn_4: 1.138  loss_dice_dn_4: 2.257  loss_bbox_dn_4: 0.7758  loss_giou_dn_4: 0.723  loss_ce_5: 1.561  loss_mask_5: 1.357  loss_dice_5: 2.583  loss_bbox_5: 2.685  loss_giou_5: 1.577  loss_ce_dn_5: 1.783  loss_mask_dn_5: 1.177  loss_dice_dn_5: 2.263  loss_bbox_dn_5: 0.7708  loss_giou_dn_5: 0.7107  loss_ce_6: 1.616  loss_mask_6: 1.345  loss_dice_6: 2.598  loss_bbox_6: 2.652  loss_giou_6: 1.576  loss_ce_dn_6: 1.831  loss_mask_dn_6: 1.143  loss_dice_dn_6: 2.238  loss_bbox_dn_6: 0.7614  loss_giou_dn_6: 0.7103  loss_ce_7: 1.607  loss_mask_7: 1.312  loss_dice_7: 2.614  loss_bbox_7: 2.623  loss_giou_7: 1.554  loss_ce_dn_7: 1.852  loss_mask_dn_7: 1.126  loss_dice_dn_7: 2.248  loss_bbox_dn_7: 0.7477  loss_giou_dn_7: 0.7108  loss_ce_8: 1.648  loss_mask_8: 1.254  loss_dice_8: 2.597  loss_bbox_8: 2.626  loss_giou_8: 1.55  loss_ce_dn_8: 1.89  loss_mask_dn_8: 1.133  loss_dice_dn_8: 2.25  loss_bbox_dn_8: 0.7536  loss_giou_dn_8: 0.7102    time: 2.2734  last_time: 2.2716  data_time: 0.0121  last_data_time: 0.0053   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:44:54 d2.utils.events]: \u001b[0m eta: 8 days, 6:17:52  iter: 4459  total_loss: 173.8  loss_ce: 1.754  loss_mask: 1.523  loss_dice: 2.403  loss_bbox: 2.976  loss_giou: 1.651  loss_ce_dn: 1.844  loss_mask_dn: 1.226  loss_dice_dn: 2.043  loss_bbox_dn: 0.7926  loss_giou_dn: 0.7085  loss_ce_0: 6.181  loss_mask_0: 1.39  loss_dice_0: 2.603  loss_bbox_0: 4.185  loss_giou_0: 1.987  loss_ce_1: 1.955  loss_mask_1: 1.524  loss_dice_1: 2.407  loss_bbox_1: 3.036  loss_giou_1: 1.809  loss_ce_dn_1: 2.276  loss_mask_dn_1: 1.349  loss_dice_dn_1: 2.163  loss_bbox_dn_1: 0.905  loss_giou_dn_1: 0.764  loss_ce_2: 1.774  loss_mask_2: 1.485  loss_dice_2: 2.413  loss_bbox_2: 3.077  loss_giou_2: 1.728  loss_ce_dn_2: 1.943  loss_mask_dn_2: 1.275  loss_dice_dn_2: 2.008  loss_bbox_dn_2: 0.8803  loss_giou_dn_2: 0.7484  loss_ce_3: 1.697  loss_mask_3: 1.473  loss_dice_3: 2.387  loss_bbox_3: 2.998  loss_giou_3: 1.733  loss_ce_dn_3: 1.851  loss_mask_dn_3: 1.192  loss_dice_dn_3: 1.992  loss_bbox_dn_3: 0.8245  loss_giou_dn_3: 0.7344  loss_ce_4: 1.719  loss_mask_4: 1.541  loss_dice_4: 2.36  loss_bbox_4: 2.998  loss_giou_4: 1.69  loss_ce_dn_4: 1.807  loss_mask_dn_4: 1.168  loss_dice_dn_4: 2.007  loss_bbox_dn_4: 0.8125  loss_giou_dn_4: 0.7255  loss_ce_5: 1.706  loss_mask_5: 1.574  loss_dice_5: 2.347  loss_bbox_5: 3.004  loss_giou_5: 1.668  loss_ce_dn_5: 1.788  loss_mask_dn_5: 1.238  loss_dice_dn_5: 1.99  loss_bbox_dn_5: 0.7934  loss_giou_dn_5: 0.7186  loss_ce_6: 1.686  loss_mask_6: 1.519  loss_dice_6: 2.342  loss_bbox_6: 2.994  loss_giou_6: 1.677  loss_ce_dn_6: 1.776  loss_mask_dn_6: 1.233  loss_dice_dn_6: 1.961  loss_bbox_dn_6: 0.7933  loss_giou_dn_6: 0.7217  loss_ce_7: 1.679  loss_mask_7: 1.524  loss_dice_7: 2.326  loss_bbox_7: 2.974  loss_giou_7: 1.662  loss_ce_dn_7: 1.82  loss_mask_dn_7: 1.223  loss_dice_dn_7: 2.008  loss_bbox_dn_7: 0.7889  loss_giou_dn_7: 0.7126  loss_ce_8: 1.68  loss_mask_8: 1.541  loss_dice_8: 2.377  loss_bbox_8: 2.968  loss_giou_8: 1.66  loss_ce_dn_8: 1.796  loss_mask_dn_8: 1.205  loss_dice_dn_8: 2.027  loss_bbox_dn_8: 0.788  loss_giou_dn_8: 0.7084    time: 2.2734  last_time: 2.2931  data_time: 0.0126  last_data_time: 0.0197   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:45:40 d2.utils.events]: \u001b[0m eta: 8 days, 6:20:02  iter: 4479  total_loss: 165.8  loss_ce: 1.431  loss_mask: 1.371  loss_dice: 2.414  loss_bbox: 2.437  loss_giou: 1.569  loss_ce_dn: 1.879  loss_mask_dn: 1.425  loss_dice_dn: 1.928  loss_bbox_dn: 0.7576  loss_giou_dn: 0.6442  loss_ce_0: 6.216  loss_mask_0: 1.358  loss_dice_0: 2.379  loss_bbox_0: 4.09  loss_giou_0: 1.894  loss_ce_1: 1.984  loss_mask_1: 1.34  loss_dice_1: 2.376  loss_bbox_1: 2.923  loss_giou_1: 1.661  loss_ce_dn_1: 2.383  loss_mask_dn_1: 1.411  loss_dice_dn_1: 2.081  loss_bbox_dn_1: 0.9718  loss_giou_dn_1: 0.751  loss_ce_2: 1.454  loss_mask_2: 1.292  loss_dice_2: 2.245  loss_bbox_2: 2.739  loss_giou_2: 1.611  loss_ce_dn_2: 2.003  loss_mask_dn_2: 1.324  loss_dice_dn_2: 1.929  loss_bbox_dn_2: 0.8859  loss_giou_dn_2: 0.7061  loss_ce_3: 1.424  loss_mask_3: 1.341  loss_dice_3: 2.288  loss_bbox_3: 2.665  loss_giou_3: 1.604  loss_ce_dn_3: 1.884  loss_mask_dn_3: 1.325  loss_dice_dn_3: 1.921  loss_bbox_dn_3: 0.835  loss_giou_dn_3: 0.6806  loss_ce_4: 1.47  loss_mask_4: 1.361  loss_dice_4: 2.335  loss_bbox_4: 2.515  loss_giou_4: 1.595  loss_ce_dn_4: 1.775  loss_mask_dn_4: 1.302  loss_dice_dn_4: 1.901  loss_bbox_dn_4: 0.8058  loss_giou_dn_4: 0.664  loss_ce_5: 1.399  loss_mask_5: 1.329  loss_dice_5: 2.353  loss_bbox_5: 2.49  loss_giou_5: 1.604  loss_ce_dn_5: 1.824  loss_mask_dn_5: 1.299  loss_dice_dn_5: 1.91  loss_bbox_dn_5: 0.7803  loss_giou_dn_5: 0.6572  loss_ce_6: 1.428  loss_mask_6: 1.323  loss_dice_6: 2.351  loss_bbox_6: 2.479  loss_giou_6: 1.577  loss_ce_dn_6: 1.782  loss_mask_dn_6: 1.371  loss_dice_dn_6: 1.932  loss_bbox_dn_6: 0.7764  loss_giou_dn_6: 0.6547  loss_ce_7: 1.441  loss_mask_7: 1.343  loss_dice_7: 2.367  loss_bbox_7: 2.446  loss_giou_7: 1.578  loss_ce_dn_7: 1.847  loss_mask_dn_7: 1.376  loss_dice_dn_7: 1.918  loss_bbox_dn_7: 0.758  loss_giou_dn_7: 0.6424  loss_ce_8: 1.376  loss_mask_8: 1.366  loss_dice_8: 2.372  loss_bbox_8: 2.444  loss_giou_8: 1.574  loss_ce_dn_8: 1.844  loss_mask_dn_8: 1.372  loss_dice_dn_8: 1.903  loss_bbox_dn_8: 0.7587  loss_giou_dn_8: 0.6439    time: 2.2734  last_time: 2.2649  data_time: 0.0124  last_data_time: 0.0072   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:46:25 d2.utils.events]: \u001b[0m eta: 8 days, 6:19:57  iter: 4499  total_loss: 170.4  loss_ce: 1.772  loss_mask: 1.205  loss_dice: 2.244  loss_bbox: 2.64  loss_giou: 1.413  loss_ce_dn: 2.394  loss_mask_dn: 1.203  loss_dice_dn: 2.062  loss_bbox_dn: 0.7999  loss_giou_dn: 0.6846  loss_ce_0: 6.299  loss_mask_0: 1.321  loss_dice_0: 2.583  loss_bbox_0: 4.378  loss_giou_0: 2.056  loss_ce_1: 2.096  loss_mask_1: 1.144  loss_dice_1: 2.333  loss_bbox_1: 3.079  loss_giou_1: 1.596  loss_ce_dn_1: 2.714  loss_mask_dn_1: 1.254  loss_dice_dn_1: 2.197  loss_bbox_dn_1: 0.9298  loss_giou_dn_1: 0.7593  loss_ce_2: 1.982  loss_mask_2: 1.131  loss_dice_2: 2.273  loss_bbox_2: 2.804  loss_giou_2: 1.607  loss_ce_dn_2: 2.314  loss_mask_dn_2: 1.311  loss_dice_dn_2: 2.082  loss_bbox_dn_2: 0.8713  loss_giou_dn_2: 0.7285  loss_ce_3: 1.807  loss_mask_3: 1.148  loss_dice_3: 2.294  loss_bbox_3: 2.643  loss_giou_3: 1.565  loss_ce_dn_3: 2.249  loss_mask_dn_3: 1.264  loss_dice_dn_3: 2.076  loss_bbox_dn_3: 0.835  loss_giou_dn_3: 0.7085  loss_ce_4: 1.757  loss_mask_4: 1.174  loss_dice_4: 2.26  loss_bbox_4: 2.675  loss_giou_4: 1.522  loss_ce_dn_4: 2.183  loss_mask_dn_4: 1.238  loss_dice_dn_4: 2.031  loss_bbox_dn_4: 0.825  loss_giou_dn_4: 0.6995  loss_ce_5: 1.727  loss_mask_5: 1.24  loss_dice_5: 2.222  loss_bbox_5: 2.65  loss_giou_5: 1.461  loss_ce_dn_5: 2.167  loss_mask_dn_5: 1.265  loss_dice_dn_5: 2.01  loss_bbox_dn_5: 0.8183  loss_giou_dn_5: 0.689  loss_ce_6: 1.627  loss_mask_6: 1.227  loss_dice_6: 2.247  loss_bbox_6: 2.676  loss_giou_6: 1.442  loss_ce_dn_6: 2.18  loss_mask_dn_6: 1.188  loss_dice_dn_6: 2.041  loss_bbox_dn_6: 0.8163  loss_giou_dn_6: 0.6853  loss_ce_7: 1.667  loss_mask_7: 1.189  loss_dice_7: 2.266  loss_bbox_7: 2.602  loss_giou_7: 1.428  loss_ce_dn_7: 2.239  loss_mask_dn_7: 1.18  loss_dice_dn_7: 2.037  loss_bbox_dn_7: 0.799  loss_giou_dn_7: 0.6888  loss_ce_8: 1.693  loss_mask_8: 1.232  loss_dice_8: 2.234  loss_bbox_8: 2.644  loss_giou_8: 1.419  loss_ce_dn_8: 2.331  loss_mask_dn_8: 1.198  loss_dice_dn_8: 2.008  loss_bbox_dn_8: 0.7996  loss_giou_dn_8: 0.6859    time: 2.2734  last_time: 2.2276  data_time: 0.0154  last_data_time: 0.0067   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:47:11 d2.utils.events]: \u001b[0m eta: 8 days, 6:19:11  iter: 4519  total_loss: 157.2  loss_ce: 1.596  loss_mask: 1.237  loss_dice: 2.138  loss_bbox: 2.463  loss_giou: 1.431  loss_ce_dn: 1.855  loss_mask_dn: 1.188  loss_dice_dn: 1.88  loss_bbox_dn: 0.7877  loss_giou_dn: 0.6502  loss_ce_0: 5.906  loss_mask_0: 1.308  loss_dice_0: 2.361  loss_bbox_0: 3.863  loss_giou_0: 1.935  loss_ce_1: 1.96  loss_mask_1: 1.281  loss_dice_1: 2.176  loss_bbox_1: 2.622  loss_giou_1: 1.529  loss_ce_dn_1: 2.42  loss_mask_dn_1: 1.284  loss_dice_dn_1: 2.041  loss_bbox_dn_1: 0.915  loss_giou_dn_1: 0.7378  loss_ce_2: 1.779  loss_mask_2: 1.296  loss_dice_2: 2.17  loss_bbox_2: 2.595  loss_giou_2: 1.542  loss_ce_dn_2: 1.948  loss_mask_dn_2: 1.285  loss_dice_dn_2: 1.964  loss_bbox_dn_2: 0.8771  loss_giou_dn_2: 0.7202  loss_ce_3: 1.789  loss_mask_3: 1.362  loss_dice_3: 2.147  loss_bbox_3: 2.496  loss_giou_3: 1.518  loss_ce_dn_3: 1.85  loss_mask_dn_3: 1.297  loss_dice_dn_3: 1.922  loss_bbox_dn_3: 0.824  loss_giou_dn_3: 0.6895  loss_ce_4: 1.701  loss_mask_4: 1.35  loss_dice_4: 2.136  loss_bbox_4: 2.472  loss_giou_4: 1.479  loss_ce_dn_4: 1.812  loss_mask_dn_4: 1.254  loss_dice_dn_4: 1.867  loss_bbox_dn_4: 0.8006  loss_giou_dn_4: 0.6701  loss_ce_5: 1.769  loss_mask_5: 1.278  loss_dice_5: 2.123  loss_bbox_5: 2.438  loss_giou_5: 1.499  loss_ce_dn_5: 1.81  loss_mask_dn_5: 1.198  loss_dice_dn_5: 1.901  loss_bbox_dn_5: 0.7843  loss_giou_dn_5: 0.6584  loss_ce_6: 1.701  loss_mask_6: 1.293  loss_dice_6: 2.166  loss_bbox_6: 2.437  loss_giou_6: 1.485  loss_ce_dn_6: 1.812  loss_mask_dn_6: 1.195  loss_dice_dn_6: 1.926  loss_bbox_dn_6: 0.7859  loss_giou_dn_6: 0.6499  loss_ce_7: 1.614  loss_mask_7: 1.238  loss_dice_7: 2.126  loss_bbox_7: 2.434  loss_giou_7: 1.482  loss_ce_dn_7: 1.823  loss_mask_dn_7: 1.206  loss_dice_dn_7: 1.89  loss_bbox_dn_7: 0.7737  loss_giou_dn_7: 0.6405  loss_ce_8: 1.624  loss_mask_8: 1.224  loss_dice_8: 2.116  loss_bbox_8: 2.435  loss_giou_8: 1.487  loss_ce_dn_8: 1.773  loss_mask_dn_8: 1.208  loss_dice_dn_8: 1.905  loss_bbox_dn_8: 0.7719  loss_giou_dn_8: 0.6394    time: 2.2734  last_time: 2.2604  data_time: 0.0105  last_data_time: 0.0047   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:47:56 d2.utils.events]: \u001b[0m eta: 8 days, 6:18:54  iter: 4539  total_loss: 166.8  loss_ce: 1.799  loss_mask: 1.137  loss_dice: 2.327  loss_bbox: 2.553  loss_giou: 1.548  loss_ce_dn: 2.02  loss_mask_dn: 0.9926  loss_dice_dn: 2.127  loss_bbox_dn: 0.7504  loss_giou_dn: 0.7012  loss_ce_0: 5.86  loss_mask_0: 1.121  loss_dice_0: 2.649  loss_bbox_0: 4.256  loss_giou_0: 2.001  loss_ce_1: 1.847  loss_mask_1: 1.175  loss_dice_1: 2.372  loss_bbox_1: 3.014  loss_giou_1: 1.744  loss_ce_dn_1: 2.303  loss_mask_dn_1: 1.083  loss_dice_dn_1: 2.303  loss_bbox_dn_1: 0.8997  loss_giou_dn_1: 0.7719  loss_ce_2: 1.729  loss_mask_2: 1.144  loss_dice_2: 2.335  loss_bbox_2: 2.786  loss_giou_2: 1.716  loss_ce_dn_2: 1.912  loss_mask_dn_2: 0.9772  loss_dice_dn_2: 2.119  loss_bbox_dn_2: 0.8695  loss_giou_dn_2: 0.7438  loss_ce_3: 1.782  loss_mask_3: 1.168  loss_dice_3: 2.362  loss_bbox_3: 2.641  loss_giou_3: 1.606  loss_ce_dn_3: 1.835  loss_mask_dn_3: 0.9716  loss_dice_dn_3: 2.166  loss_bbox_dn_3: 0.8132  loss_giou_dn_3: 0.7269  loss_ce_4: 1.778  loss_mask_4: 1.212  loss_dice_4: 2.312  loss_bbox_4: 2.668  loss_giou_4: 1.574  loss_ce_dn_4: 1.757  loss_mask_dn_4: 0.9983  loss_dice_dn_4: 2.143  loss_bbox_dn_4: 0.784  loss_giou_dn_4: 0.7186  loss_ce_5: 1.738  loss_mask_5: 1.179  loss_dice_5: 2.304  loss_bbox_5: 2.601  loss_giou_5: 1.542  loss_ce_dn_5: 1.772  loss_mask_dn_5: 0.9846  loss_dice_dn_5: 2.12  loss_bbox_dn_5: 0.777  loss_giou_dn_5: 0.7072  loss_ce_6: 1.73  loss_mask_6: 1.16  loss_dice_6: 2.321  loss_bbox_6: 2.612  loss_giou_6: 1.538  loss_ce_dn_6: 1.741  loss_mask_dn_6: 0.9986  loss_dice_dn_6: 2.101  loss_bbox_dn_6: 0.7746  loss_giou_dn_6: 0.7046  loss_ce_7: 1.774  loss_mask_7: 1.133  loss_dice_7: 2.334  loss_bbox_7: 2.599  loss_giou_7: 1.534  loss_ce_dn_7: 1.76  loss_mask_dn_7: 0.9907  loss_dice_dn_7: 2.076  loss_bbox_dn_7: 0.7307  loss_giou_dn_7: 0.6859  loss_ce_8: 1.794  loss_mask_8: 1.145  loss_dice_8: 2.327  loss_bbox_8: 2.561  loss_giou_8: 1.538  loss_ce_dn_8: 1.908  loss_mask_dn_8: 0.9747  loss_dice_dn_8: 2.122  loss_bbox_dn_8: 0.7505  loss_giou_dn_8: 0.6952    time: 2.2734  last_time: 2.2862  data_time: 0.0131  last_data_time: 0.0104   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:48:42 d2.utils.events]: \u001b[0m eta: 8 days, 6:20:50  iter: 4559  total_loss: 166.3  loss_ce: 1.809  loss_mask: 1.493  loss_dice: 2.18  loss_bbox: 2.359  loss_giou: 1.617  loss_ce_dn: 2.109  loss_mask_dn: 1.216  loss_dice_dn: 1.924  loss_bbox_dn: 0.8112  loss_giou_dn: 0.6819  loss_ce_0: 5.891  loss_mask_0: 1.431  loss_dice_0: 2.372  loss_bbox_0: 4.082  loss_giou_0: 1.945  loss_ce_1: 2.028  loss_mask_1: 1.508  loss_dice_1: 2.23  loss_bbox_1: 2.917  loss_giou_1: 1.723  loss_ce_dn_1: 2.562  loss_mask_dn_1: 1.285  loss_dice_dn_1: 2.145  loss_bbox_dn_1: 0.9345  loss_giou_dn_1: 0.7567  loss_ce_2: 1.728  loss_mask_2: 1.424  loss_dice_2: 2.174  loss_bbox_2: 2.736  loss_giou_2: 1.714  loss_ce_dn_2: 2.022  loss_mask_dn_2: 1.304  loss_dice_dn_2: 1.964  loss_bbox_dn_2: 0.8801  loss_giou_dn_2: 0.734  loss_ce_3: 1.668  loss_mask_3: 1.422  loss_dice_3: 2.152  loss_bbox_3: 2.503  loss_giou_3: 1.688  loss_ce_dn_3: 1.896  loss_mask_dn_3: 1.217  loss_dice_dn_3: 1.918  loss_bbox_dn_3: 0.8233  loss_giou_dn_3: 0.7088  loss_ce_4: 1.675  loss_mask_4: 1.441  loss_dice_4: 2.14  loss_bbox_4: 2.452  loss_giou_4: 1.669  loss_ce_dn_4: 1.888  loss_mask_dn_4: 1.203  loss_dice_dn_4: 1.864  loss_bbox_dn_4: 0.8051  loss_giou_dn_4: 0.7077  loss_ce_5: 1.747  loss_mask_5: 1.468  loss_dice_5: 2.152  loss_bbox_5: 2.39  loss_giou_5: 1.687  loss_ce_dn_5: 1.841  loss_mask_dn_5: 1.206  loss_dice_dn_5: 1.896  loss_bbox_dn_5: 0.7902  loss_giou_dn_5: 0.7007  loss_ce_6: 1.736  loss_mask_6: 1.46  loss_dice_6: 2.257  loss_bbox_6: 2.37  loss_giou_6: 1.68  loss_ce_dn_6: 1.84  loss_mask_dn_6: 1.198  loss_dice_dn_6: 1.907  loss_bbox_dn_6: 0.8076  loss_giou_dn_6: 0.6986  loss_ce_7: 1.711  loss_mask_7: 1.498  loss_dice_7: 2.233  loss_bbox_7: 2.363  loss_giou_7: 1.627  loss_ce_dn_7: 1.92  loss_mask_dn_7: 1.202  loss_dice_dn_7: 1.932  loss_bbox_dn_7: 0.8034  loss_giou_dn_7: 0.685  loss_ce_8: 1.712  loss_mask_8: 1.475  loss_dice_8: 2.201  loss_bbox_8: 2.357  loss_giou_8: 1.624  loss_ce_dn_8: 1.975  loss_mask_dn_8: 1.204  loss_dice_dn_8: 1.923  loss_bbox_dn_8: 0.8021  loss_giou_dn_8: 0.6808    time: 2.2734  last_time: 2.2882  data_time: 0.0119  last_data_time: 0.0150   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:49:27 d2.utils.events]: \u001b[0m eta: 8 days, 6:20:05  iter: 4579  total_loss: 170.6  loss_ce: 1.921  loss_mask: 1.535  loss_dice: 2.404  loss_bbox: 2.686  loss_giou: 1.599  loss_ce_dn: 2.091  loss_mask_dn: 1.335  loss_dice_dn: 2.004  loss_bbox_dn: 0.7827  loss_giou_dn: 0.6985  loss_ce_0: 5.906  loss_mask_0: 1.528  loss_dice_0: 2.645  loss_bbox_0: 4.275  loss_giou_0: 2.041  loss_ce_1: 2.021  loss_mask_1: 1.586  loss_dice_1: 2.491  loss_bbox_1: 3.115  loss_giou_1: 1.678  loss_ce_dn_1: 2.547  loss_mask_dn_1: 1.323  loss_dice_dn_1: 2.226  loss_bbox_dn_1: 0.92  loss_giou_dn_1: 0.7657  loss_ce_2: 1.854  loss_mask_2: 1.537  loss_dice_2: 2.33  loss_bbox_2: 2.968  loss_giou_2: 1.662  loss_ce_dn_2: 2.136  loss_mask_dn_2: 1.28  loss_dice_dn_2: 2.013  loss_bbox_dn_2: 0.8703  loss_giou_dn_2: 0.7432  loss_ce_3: 1.902  loss_mask_3: 1.501  loss_dice_3: 2.321  loss_bbox_3: 2.832  loss_giou_3: 1.666  loss_ce_dn_3: 2.01  loss_mask_dn_3: 1.267  loss_dice_dn_3: 2.005  loss_bbox_dn_3: 0.8119  loss_giou_dn_3: 0.7253  loss_ce_4: 1.889  loss_mask_4: 1.505  loss_dice_4: 2.35  loss_bbox_4: 2.808  loss_giou_4: 1.638  loss_ce_dn_4: 1.958  loss_mask_dn_4: 1.325  loss_dice_dn_4: 1.996  loss_bbox_dn_4: 0.7893  loss_giou_dn_4: 0.7137  loss_ce_5: 1.833  loss_mask_5: 1.517  loss_dice_5: 2.326  loss_bbox_5: 2.682  loss_giou_5: 1.63  loss_ce_dn_5: 2.091  loss_mask_dn_5: 1.292  loss_dice_dn_5: 1.989  loss_bbox_dn_5: 0.7977  loss_giou_dn_5: 0.7058  loss_ce_6: 1.876  loss_mask_6: 1.54  loss_dice_6: 2.313  loss_bbox_6: 2.677  loss_giou_6: 1.622  loss_ce_dn_6: 2.084  loss_mask_dn_6: 1.356  loss_dice_dn_6: 1.967  loss_bbox_dn_6: 0.7959  loss_giou_dn_6: 0.7049  loss_ce_7: 1.863  loss_mask_7: 1.536  loss_dice_7: 2.396  loss_bbox_7: 2.697  loss_giou_7: 1.6  loss_ce_dn_7: 1.943  loss_mask_dn_7: 1.335  loss_dice_dn_7: 1.979  loss_bbox_dn_7: 0.7823  loss_giou_dn_7: 0.694  loss_ce_8: 1.916  loss_mask_8: 1.51  loss_dice_8: 2.425  loss_bbox_8: 2.689  loss_giou_8: 1.601  loss_ce_dn_8: 2.03  loss_mask_dn_8: 1.323  loss_dice_dn_8: 2.009  loss_bbox_dn_8: 0.7817  loss_giou_dn_8: 0.6994    time: 2.2734  last_time: 2.2460  data_time: 0.0125  last_data_time: 0.0046   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:50:13 d2.utils.events]: \u001b[0m eta: 8 days, 6:19:42  iter: 4599  total_loss: 173.7  loss_ce: 1.881  loss_mask: 1.506  loss_dice: 2.434  loss_bbox: 2.777  loss_giou: 1.438  loss_ce_dn: 2.137  loss_mask_dn: 1.276  loss_dice_dn: 2.148  loss_bbox_dn: 0.8241  loss_giou_dn: 0.6782  loss_ce_0: 5.95  loss_mask_0: 1.309  loss_dice_0: 2.466  loss_bbox_0: 4.04  loss_giou_0: 1.95  loss_ce_1: 2.075  loss_mask_1: 1.478  loss_dice_1: 2.398  loss_bbox_1: 2.892  loss_giou_1: 1.559  loss_ce_dn_1: 2.492  loss_mask_dn_1: 1.324  loss_dice_dn_1: 2.369  loss_bbox_dn_1: 0.9568  loss_giou_dn_1: 0.7608  loss_ce_2: 1.781  loss_mask_2: 1.506  loss_dice_2: 2.365  loss_bbox_2: 2.858  loss_giou_2: 1.526  loss_ce_dn_2: 2.022  loss_mask_dn_2: 1.265  loss_dice_dn_2: 2.218  loss_bbox_dn_2: 0.8915  loss_giou_dn_2: 0.7338  loss_ce_3: 1.75  loss_mask_3: 1.541  loss_dice_3: 2.46  loss_bbox_3: 2.806  loss_giou_3: 1.525  loss_ce_dn_3: 1.902  loss_mask_dn_3: 1.261  loss_dice_dn_3: 2.185  loss_bbox_dn_3: 0.8593  loss_giou_dn_3: 0.7137  loss_ce_4: 1.778  loss_mask_4: 1.484  loss_dice_4: 2.396  loss_bbox_4: 2.759  loss_giou_4: 1.442  loss_ce_dn_4: 1.827  loss_mask_dn_4: 1.253  loss_dice_dn_4: 2.168  loss_bbox_dn_4: 0.8512  loss_giou_dn_4: 0.703  loss_ce_5: 1.793  loss_mask_5: 1.527  loss_dice_5: 2.39  loss_bbox_5: 2.743  loss_giou_5: 1.437  loss_ce_dn_5: 1.904  loss_mask_dn_5: 1.258  loss_dice_dn_5: 2.173  loss_bbox_dn_5: 0.8411  loss_giou_dn_5: 0.6899  loss_ce_6: 1.874  loss_mask_6: 1.522  loss_dice_6: 2.342  loss_bbox_6: 2.681  loss_giou_6: 1.447  loss_ce_dn_6: 1.869  loss_mask_dn_6: 1.259  loss_dice_dn_6: 2.15  loss_bbox_dn_6: 0.8401  loss_giou_dn_6: 0.6855  loss_ce_7: 1.858  loss_mask_7: 1.518  loss_dice_7: 2.367  loss_bbox_7: 2.692  loss_giou_7: 1.449  loss_ce_dn_7: 1.819  loss_mask_dn_7: 1.275  loss_dice_dn_7: 2.177  loss_bbox_dn_7: 0.8215  loss_giou_dn_7: 0.6732  loss_ce_8: 1.84  loss_mask_8: 1.464  loss_dice_8: 2.395  loss_bbox_8: 2.778  loss_giou_8: 1.453  loss_ce_dn_8: 1.992  loss_mask_dn_8: 1.264  loss_dice_dn_8: 2.149  loss_bbox_dn_8: 0.8219  loss_giou_dn_8: 0.6755    time: 2.2734  last_time: 2.3072  data_time: 0.0157  last_data_time: 0.0274   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:50:58 d2.utils.events]: \u001b[0m eta: 8 days, 6:19:30  iter: 4619  total_loss: 173.2  loss_ce: 1.714  loss_mask: 1.404  loss_dice: 2.23  loss_bbox: 2.529  loss_giou: 1.452  loss_ce_dn: 2.279  loss_mask_dn: 1.284  loss_dice_dn: 2.096  loss_bbox_dn: 0.809  loss_giou_dn: 0.6591  loss_ce_0: 5.864  loss_mask_0: 1.411  loss_dice_0: 2.46  loss_bbox_0: 4.116  loss_giou_0: 1.908  loss_ce_1: 2.086  loss_mask_1: 1.534  loss_dice_1: 2.354  loss_bbox_1: 2.944  loss_giou_1: 1.612  loss_ce_dn_1: 2.611  loss_mask_dn_1: 1.338  loss_dice_dn_1: 2.237  loss_bbox_dn_1: 0.9669  loss_giou_dn_1: 0.7438  loss_ce_2: 1.831  loss_mask_2: 1.46  loss_dice_2: 2.273  loss_bbox_2: 2.771  loss_giou_2: 1.603  loss_ce_dn_2: 2.25  loss_mask_dn_2: 1.296  loss_dice_dn_2: 2.13  loss_bbox_dn_2: 0.9163  loss_giou_dn_2: 0.7158  loss_ce_3: 1.71  loss_mask_3: 1.468  loss_dice_3: 2.217  loss_bbox_3: 2.638  loss_giou_3: 1.517  loss_ce_dn_3: 2.086  loss_mask_dn_3: 1.301  loss_dice_dn_3: 2.058  loss_bbox_dn_3: 0.8655  loss_giou_dn_3: 0.6848  loss_ce_4: 1.638  loss_mask_4: 1.429  loss_dice_4: 2.216  loss_bbox_4: 2.608  loss_giou_4: 1.503  loss_ce_dn_4: 2.087  loss_mask_dn_4: 1.259  loss_dice_dn_4: 2.032  loss_bbox_dn_4: 0.8502  loss_giou_dn_4: 0.6753  loss_ce_5: 1.616  loss_mask_5: 1.423  loss_dice_5: 2.203  loss_bbox_5: 2.579  loss_giou_5: 1.489  loss_ce_dn_5: 2.115  loss_mask_dn_5: 1.268  loss_dice_dn_5: 2.04  loss_bbox_dn_5: 0.8425  loss_giou_dn_5: 0.6683  loss_ce_6: 1.697  loss_mask_6: 1.397  loss_dice_6: 2.211  loss_bbox_6: 2.584  loss_giou_6: 1.487  loss_ce_dn_6: 2.135  loss_mask_dn_6: 1.304  loss_dice_dn_6: 2.027  loss_bbox_dn_6: 0.8355  loss_giou_dn_6: 0.6663  loss_ce_7: 1.791  loss_mask_7: 1.408  loss_dice_7: 2.21  loss_bbox_7: 2.54  loss_giou_7: 1.457  loss_ce_dn_7: 2.197  loss_mask_dn_7: 1.285  loss_dice_dn_7: 2.018  loss_bbox_dn_7: 0.8135  loss_giou_dn_7: 0.6537  loss_ce_8: 1.698  loss_mask_8: 1.407  loss_dice_8: 2.218  loss_bbox_8: 2.541  loss_giou_8: 1.453  loss_ce_dn_8: 2.191  loss_mask_dn_8: 1.289  loss_dice_dn_8: 2.019  loss_bbox_dn_8: 0.8112  loss_giou_dn_8: 0.6542    time: 2.2734  last_time: 2.2510  data_time: 0.0113  last_data_time: 0.0076   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:51:44 d2.utils.events]: \u001b[0m eta: 8 days, 6:18:45  iter: 4639  total_loss: 159.8  loss_ce: 1.631  loss_mask: 1.394  loss_dice: 2.286  loss_bbox: 2.484  loss_giou: 1.643  loss_ce_dn: 1.85  loss_mask_dn: 1.135  loss_dice_dn: 1.888  loss_bbox_dn: 0.7813  loss_giou_dn: 0.7202  loss_ce_0: 5.516  loss_mask_0: 1.19  loss_dice_0: 2.589  loss_bbox_0: 3.865  loss_giou_0: 2.005  loss_ce_1: 2.148  loss_mask_1: 1.21  loss_dice_1: 2.352  loss_bbox_1: 2.844  loss_giou_1: 1.799  loss_ce_dn_1: 2.598  loss_mask_dn_1: 1.236  loss_dice_dn_1: 2.019  loss_bbox_dn_1: 0.8992  loss_giou_dn_1: 0.7792  loss_ce_2: 1.802  loss_mask_2: 1.191  loss_dice_2: 2.29  loss_bbox_2: 2.654  loss_giou_2: 1.689  loss_ce_dn_2: 2.142  loss_mask_dn_2: 1.127  loss_dice_dn_2: 1.886  loss_bbox_dn_2: 0.8515  loss_giou_dn_2: 0.7558  loss_ce_3: 1.729  loss_mask_3: 1.251  loss_dice_3: 2.308  loss_bbox_3: 2.576  loss_giou_3: 1.697  loss_ce_dn_3: 2.025  loss_mask_dn_3: 1.088  loss_dice_dn_3: 1.902  loss_bbox_dn_3: 0.8054  loss_giou_dn_3: 0.7327  loss_ce_4: 1.714  loss_mask_4: 1.243  loss_dice_4: 2.261  loss_bbox_4: 2.52  loss_giou_4: 1.69  loss_ce_dn_4: 1.952  loss_mask_dn_4: 1.13  loss_dice_dn_4: 1.877  loss_bbox_dn_4: 0.7835  loss_giou_dn_4: 0.7152  loss_ce_5: 1.658  loss_mask_5: 1.274  loss_dice_5: 2.23  loss_bbox_5: 2.486  loss_giou_5: 1.678  loss_ce_dn_5: 1.876  loss_mask_dn_5: 1.132  loss_dice_dn_5: 1.911  loss_bbox_dn_5: 0.7845  loss_giou_dn_5: 0.7162  loss_ce_6: 1.642  loss_mask_6: 1.287  loss_dice_6: 2.237  loss_bbox_6: 2.486  loss_giou_6: 1.655  loss_ce_dn_6: 1.824  loss_mask_dn_6: 1.094  loss_dice_dn_6: 1.868  loss_bbox_dn_6: 0.7853  loss_giou_dn_6: 0.7195  loss_ce_7: 1.695  loss_mask_7: 1.343  loss_dice_7: 2.285  loss_bbox_7: 2.477  loss_giou_7: 1.644  loss_ce_dn_7: 1.807  loss_mask_dn_7: 1.101  loss_dice_dn_7: 1.868  loss_bbox_dn_7: 0.7646  loss_giou_dn_7: 0.7083  loss_ce_8: 1.644  loss_mask_8: 1.397  loss_dice_8: 2.284  loss_bbox_8: 2.49  loss_giou_8: 1.638  loss_ce_dn_8: 1.824  loss_mask_dn_8: 1.141  loss_dice_dn_8: 1.866  loss_bbox_dn_8: 0.7767  loss_giou_dn_8: 0.7168    time: 2.2735  last_time: 2.2509  data_time: 0.0125  last_data_time: 0.0175   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:52:29 d2.utils.events]: \u001b[0m eta: 8 days, 6:19:08  iter: 4659  total_loss: 172.8  loss_ce: 1.717  loss_mask: 1.409  loss_dice: 2.352  loss_bbox: 2.342  loss_giou: 1.528  loss_ce_dn: 2.098  loss_mask_dn: 1.212  loss_dice_dn: 1.967  loss_bbox_dn: 0.7841  loss_giou_dn: 0.69  loss_ce_0: 5.452  loss_mask_0: 1.417  loss_dice_0: 2.592  loss_bbox_0: 4.496  loss_giou_0: 2.133  loss_ce_1: 1.877  loss_mask_1: 1.471  loss_dice_1: 2.495  loss_bbox_1: 2.812  loss_giou_1: 1.747  loss_ce_dn_1: 2.182  loss_mask_dn_1: 1.332  loss_dice_dn_1: 2.096  loss_bbox_dn_1: 0.8997  loss_giou_dn_1: 0.7678  loss_ce_2: 1.645  loss_mask_2: 1.403  loss_dice_2: 2.407  loss_bbox_2: 2.688  loss_giou_2: 1.679  loss_ce_dn_2: 1.957  loss_mask_dn_2: 1.286  loss_dice_dn_2: 1.932  loss_bbox_dn_2: 0.8358  loss_giou_dn_2: 0.7461  loss_ce_3: 1.518  loss_mask_3: 1.384  loss_dice_3: 2.428  loss_bbox_3: 2.499  loss_giou_3: 1.582  loss_ce_dn_3: 1.872  loss_mask_dn_3: 1.26  loss_dice_dn_3: 1.926  loss_bbox_dn_3: 0.7842  loss_giou_dn_3: 0.719  loss_ce_4: 1.577  loss_mask_4: 1.383  loss_dice_4: 2.438  loss_bbox_4: 2.447  loss_giou_4: 1.571  loss_ce_dn_4: 1.886  loss_mask_dn_4: 1.206  loss_dice_dn_4: 1.952  loss_bbox_dn_4: 0.7545  loss_giou_dn_4: 0.7053  loss_ce_5: 1.602  loss_mask_5: 1.388  loss_dice_5: 2.404  loss_bbox_5: 2.379  loss_giou_5: 1.528  loss_ce_dn_5: 1.823  loss_mask_dn_5: 1.232  loss_dice_dn_5: 1.964  loss_bbox_dn_5: 0.7376  loss_giou_dn_5: 0.6892  loss_ce_6: 1.728  loss_mask_6: 1.382  loss_dice_6: 2.414  loss_bbox_6: 2.384  loss_giou_6: 1.531  loss_ce_dn_6: 1.839  loss_mask_dn_6: 1.23  loss_dice_dn_6: 1.936  loss_bbox_dn_6: 0.7391  loss_giou_dn_6: 0.6895  loss_ce_7: 1.788  loss_mask_7: 1.413  loss_dice_7: 2.424  loss_bbox_7: 2.392  loss_giou_7: 1.525  loss_ce_dn_7: 1.943  loss_mask_dn_7: 1.252  loss_dice_dn_7: 1.974  loss_bbox_dn_7: 0.7644  loss_giou_dn_7: 0.6693  loss_ce_8: 1.695  loss_mask_8: 1.418  loss_dice_8: 2.373  loss_bbox_8: 2.391  loss_giou_8: 1.523  loss_ce_dn_8: 1.958  loss_mask_dn_8: 1.251  loss_dice_dn_8: 1.977  loss_bbox_dn_8: 0.7738  loss_giou_dn_8: 0.6756    time: 2.2735  last_time: 2.2638  data_time: 0.0123  last_data_time: 0.0051   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:53:15 d2.utils.events]: \u001b[0m eta: 8 days, 6:18:23  iter: 4679  total_loss: 178.6  loss_ce: 1.863  loss_mask: 1.685  loss_dice: 2.48  loss_bbox: 2.653  loss_giou: 1.537  loss_ce_dn: 2.292  loss_mask_dn: 1.269  loss_dice_dn: 2.267  loss_bbox_dn: 0.7757  loss_giou_dn: 0.6585  loss_ce_0: 5.817  loss_mask_0: 1.589  loss_dice_0: 2.703  loss_bbox_0: 4.231  loss_giou_0: 1.944  loss_ce_1: 2.102  loss_mask_1: 1.626  loss_dice_1: 2.482  loss_bbox_1: 3.036  loss_giou_1: 1.686  loss_ce_dn_1: 2.711  loss_mask_dn_1: 1.428  loss_dice_dn_1: 2.386  loss_bbox_dn_1: 0.9156  loss_giou_dn_1: 0.7557  loss_ce_2: 1.954  loss_mask_2: 1.599  loss_dice_2: 2.442  loss_bbox_2: 2.839  loss_giou_2: 1.664  loss_ce_dn_2: 2.281  loss_mask_dn_2: 1.34  loss_dice_dn_2: 2.304  loss_bbox_dn_2: 0.85  loss_giou_dn_2: 0.7237  loss_ce_3: 1.787  loss_mask_3: 1.569  loss_dice_3: 2.443  loss_bbox_3: 2.744  loss_giou_3: 1.623  loss_ce_dn_3: 2.214  loss_mask_dn_3: 1.349  loss_dice_dn_3: 2.24  loss_bbox_dn_3: 0.8131  loss_giou_dn_3: 0.7029  loss_ce_4: 1.849  loss_mask_4: 1.551  loss_dice_4: 2.454  loss_bbox_4: 2.652  loss_giou_4: 1.626  loss_ce_dn_4: 2.223  loss_mask_dn_4: 1.277  loss_dice_dn_4: 2.249  loss_bbox_dn_4: 0.8041  loss_giou_dn_4: 0.6853  loss_ce_5: 1.731  loss_mask_5: 1.711  loss_dice_5: 2.424  loss_bbox_5: 2.656  loss_giou_5: 1.601  loss_ce_dn_5: 2.157  loss_mask_dn_5: 1.263  loss_dice_dn_5: 2.247  loss_bbox_dn_5: 0.7881  loss_giou_dn_5: 0.6706  loss_ce_6: 1.866  loss_mask_6: 1.605  loss_dice_6: 2.417  loss_bbox_6: 2.71  loss_giou_6: 1.579  loss_ce_dn_6: 2.13  loss_mask_dn_6: 1.243  loss_dice_dn_6: 2.233  loss_bbox_dn_6: 0.7839  loss_giou_dn_6: 0.6672  loss_ce_7: 1.863  loss_mask_7: 1.637  loss_dice_7: 2.424  loss_bbox_7: 2.697  loss_giou_7: 1.55  loss_ce_dn_7: 2.219  loss_mask_dn_7: 1.296  loss_dice_dn_7: 2.276  loss_bbox_dn_7: 0.7785  loss_giou_dn_7: 0.6566  loss_ce_8: 1.912  loss_mask_8: 1.614  loss_dice_8: 2.456  loss_bbox_8: 2.65  loss_giou_8: 1.544  loss_ce_dn_8: 2.285  loss_mask_dn_8: 1.24  loss_dice_dn_8: 2.283  loss_bbox_dn_8: 0.7774  loss_giou_dn_8: 0.6578    time: 2.2735  last_time: 2.3673  data_time: 0.0115  last_data_time: 0.0260   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:54:01 d2.utils.events]: \u001b[0m eta: 8 days, 6:18:59  iter: 4699  total_loss: 165.9  loss_ce: 1.53  loss_mask: 1.241  loss_dice: 2.316  loss_bbox: 3.119  loss_giou: 1.615  loss_ce_dn: 1.876  loss_mask_dn: 1.207  loss_dice_dn: 2.034  loss_bbox_dn: 0.7976  loss_giou_dn: 0.677  loss_ce_0: 5.763  loss_mask_0: 1.248  loss_dice_0: 2.673  loss_bbox_0: 4.052  loss_giou_0: 2.029  loss_ce_1: 1.866  loss_mask_1: 1.254  loss_dice_1: 2.348  loss_bbox_1: 3.34  loss_giou_1: 1.728  loss_ce_dn_1: 2.263  loss_mask_dn_1: 1.265  loss_dice_dn_1: 2.154  loss_bbox_dn_1: 0.9214  loss_giou_dn_1: 0.7563  loss_ce_2: 1.7  loss_mask_2: 1.234  loss_dice_2: 2.334  loss_bbox_2: 3.194  loss_giou_2: 1.682  loss_ce_dn_2: 1.846  loss_mask_dn_2: 1.234  loss_dice_dn_2: 2.001  loss_bbox_dn_2: 0.8625  loss_giou_dn_2: 0.7317  loss_ce_3: 1.535  loss_mask_3: 1.235  loss_dice_3: 2.314  loss_bbox_3: 3.165  loss_giou_3: 1.662  loss_ce_dn_3: 1.712  loss_mask_dn_3: 1.218  loss_dice_dn_3: 2  loss_bbox_dn_3: 0.8376  loss_giou_dn_3: 0.7106  loss_ce_4: 1.452  loss_mask_4: 1.257  loss_dice_4: 2.36  loss_bbox_4: 3.149  loss_giou_4: 1.676  loss_ce_dn_4: 1.686  loss_mask_dn_4: 1.213  loss_dice_dn_4: 2.003  loss_bbox_dn_4: 0.8157  loss_giou_dn_4: 0.6875  loss_ce_5: 1.43  loss_mask_5: 1.249  loss_dice_5: 2.35  loss_bbox_5: 3.112  loss_giou_5: 1.637  loss_ce_dn_5: 1.763  loss_mask_dn_5: 1.206  loss_dice_dn_5: 2.011  loss_bbox_dn_5: 0.803  loss_giou_dn_5: 0.6814  loss_ce_6: 1.53  loss_mask_6: 1.265  loss_dice_6: 2.368  loss_bbox_6: 3.129  loss_giou_6: 1.638  loss_ce_dn_6: 1.725  loss_mask_dn_6: 1.206  loss_dice_dn_6: 2.013  loss_bbox_dn_6: 0.7966  loss_giou_dn_6: 0.682  loss_ce_7: 1.552  loss_mask_7: 1.267  loss_dice_7: 2.38  loss_bbox_7: 3.116  loss_giou_7: 1.611  loss_ce_dn_7: 1.797  loss_mask_dn_7: 1.199  loss_dice_dn_7: 2.046  loss_bbox_dn_7: 0.7946  loss_giou_dn_7: 0.6714  loss_ce_8: 1.529  loss_mask_8: 1.258  loss_dice_8: 2.351  loss_bbox_8: 3.117  loss_giou_8: 1.613  loss_ce_dn_8: 1.826  loss_mask_dn_8: 1.196  loss_dice_dn_8: 2.021  loss_bbox_dn_8: 0.7953  loss_giou_dn_8: 0.6748    time: 2.2735  last_time: 2.2270  data_time: 0.0123  last_data_time: 0.0103   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:54:46 d2.utils.events]: \u001b[0m eta: 8 days, 6:18:31  iter: 4719  total_loss: 169  loss_ce: 1.499  loss_mask: 1.509  loss_dice: 2.39  loss_bbox: 2.534  loss_giou: 1.41  loss_ce_dn: 2.176  loss_mask_dn: 1.385  loss_dice_dn: 1.995  loss_bbox_dn: 0.8645  loss_giou_dn: 0.6519  loss_ce_0: 5.763  loss_mask_0: 1.524  loss_dice_0: 2.658  loss_bbox_0: 3.935  loss_giou_0: 1.873  loss_ce_1: 1.942  loss_mask_1: 1.543  loss_dice_1: 2.434  loss_bbox_1: 2.873  loss_giou_1: 1.591  loss_ce_dn_1: 2.577  loss_mask_dn_1: 1.481  loss_dice_dn_1: 2.215  loss_bbox_dn_1: 1.04  loss_giou_dn_1: 0.7457  loss_ce_2: 1.644  loss_mask_2: 1.55  loss_dice_2: 2.44  loss_bbox_2: 2.706  loss_giou_2: 1.529  loss_ce_dn_2: 2.042  loss_mask_dn_2: 1.419  loss_dice_dn_2: 2.042  loss_bbox_dn_2: 0.9913  loss_giou_dn_2: 0.7152  loss_ce_3: 1.531  loss_mask_3: 1.567  loss_dice_3: 2.402  loss_bbox_3: 2.577  loss_giou_3: 1.47  loss_ce_dn_3: 1.959  loss_mask_dn_3: 1.409  loss_dice_dn_3: 2.022  loss_bbox_dn_3: 0.9496  loss_giou_dn_3: 0.6839  loss_ce_4: 1.592  loss_mask_4: 1.597  loss_dice_4: 2.339  loss_bbox_4: 2.534  loss_giou_4: 1.433  loss_ce_dn_4: 1.95  loss_mask_dn_4: 1.313  loss_dice_dn_4: 2.01  loss_bbox_dn_4: 0.8996  loss_giou_dn_4: 0.6733  loss_ce_5: 1.583  loss_mask_5: 1.51  loss_dice_5: 2.353  loss_bbox_5: 2.535  loss_giou_5: 1.436  loss_ce_dn_5: 1.907  loss_mask_dn_5: 1.367  loss_dice_dn_5: 2  loss_bbox_dn_5: 0.8816  loss_giou_dn_5: 0.6644  loss_ce_6: 1.55  loss_mask_6: 1.511  loss_dice_6: 2.375  loss_bbox_6: 2.543  loss_giou_6: 1.433  loss_ce_dn_6: 1.966  loss_mask_dn_6: 1.335  loss_dice_dn_6: 1.982  loss_bbox_dn_6: 0.8775  loss_giou_dn_6: 0.6605  loss_ce_7: 1.477  loss_mask_7: 1.512  loss_dice_7: 2.381  loss_bbox_7: 2.541  loss_giou_7: 1.417  loss_ce_dn_7: 2.052  loss_mask_dn_7: 1.353  loss_dice_dn_7: 1.988  loss_bbox_dn_7: 0.866  loss_giou_dn_7: 0.6541  loss_ce_8: 1.441  loss_mask_8: 1.549  loss_dice_8: 2.403  loss_bbox_8: 2.539  loss_giou_8: 1.409  loss_ce_dn_8: 2.093  loss_mask_dn_8: 1.399  loss_dice_dn_8: 2  loss_bbox_dn_8: 0.8623  loss_giou_dn_8: 0.6514    time: 2.2735  last_time: 2.2635  data_time: 0.0124  last_data_time: 0.0231   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:55:32 d2.utils.events]: \u001b[0m eta: 8 days, 6:18:22  iter: 4739  total_loss: 170.4  loss_ce: 1.805  loss_mask: 1.292  loss_dice: 2.326  loss_bbox: 2.688  loss_giou: 1.49  loss_ce_dn: 2.237  loss_mask_dn: 1.3  loss_dice_dn: 2.027  loss_bbox_dn: 0.8351  loss_giou_dn: 0.7014  loss_ce_0: 6  loss_mask_0: 1.216  loss_dice_0: 2.466  loss_bbox_0: 3.951  loss_giou_0: 1.996  loss_ce_1: 1.886  loss_mask_1: 1.252  loss_dice_1: 2.396  loss_bbox_1: 2.764  loss_giou_1: 1.654  loss_ce_dn_1: 2.556  loss_mask_dn_1: 1.317  loss_dice_dn_1: 2.129  loss_bbox_dn_1: 0.9504  loss_giou_dn_1: 0.7658  loss_ce_2: 1.848  loss_mask_2: 1.244  loss_dice_2: 2.354  loss_bbox_2: 2.591  loss_giou_2: 1.599  loss_ce_dn_2: 2.27  loss_mask_dn_2: 1.252  loss_dice_dn_2: 2.015  loss_bbox_dn_2: 0.9018  loss_giou_dn_2: 0.7392  loss_ce_3: 1.8  loss_mask_3: 1.268  loss_dice_3: 2.313  loss_bbox_3: 2.599  loss_giou_3: 1.555  loss_ce_dn_3: 2.196  loss_mask_dn_3: 1.228  loss_dice_dn_3: 2.028  loss_bbox_dn_3: 0.8814  loss_giou_dn_3: 0.726  loss_ce_4: 1.769  loss_mask_4: 1.248  loss_dice_4: 2.352  loss_bbox_4: 2.588  loss_giou_4: 1.53  loss_ce_dn_4: 2.214  loss_mask_dn_4: 1.249  loss_dice_dn_4: 2  loss_bbox_dn_4: 0.857  loss_giou_dn_4: 0.717  loss_ce_5: 1.807  loss_mask_5: 1.25  loss_dice_5: 2.347  loss_bbox_5: 2.559  loss_giou_5: 1.524  loss_ce_dn_5: 2.143  loss_mask_dn_5: 1.197  loss_dice_dn_5: 2.035  loss_bbox_dn_5: 0.8447  loss_giou_dn_5: 0.7097  loss_ce_6: 1.787  loss_mask_6: 1.309  loss_dice_6: 2.303  loss_bbox_6: 2.561  loss_giou_6: 1.502  loss_ce_dn_6: 2.089  loss_mask_dn_6: 1.189  loss_dice_dn_6: 2.026  loss_bbox_dn_6: 0.8442  loss_giou_dn_6: 0.7099  loss_ce_7: 1.803  loss_mask_7: 1.242  loss_dice_7: 2.303  loss_bbox_7: 2.544  loss_giou_7: 1.494  loss_ce_dn_7: 2.098  loss_mask_dn_7: 1.175  loss_dice_dn_7: 2.048  loss_bbox_dn_7: 0.8323  loss_giou_dn_7: 0.6976  loss_ce_8: 1.785  loss_mask_8: 1.246  loss_dice_8: 2.323  loss_bbox_8: 2.644  loss_giou_8: 1.489  loss_ce_dn_8: 2.22  loss_mask_dn_8: 1.266  loss_dice_dn_8: 2.038  loss_bbox_dn_8: 0.8327  loss_giou_dn_8: 0.7    time: 2.2735  last_time: 2.2585  data_time: 0.0127  last_data_time: 0.0061   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:56:17 d2.utils.events]: \u001b[0m eta: 8 days, 6:19:38  iter: 4759  total_loss: 156.4  loss_ce: 1.618  loss_mask: 1.491  loss_dice: 2.19  loss_bbox: 2.417  loss_giou: 1.445  loss_ce_dn: 2.075  loss_mask_dn: 1.342  loss_dice_dn: 1.922  loss_bbox_dn: 0.7813  loss_giou_dn: 0.6607  loss_ce_0: 5.598  loss_mask_0: 1.436  loss_dice_0: 2.552  loss_bbox_0: 3.955  loss_giou_0: 1.874  loss_ce_1: 1.926  loss_mask_1: 1.477  loss_dice_1: 2.373  loss_bbox_1: 2.8  loss_giou_1: 1.559  loss_ce_dn_1: 2.562  loss_mask_dn_1: 1.352  loss_dice_dn_1: 2.116  loss_bbox_dn_1: 0.9423  loss_giou_dn_1: 0.7429  loss_ce_2: 1.776  loss_mask_2: 1.464  loss_dice_2: 2.246  loss_bbox_2: 2.491  loss_giou_2: 1.519  loss_ce_dn_2: 2.096  loss_mask_dn_2: 1.362  loss_dice_dn_2: 1.93  loss_bbox_dn_2: 0.877  loss_giou_dn_2: 0.7185  loss_ce_3: 1.617  loss_mask_3: 1.444  loss_dice_3: 2.246  loss_bbox_3: 2.417  loss_giou_3: 1.501  loss_ce_dn_3: 1.99  loss_mask_dn_3: 1.355  loss_dice_dn_3: 1.893  loss_bbox_dn_3: 0.8548  loss_giou_dn_3: 0.7016  loss_ce_4: 1.69  loss_mask_4: 1.416  loss_dice_4: 2.244  loss_bbox_4: 2.406  loss_giou_4: 1.48  loss_ce_dn_4: 1.912  loss_mask_dn_4: 1.355  loss_dice_dn_4: 1.891  loss_bbox_dn_4: 0.82  loss_giou_dn_4: 0.6835  loss_ce_5: 1.651  loss_mask_5: 1.387  loss_dice_5: 2.224  loss_bbox_5: 2.43  loss_giou_5: 1.441  loss_ce_dn_5: 1.942  loss_mask_dn_5: 1.348  loss_dice_dn_5: 1.891  loss_bbox_dn_5: 0.8014  loss_giou_dn_5: 0.6694  loss_ce_6: 1.68  loss_mask_6: 1.415  loss_dice_6: 2.219  loss_bbox_6: 2.401  loss_giou_6: 1.444  loss_ce_dn_6: 1.929  loss_mask_dn_6: 1.33  loss_dice_dn_6: 1.854  loss_bbox_dn_6: 0.797  loss_giou_dn_6: 0.6669  loss_ce_7: 1.723  loss_mask_7: 1.457  loss_dice_7: 2.183  loss_bbox_7: 2.391  loss_giou_7: 1.425  loss_ce_dn_7: 1.979  loss_mask_dn_7: 1.311  loss_dice_dn_7: 1.873  loss_bbox_dn_7: 0.7795  loss_giou_dn_7: 0.6603  loss_ce_8: 1.646  loss_mask_8: 1.507  loss_dice_8: 2.206  loss_bbox_8: 2.422  loss_giou_8: 1.438  loss_ce_dn_8: 1.983  loss_mask_dn_8: 1.32  loss_dice_dn_8: 1.906  loss_bbox_dn_8: 0.7776  loss_giou_dn_8: 0.6597    time: 2.2735  last_time: 2.2699  data_time: 0.0113  last_data_time: 0.0041   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:57:03 d2.utils.events]: \u001b[0m eta: 8 days, 6:18:46  iter: 4779  total_loss: 163.9  loss_ce: 1.496  loss_mask: 1.463  loss_dice: 2.467  loss_bbox: 2.683  loss_giou: 1.542  loss_ce_dn: 2.06  loss_mask_dn: 1.327  loss_dice_dn: 2.115  loss_bbox_dn: 0.8076  loss_giou_dn: 0.6929  loss_ce_0: 5.647  loss_mask_0: 1.418  loss_dice_0: 2.614  loss_bbox_0: 4.139  loss_giou_0: 1.879  loss_ce_1: 1.862  loss_mask_1: 1.391  loss_dice_1: 2.452  loss_bbox_1: 2.81  loss_giou_1: 1.677  loss_ce_dn_1: 2.606  loss_mask_dn_1: 1.411  loss_dice_dn_1: 2.284  loss_bbox_dn_1: 0.9268  loss_giou_dn_1: 0.7667  loss_ce_2: 1.687  loss_mask_2: 1.456  loss_dice_2: 2.505  loss_bbox_2: 2.7  loss_giou_2: 1.68  loss_ce_dn_2: 2.268  loss_mask_dn_2: 1.351  loss_dice_dn_2: 2.14  loss_bbox_dn_2: 0.8807  loss_giou_dn_2: 0.7398  loss_ce_3: 1.528  loss_mask_3: 1.491  loss_dice_3: 2.522  loss_bbox_3: 2.707  loss_giou_3: 1.637  loss_ce_dn_3: 2.116  loss_mask_dn_3: 1.289  loss_dice_dn_3: 2.105  loss_bbox_dn_3: 0.84  loss_giou_dn_3: 0.7244  loss_ce_4: 1.488  loss_mask_4: 1.512  loss_dice_4: 2.483  loss_bbox_4: 2.695  loss_giou_4: 1.609  loss_ce_dn_4: 2.068  loss_mask_dn_4: 1.319  loss_dice_dn_4: 2.084  loss_bbox_dn_4: 0.8184  loss_giou_dn_4: 0.7136  loss_ce_5: 1.522  loss_mask_5: 1.492  loss_dice_5: 2.474  loss_bbox_5: 2.682  loss_giou_5: 1.603  loss_ce_dn_5: 2.049  loss_mask_dn_5: 1.332  loss_dice_dn_5: 2.086  loss_bbox_dn_5: 0.811  loss_giou_dn_5: 0.7076  loss_ce_6: 1.442  loss_mask_6: 1.503  loss_dice_6: 2.488  loss_bbox_6: 2.684  loss_giou_6: 1.608  loss_ce_dn_6: 2.034  loss_mask_dn_6: 1.318  loss_dice_dn_6: 2.079  loss_bbox_dn_6: 0.8118  loss_giou_dn_6: 0.7046  loss_ce_7: 1.458  loss_mask_7: 1.492  loss_dice_7: 2.502  loss_bbox_7: 2.699  loss_giou_7: 1.581  loss_ce_dn_7: 2.101  loss_mask_dn_7: 1.324  loss_dice_dn_7: 2.11  loss_bbox_dn_7: 0.802  loss_giou_dn_7: 0.6946  loss_ce_8: 1.532  loss_mask_8: 1.485  loss_dice_8: 2.518  loss_bbox_8: 2.69  loss_giou_8: 1.581  loss_ce_dn_8: 2.053  loss_mask_dn_8: 1.315  loss_dice_dn_8: 2.105  loss_bbox_dn_8: 0.8046  loss_giou_dn_8: 0.6933    time: 2.2735  last_time: 2.2552  data_time: 0.0152  last_data_time: 0.0076   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:57:48 d2.utils.events]: \u001b[0m eta: 8 days, 6:16:47  iter: 4799  total_loss: 163.5  loss_ce: 1.816  loss_mask: 1.374  loss_dice: 2.382  loss_bbox: 2.495  loss_giou: 1.475  loss_ce_dn: 2.209  loss_mask_dn: 1.313  loss_dice_dn: 2.122  loss_bbox_dn: 0.7689  loss_giou_dn: 0.6621  loss_ce_0: 5.786  loss_mask_0: 1.266  loss_dice_0: 2.531  loss_bbox_0: 3.836  loss_giou_0: 1.969  loss_ce_1: 2.019  loss_mask_1: 1.416  loss_dice_1: 2.46  loss_bbox_1: 2.895  loss_giou_1: 1.584  loss_ce_dn_1: 2.51  loss_mask_dn_1: 1.308  loss_dice_dn_1: 2.202  loss_bbox_dn_1: 0.9385  loss_giou_dn_1: 0.7581  loss_ce_2: 1.629  loss_mask_2: 1.491  loss_dice_2: 2.435  loss_bbox_2: 2.683  loss_giou_2: 1.568  loss_ce_dn_2: 1.971  loss_mask_dn_2: 1.278  loss_dice_dn_2: 2.109  loss_bbox_dn_2: 0.8687  loss_giou_dn_2: 0.7314  loss_ce_3: 1.595  loss_mask_3: 1.458  loss_dice_3: 2.381  loss_bbox_3: 2.597  loss_giou_3: 1.566  loss_ce_dn_3: 1.935  loss_mask_dn_3: 1.343  loss_dice_dn_3: 2.134  loss_bbox_dn_3: 0.823  loss_giou_dn_3: 0.6992  loss_ce_4: 1.68  loss_mask_4: 1.442  loss_dice_4: 2.378  loss_bbox_4: 2.493  loss_giou_4: 1.56  loss_ce_dn_4: 2.022  loss_mask_dn_4: 1.283  loss_dice_dn_4: 2.153  loss_bbox_dn_4: 0.8267  loss_giou_dn_4: 0.6785  loss_ce_5: 1.704  loss_mask_5: 1.412  loss_dice_5: 2.348  loss_bbox_5: 2.476  loss_giou_5: 1.528  loss_ce_dn_5: 1.968  loss_mask_dn_5: 1.287  loss_dice_dn_5: 2.186  loss_bbox_dn_5: 0.7998  loss_giou_dn_5: 0.6687  loss_ce_6: 1.649  loss_mask_6: 1.363  loss_dice_6: 2.375  loss_bbox_6: 2.48  loss_giou_6: 1.484  loss_ce_dn_6: 1.957  loss_mask_dn_6: 1.271  loss_dice_dn_6: 2.14  loss_bbox_dn_6: 0.7971  loss_giou_dn_6: 0.6705  loss_ce_7: 1.738  loss_mask_7: 1.382  loss_dice_7: 2.36  loss_bbox_7: 2.581  loss_giou_7: 1.478  loss_ce_dn_7: 2.081  loss_mask_dn_7: 1.337  loss_dice_dn_7: 2.153  loss_bbox_dn_7: 0.7795  loss_giou_dn_7: 0.6604  loss_ce_8: 1.765  loss_mask_8: 1.354  loss_dice_8: 2.383  loss_bbox_8: 2.491  loss_giou_8: 1.469  loss_ce_dn_8: 2.158  loss_mask_dn_8: 1.287  loss_dice_dn_8: 2.118  loss_bbox_dn_8: 0.7717  loss_giou_dn_8: 0.6622    time: 2.2734  last_time: 2.2421  data_time: 0.0115  last_data_time: 0.0135   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:58:33 d2.utils.events]: \u001b[0m eta: 8 days, 6:15:42  iter: 4819  total_loss: 159.1  loss_ce: 1.321  loss_mask: 1.386  loss_dice: 2.252  loss_bbox: 2.438  loss_giou: 1.389  loss_ce_dn: 1.594  loss_mask_dn: 1.331  loss_dice_dn: 2.095  loss_bbox_dn: 0.8151  loss_giou_dn: 0.6614  loss_ce_0: 5.557  loss_mask_0: 1.332  loss_dice_0: 2.514  loss_bbox_0: 4.038  loss_giou_0: 1.89  loss_ce_1: 1.681  loss_mask_1: 1.351  loss_dice_1: 2.348  loss_bbox_1: 2.801  loss_giou_1: 1.522  loss_ce_dn_1: 2.028  loss_mask_dn_1: 1.361  loss_dice_dn_1: 2.276  loss_bbox_dn_1: 0.9642  loss_giou_dn_1: 0.733  loss_ce_2: 1.469  loss_mask_2: 1.307  loss_dice_2: 2.29  loss_bbox_2: 2.563  loss_giou_2: 1.486  loss_ce_dn_2: 1.557  loss_mask_dn_2: 1.344  loss_dice_dn_2: 2.16  loss_bbox_dn_2: 0.883  loss_giou_dn_2: 0.7138  loss_ce_3: 1.484  loss_mask_3: 1.401  loss_dice_3: 2.345  loss_bbox_3: 2.519  loss_giou_3: 1.466  loss_ce_dn_3: 1.508  loss_mask_dn_3: 1.322  loss_dice_dn_3: 2.097  loss_bbox_dn_3: 0.8381  loss_giou_dn_3: 0.688  loss_ce_4: 1.387  loss_mask_4: 1.404  loss_dice_4: 2.348  loss_bbox_4: 2.496  loss_giou_4: 1.441  loss_ce_dn_4: 1.487  loss_mask_dn_4: 1.323  loss_dice_dn_4: 2.066  loss_bbox_dn_4: 0.8057  loss_giou_dn_4: 0.6829  loss_ce_5: 1.425  loss_mask_5: 1.451  loss_dice_5: 2.334  loss_bbox_5: 2.462  loss_giou_5: 1.425  loss_ce_dn_5: 1.488  loss_mask_dn_5: 1.3  loss_dice_dn_5: 2.088  loss_bbox_dn_5: 0.8034  loss_giou_dn_5: 0.6755  loss_ce_6: 1.357  loss_mask_6: 1.363  loss_dice_6: 2.352  loss_bbox_6: 2.454  loss_giou_6: 1.409  loss_ce_dn_6: 1.505  loss_mask_dn_6: 1.323  loss_dice_dn_6: 2.064  loss_bbox_dn_6: 0.8071  loss_giou_dn_6: 0.6718  loss_ce_7: 1.399  loss_mask_7: 1.341  loss_dice_7: 2.272  loss_bbox_7: 2.427  loss_giou_7: 1.399  loss_ce_dn_7: 1.545  loss_mask_dn_7: 1.329  loss_dice_dn_7: 2.096  loss_bbox_dn_7: 0.8036  loss_giou_dn_7: 0.6562  loss_ce_8: 1.401  loss_mask_8: 1.377  loss_dice_8: 2.271  loss_bbox_8: 2.434  loss_giou_8: 1.393  loss_ce_dn_8: 1.51  loss_mask_dn_8: 1.341  loss_dice_dn_8: 2.112  loss_bbox_dn_8: 0.8098  loss_giou_dn_8: 0.6582    time: 2.2734  last_time: 2.2815  data_time: 0.0132  last_data_time: 0.0250   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 17:59:19 d2.utils.events]: \u001b[0m eta: 8 days, 6:16:49  iter: 4839  total_loss: 163.4  loss_ce: 1.836  loss_mask: 1.446  loss_dice: 2.351  loss_bbox: 2.259  loss_giou: 1.521  loss_ce_dn: 2.384  loss_mask_dn: 1.226  loss_dice_dn: 2.078  loss_bbox_dn: 0.7543  loss_giou_dn: 0.672  loss_ce_0: 5.604  loss_mask_0: 1.329  loss_dice_0: 2.556  loss_bbox_0: 4.166  loss_giou_0: 1.948  loss_ce_1: 1.93  loss_mask_1: 1.405  loss_dice_1: 2.376  loss_bbox_1: 2.73  loss_giou_1: 1.652  loss_ce_dn_1: 2.796  loss_mask_dn_1: 1.362  loss_dice_dn_1: 2.261  loss_bbox_dn_1: 0.9351  loss_giou_dn_1: 0.7491  loss_ce_2: 1.772  loss_mask_2: 1.282  loss_dice_2: 2.335  loss_bbox_2: 2.399  loss_giou_2: 1.606  loss_ce_dn_2: 2.3  loss_mask_dn_2: 1.268  loss_dice_dn_2: 2.104  loss_bbox_dn_2: 0.8674  loss_giou_dn_2: 0.7139  loss_ce_3: 1.735  loss_mask_3: 1.364  loss_dice_3: 2.381  loss_bbox_3: 2.319  loss_giou_3: 1.576  loss_ce_dn_3: 2.248  loss_mask_dn_3: 1.219  loss_dice_dn_3: 2.126  loss_bbox_dn_3: 0.8014  loss_giou_dn_3: 0.6919  loss_ce_4: 1.75  loss_mask_4: 1.499  loss_dice_4: 2.403  loss_bbox_4: 2.258  loss_giou_4: 1.556  loss_ce_dn_4: 2.196  loss_mask_dn_4: 1.247  loss_dice_dn_4: 2.127  loss_bbox_dn_4: 0.777  loss_giou_dn_4: 0.6819  loss_ce_5: 1.668  loss_mask_5: 1.408  loss_dice_5: 2.354  loss_bbox_5: 2.248  loss_giou_5: 1.541  loss_ce_dn_5: 2.128  loss_mask_dn_5: 1.223  loss_dice_dn_5: 2.104  loss_bbox_dn_5: 0.7551  loss_giou_dn_5: 0.6739  loss_ce_6: 1.746  loss_mask_6: 1.465  loss_dice_6: 2.362  loss_bbox_6: 2.252  loss_giou_6: 1.541  loss_ce_dn_6: 2.231  loss_mask_dn_6: 1.255  loss_dice_dn_6: 2.143  loss_bbox_dn_6: 0.748  loss_giou_dn_6: 0.6751  loss_ce_7: 1.722  loss_mask_7: 1.422  loss_dice_7: 2.365  loss_bbox_7: 2.268  loss_giou_7: 1.53  loss_ce_dn_7: 2.246  loss_mask_dn_7: 1.228  loss_dice_dn_7: 2.125  loss_bbox_dn_7: 0.7415  loss_giou_dn_7: 0.6724  loss_ce_8: 1.728  loss_mask_8: 1.457  loss_dice_8: 2.34  loss_bbox_8: 2.265  loss_giou_8: 1.526  loss_ce_dn_8: 2.299  loss_mask_dn_8: 1.222  loss_dice_dn_8: 2.07  loss_bbox_dn_8: 0.7473  loss_giou_dn_8: 0.6719    time: 2.2734  last_time: 2.2786  data_time: 0.0113  last_data_time: 0.0124   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:00:04 d2.utils.events]: \u001b[0m eta: 8 days, 6:17:44  iter: 4859  total_loss: 170.6  loss_ce: 1.608  loss_mask: 1.78  loss_dice: 2.388  loss_bbox: 2.496  loss_giou: 1.511  loss_ce_dn: 2.165  loss_mask_dn: 1.376  loss_dice_dn: 2.174  loss_bbox_dn: 0.8111  loss_giou_dn: 0.6525  loss_ce_0: 5.627  loss_mask_0: 1.584  loss_dice_0: 2.6  loss_bbox_0: 4.099  loss_giou_0: 2.022  loss_ce_1: 2.049  loss_mask_1: 1.783  loss_dice_1: 2.434  loss_bbox_1: 3.025  loss_giou_1: 1.63  loss_ce_dn_1: 2.431  loss_mask_dn_1: 1.44  loss_dice_dn_1: 2.311  loss_bbox_dn_1: 0.9535  loss_giou_dn_1: 0.7349  loss_ce_2: 1.75  loss_mask_2: 1.658  loss_dice_2: 2.458  loss_bbox_2: 2.637  loss_giou_2: 1.545  loss_ce_dn_2: 2.015  loss_mask_dn_2: 1.421  loss_dice_dn_2: 2.215  loss_bbox_dn_2: 0.8992  loss_giou_dn_2: 0.6989  loss_ce_3: 1.659  loss_mask_3: 1.664  loss_dice_3: 2.484  loss_bbox_3: 2.436  loss_giou_3: 1.556  loss_ce_dn_3: 1.994  loss_mask_dn_3: 1.395  loss_dice_dn_3: 2.183  loss_bbox_dn_3: 0.8598  loss_giou_dn_3: 0.6768  loss_ce_4: 1.641  loss_mask_4: 1.722  loss_dice_4: 2.434  loss_bbox_4: 2.446  loss_giou_4: 1.548  loss_ce_dn_4: 1.971  loss_mask_dn_4: 1.399  loss_dice_dn_4: 2.18  loss_bbox_dn_4: 0.8478  loss_giou_dn_4: 0.663  loss_ce_5: 1.621  loss_mask_5: 1.772  loss_dice_5: 2.43  loss_bbox_5: 2.561  loss_giou_5: 1.54  loss_ce_dn_5: 1.971  loss_mask_dn_5: 1.433  loss_dice_dn_5: 2.172  loss_bbox_dn_5: 0.8377  loss_giou_dn_5: 0.654  loss_ce_6: 1.631  loss_mask_6: 1.752  loss_dice_6: 2.441  loss_bbox_6: 2.572  loss_giou_6: 1.535  loss_ce_dn_6: 2.114  loss_mask_dn_6: 1.431  loss_dice_dn_6: 2.152  loss_bbox_dn_6: 0.8351  loss_giou_dn_6: 0.6524  loss_ce_7: 1.561  loss_mask_7: 1.742  loss_dice_7: 2.42  loss_bbox_7: 2.536  loss_giou_7: 1.509  loss_ce_dn_7: 2.107  loss_mask_dn_7: 1.437  loss_dice_dn_7: 2.143  loss_bbox_dn_7: 0.8089  loss_giou_dn_7: 0.6421  loss_ce_8: 1.554  loss_mask_8: 1.786  loss_dice_8: 2.412  loss_bbox_8: 2.505  loss_giou_8: 1.511  loss_ce_dn_8: 2.162  loss_mask_dn_8: 1.407  loss_dice_dn_8: 2.166  loss_bbox_dn_8: 0.8109  loss_giou_dn_8: 0.6465    time: 2.2734  last_time: 2.2601  data_time: 0.0115  last_data_time: 0.0211   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:00:50 d2.utils.events]: \u001b[0m eta: 8 days, 6:17:50  iter: 4879  total_loss: 163.7  loss_ce: 1.7  loss_mask: 1.421  loss_dice: 2.146  loss_bbox: 2.468  loss_giou: 1.603  loss_ce_dn: 1.988  loss_mask_dn: 1.111  loss_dice_dn: 1.788  loss_bbox_dn: 0.7749  loss_giou_dn: 0.651  loss_ce_0: 5.406  loss_mask_0: 1.378  loss_dice_0: 2.302  loss_bbox_0: 4.186  loss_giou_0: 2.066  loss_ce_1: 1.956  loss_mask_1: 1.376  loss_dice_1: 2.181  loss_bbox_1: 2.668  loss_giou_1: 1.752  loss_ce_dn_1: 2.416  loss_mask_dn_1: 1.227  loss_dice_dn_1: 2.062  loss_bbox_dn_1: 0.9513  loss_giou_dn_1: 0.7366  loss_ce_2: 1.846  loss_mask_2: 1.365  loss_dice_2: 2.129  loss_bbox_2: 2.422  loss_giou_2: 1.618  loss_ce_dn_2: 1.943  loss_mask_dn_2: 1.123  loss_dice_dn_2: 1.908  loss_bbox_dn_2: 0.8875  loss_giou_dn_2: 0.7064  loss_ce_3: 1.686  loss_mask_3: 1.377  loss_dice_3: 2.129  loss_bbox_3: 2.51  loss_giou_3: 1.632  loss_ce_dn_3: 2.043  loss_mask_dn_3: 1.098  loss_dice_dn_3: 1.767  loss_bbox_dn_3: 0.8208  loss_giou_dn_3: 0.6722  loss_ce_4: 1.721  loss_mask_4: 1.416  loss_dice_4: 2.142  loss_bbox_4: 2.524  loss_giou_4: 1.601  loss_ce_dn_4: 1.943  loss_mask_dn_4: 1.032  loss_dice_dn_4: 1.751  loss_bbox_dn_4: 0.7991  loss_giou_dn_4: 0.658  loss_ce_5: 1.671  loss_mask_5: 1.449  loss_dice_5: 2.162  loss_bbox_5: 2.478  loss_giou_5: 1.627  loss_ce_dn_5: 1.846  loss_mask_dn_5: 1.07  loss_dice_dn_5: 1.756  loss_bbox_dn_5: 0.7743  loss_giou_dn_5: 0.6468  loss_ce_6: 1.671  loss_mask_6: 1.436  loss_dice_6: 2.185  loss_bbox_6: 2.474  loss_giou_6: 1.624  loss_ce_dn_6: 1.869  loss_mask_dn_6: 1.065  loss_dice_dn_6: 1.745  loss_bbox_dn_6: 0.7696  loss_giou_dn_6: 0.6453  loss_ce_7: 1.691  loss_mask_7: 1.425  loss_dice_7: 2.168  loss_bbox_7: 2.466  loss_giou_7: 1.597  loss_ce_dn_7: 1.957  loss_mask_dn_7: 1.1  loss_dice_dn_7: 1.798  loss_bbox_dn_7: 0.768  loss_giou_dn_7: 0.6416  loss_ce_8: 1.747  loss_mask_8: 1.431  loss_dice_8: 2.118  loss_bbox_8: 2.474  loss_giou_8: 1.606  loss_ce_dn_8: 2.112  loss_mask_dn_8: 1.09  loss_dice_dn_8: 1.75  loss_bbox_dn_8: 0.7717  loss_giou_dn_8: 0.6462    time: 2.2734  last_time: 2.2253  data_time: 0.0121  last_data_time: 0.0041   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:01:35 d2.utils.events]: \u001b[0m eta: 8 days, 6:17:04  iter: 4899  total_loss: 162.1  loss_ce: 1.33  loss_mask: 1.635  loss_dice: 2.205  loss_bbox: 2.798  loss_giou: 1.321  loss_ce_dn: 1.916  loss_mask_dn: 1.492  loss_dice_dn: 2.118  loss_bbox_dn: 0.8056  loss_giou_dn: 0.6476  loss_ce_0: 5.538  loss_mask_0: 1.593  loss_dice_0: 2.524  loss_bbox_0: 3.862  loss_giou_0: 1.884  loss_ce_1: 1.633  loss_mask_1: 1.577  loss_dice_1: 2.282  loss_bbox_1: 3.046  loss_giou_1: 1.515  loss_ce_dn_1: 2.244  loss_mask_dn_1: 1.458  loss_dice_dn_1: 2.22  loss_bbox_dn_1: 0.9754  loss_giou_dn_1: 0.7207  loss_ce_2: 1.454  loss_mask_2: 1.553  loss_dice_2: 2.218  loss_bbox_2: 2.958  loss_giou_2: 1.422  loss_ce_dn_2: 1.987  loss_mask_dn_2: 1.484  loss_dice_dn_2: 2.139  loss_bbox_dn_2: 0.9243  loss_giou_dn_2: 0.6976  loss_ce_3: 1.351  loss_mask_3: 1.633  loss_dice_3: 2.249  loss_bbox_3: 2.908  loss_giou_3: 1.419  loss_ce_dn_3: 1.898  loss_mask_dn_3: 1.477  loss_dice_dn_3: 2.137  loss_bbox_dn_3: 0.8961  loss_giou_dn_3: 0.6758  loss_ce_4: 1.36  loss_mask_4: 1.623  loss_dice_4: 2.234  loss_bbox_4: 2.897  loss_giou_4: 1.353  loss_ce_dn_4: 1.887  loss_mask_dn_4: 1.442  loss_dice_dn_4: 2.124  loss_bbox_dn_4: 0.8524  loss_giou_dn_4: 0.6537  loss_ce_5: 1.378  loss_mask_5: 1.64  loss_dice_5: 2.243  loss_bbox_5: 2.857  loss_giou_5: 1.356  loss_ce_dn_5: 1.874  loss_mask_dn_5: 1.465  loss_dice_dn_5: 2.149  loss_bbox_dn_5: 0.8236  loss_giou_dn_5: 0.6457  loss_ce_6: 1.352  loss_mask_6: 1.625  loss_dice_6: 2.224  loss_bbox_6: 2.854  loss_giou_6: 1.332  loss_ce_dn_6: 1.925  loss_mask_dn_6: 1.514  loss_dice_dn_6: 2.146  loss_bbox_dn_6: 0.8246  loss_giou_dn_6: 0.6474  loss_ce_7: 1.318  loss_mask_7: 1.616  loss_dice_7: 2.201  loss_bbox_7: 2.854  loss_giou_7: 1.336  loss_ce_dn_7: 1.947  loss_mask_dn_7: 1.476  loss_dice_dn_7: 2.132  loss_bbox_dn_7: 0.8069  loss_giou_dn_7: 0.6423  loss_ce_8: 1.352  loss_mask_8: 1.617  loss_dice_8: 2.206  loss_bbox_8: 2.848  loss_giou_8: 1.32  loss_ce_dn_8: 1.904  loss_mask_dn_8: 1.492  loss_dice_dn_8: 2.098  loss_bbox_dn_8: 0.8064  loss_giou_dn_8: 0.644    time: 2.2734  last_time: 2.3731  data_time: 0.0124  last_data_time: 0.0146   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:02:21 d2.utils.events]: \u001b[0m eta: 8 days, 6:16:31  iter: 4919  total_loss: 173.3  loss_ce: 1.873  loss_mask: 1.237  loss_dice: 2.517  loss_bbox: 2.475  loss_giou: 1.67  loss_ce_dn: 2.35  loss_mask_dn: 1.178  loss_dice_dn: 2.166  loss_bbox_dn: 0.6834  loss_giou_dn: 0.6865  loss_ce_0: 5.601  loss_mask_0: 1.154  loss_dice_0: 2.552  loss_bbox_0: 4.019  loss_giou_0: 1.941  loss_ce_1: 1.89  loss_mask_1: 1.155  loss_dice_1: 2.476  loss_bbox_1: 2.921  loss_giou_1: 1.744  loss_ce_dn_1: 2.667  loss_mask_dn_1: 1.185  loss_dice_dn_1: 2.359  loss_bbox_dn_1: 0.8036  loss_giou_dn_1: 0.7401  loss_ce_2: 1.712  loss_mask_2: 1.086  loss_dice_2: 2.466  loss_bbox_2: 2.626  loss_giou_2: 1.729  loss_ce_dn_2: 2.267  loss_mask_dn_2: 1.216  loss_dice_dn_2: 2.227  loss_bbox_dn_2: 0.753  loss_giou_dn_2: 0.7206  loss_ce_3: 1.679  loss_mask_3: 1.189  loss_dice_3: 2.471  loss_bbox_3: 2.527  loss_giou_3: 1.73  loss_ce_dn_3: 2.128  loss_mask_dn_3: 1.188  loss_dice_dn_3: 2.16  loss_bbox_dn_3: 0.7159  loss_giou_dn_3: 0.7021  loss_ce_4: 1.667  loss_mask_4: 1.22  loss_dice_4: 2.474  loss_bbox_4: 2.526  loss_giou_4: 1.721  loss_ce_dn_4: 2.182  loss_mask_dn_4: 1.165  loss_dice_dn_4: 2.12  loss_bbox_dn_4: 0.7035  loss_giou_dn_4: 0.6892  loss_ce_5: 1.775  loss_mask_5: 1.213  loss_dice_5: 2.497  loss_bbox_5: 2.482  loss_giou_5: 1.697  loss_ce_dn_5: 2.253  loss_mask_dn_5: 1.156  loss_dice_dn_5: 2.122  loss_bbox_dn_5: 0.6888  loss_giou_dn_5: 0.6834  loss_ce_6: 1.762  loss_mask_6: 1.208  loss_dice_6: 2.455  loss_bbox_6: 2.484  loss_giou_6: 1.702  loss_ce_dn_6: 2.254  loss_mask_dn_6: 1.207  loss_dice_dn_6: 2.11  loss_bbox_dn_6: 0.689  loss_giou_dn_6: 0.6855  loss_ce_7: 1.752  loss_mask_7: 1.243  loss_dice_7: 2.499  loss_bbox_7: 2.482  loss_giou_7: 1.678  loss_ce_dn_7: 2.214  loss_mask_dn_7: 1.203  loss_dice_dn_7: 2.113  loss_bbox_dn_7: 0.6805  loss_giou_dn_7: 0.6778  loss_ce_8: 1.774  loss_mask_8: 1.219  loss_dice_8: 2.501  loss_bbox_8: 2.478  loss_giou_8: 1.672  loss_ce_dn_8: 2.308  loss_mask_dn_8: 1.164  loss_dice_dn_8: 2.152  loss_bbox_dn_8: 0.6796  loss_giou_dn_8: 0.6821    time: 2.2734  last_time: 2.2506  data_time: 0.0144  last_data_time: 0.0268   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:03:06 d2.utils.events]: \u001b[0m eta: 8 days, 6:15:02  iter: 4939  total_loss: 175.8  loss_ce: 1.51  loss_mask: 1.812  loss_dice: 2.372  loss_bbox: 2.718  loss_giou: 1.547  loss_ce_dn: 2.17  loss_mask_dn: 1.469  loss_dice_dn: 2.196  loss_bbox_dn: 0.8004  loss_giou_dn: 0.6386  loss_ce_0: 5.587  loss_mask_0: 1.675  loss_dice_0: 2.746  loss_bbox_0: 4.004  loss_giou_0: 1.979  loss_ce_1: 1.844  loss_mask_1: 1.897  loss_dice_1: 2.371  loss_bbox_1: 2.905  loss_giou_1: 1.507  loss_ce_dn_1: 2.737  loss_mask_dn_1: 1.554  loss_dice_dn_1: 2.331  loss_bbox_dn_1: 0.8667  loss_giou_dn_1: 0.7073  loss_ce_2: 1.588  loss_mask_2: 1.885  loss_dice_2: 2.372  loss_bbox_2: 2.807  loss_giou_2: 1.481  loss_ce_dn_2: 2.171  loss_mask_dn_2: 1.565  loss_dice_dn_2: 2.24  loss_bbox_dn_2: 0.8262  loss_giou_dn_2: 0.6745  loss_ce_3: 1.499  loss_mask_3: 1.863  loss_dice_3: 2.372  loss_bbox_3: 2.727  loss_giou_3: 1.486  loss_ce_dn_3: 2.104  loss_mask_dn_3: 1.512  loss_dice_dn_3: 2.207  loss_bbox_dn_3: 0.806  loss_giou_dn_3: 0.6623  loss_ce_4: 1.517  loss_mask_4: 1.842  loss_dice_4: 2.359  loss_bbox_4: 2.72  loss_giou_4: 1.548  loss_ce_dn_4: 2.045  loss_mask_dn_4: 1.503  loss_dice_dn_4: 2.185  loss_bbox_dn_4: 0.7885  loss_giou_dn_4: 0.6537  loss_ce_5: 1.555  loss_mask_5: 1.812  loss_dice_5: 2.367  loss_bbox_5: 2.672  loss_giou_5: 1.522  loss_ce_dn_5: 2.167  loss_mask_dn_5: 1.468  loss_dice_dn_5: 2.182  loss_bbox_dn_5: 0.7921  loss_giou_dn_5: 0.6455  loss_ce_6: 1.578  loss_mask_6: 1.781  loss_dice_6: 2.375  loss_bbox_6: 2.693  loss_giou_6: 1.53  loss_ce_dn_6: 2.132  loss_mask_dn_6: 1.455  loss_dice_dn_6: 2.202  loss_bbox_dn_6: 0.7903  loss_giou_dn_6: 0.6425  loss_ce_7: 1.524  loss_mask_7: 1.823  loss_dice_7: 2.408  loss_bbox_7: 2.707  loss_giou_7: 1.551  loss_ce_dn_7: 2.204  loss_mask_dn_7: 1.475  loss_dice_dn_7: 2.209  loss_bbox_dn_7: 0.7868  loss_giou_dn_7: 0.6304  loss_ce_8: 1.513  loss_mask_8: 1.797  loss_dice_8: 2.4  loss_bbox_8: 2.711  loss_giou_8: 1.551  loss_ce_dn_8: 2.165  loss_mask_dn_8: 1.464  loss_dice_dn_8: 2.205  loss_bbox_dn_8: 0.7945  loss_giou_dn_8: 0.6349    time: 2.2733  last_time: 2.2397  data_time: 0.0122  last_data_time: 0.0053   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:03:51 d2.utils.events]: \u001b[0m eta: 8 days, 6:15:26  iter: 4959  total_loss: 159.5  loss_ce: 1.461  loss_mask: 1.369  loss_dice: 2.245  loss_bbox: 2.585  loss_giou: 1.398  loss_ce_dn: 2.201  loss_mask_dn: 1.281  loss_dice_dn: 2.197  loss_bbox_dn: 0.915  loss_giou_dn: 0.6595  loss_ce_0: 4.83  loss_mask_0: 1.531  loss_dice_0: 2.472  loss_bbox_0: 4.176  loss_giou_0: 1.917  loss_ce_1: 1.836  loss_mask_1: 1.397  loss_dice_1: 2.326  loss_bbox_1: 2.726  loss_giou_1: 1.576  loss_ce_dn_1: 2.564  loss_mask_dn_1: 1.399  loss_dice_dn_1: 2.289  loss_bbox_dn_1: 0.9236  loss_giou_dn_1: 0.7246  loss_ce_2: 1.588  loss_mask_2: 1.467  loss_dice_2: 2.296  loss_bbox_2: 2.676  loss_giou_2: 1.503  loss_ce_dn_2: 2.168  loss_mask_dn_2: 1.352  loss_dice_dn_2: 2.214  loss_bbox_dn_2: 0.9279  loss_giou_dn_2: 0.7032  loss_ce_3: 1.377  loss_mask_3: 1.442  loss_dice_3: 2.279  loss_bbox_3: 2.677  loss_giou_3: 1.49  loss_ce_dn_3: 2.1  loss_mask_dn_3: 1.334  loss_dice_dn_3: 2.227  loss_bbox_dn_3: 0.9061  loss_giou_dn_3: 0.682  loss_ce_4: 1.362  loss_mask_4: 1.439  loss_dice_4: 2.29  loss_bbox_4: 2.667  loss_giou_4: 1.479  loss_ce_dn_4: 2.197  loss_mask_dn_4: 1.326  loss_dice_dn_4: 2.196  loss_bbox_dn_4: 0.914  loss_giou_dn_4: 0.6687  loss_ce_5: 1.385  loss_mask_5: 1.434  loss_dice_5: 2.282  loss_bbox_5: 2.638  loss_giou_5: 1.466  loss_ce_dn_5: 2.072  loss_mask_dn_5: 1.291  loss_dice_dn_5: 2.203  loss_bbox_dn_5: 0.9141  loss_giou_dn_5: 0.663  loss_ce_6: 1.488  loss_mask_6: 1.424  loss_dice_6: 2.216  loss_bbox_6: 2.579  loss_giou_6: 1.424  loss_ce_dn_6: 2.121  loss_mask_dn_6: 1.297  loss_dice_dn_6: 2.218  loss_bbox_dn_6: 0.9213  loss_giou_dn_6: 0.6617  loss_ce_7: 1.495  loss_mask_7: 1.394  loss_dice_7: 2.212  loss_bbox_7: 2.517  loss_giou_7: 1.408  loss_ce_dn_7: 2.122  loss_mask_dn_7: 1.273  loss_dice_dn_7: 2.191  loss_bbox_dn_7: 0.9102  loss_giou_dn_7: 0.6559  loss_ce_8: 1.398  loss_mask_8: 1.386  loss_dice_8: 2.245  loss_bbox_8: 2.512  loss_giou_8: 1.408  loss_ce_dn_8: 2.151  loss_mask_dn_8: 1.311  loss_dice_dn_8: 2.207  loss_bbox_dn_8: 0.91  loss_giou_dn_8: 0.6566    time: 2.2733  last_time: 2.3461  data_time: 0.0140  last_data_time: 0.0161   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:04:37 d2.utils.events]: \u001b[0m eta: 8 days, 6:17:32  iter: 4979  total_loss: 177.8  loss_ce: 1.678  loss_mask: 1.416  loss_dice: 2.505  loss_bbox: 2.673  loss_giou: 1.66  loss_ce_dn: 2.28  loss_mask_dn: 1.227  loss_dice_dn: 2.446  loss_bbox_dn: 0.7516  loss_giou_dn: 0.6815  loss_ce_0: 5.223  loss_mask_0: 1.314  loss_dice_0: 2.849  loss_bbox_0: 4.105  loss_giou_0: 2.033  loss_ce_1: 2  loss_mask_1: 1.437  loss_dice_1: 2.656  loss_bbox_1: 3.078  loss_giou_1: 1.732  loss_ce_dn_1: 2.707  loss_mask_dn_1: 1.249  loss_dice_dn_1: 2.475  loss_bbox_dn_1: 0.9392  loss_giou_dn_1: 0.7573  loss_ce_2: 1.738  loss_mask_2: 1.424  loss_dice_2: 2.593  loss_bbox_2: 2.855  loss_giou_2: 1.771  loss_ce_dn_2: 2.324  loss_mask_dn_2: 1.234  loss_dice_dn_2: 2.363  loss_bbox_dn_2: 0.8828  loss_giou_dn_2: 0.7361  loss_ce_3: 1.706  loss_mask_3: 1.431  loss_dice_3: 2.551  loss_bbox_3: 2.781  loss_giou_3: 1.731  loss_ce_dn_3: 2.254  loss_mask_dn_3: 1.209  loss_dice_dn_3: 2.36  loss_bbox_dn_3: 0.8335  loss_giou_dn_3: 0.7123  loss_ce_4: 1.604  loss_mask_4: 1.415  loss_dice_4: 2.58  loss_bbox_4: 2.759  loss_giou_4: 1.693  loss_ce_dn_4: 2.209  loss_mask_dn_4: 1.199  loss_dice_dn_4: 2.379  loss_bbox_dn_4: 0.8073  loss_giou_dn_4: 0.7013  loss_ce_5: 1.614  loss_mask_5: 1.453  loss_dice_5: 2.547  loss_bbox_5: 2.738  loss_giou_5: 1.695  loss_ce_dn_5: 2.231  loss_mask_dn_5: 1.203  loss_dice_dn_5: 2.415  loss_bbox_dn_5: 0.7843  loss_giou_dn_5: 0.6958  loss_ce_6: 1.635  loss_mask_6: 1.442  loss_dice_6: 2.533  loss_bbox_6: 2.743  loss_giou_6: 1.673  loss_ce_dn_6: 2.194  loss_mask_dn_6: 1.252  loss_dice_dn_6: 2.443  loss_bbox_dn_6: 0.775  loss_giou_dn_6: 0.688  loss_ce_7: 1.727  loss_mask_7: 1.445  loss_dice_7: 2.528  loss_bbox_7: 2.678  loss_giou_7: 1.657  loss_ce_dn_7: 2.22  loss_mask_dn_7: 1.254  loss_dice_dn_7: 2.454  loss_bbox_dn_7: 0.7561  loss_giou_dn_7: 0.6797  loss_ce_8: 1.747  loss_mask_8: 1.409  loss_dice_8: 2.492  loss_bbox_8: 2.671  loss_giou_8: 1.654  loss_ce_dn_8: 2.256  loss_mask_dn_8: 1.237  loss_dice_dn_8: 2.419  loss_bbox_dn_8: 0.7519  loss_giou_dn_8: 0.6797    time: 2.2733  last_time: 2.3049  data_time: 0.0128  last_data_time: 0.0073   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:05:23 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to maskdino_SwinL_bs16_160k_steplr_workdir/model_0004999.pth\n",
            "\u001b[32m[07/26 18:05:44 d2.data.datasets.coco]: \u001b[0mLoaded 2000 images with semantic segmentation from datasets/ADEChallengeData2016/images/validation\n",
            "\u001b[32m[07/26 18:05:44 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=2048, sample_style='choice')]\n",
            "\u001b[32m[07/26 18:05:44 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[07/26 18:05:44 d2.data.common]: \u001b[0mSerializing 2000 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[07/26 18:05:44 d2.data.common]: \u001b[0mSerialized dataset takes 0.39 MiB\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\u001b[32m[07/26 18:05:44 d2.data.datasets.coco]: \u001b[0mLoaded 2000 images with semantic segmentation from datasets/ADEChallengeData2016/images/validation\n",
            "\u001b[32m[07/26 18:05:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 2000 batches\n",
            "\u001b[32m[07/26 18:05:50 d2.evaluation.evaluator]: \u001b[0mInference done 11/2000. Dataloading: 0.0018 s/iter. Inference: 0.4398 s/iter. Eval: 0.0228 s/iter. Total: 0.4645 s/iter. ETA=0:15:23\n",
            "\u001b[32m[07/26 18:05:55 d2.evaluation.evaluator]: \u001b[0mInference done 23/2000. Dataloading: 0.0029 s/iter. Inference: 0.4264 s/iter. Eval: 0.0248 s/iter. Total: 0.4542 s/iter. ETA=0:14:57\n",
            "\u001b[32m[07/26 18:06:01 d2.evaluation.evaluator]: \u001b[0mInference done 35/2000. Dataloading: 0.0027 s/iter. Inference: 0.4275 s/iter. Eval: 0.0234 s/iter. Total: 0.4536 s/iter. ETA=0:14:51\n",
            "\u001b[32m[07/26 18:06:06 d2.evaluation.evaluator]: \u001b[0mInference done 46/2000. Dataloading: 0.0030 s/iter. Inference: 0.4358 s/iter. Eval: 0.0225 s/iter. Total: 0.4615 s/iter. ETA=0:15:01\n",
            "\u001b[32m[07/26 18:06:11 d2.evaluation.evaluator]: \u001b[0mInference done 56/2000. Dataloading: 0.0029 s/iter. Inference: 0.4434 s/iter. Eval: 0.0234 s/iter. Total: 0.4697 s/iter. ETA=0:15:13\n",
            "\u001b[32m[07/26 18:06:16 d2.evaluation.evaluator]: \u001b[0mInference done 67/2000. Dataloading: 0.0031 s/iter. Inference: 0.4464 s/iter. Eval: 0.0227 s/iter. Total: 0.4723 s/iter. ETA=0:15:12\n",
            "\u001b[32m[07/26 18:06:22 d2.evaluation.evaluator]: \u001b[0mInference done 78/2000. Dataloading: 0.0030 s/iter. Inference: 0.4467 s/iter. Eval: 0.0223 s/iter. Total: 0.4721 s/iter. ETA=0:15:07\n",
            "\u001b[32m[07/26 18:06:27 d2.evaluation.evaluator]: \u001b[0mInference done 90/2000. Dataloading: 0.0030 s/iter. Inference: 0.4436 s/iter. Eval: 0.0218 s/iter. Total: 0.4685 s/iter. ETA=0:14:54\n",
            "\u001b[32m[07/26 18:06:33 d2.evaluation.evaluator]: \u001b[0mInference done 101/2000. Dataloading: 0.0031 s/iter. Inference: 0.4452 s/iter. Eval: 0.0235 s/iter. Total: 0.4718 s/iter. ETA=0:14:55\n",
            "\u001b[32m[07/26 18:06:38 d2.evaluation.evaluator]: \u001b[0mInference done 113/2000. Dataloading: 0.0030 s/iter. Inference: 0.4405 s/iter. Eval: 0.0231 s/iter. Total: 0.4666 s/iter. ETA=0:14:40\n",
            "\u001b[32m[07/26 18:06:43 d2.evaluation.evaluator]: \u001b[0mInference done 125/2000. Dataloading: 0.0032 s/iter. Inference: 0.4377 s/iter. Eval: 0.0238 s/iter. Total: 0.4647 s/iter. ETA=0:14:31\n",
            "\u001b[32m[07/26 18:06:48 d2.evaluation.evaluator]: \u001b[0mInference done 137/2000. Dataloading: 0.0031 s/iter. Inference: 0.4366 s/iter. Eval: 0.0240 s/iter. Total: 0.4637 s/iter. ETA=0:14:23\n",
            "\u001b[32m[07/26 18:06:54 d2.evaluation.evaluator]: \u001b[0mInference done 148/2000. Dataloading: 0.0032 s/iter. Inference: 0.4368 s/iter. Eval: 0.0247 s/iter. Total: 0.4647 s/iter. ETA=0:14:20\n",
            "\u001b[32m[07/26 18:06:59 d2.evaluation.evaluator]: \u001b[0mInference done 159/2000. Dataloading: 0.0032 s/iter. Inference: 0.4378 s/iter. Eval: 0.0248 s/iter. Total: 0.4658 s/iter. ETA=0:14:17\n",
            "\u001b[32m[07/26 18:07:04 d2.evaluation.evaluator]: \u001b[0mInference done 171/2000. Dataloading: 0.0032 s/iter. Inference: 0.4365 s/iter. Eval: 0.0249 s/iter. Total: 0.4647 s/iter. ETA=0:14:09\n",
            "\u001b[32m[07/26 18:07:10 d2.evaluation.evaluator]: \u001b[0mInference done 183/2000. Dataloading: 0.0031 s/iter. Inference: 0.4351 s/iter. Eval: 0.0251 s/iter. Total: 0.4634 s/iter. ETA=0:14:01\n",
            "\u001b[32m[07/26 18:07:15 d2.evaluation.evaluator]: \u001b[0mInference done 194/2000. Dataloading: 0.0031 s/iter. Inference: 0.4354 s/iter. Eval: 0.0244 s/iter. Total: 0.4630 s/iter. ETA=0:13:56\n",
            "\u001b[32m[07/26 18:07:20 d2.evaluation.evaluator]: \u001b[0mInference done 206/2000. Dataloading: 0.0031 s/iter. Inference: 0.4341 s/iter. Eval: 0.0242 s/iter. Total: 0.4615 s/iter. ETA=0:13:47\n",
            "\u001b[32m[07/26 18:07:25 d2.evaluation.evaluator]: \u001b[0mInference done 219/2000. Dataloading: 0.0030 s/iter. Inference: 0.4305 s/iter. Eval: 0.0234 s/iter. Total: 0.4571 s/iter. ETA=0:13:34\n",
            "\u001b[32m[07/26 18:07:30 d2.evaluation.evaluator]: \u001b[0mInference done 230/2000. Dataloading: 0.0031 s/iter. Inference: 0.4309 s/iter. Eval: 0.0235 s/iter. Total: 0.4576 s/iter. ETA=0:13:29\n",
            "\u001b[32m[07/26 18:07:36 d2.evaluation.evaluator]: \u001b[0mInference done 241/2000. Dataloading: 0.0031 s/iter. Inference: 0.4327 s/iter. Eval: 0.0232 s/iter. Total: 0.4591 s/iter. ETA=0:13:27\n",
            "\u001b[32m[07/26 18:07:41 d2.evaluation.evaluator]: \u001b[0mInference done 253/2000. Dataloading: 0.0030 s/iter. Inference: 0.4325 s/iter. Eval: 0.0231 s/iter. Total: 0.4587 s/iter. ETA=0:13:21\n",
            "\u001b[32m[07/26 18:07:46 d2.evaluation.evaluator]: \u001b[0mInference done 265/2000. Dataloading: 0.0031 s/iter. Inference: 0.4306 s/iter. Eval: 0.0230 s/iter. Total: 0.4567 s/iter. ETA=0:13:12\n",
            "\u001b[32m[07/26 18:07:51 d2.evaluation.evaluator]: \u001b[0mInference done 277/2000. Dataloading: 0.0031 s/iter. Inference: 0.4296 s/iter. Eval: 0.0230 s/iter. Total: 0.4557 s/iter. ETA=0:13:05\n",
            "\u001b[32m[07/26 18:07:56 d2.evaluation.evaluator]: \u001b[0mInference done 288/2000. Dataloading: 0.0031 s/iter. Inference: 0.4299 s/iter. Eval: 0.0233 s/iter. Total: 0.4563 s/iter. ETA=0:13:01\n",
            "\u001b[32m[07/26 18:08:01 d2.evaluation.evaluator]: \u001b[0mInference done 299/2000. Dataloading: 0.0031 s/iter. Inference: 0.4298 s/iter. Eval: 0.0234 s/iter. Total: 0.4564 s/iter. ETA=0:12:56\n",
            "\u001b[32m[07/26 18:08:07 d2.evaluation.evaluator]: \u001b[0mInference done 311/2000. Dataloading: 0.0030 s/iter. Inference: 0.4290 s/iter. Eval: 0.0237 s/iter. Total: 0.4559 s/iter. ETA=0:12:49\n",
            "\u001b[32m[07/26 18:08:12 d2.evaluation.evaluator]: \u001b[0mInference done 323/2000. Dataloading: 0.0030 s/iter. Inference: 0.4286 s/iter. Eval: 0.0240 s/iter. Total: 0.4557 s/iter. ETA=0:12:44\n",
            "\u001b[32m[07/26 18:08:17 d2.evaluation.evaluator]: \u001b[0mInference done 335/2000. Dataloading: 0.0030 s/iter. Inference: 0.4282 s/iter. Eval: 0.0239 s/iter. Total: 0.4552 s/iter. ETA=0:12:37\n",
            "\u001b[32m[07/26 18:08:23 d2.evaluation.evaluator]: \u001b[0mInference done 346/2000. Dataloading: 0.0030 s/iter. Inference: 0.4288 s/iter. Eval: 0.0239 s/iter. Total: 0.4557 s/iter. ETA=0:12:33\n",
            "\u001b[32m[07/26 18:08:28 d2.evaluation.evaluator]: \u001b[0mInference done 358/2000. Dataloading: 0.0030 s/iter. Inference: 0.4279 s/iter. Eval: 0.0238 s/iter. Total: 0.4547 s/iter. ETA=0:12:26\n",
            "\u001b[32m[07/26 18:08:33 d2.evaluation.evaluator]: \u001b[0mInference done 371/2000. Dataloading: 0.0030 s/iter. Inference: 0.4264 s/iter. Eval: 0.0234 s/iter. Total: 0.4529 s/iter. ETA=0:12:17\n",
            "\u001b[32m[07/26 18:08:38 d2.evaluation.evaluator]: \u001b[0mInference done 382/2000. Dataloading: 0.0030 s/iter. Inference: 0.4266 s/iter. Eval: 0.0236 s/iter. Total: 0.4532 s/iter. ETA=0:12:13\n",
            "\u001b[32m[07/26 18:08:43 d2.evaluation.evaluator]: \u001b[0mInference done 393/2000. Dataloading: 0.0030 s/iter. Inference: 0.4268 s/iter. Eval: 0.0236 s/iter. Total: 0.4535 s/iter. ETA=0:12:08\n",
            "\u001b[32m[07/26 18:08:48 d2.evaluation.evaluator]: \u001b[0mInference done 405/2000. Dataloading: 0.0030 s/iter. Inference: 0.4264 s/iter. Eval: 0.0234 s/iter. Total: 0.4529 s/iter. ETA=0:12:02\n",
            "\u001b[32m[07/26 18:08:53 d2.evaluation.evaluator]: \u001b[0mInference done 420/2000. Dataloading: 0.0030 s/iter. Inference: 0.4229 s/iter. Eval: 0.0229 s/iter. Total: 0.4489 s/iter. ETA=0:11:49\n",
            "\u001b[32m[07/26 18:08:59 d2.evaluation.evaluator]: \u001b[0mInference done 432/2000. Dataloading: 0.0030 s/iter. Inference: 0.4227 s/iter. Eval: 0.0233 s/iter. Total: 0.4491 s/iter. ETA=0:11:44\n",
            "\u001b[32m[07/26 18:09:04 d2.evaluation.evaluator]: \u001b[0mInference done 444/2000. Dataloading: 0.0030 s/iter. Inference: 0.4229 s/iter. Eval: 0.0232 s/iter. Total: 0.4492 s/iter. ETA=0:11:38\n",
            "\u001b[32m[07/26 18:09:10 d2.evaluation.evaluator]: \u001b[0mInference done 456/2000. Dataloading: 0.0030 s/iter. Inference: 0.4228 s/iter. Eval: 0.0233 s/iter. Total: 0.4491 s/iter. ETA=0:11:33\n",
            "\u001b[32m[07/26 18:09:15 d2.evaluation.evaluator]: \u001b[0mInference done 467/2000. Dataloading: 0.0030 s/iter. Inference: 0.4229 s/iter. Eval: 0.0234 s/iter. Total: 0.4493 s/iter. ETA=0:11:28\n",
            "\u001b[32m[07/26 18:09:20 d2.evaluation.evaluator]: \u001b[0mInference done 478/2000. Dataloading: 0.0029 s/iter. Inference: 0.4231 s/iter. Eval: 0.0236 s/iter. Total: 0.4497 s/iter. ETA=0:11:24\n",
            "\u001b[32m[07/26 18:09:25 d2.evaluation.evaluator]: \u001b[0mInference done 490/2000. Dataloading: 0.0030 s/iter. Inference: 0.4225 s/iter. Eval: 0.0236 s/iter. Total: 0.4492 s/iter. ETA=0:11:18\n",
            "\u001b[32m[07/26 18:09:30 d2.evaluation.evaluator]: \u001b[0mInference done 502/2000. Dataloading: 0.0029 s/iter. Inference: 0.4224 s/iter. Eval: 0.0236 s/iter. Total: 0.4490 s/iter. ETA=0:11:12\n",
            "\u001b[32m[07/26 18:09:36 d2.evaluation.evaluator]: \u001b[0mInference done 513/2000. Dataloading: 0.0029 s/iter. Inference: 0.4229 s/iter. Eval: 0.0238 s/iter. Total: 0.4497 s/iter. ETA=0:11:08\n",
            "\u001b[32m[07/26 18:09:41 d2.evaluation.evaluator]: \u001b[0mInference done 524/2000. Dataloading: 0.0029 s/iter. Inference: 0.4235 s/iter. Eval: 0.0240 s/iter. Total: 0.4505 s/iter. ETA=0:11:04\n",
            "\u001b[32m[07/26 18:09:46 d2.evaluation.evaluator]: \u001b[0mInference done 535/2000. Dataloading: 0.0029 s/iter. Inference: 0.4239 s/iter. Eval: 0.0243 s/iter. Total: 0.4512 s/iter. ETA=0:11:00\n",
            "\u001b[32m[07/26 18:09:52 d2.evaluation.evaluator]: \u001b[0mInference done 546/2000. Dataloading: 0.0029 s/iter. Inference: 0.4244 s/iter. Eval: 0.0242 s/iter. Total: 0.4516 s/iter. ETA=0:10:56\n",
            "\u001b[32m[07/26 18:09:57 d2.evaluation.evaluator]: \u001b[0mInference done 558/2000. Dataloading: 0.0029 s/iter. Inference: 0.4239 s/iter. Eval: 0.0241 s/iter. Total: 0.4510 s/iter. ETA=0:10:50\n",
            "\u001b[32m[07/26 18:10:02 d2.evaluation.evaluator]: \u001b[0mInference done 570/2000. Dataloading: 0.0029 s/iter. Inference: 0.4236 s/iter. Eval: 0.0241 s/iter. Total: 0.4507 s/iter. ETA=0:10:44\n",
            "\u001b[32m[07/26 18:10:07 d2.evaluation.evaluator]: \u001b[0mInference done 582/2000. Dataloading: 0.0029 s/iter. Inference: 0.4236 s/iter. Eval: 0.0242 s/iter. Total: 0.4507 s/iter. ETA=0:10:39\n",
            "\u001b[32m[07/26 18:10:12 d2.evaluation.evaluator]: \u001b[0mInference done 593/2000. Dataloading: 0.0029 s/iter. Inference: 0.4236 s/iter. Eval: 0.0244 s/iter. Total: 0.4510 s/iter. ETA=0:10:34\n",
            "\u001b[32m[07/26 18:10:17 d2.evaluation.evaluator]: \u001b[0mInference done 604/2000. Dataloading: 0.0029 s/iter. Inference: 0.4237 s/iter. Eval: 0.0244 s/iter. Total: 0.4511 s/iter. ETA=0:10:29\n",
            "\u001b[32m[07/26 18:10:23 d2.evaluation.evaluator]: \u001b[0mInference done 616/2000. Dataloading: 0.0029 s/iter. Inference: 0.4233 s/iter. Eval: 0.0243 s/iter. Total: 0.4507 s/iter. ETA=0:10:23\n",
            "\u001b[32m[07/26 18:10:28 d2.evaluation.evaluator]: \u001b[0mInference done 628/2000. Dataloading: 0.0029 s/iter. Inference: 0.4233 s/iter. Eval: 0.0241 s/iter. Total: 0.4504 s/iter. ETA=0:10:18\n",
            "\u001b[32m[07/26 18:10:33 d2.evaluation.evaluator]: \u001b[0mInference done 640/2000. Dataloading: 0.0029 s/iter. Inference: 0.4234 s/iter. Eval: 0.0240 s/iter. Total: 0.4504 s/iter. ETA=0:10:12\n",
            "\u001b[32m[07/26 18:10:38 d2.evaluation.evaluator]: \u001b[0mInference done 651/2000. Dataloading: 0.0030 s/iter. Inference: 0.4236 s/iter. Eval: 0.0239 s/iter. Total: 0.4505 s/iter. ETA=0:10:07\n",
            "\u001b[32m[07/26 18:10:43 d2.evaluation.evaluator]: \u001b[0mInference done 662/2000. Dataloading: 0.0029 s/iter. Inference: 0.4239 s/iter. Eval: 0.0239 s/iter. Total: 0.4508 s/iter. ETA=0:10:03\n",
            "\u001b[32m[07/26 18:10:48 d2.evaluation.evaluator]: \u001b[0mInference done 673/2000. Dataloading: 0.0030 s/iter. Inference: 0.4241 s/iter. Eval: 0.0238 s/iter. Total: 0.4510 s/iter. ETA=0:09:58\n",
            "\u001b[32m[07/26 18:10:54 d2.evaluation.evaluator]: \u001b[0mInference done 684/2000. Dataloading: 0.0030 s/iter. Inference: 0.4244 s/iter. Eval: 0.0237 s/iter. Total: 0.4511 s/iter. ETA=0:09:53\n",
            "\u001b[32m[07/26 18:10:59 d2.evaluation.evaluator]: \u001b[0mInference done 696/2000. Dataloading: 0.0029 s/iter. Inference: 0.4244 s/iter. Eval: 0.0238 s/iter. Total: 0.4512 s/iter. ETA=0:09:48\n",
            "\u001b[32m[07/26 18:11:04 d2.evaluation.evaluator]: \u001b[0mInference done 708/2000. Dataloading: 0.0030 s/iter. Inference: 0.4245 s/iter. Eval: 0.0237 s/iter. Total: 0.4512 s/iter. ETA=0:09:42\n",
            "\u001b[32m[07/26 18:11:10 d2.evaluation.evaluator]: \u001b[0mInference done 719/2000. Dataloading: 0.0030 s/iter. Inference: 0.4249 s/iter. Eval: 0.0237 s/iter. Total: 0.4515 s/iter. ETA=0:09:38\n",
            "\u001b[32m[07/26 18:11:15 d2.evaluation.evaluator]: \u001b[0mInference done 731/2000. Dataloading: 0.0030 s/iter. Inference: 0.4249 s/iter. Eval: 0.0236 s/iter. Total: 0.4516 s/iter. ETA=0:09:33\n",
            "\u001b[32m[07/26 18:11:20 d2.evaluation.evaluator]: \u001b[0mInference done 742/2000. Dataloading: 0.0030 s/iter. Inference: 0.4252 s/iter. Eval: 0.0235 s/iter. Total: 0.4517 s/iter. ETA=0:09:28\n",
            "\u001b[32m[07/26 18:11:25 d2.evaluation.evaluator]: \u001b[0mInference done 754/2000. Dataloading: 0.0030 s/iter. Inference: 0.4250 s/iter. Eval: 0.0235 s/iter. Total: 0.4515 s/iter. ETA=0:09:22\n",
            "\u001b[32m[07/26 18:11:30 d2.evaluation.evaluator]: \u001b[0mInference done 768/2000. Dataloading: 0.0030 s/iter. Inference: 0.4236 s/iter. Eval: 0.0232 s/iter. Total: 0.4498 s/iter. ETA=0:09:14\n",
            "\u001b[32m[07/26 18:11:36 d2.evaluation.evaluator]: \u001b[0mInference done 780/2000. Dataloading: 0.0029 s/iter. Inference: 0.4234 s/iter. Eval: 0.0232 s/iter. Total: 0.4496 s/iter. ETA=0:09:08\n",
            "\u001b[32m[07/26 18:11:41 d2.evaluation.evaluator]: \u001b[0mInference done 791/2000. Dataloading: 0.0030 s/iter. Inference: 0.4234 s/iter. Eval: 0.0234 s/iter. Total: 0.4498 s/iter. ETA=0:09:03\n",
            "\u001b[32m[07/26 18:11:46 d2.evaluation.evaluator]: \u001b[0mInference done 803/2000. Dataloading: 0.0029 s/iter. Inference: 0.4233 s/iter. Eval: 0.0235 s/iter. Total: 0.4498 s/iter. ETA=0:08:58\n",
            "\u001b[32m[07/26 18:11:51 d2.evaluation.evaluator]: \u001b[0mInference done 814/2000. Dataloading: 0.0030 s/iter. Inference: 0.4233 s/iter. Eval: 0.0237 s/iter. Total: 0.4500 s/iter. ETA=0:08:53\n",
            "\u001b[32m[07/26 18:11:57 d2.evaluation.evaluator]: \u001b[0mInference done 826/2000. Dataloading: 0.0030 s/iter. Inference: 0.4232 s/iter. Eval: 0.0238 s/iter. Total: 0.4500 s/iter. ETA=0:08:48\n",
            "\u001b[32m[07/26 18:12:02 d2.evaluation.evaluator]: \u001b[0mInference done 837/2000. Dataloading: 0.0030 s/iter. Inference: 0.4232 s/iter. Eval: 0.0240 s/iter. Total: 0.4503 s/iter. ETA=0:08:43\n",
            "\u001b[32m[07/26 18:12:07 d2.evaluation.evaluator]: \u001b[0mInference done 848/2000. Dataloading: 0.0030 s/iter. Inference: 0.4232 s/iter. Eval: 0.0241 s/iter. Total: 0.4504 s/iter. ETA=0:08:38\n",
            "\u001b[32m[07/26 18:12:12 d2.evaluation.evaluator]: \u001b[0mInference done 860/2000. Dataloading: 0.0030 s/iter. Inference: 0.4231 s/iter. Eval: 0.0242 s/iter. Total: 0.4504 s/iter. ETA=0:08:33\n",
            "\u001b[32m[07/26 18:12:18 d2.evaluation.evaluator]: \u001b[0mInference done 875/2000. Dataloading: 0.0030 s/iter. Inference: 0.4216 s/iter. Eval: 0.0240 s/iter. Total: 0.4486 s/iter. ETA=0:08:24\n",
            "\u001b[32m[07/26 18:12:23 d2.evaluation.evaluator]: \u001b[0mInference done 887/2000. Dataloading: 0.0030 s/iter. Inference: 0.4216 s/iter. Eval: 0.0238 s/iter. Total: 0.4485 s/iter. ETA=0:08:19\n",
            "\u001b[32m[07/26 18:12:28 d2.evaluation.evaluator]: \u001b[0mInference done 899/2000. Dataloading: 0.0030 s/iter. Inference: 0.4215 s/iter. Eval: 0.0238 s/iter. Total: 0.4483 s/iter. ETA=0:08:13\n",
            "\u001b[32m[07/26 18:12:33 d2.evaluation.evaluator]: \u001b[0mInference done 911/2000. Dataloading: 0.0030 s/iter. Inference: 0.4215 s/iter. Eval: 0.0237 s/iter. Total: 0.4482 s/iter. ETA=0:08:08\n",
            "\u001b[32m[07/26 18:12:38 d2.evaluation.evaluator]: \u001b[0mInference done 922/2000. Dataloading: 0.0030 s/iter. Inference: 0.4216 s/iter. Eval: 0.0237 s/iter. Total: 0.4483 s/iter. ETA=0:08:03\n",
            "\u001b[32m[07/26 18:12:43 d2.evaluation.evaluator]: \u001b[0mInference done 933/2000. Dataloading: 0.0030 s/iter. Inference: 0.4217 s/iter. Eval: 0.0236 s/iter. Total: 0.4484 s/iter. ETA=0:07:58\n",
            "\u001b[32m[07/26 18:12:48 d2.evaluation.evaluator]: \u001b[0mInference done 945/2000. Dataloading: 0.0030 s/iter. Inference: 0.4216 s/iter. Eval: 0.0236 s/iter. Total: 0.4482 s/iter. ETA=0:07:52\n",
            "\u001b[32m[07/26 18:12:54 d2.evaluation.evaluator]: \u001b[0mInference done 957/2000. Dataloading: 0.0030 s/iter. Inference: 0.4214 s/iter. Eval: 0.0236 s/iter. Total: 0.4481 s/iter. ETA=0:07:47\n",
            "\u001b[32m[07/26 18:12:59 d2.evaluation.evaluator]: \u001b[0mInference done 968/2000. Dataloading: 0.0030 s/iter. Inference: 0.4214 s/iter. Eval: 0.0237 s/iter. Total: 0.4482 s/iter. ETA=0:07:42\n",
            "\u001b[32m[07/26 18:13:04 d2.evaluation.evaluator]: \u001b[0mInference done 981/2000. Dataloading: 0.0030 s/iter. Inference: 0.4209 s/iter. Eval: 0.0237 s/iter. Total: 0.4477 s/iter. ETA=0:07:36\n",
            "\u001b[32m[07/26 18:13:09 d2.evaluation.evaluator]: \u001b[0mInference done 993/2000. Dataloading: 0.0030 s/iter. Inference: 0.4208 s/iter. Eval: 0.0236 s/iter. Total: 0.4474 s/iter. ETA=0:07:30\n",
            "\u001b[32m[07/26 18:13:14 d2.evaluation.evaluator]: \u001b[0mInference done 1005/2000. Dataloading: 0.0030 s/iter. Inference: 0.4206 s/iter. Eval: 0.0236 s/iter. Total: 0.4473 s/iter. ETA=0:07:25\n",
            "\u001b[32m[07/26 18:13:20 d2.evaluation.evaluator]: \u001b[0mInference done 1016/2000. Dataloading: 0.0030 s/iter. Inference: 0.4208 s/iter. Eval: 0.0237 s/iter. Total: 0.4475 s/iter. ETA=0:07:20\n",
            "\u001b[32m[07/26 18:13:25 d2.evaluation.evaluator]: \u001b[0mInference done 1028/2000. Dataloading: 0.0030 s/iter. Inference: 0.4206 s/iter. Eval: 0.0236 s/iter. Total: 0.4473 s/iter. ETA=0:07:14\n",
            "\u001b[32m[07/26 18:13:30 d2.evaluation.evaluator]: \u001b[0mInference done 1039/2000. Dataloading: 0.0030 s/iter. Inference: 0.4208 s/iter. Eval: 0.0237 s/iter. Total: 0.4476 s/iter. ETA=0:07:10\n",
            "\u001b[32m[07/26 18:13:36 d2.evaluation.evaluator]: \u001b[0mInference done 1051/2000. Dataloading: 0.0030 s/iter. Inference: 0.4210 s/iter. Eval: 0.0237 s/iter. Total: 0.4477 s/iter. ETA=0:07:04\n",
            "\u001b[32m[07/26 18:13:41 d2.evaluation.evaluator]: \u001b[0mInference done 1063/2000. Dataloading: 0.0030 s/iter. Inference: 0.4209 s/iter. Eval: 0.0236 s/iter. Total: 0.4476 s/iter. ETA=0:06:59\n",
            "\u001b[32m[07/26 18:13:46 d2.evaluation.evaluator]: \u001b[0mInference done 1074/2000. Dataloading: 0.0030 s/iter. Inference: 0.4210 s/iter. Eval: 0.0236 s/iter. Total: 0.4476 s/iter. ETA=0:06:54\n",
            "\u001b[32m[07/26 18:13:51 d2.evaluation.evaluator]: \u001b[0mInference done 1085/2000. Dataloading: 0.0030 s/iter. Inference: 0.4211 s/iter. Eval: 0.0237 s/iter. Total: 0.4478 s/iter. ETA=0:06:49\n",
            "\u001b[32m[07/26 18:13:56 d2.evaluation.evaluator]: \u001b[0mInference done 1097/2000. Dataloading: 0.0030 s/iter. Inference: 0.4209 s/iter. Eval: 0.0237 s/iter. Total: 0.4477 s/iter. ETA=0:06:44\n",
            "\u001b[32m[07/26 18:14:01 d2.evaluation.evaluator]: \u001b[0mInference done 1110/2000. Dataloading: 0.0030 s/iter. Inference: 0.4206 s/iter. Eval: 0.0237 s/iter. Total: 0.4473 s/iter. ETA=0:06:38\n",
            "\u001b[32m[07/26 18:14:07 d2.evaluation.evaluator]: \u001b[0mInference done 1122/2000. Dataloading: 0.0030 s/iter. Inference: 0.4204 s/iter. Eval: 0.0237 s/iter. Total: 0.4472 s/iter. ETA=0:06:32\n",
            "\u001b[32m[07/26 18:14:12 d2.evaluation.evaluator]: \u001b[0mInference done 1134/2000. Dataloading: 0.0030 s/iter. Inference: 0.4203 s/iter. Eval: 0.0237 s/iter. Total: 0.4471 s/iter. ETA=0:06:27\n",
            "\u001b[32m[07/26 18:14:17 d2.evaluation.evaluator]: \u001b[0mInference done 1146/2000. Dataloading: 0.0030 s/iter. Inference: 0.4202 s/iter. Eval: 0.0237 s/iter. Total: 0.4470 s/iter. ETA=0:06:21\n",
            "\u001b[32m[07/26 18:14:22 d2.evaluation.evaluator]: \u001b[0mInference done 1157/2000. Dataloading: 0.0030 s/iter. Inference: 0.4203 s/iter. Eval: 0.0239 s/iter. Total: 0.4473 s/iter. ETA=0:06:17\n",
            "\u001b[32m[07/26 18:14:28 d2.evaluation.evaluator]: \u001b[0mInference done 1169/2000. Dataloading: 0.0030 s/iter. Inference: 0.4202 s/iter. Eval: 0.0239 s/iter. Total: 0.4472 s/iter. ETA=0:06:11\n",
            "\u001b[32m[07/26 18:14:33 d2.evaluation.evaluator]: \u001b[0mInference done 1181/2000. Dataloading: 0.0030 s/iter. Inference: 0.4203 s/iter. Eval: 0.0239 s/iter. Total: 0.4473 s/iter. ETA=0:06:06\n",
            "\u001b[32m[07/26 18:14:39 d2.evaluation.evaluator]: \u001b[0mInference done 1193/2000. Dataloading: 0.0030 s/iter. Inference: 0.4203 s/iter. Eval: 0.0239 s/iter. Total: 0.4472 s/iter. ETA=0:06:00\n",
            "\u001b[32m[07/26 18:14:44 d2.evaluation.evaluator]: \u001b[0mInference done 1206/2000. Dataloading: 0.0030 s/iter. Inference: 0.4197 s/iter. Eval: 0.0238 s/iter. Total: 0.4466 s/iter. ETA=0:05:54\n",
            "\u001b[32m[07/26 18:14:49 d2.evaluation.evaluator]: \u001b[0mInference done 1219/2000. Dataloading: 0.0030 s/iter. Inference: 0.4194 s/iter. Eval: 0.0237 s/iter. Total: 0.4462 s/iter. ETA=0:05:48\n",
            "\u001b[32m[07/26 18:14:54 d2.evaluation.evaluator]: \u001b[0mInference done 1231/2000. Dataloading: 0.0030 s/iter. Inference: 0.4195 s/iter. Eval: 0.0237 s/iter. Total: 0.4463 s/iter. ETA=0:05:43\n",
            "\u001b[32m[07/26 18:14:59 d2.evaluation.evaluator]: \u001b[0mInference done 1243/2000. Dataloading: 0.0030 s/iter. Inference: 0.4193 s/iter. Eval: 0.0236 s/iter. Total: 0.4460 s/iter. ETA=0:05:37\n",
            "\u001b[32m[07/26 18:15:04 d2.evaluation.evaluator]: \u001b[0mInference done 1255/2000. Dataloading: 0.0030 s/iter. Inference: 0.4192 s/iter. Eval: 0.0235 s/iter. Total: 0.4458 s/iter. ETA=0:05:32\n",
            "\u001b[32m[07/26 18:15:10 d2.evaluation.evaluator]: \u001b[0mInference done 1268/2000. Dataloading: 0.0030 s/iter. Inference: 0.4188 s/iter. Eval: 0.0235 s/iter. Total: 0.4453 s/iter. ETA=0:05:25\n",
            "\u001b[32m[07/26 18:15:15 d2.evaluation.evaluator]: \u001b[0mInference done 1281/2000. Dataloading: 0.0030 s/iter. Inference: 0.4184 s/iter. Eval: 0.0234 s/iter. Total: 0.4449 s/iter. ETA=0:05:19\n",
            "\u001b[32m[07/26 18:15:20 d2.evaluation.evaluator]: \u001b[0mInference done 1293/2000. Dataloading: 0.0030 s/iter. Inference: 0.4183 s/iter. Eval: 0.0233 s/iter. Total: 0.4447 s/iter. ETA=0:05:14\n",
            "\u001b[32m[07/26 18:15:25 d2.evaluation.evaluator]: \u001b[0mInference done 1305/2000. Dataloading: 0.0030 s/iter. Inference: 0.4182 s/iter. Eval: 0.0233 s/iter. Total: 0.4446 s/iter. ETA=0:05:09\n",
            "\u001b[32m[07/26 18:15:30 d2.evaluation.evaluator]: \u001b[0mInference done 1317/2000. Dataloading: 0.0030 s/iter. Inference: 0.4180 s/iter. Eval: 0.0233 s/iter. Total: 0.4444 s/iter. ETA=0:05:03\n",
            "\u001b[32m[07/26 18:15:36 d2.evaluation.evaluator]: \u001b[0mInference done 1329/2000. Dataloading: 0.0030 s/iter. Inference: 0.4180 s/iter. Eval: 0.0233 s/iter. Total: 0.4444 s/iter. ETA=0:04:58\n",
            "\u001b[32m[07/26 18:15:41 d2.evaluation.evaluator]: \u001b[0mInference done 1340/2000. Dataloading: 0.0030 s/iter. Inference: 0.4181 s/iter. Eval: 0.0234 s/iter. Total: 0.4445 s/iter. ETA=0:04:53\n",
            "\u001b[32m[07/26 18:15:46 d2.evaluation.evaluator]: \u001b[0mInference done 1353/2000. Dataloading: 0.0030 s/iter. Inference: 0.4178 s/iter. Eval: 0.0233 s/iter. Total: 0.4442 s/iter. ETA=0:04:47\n",
            "\u001b[32m[07/26 18:15:51 d2.evaluation.evaluator]: \u001b[0mInference done 1367/2000. Dataloading: 0.0030 s/iter. Inference: 0.4172 s/iter. Eval: 0.0232 s/iter. Total: 0.4435 s/iter. ETA=0:04:40\n",
            "\u001b[32m[07/26 18:15:56 d2.evaluation.evaluator]: \u001b[0mInference done 1378/2000. Dataloading: 0.0030 s/iter. Inference: 0.4173 s/iter. Eval: 0.0233 s/iter. Total: 0.4436 s/iter. ETA=0:04:35\n",
            "\u001b[32m[07/26 18:16:02 d2.evaluation.evaluator]: \u001b[0mInference done 1390/2000. Dataloading: 0.0030 s/iter. Inference: 0.4174 s/iter. Eval: 0.0233 s/iter. Total: 0.4437 s/iter. ETA=0:04:30\n",
            "\u001b[32m[07/26 18:16:07 d2.evaluation.evaluator]: \u001b[0mInference done 1401/2000. Dataloading: 0.0030 s/iter. Inference: 0.4176 s/iter. Eval: 0.0232 s/iter. Total: 0.4439 s/iter. ETA=0:04:25\n",
            "\u001b[32m[07/26 18:16:12 d2.evaluation.evaluator]: \u001b[0mInference done 1416/2000. Dataloading: 0.0030 s/iter. Inference: 0.4167 s/iter. Eval: 0.0231 s/iter. Total: 0.4429 s/iter. ETA=0:04:18\n",
            "\u001b[32m[07/26 18:16:17 d2.evaluation.evaluator]: \u001b[0mInference done 1429/2000. Dataloading: 0.0030 s/iter. Inference: 0.4165 s/iter. Eval: 0.0231 s/iter. Total: 0.4426 s/iter. ETA=0:04:12\n",
            "\u001b[32m[07/26 18:16:23 d2.evaluation.evaluator]: \u001b[0mInference done 1440/2000. Dataloading: 0.0030 s/iter. Inference: 0.4165 s/iter. Eval: 0.0231 s/iter. Total: 0.4427 s/iter. ETA=0:04:07\n",
            "\u001b[32m[07/26 18:16:28 d2.evaluation.evaluator]: \u001b[0mInference done 1452/2000. Dataloading: 0.0030 s/iter. Inference: 0.4165 s/iter. Eval: 0.0231 s/iter. Total: 0.4427 s/iter. ETA=0:04:02\n",
            "\u001b[32m[07/26 18:16:33 d2.evaluation.evaluator]: \u001b[0mInference done 1464/2000. Dataloading: 0.0030 s/iter. Inference: 0.4165 s/iter. Eval: 0.0232 s/iter. Total: 0.4428 s/iter. ETA=0:03:57\n",
            "\u001b[32m[07/26 18:16:39 d2.evaluation.evaluator]: \u001b[0mInference done 1476/2000. Dataloading: 0.0030 s/iter. Inference: 0.4164 s/iter. Eval: 0.0232 s/iter. Total: 0.4428 s/iter. ETA=0:03:52\n",
            "\u001b[32m[07/26 18:16:44 d2.evaluation.evaluator]: \u001b[0mInference done 1489/2000. Dataloading: 0.0030 s/iter. Inference: 0.4162 s/iter. Eval: 0.0232 s/iter. Total: 0.4425 s/iter. ETA=0:03:46\n",
            "\u001b[32m[07/26 18:16:49 d2.evaluation.evaluator]: \u001b[0mInference done 1501/2000. Dataloading: 0.0030 s/iter. Inference: 0.4161 s/iter. Eval: 0.0232 s/iter. Total: 0.4424 s/iter. ETA=0:03:40\n",
            "\u001b[32m[07/26 18:16:54 d2.evaluation.evaluator]: \u001b[0mInference done 1513/2000. Dataloading: 0.0030 s/iter. Inference: 0.4162 s/iter. Eval: 0.0232 s/iter. Total: 0.4425 s/iter. ETA=0:03:35\n",
            "\u001b[32m[07/26 18:17:00 d2.evaluation.evaluator]: \u001b[0mInference done 1524/2000. Dataloading: 0.0030 s/iter. Inference: 0.4164 s/iter. Eval: 0.0233 s/iter. Total: 0.4428 s/iter. ETA=0:03:30\n",
            "\u001b[32m[07/26 18:17:05 d2.evaluation.evaluator]: \u001b[0mInference done 1536/2000. Dataloading: 0.0030 s/iter. Inference: 0.4163 s/iter. Eval: 0.0233 s/iter. Total: 0.4427 s/iter. ETA=0:03:25\n",
            "\u001b[32m[07/26 18:17:10 d2.evaluation.evaluator]: \u001b[0mInference done 1548/2000. Dataloading: 0.0030 s/iter. Inference: 0.4164 s/iter. Eval: 0.0233 s/iter. Total: 0.4428 s/iter. ETA=0:03:20\n",
            "\u001b[32m[07/26 18:17:15 d2.evaluation.evaluator]: \u001b[0mInference done 1561/2000. Dataloading: 0.0030 s/iter. Inference: 0.4160 s/iter. Eval: 0.0232 s/iter. Total: 0.4423 s/iter. ETA=0:03:14\n",
            "\u001b[32m[07/26 18:17:21 d2.evaluation.evaluator]: \u001b[0mInference done 1573/2000. Dataloading: 0.0030 s/iter. Inference: 0.4159 s/iter. Eval: 0.0232 s/iter. Total: 0.4422 s/iter. ETA=0:03:08\n",
            "\u001b[32m[07/26 18:17:26 d2.evaluation.evaluator]: \u001b[0mInference done 1585/2000. Dataloading: 0.0030 s/iter. Inference: 0.4159 s/iter. Eval: 0.0233 s/iter. Total: 0.4423 s/iter. ETA=0:03:03\n",
            "\u001b[32m[07/26 18:17:31 d2.evaluation.evaluator]: \u001b[0mInference done 1597/2000. Dataloading: 0.0030 s/iter. Inference: 0.4158 s/iter. Eval: 0.0234 s/iter. Total: 0.4423 s/iter. ETA=0:02:58\n",
            "\u001b[32m[07/26 18:17:37 d2.evaluation.evaluator]: \u001b[0mInference done 1610/2000. Dataloading: 0.0030 s/iter. Inference: 0.4156 s/iter. Eval: 0.0233 s/iter. Total: 0.4420 s/iter. ETA=0:02:52\n",
            "\u001b[32m[07/26 18:17:42 d2.evaluation.evaluator]: \u001b[0mInference done 1622/2000. Dataloading: 0.0030 s/iter. Inference: 0.4157 s/iter. Eval: 0.0232 s/iter. Total: 0.4420 s/iter. ETA=0:02:47\n",
            "\u001b[32m[07/26 18:17:47 d2.evaluation.evaluator]: \u001b[0mInference done 1634/2000. Dataloading: 0.0030 s/iter. Inference: 0.4158 s/iter. Eval: 0.0232 s/iter. Total: 0.4420 s/iter. ETA=0:02:41\n",
            "\u001b[32m[07/26 18:17:52 d2.evaluation.evaluator]: \u001b[0mInference done 1646/2000. Dataloading: 0.0030 s/iter. Inference: 0.4156 s/iter. Eval: 0.0232 s/iter. Total: 0.4419 s/iter. ETA=0:02:36\n",
            "\u001b[32m[07/26 18:17:58 d2.evaluation.evaluator]: \u001b[0mInference done 1658/2000. Dataloading: 0.0030 s/iter. Inference: 0.4157 s/iter. Eval: 0.0231 s/iter. Total: 0.4419 s/iter. ETA=0:02:31\n",
            "\u001b[32m[07/26 18:18:03 d2.evaluation.evaluator]: \u001b[0mInference done 1670/2000. Dataloading: 0.0030 s/iter. Inference: 0.4157 s/iter. Eval: 0.0231 s/iter. Total: 0.4419 s/iter. ETA=0:02:25\n",
            "\u001b[32m[07/26 18:18:08 d2.evaluation.evaluator]: \u001b[0mInference done 1681/2000. Dataloading: 0.0030 s/iter. Inference: 0.4160 s/iter. Eval: 0.0230 s/iter. Total: 0.4421 s/iter. ETA=0:02:21\n",
            "\u001b[32m[07/26 18:18:13 d2.evaluation.evaluator]: \u001b[0mInference done 1692/2000. Dataloading: 0.0030 s/iter. Inference: 0.4161 s/iter. Eval: 0.0230 s/iter. Total: 0.4422 s/iter. ETA=0:02:16\n",
            "\u001b[32m[07/26 18:18:19 d2.evaluation.evaluator]: \u001b[0mInference done 1704/2000. Dataloading: 0.0030 s/iter. Inference: 0.4161 s/iter. Eval: 0.0231 s/iter. Total: 0.4423 s/iter. ETA=0:02:10\n",
            "\u001b[32m[07/26 18:18:24 d2.evaluation.evaluator]: \u001b[0mInference done 1715/2000. Dataloading: 0.0030 s/iter. Inference: 0.4163 s/iter. Eval: 0.0231 s/iter. Total: 0.4424 s/iter. ETA=0:02:06\n",
            "\u001b[32m[07/26 18:18:29 d2.evaluation.evaluator]: \u001b[0mInference done 1727/2000. Dataloading: 0.0030 s/iter. Inference: 0.4163 s/iter. Eval: 0.0231 s/iter. Total: 0.4425 s/iter. ETA=0:02:00\n",
            "\u001b[32m[07/26 18:18:35 d2.evaluation.evaluator]: \u001b[0mInference done 1739/2000. Dataloading: 0.0030 s/iter. Inference: 0.4164 s/iter. Eval: 0.0231 s/iter. Total: 0.4426 s/iter. ETA=0:01:55\n",
            "\u001b[32m[07/26 18:18:40 d2.evaluation.evaluator]: \u001b[0mInference done 1751/2000. Dataloading: 0.0030 s/iter. Inference: 0.4165 s/iter. Eval: 0.0231 s/iter. Total: 0.4426 s/iter. ETA=0:01:50\n",
            "\u001b[32m[07/26 18:18:45 d2.evaluation.evaluator]: \u001b[0mInference done 1766/2000. Dataloading: 0.0030 s/iter. Inference: 0.4159 s/iter. Eval: 0.0229 s/iter. Total: 0.4419 s/iter. ETA=0:01:43\n",
            "\u001b[32m[07/26 18:18:50 d2.evaluation.evaluator]: \u001b[0mInference done 1777/2000. Dataloading: 0.0030 s/iter. Inference: 0.4160 s/iter. Eval: 0.0229 s/iter. Total: 0.4420 s/iter. ETA=0:01:38\n",
            "\u001b[32m[07/26 18:18:56 d2.evaluation.evaluator]: \u001b[0mInference done 1789/2000. Dataloading: 0.0030 s/iter. Inference: 0.4160 s/iter. Eval: 0.0230 s/iter. Total: 0.4421 s/iter. ETA=0:01:33\n",
            "\u001b[32m[07/26 18:19:01 d2.evaluation.evaluator]: \u001b[0mInference done 1800/2000. Dataloading: 0.0030 s/iter. Inference: 0.4160 s/iter. Eval: 0.0231 s/iter. Total: 0.4422 s/iter. ETA=0:01:28\n",
            "\u001b[32m[07/26 18:19:06 d2.evaluation.evaluator]: \u001b[0mInference done 1812/2000. Dataloading: 0.0030 s/iter. Inference: 0.4160 s/iter. Eval: 0.0231 s/iter. Total: 0.4422 s/iter. ETA=0:01:23\n",
            "\u001b[32m[07/26 18:19:12 d2.evaluation.evaluator]: \u001b[0mInference done 1824/2000. Dataloading: 0.0030 s/iter. Inference: 0.4160 s/iter. Eval: 0.0232 s/iter. Total: 0.4423 s/iter. ETA=0:01:17\n",
            "\u001b[32m[07/26 18:19:17 d2.evaluation.evaluator]: \u001b[0mInference done 1835/2000. Dataloading: 0.0030 s/iter. Inference: 0.4160 s/iter. Eval: 0.0233 s/iter. Total: 0.4424 s/iter. ETA=0:01:12\n",
            "\u001b[32m[07/26 18:19:22 d2.evaluation.evaluator]: \u001b[0mInference done 1847/2000. Dataloading: 0.0030 s/iter. Inference: 0.4160 s/iter. Eval: 0.0233 s/iter. Total: 0.4424 s/iter. ETA=0:01:07\n",
            "\u001b[32m[07/26 18:19:27 d2.evaluation.evaluator]: \u001b[0mInference done 1858/2000. Dataloading: 0.0030 s/iter. Inference: 0.4160 s/iter. Eval: 0.0234 s/iter. Total: 0.4425 s/iter. ETA=0:01:02\n",
            "\u001b[32m[07/26 18:19:33 d2.evaluation.evaluator]: \u001b[0mInference done 1874/2000. Dataloading: 0.0030 s/iter. Inference: 0.4153 s/iter. Eval: 0.0233 s/iter. Total: 0.4416 s/iter. ETA=0:00:55\n",
            "\u001b[32m[07/26 18:19:38 d2.evaluation.evaluator]: \u001b[0mInference done 1885/2000. Dataloading: 0.0030 s/iter. Inference: 0.4153 s/iter. Eval: 0.0233 s/iter. Total: 0.4417 s/iter. ETA=0:00:50\n",
            "\u001b[32m[07/26 18:19:43 d2.evaluation.evaluator]: \u001b[0mInference done 1896/2000. Dataloading: 0.0030 s/iter. Inference: 0.4155 s/iter. Eval: 0.0232 s/iter. Total: 0.4419 s/iter. ETA=0:00:45\n",
            "\u001b[32m[07/26 18:19:48 d2.evaluation.evaluator]: \u001b[0mInference done 1909/2000. Dataloading: 0.0030 s/iter. Inference: 0.4153 s/iter. Eval: 0.0232 s/iter. Total: 0.4416 s/iter. ETA=0:00:40\n",
            "\u001b[32m[07/26 18:19:53 d2.evaluation.evaluator]: \u001b[0mInference done 1920/2000. Dataloading: 0.0030 s/iter. Inference: 0.4154 s/iter. Eval: 0.0231 s/iter. Total: 0.4416 s/iter. ETA=0:00:35\n",
            "\u001b[32m[07/26 18:19:58 d2.evaluation.evaluator]: \u001b[0mInference done 1931/2000. Dataloading: 0.0030 s/iter. Inference: 0.4156 s/iter. Eval: 0.0231 s/iter. Total: 0.4418 s/iter. ETA=0:00:30\n",
            "\u001b[32m[07/26 18:20:03 d2.evaluation.evaluator]: \u001b[0mInference done 1943/2000. Dataloading: 0.0030 s/iter. Inference: 0.4156 s/iter. Eval: 0.0231 s/iter. Total: 0.4418 s/iter. ETA=0:00:25\n",
            "\u001b[32m[07/26 18:20:09 d2.evaluation.evaluator]: \u001b[0mInference done 1956/2000. Dataloading: 0.0030 s/iter. Inference: 0.4154 s/iter. Eval: 0.0231 s/iter. Total: 0.4416 s/iter. ETA=0:00:19\n",
            "\u001b[32m[07/26 18:20:14 d2.evaluation.evaluator]: \u001b[0mInference done 1967/2000. Dataloading: 0.0030 s/iter. Inference: 0.4155 s/iter. Eval: 0.0231 s/iter. Total: 0.4417 s/iter. ETA=0:00:14\n",
            "\u001b[32m[07/26 18:20:19 d2.evaluation.evaluator]: \u001b[0mInference done 1979/2000. Dataloading: 0.0030 s/iter. Inference: 0.4153 s/iter. Eval: 0.0232 s/iter. Total: 0.4416 s/iter. ETA=0:00:09\n",
            "\u001b[32m[07/26 18:20:24 d2.evaluation.evaluator]: \u001b[0mInference done 1991/2000. Dataloading: 0.0030 s/iter. Inference: 0.4153 s/iter. Eval: 0.0231 s/iter. Total: 0.4416 s/iter. ETA=0:00:03\n",
            "\u001b[32m[07/26 18:20:28 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:14:40.818124 (0.441513 s / iter per device, on 1 devices)\n",
            "\u001b[32m[07/26 18:20:28 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:13:48 (0.415248 s / iter per device, on 1 devices)\n",
            "\u001b[32m[07/26 18:20:28 d2.evaluation.sem_seg_evaluation]: \u001b[0mOrderedDict([('sem_seg', {'mIoU': 14.642490108635139, 'fwIoU': 57.45511342035804, 'IoU-wall': 69.32283054631424, 'BoundaryIoU-wall': 71.17622606785102, 'min(IoU, B-Iou)-wall': 69.32283054631424, 'IoU-building': 74.57454867141277, 'BoundaryIoU-building': 8.192978826715542, 'min(IoU, B-Iou)-building': 8.192978826715542, 'IoU-sky': 93.47213157812904, 'BoundaryIoU-sky': 0.0, 'min(IoU, B-Iou)-sky': 0.0, 'IoU-floor': 75.1996851029977, 'BoundaryIoU-floor': 0.0, 'min(IoU, B-Iou)-floor': 0.0, 'IoU-tree': 71.85692605069246, 'BoundaryIoU-tree': 0.0, 'min(IoU, B-Iou)-tree': 0.0, 'IoU-ceiling': 72.69137620426895, 'BoundaryIoU-ceiling': 0.0, 'min(IoU, B-Iou)-ceiling': 0.0, 'IoU-road, route': 74.08831039956428, 'BoundaryIoU-road, route': 0.0, 'min(IoU, B-Iou)-road, route': 0.0, 'IoU-bed': 47.597599155801404, 'BoundaryIoU-bed': 0.0, 'min(IoU, B-Iou)-bed': 0.0, 'IoU-window ': 50.40800550317207, 'BoundaryIoU-window ': 0.0, 'min(IoU, B-Iou)-window ': 0.0, 'IoU-grass': 53.882817502332294, 'BoundaryIoU-grass': 0.0, 'min(IoU, B-Iou)-grass': 0.0, 'IoU-cabinet': 48.368867961770434, 'BoundaryIoU-cabinet': 0.0, 'min(IoU, B-Iou)-cabinet': 0.0, 'IoU-sidewalk, pavement': 53.47309389878878, 'BoundaryIoU-sidewalk, pavement': 0.0, 'min(IoU, B-Iou)-sidewalk, pavement': 0.0, 'IoU-person': 75.18844693549312, 'BoundaryIoU-person': 0.0, 'min(IoU, B-Iou)-person': 0.0, 'IoU-earth, ground': 6.729975502896804, 'BoundaryIoU-earth, ground': 0.0, 'min(IoU, B-Iou)-earth, ground': 0.0, 'IoU-door': 25.458796371143, 'BoundaryIoU-door': 0.0, 'min(IoU, B-Iou)-door': 0.0, 'IoU-table': 38.726623714360365, 'BoundaryIoU-table': 0.0, 'min(IoU, B-Iou)-table': 0.0, 'IoU-mountain, mount': 39.17628548182825, 'BoundaryIoU-mountain, mount': 0.0, 'min(IoU, B-Iou)-mountain, mount': 0.0, 'IoU-plant': 45.38533272657992, 'BoundaryIoU-plant': 0.0, 'min(IoU, B-Iou)-plant': 0.0, 'IoU-curtain': 61.864586973363046, 'BoundaryIoU-curtain': 0.0, 'min(IoU, B-Iou)-curtain': 0.0, 'IoU-chair': 32.043303991192204, 'BoundaryIoU-chair': 0.0, 'min(IoU, B-Iou)-chair': 0.0, 'IoU-car': 72.13447529336473, 'BoundaryIoU-car': 0.0, 'min(IoU, B-Iou)-car': 0.0, 'IoU-water': 14.918521371347534, 'BoundaryIoU-water': 0.0, 'min(IoU, B-Iou)-water': 0.0, 'IoU-painting, picture': 56.82515602013183, 'BoundaryIoU-painting, picture': 0.0, 'min(IoU, B-Iou)-painting, picture': 0.0, 'IoU-sofa': 8.54377565954843, 'BoundaryIoU-sofa': 0.0, 'min(IoU, B-Iou)-sofa': 0.0, 'IoU-shelf': 23.293495868352863, 'BoundaryIoU-shelf': 0.0, 'min(IoU, B-Iou)-shelf': 0.0, 'IoU-house': 0.02422309135313064, 'BoundaryIoU-house': 0.0, 'min(IoU, B-Iou)-house': 0.0, 'IoU-sea': 31.180543122066908, 'BoundaryIoU-sea': 0.0, 'min(IoU, B-Iou)-sea': 0.0, 'IoU-mirror': 11.864791467142464, 'BoundaryIoU-mirror': 0.0, 'min(IoU, B-Iou)-mirror': 0.0, 'IoU-rug': 50.245673477898244, 'BoundaryIoU-rug': 0.0, 'min(IoU, B-Iou)-rug': 0.0, 'IoU-field': 9.34083252759997, 'BoundaryIoU-field': 0.0, 'min(IoU, B-Iou)-field': 0.0, 'IoU-armchair': 0.8083785791191898, 'BoundaryIoU-armchair': 0.0, 'min(IoU, B-Iou)-armchair': 0.0, 'IoU-seat': 7.7729067600497155, 'BoundaryIoU-seat': 0.0, 'min(IoU, B-Iou)-seat': 0.0, 'IoU-fence': 9.146023666225075, 'BoundaryIoU-fence': 0.0, 'min(IoU, B-Iou)-fence': 0.0, 'IoU-desk': 1.0986215510041453, 'BoundaryIoU-desk': 0.0, 'min(IoU, B-Iou)-desk': 0.0, 'IoU-rock, stone': 22.357210870581, 'BoundaryIoU-rock, stone': 0.0, 'min(IoU, B-Iou)-rock, stone': 0.0, 'IoU-wardrobe, closet, press': 19.509248078612412, 'BoundaryIoU-wardrobe, closet, press': 0.0, 'min(IoU, B-Iou)-wardrobe, closet, press': 0.0, 'IoU-lamp': 42.56489776840447, 'BoundaryIoU-lamp': 0.0, 'min(IoU, B-Iou)-lamp': 0.0, 'IoU-tub': 19.27378541443266, 'BoundaryIoU-tub': 0.0, 'min(IoU, B-Iou)-tub': 0.0, 'IoU-rail': 10.248319968315338, 'BoundaryIoU-rail': 0.0, 'min(IoU, B-Iou)-rail': 0.0, 'IoU-cushion': 16.330227117181163, 'BoundaryIoU-cushion': 0.0, 'min(IoU, B-Iou)-cushion': 0.0, 'IoU-base, pedestal, stand': 0.8182499982057889, 'BoundaryIoU-base, pedestal, stand': 0.0, 'min(IoU, B-Iou)-base, pedestal, stand': 0.0, 'IoU-box': 3.787867657261889, 'BoundaryIoU-box': 0.0, 'min(IoU, B-Iou)-box': 0.0, 'IoU-column, pillar': 28.112500951033738, 'BoundaryIoU-column, pillar': 0.0, 'min(IoU, B-Iou)-column, pillar': 0.0, 'IoU-signboard, sign': 20.782114373113192, 'BoundaryIoU-signboard, sign': 0.0, 'min(IoU, B-Iou)-signboard, sign': 0.0, 'IoU-chest of drawers, chest, bureau, dresser': 2.5312130438703795, 'BoundaryIoU-chest of drawers, chest, bureau, dresser': 0.0, 'min(IoU, B-Iou)-chest of drawers, chest, bureau, dresser': 0.0, 'IoU-counter': 0.0, 'BoundaryIoU-counter': 0.0, 'min(IoU, B-Iou)-counter': 0.0, 'IoU-sand': 10.552351379125794, 'BoundaryIoU-sand': 0.0, 'min(IoU, B-Iou)-sand': 0.0, 'IoU-sink': 39.3936244188355, 'BoundaryIoU-sink': 0.0, 'min(IoU, B-Iou)-sink': 0.0, 'IoU-skyscraper': 40.17661867844704, 'BoundaryIoU-skyscraper': 0.0, 'min(IoU, B-Iou)-skyscraper': 0.0, 'IoU-fireplace': 23.16377062430878, 'BoundaryIoU-fireplace': 0.0, 'min(IoU, B-Iou)-fireplace': 0.0, 'IoU-refrigerator, icebox': 21.737638550295156, 'BoundaryIoU-refrigerator, icebox': 0.0, 'min(IoU, B-Iou)-refrigerator, icebox': 0.0, 'IoU-grandstand, covered stand': 0.0, 'BoundaryIoU-grandstand, covered stand': 0.0, 'min(IoU, B-Iou)-grandstand, covered stand': 0.0, 'IoU-path': 18.55507494865701, 'BoundaryIoU-path': 0.0, 'min(IoU, B-Iou)-path': 0.0, 'IoU-stairs': 26.054342063060883, 'BoundaryIoU-stairs': 0.0, 'min(IoU, B-Iou)-stairs': 0.0, 'IoU-runway': 0.0, 'BoundaryIoU-runway': 0.0, 'min(IoU, B-Iou)-runway': 0.0, 'IoU-case, display case, showcase, vitrine': 0.3157795332852633, 'BoundaryIoU-case, display case, showcase, vitrine': 0.0, 'min(IoU, B-Iou)-case, display case, showcase, vitrine': 0.0, 'IoU-pool table, billiard table, snooker table': 0.0, 'BoundaryIoU-pool table, billiard table, snooker table': 0.0, 'min(IoU, B-Iou)-pool table, billiard table, snooker table': 0.0, 'IoU-pillow': 26.494198756142207, 'BoundaryIoU-pillow': 0.0, 'min(IoU, B-Iou)-pillow': 0.0, 'IoU-screen door, screen': 0.0, 'BoundaryIoU-screen door, screen': 0.0, 'min(IoU, B-Iou)-screen door, screen': 0.0, 'IoU-stairway, staircase': 9.857302536025756, 'BoundaryIoU-stairway, staircase': 0.0, 'min(IoU, B-Iou)-stairway, staircase': 0.0, 'IoU-river': 0.0, 'BoundaryIoU-river': 0.0, 'min(IoU, B-Iou)-river': 0.0, 'IoU-bridge, span': 11.005156220577065, 'BoundaryIoU-bridge, span': 0.0, 'min(IoU, B-Iou)-bridge, span': 0.0, 'IoU-bookcase': 0.0, 'BoundaryIoU-bookcase': 0.0, 'min(IoU, B-Iou)-bookcase': 0.0, 'IoU-blind, screen': 0.0, 'BoundaryIoU-blind, screen': 0.0, 'min(IoU, B-Iou)-blind, screen': 0.0, 'IoU-coffee table': 18.496590535387217, 'BoundaryIoU-coffee table': 0.0, 'min(IoU, B-Iou)-coffee table': 0.0, 'IoU-toilet, can, commode, crapper, pot, potty, stool, throne': 36.75289504163905, 'BoundaryIoU-toilet, can, commode, crapper, pot, potty, stool, throne': 0.0, 'min(IoU, B-Iou)-toilet, can, commode, crapper, pot, potty, stool, throne': 0.0, 'IoU-flower': 3.4140222056321994, 'BoundaryIoU-flower': 0.0, 'min(IoU, B-Iou)-flower': 0.0, 'IoU-book': 0.3408204033996119, 'BoundaryIoU-book': 0.0, 'min(IoU, B-Iou)-book': 0.0, 'IoU-hill': 0.0, 'BoundaryIoU-hill': 0.0, 'min(IoU, B-Iou)-hill': 0.0, 'IoU-bench': 7.137600064193675, 'BoundaryIoU-bench': 0.0, 'min(IoU, B-Iou)-bench': 0.0, 'IoU-countertop': 22.734352007254273, 'BoundaryIoU-countertop': 0.0, 'min(IoU, B-Iou)-countertop': 0.0, 'IoU-stove': 7.332478384400922, 'BoundaryIoU-stove': 0.0, 'min(IoU, B-Iou)-stove': 0.0, 'IoU-palm, palm tree': 29.194738603685344, 'BoundaryIoU-palm, palm tree': 0.0, 'min(IoU, B-Iou)-palm, palm tree': 0.0, 'IoU-kitchen island': 0.0, 'BoundaryIoU-kitchen island': 0.0, 'min(IoU, B-Iou)-kitchen island': 0.0, 'IoU-computer': 6.768061438760363, 'BoundaryIoU-computer': 0.0, 'min(IoU, B-Iou)-computer': 0.0, 'IoU-swivel chair': 0.0, 'BoundaryIoU-swivel chair': 0.0, 'min(IoU, B-Iou)-swivel chair': 0.0, 'IoU-boat': 0.6854249547920434, 'BoundaryIoU-boat': 0.0, 'min(IoU, B-Iou)-boat': 0.0, 'IoU-bar': 0.041272207807166006, 'BoundaryIoU-bar': 0.0, 'min(IoU, B-Iou)-bar': 0.0, 'IoU-arcade machine': 0.0, 'BoundaryIoU-arcade machine': 0.0, 'min(IoU, B-Iou)-arcade machine': 0.0, 'IoU-hovel, hut, hutch, shack, shanty': 0.0, 'BoundaryIoU-hovel, hut, hutch, shack, shanty': 0.0, 'min(IoU, B-Iou)-hovel, hut, hutch, shack, shanty': 0.0, 'IoU-bus': 0.0028218827601775905, 'BoundaryIoU-bus': 0.0, 'min(IoU, B-Iou)-bus': 0.0, 'IoU-towel': 19.787779872533395, 'BoundaryIoU-towel': 0.0, 'min(IoU, B-Iou)-towel': 0.0, 'IoU-light': 38.63827495134834, 'BoundaryIoU-light': 0.0, 'min(IoU, B-Iou)-light': 0.0, 'IoU-truck': 1.593494577945618, 'BoundaryIoU-truck': 0.0, 'min(IoU, B-Iou)-truck': 0.0, 'IoU-tower': 0.0, 'BoundaryIoU-tower': 0.0, 'min(IoU, B-Iou)-tower': 0.0, 'IoU-chandelier': 30.488375310463866, 'BoundaryIoU-chandelier': 0.0, 'min(IoU, B-Iou)-chandelier': 0.0, 'IoU-awning, sunshade, sunblind': 11.216639185186265, 'BoundaryIoU-awning, sunshade, sunblind': 0.0, 'min(IoU, B-Iou)-awning, sunshade, sunblind': 0.0, 'IoU-street lamp': 14.308030810752381, 'BoundaryIoU-street lamp': 0.0, 'min(IoU, B-Iou)-street lamp': 0.0, 'IoU-booth': 0.0, 'BoundaryIoU-booth': 0.0, 'min(IoU, B-Iou)-booth': 0.0, 'IoU-tv': 0.5545872286483917, 'BoundaryIoU-tv': 0.0, 'min(IoU, B-Iou)-tv': 0.0, 'IoU-plane': 13.370395567361754, 'BoundaryIoU-plane': 0.0, 'min(IoU, B-Iou)-plane': 0.0, 'IoU-dirt track': 0.0, 'BoundaryIoU-dirt track': 0.0, 'min(IoU, B-Iou)-dirt track': 0.0, 'IoU-clothes': 0.0, 'BoundaryIoU-clothes': 0.0, 'min(IoU, B-Iou)-clothes': 0.0, 'IoU-pole': 12.657993189086774, 'BoundaryIoU-pole': 0.0, 'min(IoU, B-Iou)-pole': 0.0, 'IoU-land, ground, soil': 0.0, 'BoundaryIoU-land, ground, soil': 0.0, 'min(IoU, B-Iou)-land, ground, soil': 0.0, 'IoU-bannister, banister, balustrade, balusters, handrail': 0.8794315230434347, 'BoundaryIoU-bannister, banister, balustrade, balusters, handrail': 0.0, 'min(IoU, B-Iou)-bannister, banister, balustrade, balusters, handrail': 0.0, 'IoU-escalator, moving staircase, moving stairway': 0.0, 'BoundaryIoU-escalator, moving staircase, moving stairway': 0.0, 'min(IoU, B-Iou)-escalator, moving staircase, moving stairway': 0.0, 'IoU-ottoman, pouf, pouffe, puff, hassock': 0.2500777027147721, 'BoundaryIoU-ottoman, pouf, pouffe, puff, hassock': 0.0, 'min(IoU, B-Iou)-ottoman, pouf, pouffe, puff, hassock': 0.0, 'IoU-bottle': 2.357058557187711, 'BoundaryIoU-bottle': 0.0, 'min(IoU, B-Iou)-bottle': 0.0, 'IoU-buffet, counter, sideboard': 0.03943387297991023, 'BoundaryIoU-buffet, counter, sideboard': 0.0, 'min(IoU, B-Iou)-buffet, counter, sideboard': 0.0, 'IoU-poster, posting, placard, notice, bill, card': 0.1937615408182699, 'BoundaryIoU-poster, posting, placard, notice, bill, card': 0.0, 'min(IoU, B-Iou)-poster, posting, placard, notice, bill, card': 0.0, 'IoU-stage': 0.0, 'BoundaryIoU-stage': 0.0, 'min(IoU, B-Iou)-stage': 0.0, 'IoU-van': 0.10133845728649829, 'BoundaryIoU-van': 0.0, 'min(IoU, B-Iou)-van': 0.0, 'IoU-ship': 0.0, 'BoundaryIoU-ship': 0.0, 'min(IoU, B-Iou)-ship': 0.0, 'IoU-fountain': 0.0, 'BoundaryIoU-fountain': 0.0, 'min(IoU, B-Iou)-fountain': 0.0, 'IoU-conveyer belt, conveyor belt, conveyer, conveyor, transporter': 0.0, 'BoundaryIoU-conveyer belt, conveyor belt, conveyer, conveyor, transporter': 0.0, 'min(IoU, B-Iou)-conveyer belt, conveyor belt, conveyer, conveyor, transporter': 0.0, 'IoU-canopy': 0.0, 'BoundaryIoU-canopy': 0.0, 'min(IoU, B-Iou)-canopy': 0.0, 'IoU-washer, automatic washer, washing machine': 0.0, 'BoundaryIoU-washer, automatic washer, washing machine': 0.0, 'min(IoU, B-Iou)-washer, automatic washer, washing machine': 0.0, 'IoU-plaything, toy': 1.7059551063669827, 'BoundaryIoU-plaything, toy': 0.0, 'min(IoU, B-Iou)-plaything, toy': 0.0, 'IoU-pool': 0.0, 'BoundaryIoU-pool': 0.0, 'min(IoU, B-Iou)-pool': 0.0, 'IoU-stool': 0.1634308369777401, 'BoundaryIoU-stool': 0.0, 'min(IoU, B-Iou)-stool': 0.0, 'IoU-barrel, cask': 0.0, 'BoundaryIoU-barrel, cask': 0.0, 'min(IoU, B-Iou)-barrel, cask': 0.0, 'IoU-basket, handbasket': 2.853180257706604, 'BoundaryIoU-basket, handbasket': 0.0, 'min(IoU, B-Iou)-basket, handbasket': 0.0, 'IoU-falls': 0.005149981506884589, 'BoundaryIoU-falls': 0.0, 'min(IoU, B-Iou)-falls': 0.0, 'IoU-tent': 0.0, 'BoundaryIoU-tent': 0.0, 'min(IoU, B-Iou)-tent': 0.0, 'IoU-bag': 4.857028604665656, 'BoundaryIoU-bag': 0.0, 'min(IoU, B-Iou)-bag': 0.0, 'IoU-minibike, motorbike': 1.1197738131301052, 'BoundaryIoU-minibike, motorbike': 0.0, 'min(IoU, B-Iou)-minibike, motorbike': 0.0, 'IoU-cradle': 0.0, 'BoundaryIoU-cradle': 0.0, 'min(IoU, B-Iou)-cradle': 0.0, 'IoU-oven': 4.475899924330687, 'BoundaryIoU-oven': 0.0, 'min(IoU, B-Iou)-oven': 0.0, 'IoU-ball': 0.0, 'BoundaryIoU-ball': 0.0, 'min(IoU, B-Iou)-ball': 0.0, 'IoU-food, solid food': 0.0, 'BoundaryIoU-food, solid food': 0.0, 'min(IoU, B-Iou)-food, solid food': 0.0, 'IoU-step, stair': 0.6668613866909957, 'BoundaryIoU-step, stair': 0.0, 'min(IoU, B-Iou)-step, stair': 0.0, 'IoU-tank, storage tank': 0.0, 'BoundaryIoU-tank, storage tank': 0.0, 'min(IoU, B-Iou)-tank, storage tank': 0.0, 'IoU-trade name': 1.6541573774742604, 'BoundaryIoU-trade name': 0.0, 'min(IoU, B-Iou)-trade name': 0.0, 'IoU-microwave': 2.291969955014961, 'BoundaryIoU-microwave': 0.0, 'min(IoU, B-Iou)-microwave': 0.0, 'IoU-pot': 4.925129353177005, 'BoundaryIoU-pot': 0.0, 'min(IoU, B-Iou)-pot': 0.0, 'IoU-animal': 0.0, 'BoundaryIoU-animal': 0.0, 'min(IoU, B-Iou)-animal': 0.0, 'IoU-bicycle': 7.714840719662883, 'BoundaryIoU-bicycle': 0.0, 'min(IoU, B-Iou)-bicycle': 0.0, 'IoU-lake': 0.0, 'BoundaryIoU-lake': 0.0, 'min(IoU, B-Iou)-lake': 0.0, 'IoU-dishwasher': 0.11033213435782245, 'BoundaryIoU-dishwasher': 0.0, 'min(IoU, B-Iou)-dishwasher': 0.0, 'IoU-screen': 0.0, 'BoundaryIoU-screen': 0.0, 'min(IoU, B-Iou)-screen': 0.0, 'IoU-blanket, cover': 0.0, 'BoundaryIoU-blanket, cover': 0.0, 'min(IoU, B-Iou)-blanket, cover': 0.0, 'IoU-sculpture': 2.2218312400750686, 'BoundaryIoU-sculpture': 0.0, 'min(IoU, B-Iou)-sculpture': 0.0, 'IoU-hood, exhaust hood': 16.147585826260826, 'BoundaryIoU-hood, exhaust hood': 0.0, 'min(IoU, B-Iou)-hood, exhaust hood': 0.0, 'IoU-sconce': 1.8831819538612424, 'BoundaryIoU-sconce': 0.0, 'min(IoU, B-Iou)-sconce': 0.0, 'IoU-vase': 7.043942675214192, 'BoundaryIoU-vase': 0.0, 'min(IoU, B-Iou)-vase': 0.0, 'IoU-traffic light': 10.743284736279483, 'BoundaryIoU-traffic light': 0.0, 'min(IoU, B-Iou)-traffic light': 0.0, 'IoU-tray': 0.0, 'BoundaryIoU-tray': 0.0, 'min(IoU, B-Iou)-tray': 0.0, 'IoU-trash can': 10.31650398022423, 'BoundaryIoU-trash can': 0.0, 'min(IoU, B-Iou)-trash can': 0.0, 'IoU-fan': 1.5647166064001257, 'BoundaryIoU-fan': 0.0, 'min(IoU, B-Iou)-fan': 0.0, 'IoU-pier': 0.0, 'BoundaryIoU-pier': 0.0, 'min(IoU, B-Iou)-pier': 0.0, 'IoU-crt screen': 0.005087616015815218, 'BoundaryIoU-crt screen': 0.0, 'min(IoU, B-Iou)-crt screen': 0.0, 'IoU-plate': 2.9274634232328576, 'BoundaryIoU-plate': 0.0, 'min(IoU, B-Iou)-plate': 0.0, 'IoU-monitor': 0.0, 'BoundaryIoU-monitor': 0.0, 'min(IoU, B-Iou)-monitor': 0.0, 'IoU-bulletin board': 0.0, 'BoundaryIoU-bulletin board': 0.0, 'min(IoU, B-Iou)-bulletin board': 0.0, 'IoU-shower': 0.0, 'BoundaryIoU-shower': 0.0, 'min(IoU, B-Iou)-shower': 0.0, 'IoU-radiator': 0.0, 'BoundaryIoU-radiator': 0.0, 'min(IoU, B-Iou)-radiator': 0.0, 'IoU-glass, drinking glass': 0.0, 'BoundaryIoU-glass, drinking glass': 0.0, 'min(IoU, B-Iou)-glass, drinking glass': 0.0, 'IoU-clock': 0.1656217834752503, 'BoundaryIoU-clock': 0.0, 'min(IoU, B-Iou)-clock': 0.0, 'IoU-flag': 11.144381249021631, 'BoundaryIoU-flag': 0.0, 'min(IoU, B-Iou)-flag': 0.0, 'mACC': 21.719156848900212, 'pACC': 70.1156011020463, 'ACC-wall': 76.41886734632082, 'ACC-building': 88.96868518387492, 'ACC-sky': 96.09662467557138, 'ACC-floor': 85.7933684129386, 'ACC-tree': 87.729058244763, 'ACC-ceiling': 96.33526445683826, 'ACC-road, route': 81.15757512601883, 'ACC-bed': 96.84926010928181, 'ACC-window ': 74.92477064343691, 'ACC-grass': 89.37559131192127, 'ACC-cabinet': 63.950771609882025, 'ACC-sidewalk, pavement': 65.97183524339385, 'ACC-person': 83.02213111302365, 'ACC-earth, ground': 7.259379982431089, 'ACC-door': 43.78703753999037, 'ACC-table': 54.74308038352264, 'ACC-mountain, mount': 49.00484322440213, 'ACC-plant': 68.98485681375243, 'ACC-curtain': 87.21917592517323, 'ACC-chair': 43.37167332936191, 'ACC-car': 89.20116587848354, 'ACC-water': 36.111387916166635, 'ACC-painting, picture': 77.02665685353692, 'ACC-sofa': 11.221919065884968, 'ACC-shelf': 35.63664235520402, 'ACC-house': 0.024225368939094768, 'ACC-sea': 81.45250823302521, 'ACC-mirror': 15.90763975493964, 'ACC-rug': 69.53786332123767, 'ACC-field': 13.818422924329605, 'ACC-armchair': 0.96469816250423, 'ACC-seat': 74.3351348230722, 'ACC-fence': 12.82612402660149, 'ACC-desk': 1.1188416111069492, 'ACC-rock, stone': 67.53578239738536, 'ACC-wardrobe, closet, press': 53.819085664131194, 'ACC-lamp': 72.0598992213867, 'ACC-tub': 24.204176325916855, 'ACC-rail': 20.523657813608796, 'ACC-cushion': 18.333282226729338, 'ACC-base, pedestal, stand': 1.0219619293691085, 'ACC-box': 4.252924653234417, 'ACC-column, pillar': 42.26032735775526, 'ACC-signboard, sign': 25.050944152814232, 'ACC-chest of drawers, chest, bureau, dresser': 2.6484195472952945, 'ACC-counter': 0.0, 'ACC-sand': 14.557304658889466, 'ACC-sink': 48.672289066884474, 'ACC-skyscraper': 61.26980969431736, 'ACC-fireplace': 36.608067147433474, 'ACC-refrigerator, icebox': 33.747998353412086, 'ACC-grandstand, covered stand': 0.0, 'ACC-path': 33.61038891387631, 'ACC-stairs': 33.142588317579005, 'ACC-runway': 0.0, 'ACC-case, display case, showcase, vitrine': 0.46739715433772777, 'ACC-pool table, billiard table, snooker table': 0.0, 'ACC-pillow': 39.97083323293288, 'ACC-screen door, screen': 0.0, 'ACC-stairway, staircase': 10.247783792810722, 'ACC-river': 0.0, 'ACC-bridge, span': 27.039878283984397, 'ACC-bookcase': 0.0, 'ACC-blind, screen': 0.0, 'ACC-coffee table': 23.23792403373117, 'ACC-toilet, can, commode, crapper, pot, potty, stool, throne': 45.23472285021442, 'ACC-flower': 3.97907756122078, 'ACC-book': 0.341337310821325, 'ACC-hill': 0.0, 'ACC-bench': 16.366049345478675, 'ACC-countertop': 30.995113466055614, 'ACC-stove': 9.638017646924709, 'ACC-palm, palm tree': 34.62080547785422, 'ACC-kitchen island': 0.0, 'ACC-computer': 7.835665961529645, 'ACC-swivel chair': 0.0, 'ACC-boat': 0.6901916454471824, 'ACC-bar': 0.04160860814367549, 'ACC-arcade machine': 0.0, 'ACC-hovel, hut, hutch, shack, shanty': 0.0, 'ACC-bus': 0.0028341792571143806, 'ACC-towel': 28.565648609938528, 'ACC-light': 51.540984366489354, 'ACC-truck': 1.7245272368049676, 'ACC-tower': 0.0, 'ACC-chandelier': 48.747936049803414, 'ACC-awning, sunshade, sunblind': 45.17821131163379, 'ACC-street lamp': 20.91087918136046, 'ACC-booth': 0.0, 'ACC-tv': 0.556771493299543, 'ACC-plane': 61.62830935912608, 'ACC-dirt track': 0.0, 'ACC-clothes': 0.0, 'ACC-pole': 23.939365496527127, 'ACC-land, ground, soil': 0.0, 'ACC-bannister, banister, balustrade, balusters, handrail': 1.11010402887332, 'ACC-escalator, moving staircase, moving stairway': 0.0, 'ACC-ottoman, pouf, pouffe, puff, hassock': 0.25444737664754674, 'ACC-bottle': 2.508967653240207, 'ACC-buffet, counter, sideboard': 0.04188805987200025, 'ACC-poster, posting, placard, notice, bill, card': 0.28114186851211076, 'ACC-stage': 0.0, 'ACC-van': 0.11073772417198383, 'ACC-ship': 0.0, 'ACC-fountain': 0.0, 'ACC-conveyer belt, conveyor belt, conveyer, conveyor, transporter': 0.0, 'ACC-canopy': 0.0, 'ACC-washer, automatic washer, washing machine': 0.0, 'ACC-plaything, toy': 16.6736305218569, 'ACC-pool': 0.0, 'ACC-stool': 0.20940784116027455, 'ACC-barrel, cask': 0.0, 'ACC-basket, handbasket': 4.143057534488662, 'ACC-falls': 0.005152248956669586, 'ACC-tent': 0.0, 'ACC-bag': 10.98143049203619, 'ACC-minibike, motorbike': 1.1544509645993941, 'ACC-cradle': 0.0, 'ACC-oven': 4.703632645848585, 'ACC-ball': 0.0, 'ACC-food, solid food': 0.0, 'ACC-step, stair': 0.771125304067003, 'ACC-tank, storage tank': 0.0, 'ACC-trade name': 1.716603879542317, 'ACC-microwave': 2.4137869797966447, 'ACC-pot': 5.220275783586155, 'ACC-animal': 0.0, 'ACC-bicycle': 10.838782805824396, 'ACC-lake': 0.0, 'ACC-dishwasher': 0.13955554999280317, 'ACC-screen': 0.0, 'ACC-blanket, cover': 0.0, 'ACC-sculpture': 4.031663910673062, 'ACC-hood, exhaust hood': 17.89045695680393, 'ACC-sconce': 1.9914210505549261, 'ACC-vase': 12.375040737288073, 'ACC-traffic light': 28.644633110976347, 'ACC-tray': 0.0, 'ACC-trash can': 21.108426823752634, 'ACC-fan': 1.5733119673683442, 'ACC-pier': 0.0, 'ACC-crt screen': 0.011971849052086094, 'ACC-plate': 3.2208016368246764, 'ACC-monitor': 0.0, 'ACC-bulletin board': 0.0, 'ACC-shower': 0.0, 'ACC-radiator': 0.0, 'ACC-glass, drinking glass': 0.0, 'ACC-clock': 0.16783614640622038, 'ACC-flag': 16.611921813580313})])\n",
            "\u001b[32m[07/26 18:20:28 d2.engine.defaults]: \u001b[0mEvaluation results for ade20k_sem_seg_val in csv format:\n",
            "\u001b[32m[07/26 18:20:28 d2.evaluation.testing]: \u001b[0mcopypaste: Task: sem_seg\n",
            "\u001b[32m[07/26 18:20:28 d2.evaluation.testing]: \u001b[0mcopypaste: mIoU,fwIoU,mACC,pACC\n",
            "\u001b[32m[07/26 18:20:28 d2.evaluation.testing]: \u001b[0mcopypaste: 14.6425,57.4551,21.7192,70.1156\n",
            "\u001b[32m[07/26 18:20:28 d2.utils.events]: \u001b[0m eta: 8 days, 6:17:50  iter: 4999  total_loss: 155.1  loss_ce: 1.642  loss_mask: 1.142  loss_dice: 2.484  loss_bbox: 2.585  loss_giou: 1.654  loss_ce_dn: 2.242  loss_mask_dn: 1.057  loss_dice_dn: 2.178  loss_bbox_dn: 0.7042  loss_giou_dn: 0.6931  loss_ce_0: 5.213  loss_mask_0: 1.069  loss_dice_0: 2.635  loss_bbox_0: 4.21  loss_giou_0: 2.038  loss_ce_1: 1.704  loss_mask_1: 1.212  loss_dice_1: 2.489  loss_bbox_1: 3.201  loss_giou_1: 1.738  loss_ce_dn_1: 2.39  loss_mask_dn_1: 1.171  loss_dice_dn_1: 2.273  loss_bbox_dn_1: 0.8825  loss_giou_dn_1: 0.7452  loss_ce_2: 1.559  loss_mask_2: 1.137  loss_dice_2: 2.487  loss_bbox_2: 2.857  loss_giou_2: 1.712  loss_ce_dn_2: 2.121  loss_mask_dn_2: 1.127  loss_dice_dn_2: 2.199  loss_bbox_dn_2: 0.8397  loss_giou_dn_2: 0.735  loss_ce_3: 1.593  loss_mask_3: 1.127  loss_dice_3: 2.499  loss_bbox_3: 2.748  loss_giou_3: 1.696  loss_ce_dn_3: 2.004  loss_mask_dn_3: 1.166  loss_dice_dn_3: 2.165  loss_bbox_dn_3: 0.7803  loss_giou_dn_3: 0.7098  loss_ce_4: 1.642  loss_mask_4: 1.123  loss_dice_4: 2.479  loss_bbox_4: 2.687  loss_giou_4: 1.673  loss_ce_dn_4: 1.98  loss_mask_dn_4: 1.126  loss_dice_dn_4: 2.165  loss_bbox_dn_4: 0.7668  loss_giou_dn_4: 0.6996  loss_ce_5: 1.633  loss_mask_5: 1.099  loss_dice_5: 2.486  loss_bbox_5: 2.548  loss_giou_5: 1.657  loss_ce_dn_5: 1.929  loss_mask_dn_5: 1.095  loss_dice_dn_5: 2.165  loss_bbox_dn_5: 0.7462  loss_giou_dn_5: 0.6909  loss_ce_6: 1.583  loss_mask_6: 1.124  loss_dice_6: 2.474  loss_bbox_6: 2.509  loss_giou_6: 1.646  loss_ce_dn_6: 2.021  loss_mask_dn_6: 1.087  loss_dice_dn_6: 2.173  loss_bbox_dn_6: 0.7334  loss_giou_dn_6: 0.6915  loss_ce_7: 1.621  loss_mask_7: 1.179  loss_dice_7: 2.456  loss_bbox_7: 2.582  loss_giou_7: 1.658  loss_ce_dn_7: 2.083  loss_mask_dn_7: 1.073  loss_dice_dn_7: 2.172  loss_bbox_dn_7: 0.7114  loss_giou_dn_7: 0.6907  loss_ce_8: 1.584  loss_mask_8: 1.155  loss_dice_8: 2.483  loss_bbox_8: 2.593  loss_giou_8: 1.616  loss_ce_dn_8: 2.044  loss_mask_dn_8: 1.077  loss_dice_dn_8: 2.157  loss_bbox_dn_8: 0.7074  loss_giou_dn_8: 0.6907    time: 2.2734  last_time: 2.2409  data_time: 0.0121  last_data_time: 0.0053   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:21:14 d2.utils.events]: \u001b[0m eta: 8 days, 6:17:45  iter: 5019  total_loss: 162.7  loss_ce: 1.209  loss_mask: 1.512  loss_dice: 2.398  loss_bbox: 2.592  loss_giou: 1.493  loss_ce_dn: 1.647  loss_mask_dn: 1.325  loss_dice_dn: 2.005  loss_bbox_dn: 0.7222  loss_giou_dn: 0.6604  loss_ce_0: 5.039  loss_mask_0: 1.699  loss_dice_0: 2.518  loss_bbox_0: 4.439  loss_giou_0: 2.06  loss_ce_1: 1.333  loss_mask_1: 1.609  loss_dice_1: 2.434  loss_bbox_1: 3.184  loss_giou_1: 1.669  loss_ce_dn_1: 2.088  loss_mask_dn_1: 1.39  loss_dice_dn_1: 2.213  loss_bbox_dn_1: 0.8742  loss_giou_dn_1: 0.7519  loss_ce_2: 1.348  loss_mask_2: 1.508  loss_dice_2: 2.44  loss_bbox_2: 2.892  loss_giou_2: 1.66  loss_ce_dn_2: 1.592  loss_mask_dn_2: 1.317  loss_dice_dn_2: 2.083  loss_bbox_dn_2: 0.8409  loss_giou_dn_2: 0.7198  loss_ce_3: 1.24  loss_mask_3: 1.527  loss_dice_3: 2.375  loss_bbox_3: 2.722  loss_giou_3: 1.547  loss_ce_dn_3: 1.471  loss_mask_dn_3: 1.364  loss_dice_dn_3: 2.043  loss_bbox_dn_3: 0.7832  loss_giou_dn_3: 0.6996  loss_ce_4: 1.207  loss_mask_4: 1.573  loss_dice_4: 2.339  loss_bbox_4: 2.685  loss_giou_4: 1.525  loss_ce_dn_4: 1.482  loss_mask_dn_4: 1.366  loss_dice_dn_4: 2.07  loss_bbox_dn_4: 0.7532  loss_giou_dn_4: 0.6807  loss_ce_5: 1.254  loss_mask_5: 1.578  loss_dice_5: 2.396  loss_bbox_5: 2.665  loss_giou_5: 1.522  loss_ce_dn_5: 1.53  loss_mask_dn_5: 1.318  loss_dice_dn_5: 2.012  loss_bbox_dn_5: 0.7425  loss_giou_dn_5: 0.667  loss_ce_6: 1.205  loss_mask_6: 1.583  loss_dice_6: 2.384  loss_bbox_6: 2.625  loss_giou_6: 1.504  loss_ce_dn_6: 1.594  loss_mask_dn_6: 1.281  loss_dice_dn_6: 2  loss_bbox_dn_6: 0.7442  loss_giou_dn_6: 0.6677  loss_ce_7: 1.217  loss_mask_7: 1.563  loss_dice_7: 2.395  loss_bbox_7: 2.606  loss_giou_7: 1.499  loss_ce_dn_7: 1.63  loss_mask_dn_7: 1.278  loss_dice_dn_7: 2.043  loss_bbox_dn_7: 0.7221  loss_giou_dn_7: 0.6572  loss_ce_8: 1.218  loss_mask_8: 1.512  loss_dice_8: 2.4  loss_bbox_8: 2.601  loss_giou_8: 1.49  loss_ce_dn_8: 1.577  loss_mask_dn_8: 1.298  loss_dice_dn_8: 2.012  loss_bbox_dn_8: 0.7208  loss_giou_dn_8: 0.6578    time: 2.2734  last_time: 2.2912  data_time: 0.0081  last_data_time: 0.0075   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:21:59 d2.utils.events]: \u001b[0m eta: 8 days, 6:16:30  iter: 5039  total_loss: 171.7  loss_ce: 1.559  loss_mask: 1.596  loss_dice: 2.58  loss_bbox: 2.661  loss_giou: 1.489  loss_ce_dn: 1.874  loss_mask_dn: 1.523  loss_dice_dn: 2.118  loss_bbox_dn: 0.8256  loss_giou_dn: 0.6841  loss_ce_0: 5.681  loss_mask_0: 1.699  loss_dice_0: 2.513  loss_bbox_0: 4.127  loss_giou_0: 1.912  loss_ce_1: 1.779  loss_mask_1: 1.363  loss_dice_1: 2.563  loss_bbox_1: 3.008  loss_giou_1: 1.63  loss_ce_dn_1: 2.644  loss_mask_dn_1: 1.649  loss_dice_dn_1: 2.24  loss_bbox_dn_1: 0.9625  loss_giou_dn_1: 0.7544  loss_ce_2: 1.592  loss_mask_2: 1.594  loss_dice_2: 2.592  loss_bbox_2: 2.996  loss_giou_2: 1.57  loss_ce_dn_2: 2.129  loss_mask_dn_2: 1.547  loss_dice_dn_2: 2.185  loss_bbox_dn_2: 0.9234  loss_giou_dn_2: 0.7314  loss_ce_3: 1.553  loss_mask_3: 1.5  loss_dice_3: 2.556  loss_bbox_3: 2.808  loss_giou_3: 1.519  loss_ce_dn_3: 1.908  loss_mask_dn_3: 1.589  loss_dice_dn_3: 2.151  loss_bbox_dn_3: 0.8922  loss_giou_dn_3: 0.7081  loss_ce_4: 1.552  loss_mask_4: 1.494  loss_dice_4: 2.505  loss_bbox_4: 2.765  loss_giou_4: 1.514  loss_ce_dn_4: 1.888  loss_mask_dn_4: 1.51  loss_dice_dn_4: 2.131  loss_bbox_dn_4: 0.8741  loss_giou_dn_4: 0.6986  loss_ce_5: 1.483  loss_mask_5: 1.591  loss_dice_5: 2.53  loss_bbox_5: 2.74  loss_giou_5: 1.509  loss_ce_dn_5: 1.804  loss_mask_dn_5: 1.488  loss_dice_dn_5: 2.151  loss_bbox_dn_5: 0.8451  loss_giou_dn_5: 0.6946  loss_ce_6: 1.455  loss_mask_6: 1.552  loss_dice_6: 2.521  loss_bbox_6: 2.725  loss_giou_6: 1.499  loss_ce_dn_6: 1.778  loss_mask_dn_6: 1.528  loss_dice_dn_6: 2.119  loss_bbox_dn_6: 0.8419  loss_giou_dn_6: 0.6912  loss_ce_7: 1.469  loss_mask_7: 1.634  loss_dice_7: 2.558  loss_bbox_7: 2.652  loss_giou_7: 1.491  loss_ce_dn_7: 1.738  loss_mask_dn_7: 1.509  loss_dice_dn_7: 2.068  loss_bbox_dn_7: 0.8245  loss_giou_dn_7: 0.682  loss_ce_8: 1.501  loss_mask_8: 1.626  loss_dice_8: 2.549  loss_bbox_8: 2.666  loss_giou_8: 1.49  loss_ce_dn_8: 1.787  loss_mask_dn_8: 1.516  loss_dice_dn_8: 2.126  loss_bbox_dn_8: 0.8257  loss_giou_dn_8: 0.6822    time: 2.2734  last_time: 2.2218  data_time: 0.0117  last_data_time: 0.0079   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:22:45 d2.utils.events]: \u001b[0m eta: 8 days, 6:16:14  iter: 5059  total_loss: 160.1  loss_ce: 1.534  loss_mask: 1.404  loss_dice: 2.363  loss_bbox: 2.419  loss_giou: 1.447  loss_ce_dn: 1.746  loss_mask_dn: 1.201  loss_dice_dn: 2.031  loss_bbox_dn: 0.787  loss_giou_dn: 0.6632  loss_ce_0: 5.075  loss_mask_0: 1.341  loss_dice_0: 2.625  loss_bbox_0: 4.161  loss_giou_0: 1.981  loss_ce_1: 1.804  loss_mask_1: 1.396  loss_dice_1: 2.435  loss_bbox_1: 2.85  loss_giou_1: 1.617  loss_ce_dn_1: 2.384  loss_mask_dn_1: 1.335  loss_dice_dn_1: 2.288  loss_bbox_dn_1: 0.9374  loss_giou_dn_1: 0.7389  loss_ce_2: 1.585  loss_mask_2: 1.397  loss_dice_2: 2.377  loss_bbox_2: 2.735  loss_giou_2: 1.54  loss_ce_dn_2: 1.969  loss_mask_dn_2: 1.19  loss_dice_dn_2: 2.169  loss_bbox_dn_2: 0.8657  loss_giou_dn_2: 0.7148  loss_ce_3: 1.494  loss_mask_3: 1.367  loss_dice_3: 2.375  loss_bbox_3: 2.627  loss_giou_3: 1.508  loss_ce_dn_3: 1.883  loss_mask_dn_3: 1.158  loss_dice_dn_3: 2.132  loss_bbox_dn_3: 0.842  loss_giou_dn_3: 0.6854  loss_ce_4: 1.439  loss_mask_4: 1.376  loss_dice_4: 2.363  loss_bbox_4: 2.555  loss_giou_4: 1.519  loss_ce_dn_4: 1.864  loss_mask_dn_4: 1.173  loss_dice_dn_4: 2.118  loss_bbox_dn_4: 0.805  loss_giou_dn_4: 0.6771  loss_ce_5: 1.503  loss_mask_5: 1.403  loss_dice_5: 2.359  loss_bbox_5: 2.451  loss_giou_5: 1.47  loss_ce_dn_5: 1.836  loss_mask_dn_5: 1.165  loss_dice_dn_5: 2.113  loss_bbox_dn_5: 0.7908  loss_giou_dn_5: 0.672  loss_ce_6: 1.482  loss_mask_6: 1.425  loss_dice_6: 2.368  loss_bbox_6: 2.418  loss_giou_6: 1.457  loss_ce_dn_6: 1.755  loss_mask_dn_6: 1.175  loss_dice_dn_6: 2.115  loss_bbox_dn_6: 0.7921  loss_giou_dn_6: 0.6689  loss_ce_7: 1.504  loss_mask_7: 1.368  loss_dice_7: 2.364  loss_bbox_7: 2.403  loss_giou_7: 1.445  loss_ce_dn_7: 1.754  loss_mask_dn_7: 1.157  loss_dice_dn_7: 2.091  loss_bbox_dn_7: 0.7904  loss_giou_dn_7: 0.6612  loss_ce_8: 1.467  loss_mask_8: 1.36  loss_dice_8: 2.34  loss_bbox_8: 2.401  loss_giou_8: 1.448  loss_ce_dn_8: 1.716  loss_mask_dn_8: 1.199  loss_dice_dn_8: 2.07  loss_bbox_dn_8: 0.786  loss_giou_dn_8: 0.6629    time: 2.2734  last_time: 2.2757  data_time: 0.0097  last_data_time: 0.0050   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:23:30 d2.utils.events]: \u001b[0m eta: 8 days, 6:13:56  iter: 5079  total_loss: 159.3  loss_ce: 1.431  loss_mask: 1.346  loss_dice: 2.348  loss_bbox: 2.456  loss_giou: 1.583  loss_ce_dn: 1.555  loss_mask_dn: 1.145  loss_dice_dn: 1.994  loss_bbox_dn: 0.8291  loss_giou_dn: 0.7145  loss_ce_0: 5.217  loss_mask_0: 1.269  loss_dice_0: 2.527  loss_bbox_0: 4.145  loss_giou_0: 1.977  loss_ce_1: 1.528  loss_mask_1: 1.237  loss_dice_1: 2.372  loss_bbox_1: 2.643  loss_giou_1: 1.669  loss_ce_dn_1: 1.996  loss_mask_dn_1: 1.16  loss_dice_dn_1: 2.303  loss_bbox_dn_1: 0.8833  loss_giou_dn_1: 0.7625  loss_ce_2: 1.474  loss_mask_2: 1.315  loss_dice_2: 2.347  loss_bbox_2: 2.565  loss_giou_2: 1.61  loss_ce_dn_2: 1.494  loss_mask_dn_2: 1.117  loss_dice_dn_2: 2.069  loss_bbox_dn_2: 0.8349  loss_giou_dn_2: 0.7378  loss_ce_3: 1.335  loss_mask_3: 1.328  loss_dice_3: 2.358  loss_bbox_3: 2.514  loss_giou_3: 1.599  loss_ce_dn_3: 1.387  loss_mask_dn_3: 1.114  loss_dice_dn_3: 2.061  loss_bbox_dn_3: 0.8237  loss_giou_dn_3: 0.7226  loss_ce_4: 1.357  loss_mask_4: 1.393  loss_dice_4: 2.337  loss_bbox_4: 2.497  loss_giou_4: 1.595  loss_ce_dn_4: 1.338  loss_mask_dn_4: 1.151  loss_dice_dn_4: 1.971  loss_bbox_dn_4: 0.8235  loss_giou_dn_4: 0.7071  loss_ce_5: 1.416  loss_mask_5: 1.353  loss_dice_5: 2.332  loss_bbox_5: 2.497  loss_giou_5: 1.585  loss_ce_dn_5: 1.386  loss_mask_dn_5: 1.148  loss_dice_dn_5: 2.011  loss_bbox_dn_5: 0.8133  loss_giou_dn_5: 0.6947  loss_ce_6: 1.375  loss_mask_6: 1.362  loss_dice_6: 2.294  loss_bbox_6: 2.472  loss_giou_6: 1.591  loss_ce_dn_6: 1.425  loss_mask_dn_6: 1.158  loss_dice_dn_6: 1.993  loss_bbox_dn_6: 0.8103  loss_giou_dn_6: 0.6949  loss_ce_7: 1.377  loss_mask_7: 1.326  loss_dice_7: 2.366  loss_bbox_7: 2.469  loss_giou_7: 1.585  loss_ce_dn_7: 1.491  loss_mask_dn_7: 1.166  loss_dice_dn_7: 1.989  loss_bbox_dn_7: 0.8151  loss_giou_dn_7: 0.7065  loss_ce_8: 1.38  loss_mask_8: 1.334  loss_dice_8: 2.349  loss_bbox_8: 2.463  loss_giou_8: 1.59  loss_ce_dn_8: 1.504  loss_mask_dn_8: 1.152  loss_dice_dn_8: 1.993  loss_bbox_dn_8: 0.8232  loss_giou_dn_8: 0.7061    time: 2.2733  last_time: 2.2954  data_time: 0.0091  last_data_time: 0.0069   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:24:16 d2.utils.events]: \u001b[0m eta: 8 days, 6:14:44  iter: 5099  total_loss: 170.2  loss_ce: 1.598  loss_mask: 1.395  loss_dice: 2.362  loss_bbox: 2.642  loss_giou: 1.601  loss_ce_dn: 2.012  loss_mask_dn: 1.291  loss_dice_dn: 2.039  loss_bbox_dn: 0.9027  loss_giou_dn: 0.7172  loss_ce_0: 4.852  loss_mask_0: 1.479  loss_dice_0: 2.723  loss_bbox_0: 4.025  loss_giou_0: 2.033  loss_ce_1: 1.721  loss_mask_1: 1.317  loss_dice_1: 2.264  loss_bbox_1: 2.95  loss_giou_1: 1.781  loss_ce_dn_1: 2.376  loss_mask_dn_1: 1.399  loss_dice_dn_1: 2.296  loss_bbox_dn_1: 0.9763  loss_giou_dn_1: 0.7596  loss_ce_2: 1.52  loss_mask_2: 1.31  loss_dice_2: 2.279  loss_bbox_2: 2.776  loss_giou_2: 1.654  loss_ce_dn_2: 1.985  loss_mask_dn_2: 1.26  loss_dice_dn_2: 2.111  loss_bbox_dn_2: 0.9428  loss_giou_dn_2: 0.7418  loss_ce_3: 1.517  loss_mask_3: 1.343  loss_dice_3: 2.232  loss_bbox_3: 2.722  loss_giou_3: 1.648  loss_ce_dn_3: 1.84  loss_mask_dn_3: 1.263  loss_dice_dn_3: 2.051  loss_bbox_dn_3: 0.9039  loss_giou_dn_3: 0.7259  loss_ce_4: 1.514  loss_mask_4: 1.358  loss_dice_4: 2.304  loss_bbox_4: 2.715  loss_giou_4: 1.646  loss_ce_dn_4: 1.8  loss_mask_dn_4: 1.314  loss_dice_dn_4: 2.048  loss_bbox_dn_4: 0.8939  loss_giou_dn_4: 0.7213  loss_ce_5: 1.494  loss_mask_5: 1.461  loss_dice_5: 2.441  loss_bbox_5: 2.712  loss_giou_5: 1.629  loss_ce_dn_5: 1.832  loss_mask_dn_5: 1.339  loss_dice_dn_5: 2.071  loss_bbox_dn_5: 0.8867  loss_giou_dn_5: 0.7145  loss_ce_6: 1.537  loss_mask_6: 1.451  loss_dice_6: 2.408  loss_bbox_6: 2.674  loss_giou_6: 1.612  loss_ce_dn_6: 1.868  loss_mask_dn_6: 1.301  loss_dice_dn_6: 2.046  loss_bbox_dn_6: 0.8873  loss_giou_dn_6: 0.7139  loss_ce_7: 1.548  loss_mask_7: 1.395  loss_dice_7: 2.427  loss_bbox_7: 2.659  loss_giou_7: 1.593  loss_ce_dn_7: 1.869  loss_mask_dn_7: 1.262  loss_dice_dn_7: 2.042  loss_bbox_dn_7: 0.892  loss_giou_dn_7: 0.7116  loss_ce_8: 1.56  loss_mask_8: 1.345  loss_dice_8: 2.329  loss_bbox_8: 2.624  loss_giou_8: 1.601  loss_ce_dn_8: 1.924  loss_mask_dn_8: 1.295  loss_dice_dn_8: 2.053  loss_bbox_dn_8: 0.8969  loss_giou_dn_8: 0.7143    time: 2.2733  last_time: 2.2646  data_time: 0.0099  last_data_time: 0.0043   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:25:02 d2.utils.events]: \u001b[0m eta: 8 days, 6:13:28  iter: 5119  total_loss: 164.5  loss_ce: 1.814  loss_mask: 1.446  loss_dice: 2.353  loss_bbox: 2.645  loss_giou: 1.606  loss_ce_dn: 1.848  loss_mask_dn: 1.171  loss_dice_dn: 1.938  loss_bbox_dn: 0.7147  loss_giou_dn: 0.6975  loss_ce_0: 5.115  loss_mask_0: 1.352  loss_dice_0: 2.654  loss_bbox_0: 4.167  loss_giou_0: 2.003  loss_ce_1: 1.965  loss_mask_1: 1.338  loss_dice_1: 2.39  loss_bbox_1: 2.887  loss_giou_1: 1.788  loss_ce_dn_1: 2.19  loss_mask_dn_1: 1.253  loss_dice_dn_1: 2.162  loss_bbox_dn_1: 0.8382  loss_giou_dn_1: 0.7648  loss_ce_2: 1.85  loss_mask_2: 1.436  loss_dice_2: 2.347  loss_bbox_2: 2.812  loss_giou_2: 1.726  loss_ce_dn_2: 1.832  loss_mask_dn_2: 1.245  loss_dice_dn_2: 2.04  loss_bbox_dn_2: 0.8026  loss_giou_dn_2: 0.7527  loss_ce_3: 1.744  loss_mask_3: 1.435  loss_dice_3: 2.368  loss_bbox_3: 2.721  loss_giou_3: 1.666  loss_ce_dn_3: 1.779  loss_mask_dn_3: 1.182  loss_dice_dn_3: 2.029  loss_bbox_dn_3: 0.7587  loss_giou_dn_3: 0.7292  loss_ce_4: 1.683  loss_mask_4: 1.449  loss_dice_4: 2.365  loss_bbox_4: 2.704  loss_giou_4: 1.645  loss_ce_dn_4: 1.739  loss_mask_dn_4: 1.162  loss_dice_dn_4: 2.019  loss_bbox_dn_4: 0.737  loss_giou_dn_4: 0.7135  loss_ce_5: 1.675  loss_mask_5: 1.446  loss_dice_5: 2.346  loss_bbox_5: 2.642  loss_giou_5: 1.639  loss_ce_dn_5: 1.67  loss_mask_dn_5: 1.187  loss_dice_dn_5: 1.994  loss_bbox_dn_5: 0.7166  loss_giou_dn_5: 0.7022  loss_ce_6: 1.68  loss_mask_6: 1.434  loss_dice_6: 2.351  loss_bbox_6: 2.623  loss_giou_6: 1.64  loss_ce_dn_6: 1.757  loss_mask_dn_6: 1.199  loss_dice_dn_6: 1.986  loss_bbox_dn_6: 0.7211  loss_giou_dn_6: 0.7006  loss_ce_7: 1.701  loss_mask_7: 1.433  loss_dice_7: 2.326  loss_bbox_7: 2.613  loss_giou_7: 1.632  loss_ce_dn_7: 1.76  loss_mask_dn_7: 1.185  loss_dice_dn_7: 1.981  loss_bbox_dn_7: 0.712  loss_giou_dn_7: 0.6988  loss_ce_8: 1.749  loss_mask_8: 1.409  loss_dice_8: 2.311  loss_bbox_8: 2.637  loss_giou_8: 1.617  loss_ce_dn_8: 1.799  loss_mask_dn_8: 1.174  loss_dice_dn_8: 1.986  loss_bbox_dn_8: 0.7138  loss_giou_dn_8: 0.6975    time: 2.2734  last_time: 2.2552  data_time: 0.0104  last_data_time: 0.0041   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:25:47 d2.utils.events]: \u001b[0m eta: 8 days, 6:13:13  iter: 5139  total_loss: 164.3  loss_ce: 1.758  loss_mask: 1.426  loss_dice: 2.338  loss_bbox: 2.545  loss_giou: 1.468  loss_ce_dn: 2.176  loss_mask_dn: 1.26  loss_dice_dn: 2.025  loss_bbox_dn: 0.768  loss_giou_dn: 0.631  loss_ce_0: 5.613  loss_mask_0: 1.4  loss_dice_0: 2.623  loss_bbox_0: 3.866  loss_giou_0: 1.892  loss_ce_1: 1.969  loss_mask_1: 1.395  loss_dice_1: 2.375  loss_bbox_1: 2.808  loss_giou_1: 1.549  loss_ce_dn_1: 2.435  loss_mask_dn_1: 1.285  loss_dice_dn_1: 2.244  loss_bbox_dn_1: 0.881  loss_giou_dn_1: 0.7389  loss_ce_2: 1.763  loss_mask_2: 1.415  loss_dice_2: 2.368  loss_bbox_2: 2.608  loss_giou_2: 1.611  loss_ce_dn_2: 2.185  loss_mask_dn_2: 1.29  loss_dice_dn_2: 2.081  loss_bbox_dn_2: 0.8355  loss_giou_dn_2: 0.7109  loss_ce_3: 1.679  loss_mask_3: 1.451  loss_dice_3: 2.363  loss_bbox_3: 2.58  loss_giou_3: 1.596  loss_ce_dn_3: 2.02  loss_mask_dn_3: 1.242  loss_dice_dn_3: 2.086  loss_bbox_dn_3: 0.8294  loss_giou_dn_3: 0.6794  loss_ce_4: 1.653  loss_mask_4: 1.482  loss_dice_4: 2.387  loss_bbox_4: 2.544  loss_giou_4: 1.582  loss_ce_dn_4: 1.948  loss_mask_dn_4: 1.171  loss_dice_dn_4: 2.073  loss_bbox_dn_4: 0.8096  loss_giou_dn_4: 0.6582  loss_ce_5: 1.57  loss_mask_5: 1.461  loss_dice_5: 2.365  loss_bbox_5: 2.518  loss_giou_5: 1.544  loss_ce_dn_5: 1.868  loss_mask_dn_5: 1.197  loss_dice_dn_5: 2.045  loss_bbox_dn_5: 0.7999  loss_giou_dn_5: 0.6454  loss_ce_6: 1.583  loss_mask_6: 1.383  loss_dice_6: 2.369  loss_bbox_6: 2.539  loss_giou_6: 1.534  loss_ce_dn_6: 1.93  loss_mask_dn_6: 1.216  loss_dice_dn_6: 2.04  loss_bbox_dn_6: 0.7928  loss_giou_dn_6: 0.6424  loss_ce_7: 1.611  loss_mask_7: 1.343  loss_dice_7: 2.351  loss_bbox_7: 2.557  loss_giou_7: 1.524  loss_ce_dn_7: 1.978  loss_mask_dn_7: 1.198  loss_dice_dn_7: 2.048  loss_bbox_dn_7: 0.7628  loss_giou_dn_7: 0.6339  loss_ce_8: 1.664  loss_mask_8: 1.34  loss_dice_8: 2.33  loss_bbox_8: 2.551  loss_giou_8: 1.519  loss_ce_dn_8: 2.128  loss_mask_dn_8: 1.241  loss_dice_dn_8: 2.015  loss_bbox_dn_8: 0.7675  loss_giou_dn_8: 0.6297    time: 2.2733  last_time: 2.2736  data_time: 0.0123  last_data_time: 0.0232   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:26:32 d2.utils.events]: \u001b[0m eta: 8 days, 6:12:56  iter: 5159  total_loss: 163.9  loss_ce: 1.794  loss_mask: 1.615  loss_dice: 2.235  loss_bbox: 2.295  loss_giou: 1.321  loss_ce_dn: 2.012  loss_mask_dn: 1.418  loss_dice_dn: 2.021  loss_bbox_dn: 0.8244  loss_giou_dn: 0.6521  loss_ce_0: 5.144  loss_mask_0: 1.523  loss_dice_0: 2.341  loss_bbox_0: 3.808  loss_giou_0: 1.888  loss_ce_1: 2.059  loss_mask_1: 1.553  loss_dice_1: 2.225  loss_bbox_1: 2.631  loss_giou_1: 1.509  loss_ce_dn_1: 2.304  loss_mask_dn_1: 1.493  loss_dice_dn_1: 2.243  loss_bbox_dn_1: 0.9817  loss_giou_dn_1: 0.7246  loss_ce_2: 1.825  loss_mask_2: 1.551  loss_dice_2: 2.193  loss_bbox_2: 2.523  loss_giou_2: 1.486  loss_ce_dn_2: 1.854  loss_mask_dn_2: 1.433  loss_dice_dn_2: 2.086  loss_bbox_dn_2: 0.909  loss_giou_dn_2: 0.6904  loss_ce_3: 1.776  loss_mask_3: 1.507  loss_dice_3: 2.18  loss_bbox_3: 2.377  loss_giou_3: 1.406  loss_ce_dn_3: 1.842  loss_mask_dn_3: 1.43  loss_dice_dn_3: 2.048  loss_bbox_dn_3: 0.8635  loss_giou_dn_3: 0.6829  loss_ce_4: 1.77  loss_mask_4: 1.544  loss_dice_4: 2.235  loss_bbox_4: 2.323  loss_giou_4: 1.386  loss_ce_dn_4: 1.774  loss_mask_dn_4: 1.433  loss_dice_dn_4: 2.013  loss_bbox_dn_4: 0.8456  loss_giou_dn_4: 0.6664  loss_ce_5: 1.714  loss_mask_5: 1.507  loss_dice_5: 2.195  loss_bbox_5: 2.283  loss_giou_5: 1.364  loss_ce_dn_5: 1.749  loss_mask_dn_5: 1.412  loss_dice_dn_5: 2.014  loss_bbox_dn_5: 0.8411  loss_giou_dn_5: 0.661  loss_ce_6: 1.766  loss_mask_6: 1.532  loss_dice_6: 2.195  loss_bbox_6: 2.264  loss_giou_6: 1.36  loss_ce_dn_6: 1.761  loss_mask_dn_6: 1.361  loss_dice_dn_6: 1.979  loss_bbox_dn_6: 0.8361  loss_giou_dn_6: 0.6601  loss_ce_7: 1.77  loss_mask_7: 1.626  loss_dice_7: 2.202  loss_bbox_7: 2.299  loss_giou_7: 1.338  loss_ce_dn_7: 1.779  loss_mask_dn_7: 1.377  loss_dice_dn_7: 1.976  loss_bbox_dn_7: 0.8263  loss_giou_dn_7: 0.6463  loss_ce_8: 1.833  loss_mask_8: 1.645  loss_dice_8: 2.23  loss_bbox_8: 2.29  loss_giou_8: 1.321  loss_ce_dn_8: 1.885  loss_mask_dn_8: 1.419  loss_dice_dn_8: 2.003  loss_bbox_dn_8: 0.8251  loss_giou_dn_8: 0.6483    time: 2.2733  last_time: 2.3155  data_time: 0.0094  last_data_time: 0.0042   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:27:18 d2.utils.events]: \u001b[0m eta: 8 days, 6:13:01  iter: 5179  total_loss: 165.6  loss_ce: 1.455  loss_mask: 1.276  loss_dice: 2.244  loss_bbox: 2.617  loss_giou: 1.522  loss_ce_dn: 1.929  loss_mask_dn: 1.213  loss_dice_dn: 1.95  loss_bbox_dn: 0.7331  loss_giou_dn: 0.6737  loss_ce_0: 5.046  loss_mask_0: 1.285  loss_dice_0: 2.462  loss_bbox_0: 4.402  loss_giou_0: 1.937  loss_ce_1: 1.687  loss_mask_1: 1.36  loss_dice_1: 2.223  loss_bbox_1: 3.01  loss_giou_1: 1.648  loss_ce_dn_1: 2.414  loss_mask_dn_1: 1.182  loss_dice_dn_1: 2.117  loss_bbox_dn_1: 0.8887  loss_giou_dn_1: 0.7576  loss_ce_2: 1.549  loss_mask_2: 1.345  loss_dice_2: 2.225  loss_bbox_2: 2.909  loss_giou_2: 1.62  loss_ce_dn_2: 1.951  loss_mask_dn_2: 1.232  loss_dice_dn_2: 2.024  loss_bbox_dn_2: 0.8208  loss_giou_dn_2: 0.7246  loss_ce_3: 1.533  loss_mask_3: 1.392  loss_dice_3: 2.217  loss_bbox_3: 2.81  loss_giou_3: 1.598  loss_ce_dn_3: 1.935  loss_mask_dn_3: 1.194  loss_dice_dn_3: 1.931  loss_bbox_dn_3: 0.7723  loss_giou_dn_3: 0.6986  loss_ce_4: 1.478  loss_mask_4: 1.344  loss_dice_4: 2.188  loss_bbox_4: 2.693  loss_giou_4: 1.577  loss_ce_dn_4: 1.88  loss_mask_dn_4: 1.192  loss_dice_dn_4: 1.891  loss_bbox_dn_4: 0.7594  loss_giou_dn_4: 0.6866  loss_ce_5: 1.406  loss_mask_5: 1.369  loss_dice_5: 2.206  loss_bbox_5: 2.685  loss_giou_5: 1.554  loss_ce_dn_5: 1.878  loss_mask_dn_5: 1.195  loss_dice_dn_5: 1.902  loss_bbox_dn_5: 0.7484  loss_giou_dn_5: 0.674  loss_ce_6: 1.4  loss_mask_6: 1.295  loss_dice_6: 2.239  loss_bbox_6: 2.661  loss_giou_6: 1.541  loss_ce_dn_6: 1.88  loss_mask_dn_6: 1.25  loss_dice_dn_6: 1.944  loss_bbox_dn_6: 0.7502  loss_giou_dn_6: 0.6737  loss_ce_7: 1.418  loss_mask_7: 1.318  loss_dice_7: 2.221  loss_bbox_7: 2.628  loss_giou_7: 1.526  loss_ce_dn_7: 1.879  loss_mask_dn_7: 1.262  loss_dice_dn_7: 1.953  loss_bbox_dn_7: 0.7382  loss_giou_dn_7: 0.6693  loss_ce_8: 1.426  loss_mask_8: 1.313  loss_dice_8: 2.257  loss_bbox_8: 2.622  loss_giou_8: 1.52  loss_ce_dn_8: 1.92  loss_mask_dn_8: 1.221  loss_dice_dn_8: 1.936  loss_bbox_dn_8: 0.7334  loss_giou_dn_8: 0.671    time: 2.2733  last_time: 2.2390  data_time: 0.0103  last_data_time: 0.0247   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:28:03 d2.utils.events]: \u001b[0m eta: 8 days, 6:09:48  iter: 5199  total_loss: 158.1  loss_ce: 1.469  loss_mask: 1.327  loss_dice: 2.412  loss_bbox: 2.646  loss_giou: 1.502  loss_ce_dn: 1.906  loss_mask_dn: 1.122  loss_dice_dn: 2.027  loss_bbox_dn: 0.8001  loss_giou_dn: 0.6867  loss_ce_0: 5.016  loss_mask_0: 1.179  loss_dice_0: 2.641  loss_bbox_0: 4.091  loss_giou_0: 2.039  loss_ce_1: 1.619  loss_mask_1: 1.354  loss_dice_1: 2.398  loss_bbox_1: 2.759  loss_giou_1: 1.595  loss_ce_dn_1: 2.41  loss_mask_dn_1: 1.224  loss_dice_dn_1: 2.245  loss_bbox_dn_1: 0.939  loss_giou_dn_1: 0.7497  loss_ce_2: 1.462  loss_mask_2: 1.349  loss_dice_2: 2.354  loss_bbox_2: 2.742  loss_giou_2: 1.583  loss_ce_dn_2: 1.95  loss_mask_dn_2: 1.186  loss_dice_dn_2: 2.059  loss_bbox_dn_2: 0.8819  loss_giou_dn_2: 0.7226  loss_ce_3: 1.442  loss_mask_3: 1.338  loss_dice_3: 2.317  loss_bbox_3: 2.692  loss_giou_3: 1.544  loss_ce_dn_3: 1.828  loss_mask_dn_3: 1.183  loss_dice_dn_3: 2.037  loss_bbox_dn_3: 0.8392  loss_giou_dn_3: 0.6995  loss_ce_4: 1.381  loss_mask_4: 1.342  loss_dice_4: 2.324  loss_bbox_4: 2.707  loss_giou_4: 1.527  loss_ce_dn_4: 1.713  loss_mask_dn_4: 1.145  loss_dice_dn_4: 2.041  loss_bbox_dn_4: 0.8192  loss_giou_dn_4: 0.692  loss_ce_5: 1.489  loss_mask_5: 1.339  loss_dice_5: 2.355  loss_bbox_5: 2.669  loss_giou_5: 1.508  loss_ce_dn_5: 1.75  loss_mask_dn_5: 1.141  loss_dice_dn_5: 2  loss_bbox_dn_5: 0.8027  loss_giou_dn_5: 0.6855  loss_ce_6: 1.429  loss_mask_6: 1.341  loss_dice_6: 2.358  loss_bbox_6: 2.68  loss_giou_6: 1.512  loss_ce_dn_6: 1.776  loss_mask_dn_6: 1.126  loss_dice_dn_6: 1.988  loss_bbox_dn_6: 0.8074  loss_giou_dn_6: 0.6847  loss_ce_7: 1.362  loss_mask_7: 1.36  loss_dice_7: 2.359  loss_bbox_7: 2.626  loss_giou_7: 1.499  loss_ce_dn_7: 1.776  loss_mask_dn_7: 1.114  loss_dice_dn_7: 2  loss_bbox_dn_7: 0.7916  loss_giou_dn_7: 0.6784  loss_ce_8: 1.376  loss_mask_8: 1.339  loss_dice_8: 2.378  loss_bbox_8: 2.644  loss_giou_8: 1.498  loss_ce_dn_8: 1.819  loss_mask_dn_8: 1.117  loss_dice_dn_8: 2.044  loss_bbox_dn_8: 0.7943  loss_giou_dn_8: 0.6802    time: 2.2733  last_time: 2.2614  data_time: 0.0123  last_data_time: 0.0013   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:28:49 d2.utils.events]: \u001b[0m eta: 8 days, 6:10:12  iter: 5219  total_loss: 162.6  loss_ce: 1.747  loss_mask: 1.275  loss_dice: 2.335  loss_bbox: 2.545  loss_giou: 1.614  loss_ce_dn: 2.187  loss_mask_dn: 1.059  loss_dice_dn: 2.085  loss_bbox_dn: 0.86  loss_giou_dn: 0.6723  loss_ce_0: 5.062  loss_mask_0: 1.247  loss_dice_0: 2.503  loss_bbox_0: 3.897  loss_giou_0: 2.009  loss_ce_1: 1.723  loss_mask_1: 1.334  loss_dice_1: 2.391  loss_bbox_1: 2.982  loss_giou_1: 1.749  loss_ce_dn_1: 2.352  loss_mask_dn_1: 1.14  loss_dice_dn_1: 2.217  loss_bbox_dn_1: 0.959  loss_giou_dn_1: 0.7414  loss_ce_2: 1.667  loss_mask_2: 1.256  loss_dice_2: 2.317  loss_bbox_2: 2.76  loss_giou_2: 1.685  loss_ce_dn_2: 1.919  loss_mask_dn_2: 1.084  loss_dice_dn_2: 2.086  loss_bbox_dn_2: 0.9044  loss_giou_dn_2: 0.7246  loss_ce_3: 1.572  loss_mask_3: 1.272  loss_dice_3: 2.282  loss_bbox_3: 2.687  loss_giou_3: 1.658  loss_ce_dn_3: 1.828  loss_mask_dn_3: 1.061  loss_dice_dn_3: 2.049  loss_bbox_dn_3: 0.8642  loss_giou_dn_3: 0.7051  loss_ce_4: 1.618  loss_mask_4: 1.173  loss_dice_4: 2.303  loss_bbox_4: 2.599  loss_giou_4: 1.651  loss_ce_dn_4: 1.888  loss_mask_dn_4: 1.092  loss_dice_dn_4: 2.067  loss_bbox_dn_4: 0.8594  loss_giou_dn_4: 0.693  loss_ce_5: 1.604  loss_mask_5: 1.148  loss_dice_5: 2.27  loss_bbox_5: 2.522  loss_giou_5: 1.636  loss_ce_dn_5: 1.929  loss_mask_dn_5: 1.098  loss_dice_dn_5: 2.064  loss_bbox_dn_5: 0.8606  loss_giou_dn_5: 0.6904  loss_ce_6: 1.677  loss_mask_6: 1.177  loss_dice_6: 2.269  loss_bbox_6: 2.525  loss_giou_6: 1.632  loss_ce_dn_6: 1.968  loss_mask_dn_6: 1.098  loss_dice_dn_6: 2.055  loss_bbox_dn_6: 0.8518  loss_giou_dn_6: 0.6855  loss_ce_7: 1.738  loss_mask_7: 1.224  loss_dice_7: 2.299  loss_bbox_7: 2.547  loss_giou_7: 1.614  loss_ce_dn_7: 2.001  loss_mask_dn_7: 1.07  loss_dice_dn_7: 2.089  loss_bbox_dn_7: 0.8535  loss_giou_dn_7: 0.674  loss_ce_8: 1.742  loss_mask_8: 1.26  loss_dice_8: 2.302  loss_bbox_8: 2.543  loss_giou_8: 1.612  loss_ce_dn_8: 2.073  loss_mask_dn_8: 1.077  loss_dice_dn_8: 2.093  loss_bbox_dn_8: 0.8535  loss_giou_dn_8: 0.6735    time: 2.2733  last_time: 2.3286  data_time: 0.0116  last_data_time: 0.0301   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:29:34 d2.utils.events]: \u001b[0m eta: 8 days, 6:09:54  iter: 5239  total_loss: 163.7  loss_ce: 1.55  loss_mask: 1.346  loss_dice: 2.304  loss_bbox: 2.575  loss_giou: 1.432  loss_ce_dn: 1.896  loss_mask_dn: 1.237  loss_dice_dn: 2.02  loss_bbox_dn: 0.8232  loss_giou_dn: 0.6927  loss_ce_0: 5.008  loss_mask_0: 1.273  loss_dice_0: 2.435  loss_bbox_0: 4.063  loss_giou_0: 1.963  loss_ce_1: 1.888  loss_mask_1: 1.387  loss_dice_1: 2.435  loss_bbox_1: 2.788  loss_giou_1: 1.596  loss_ce_dn_1: 2.376  loss_mask_dn_1: 1.296  loss_dice_dn_1: 2.172  loss_bbox_dn_1: 0.9411  loss_giou_dn_1: 0.7656  loss_ce_2: 1.602  loss_mask_2: 1.413  loss_dice_2: 2.376  loss_bbox_2: 2.586  loss_giou_2: 1.626  loss_ce_dn_2: 2.016  loss_mask_dn_2: 1.319  loss_dice_dn_2: 2.034  loss_bbox_dn_2: 0.8716  loss_giou_dn_2: 0.7383  loss_ce_3: 1.514  loss_mask_3: 1.324  loss_dice_3: 2.414  loss_bbox_3: 2.528  loss_giou_3: 1.576  loss_ce_dn_3: 1.854  loss_mask_dn_3: 1.268  loss_dice_dn_3: 2.035  loss_bbox_dn_3: 0.8532  loss_giou_dn_3: 0.7092  loss_ce_4: 1.507  loss_mask_4: 1.33  loss_dice_4: 2.334  loss_bbox_4: 2.632  loss_giou_4: 1.518  loss_ce_dn_4: 1.789  loss_mask_dn_4: 1.235  loss_dice_dn_4: 2.021  loss_bbox_dn_4: 0.8356  loss_giou_dn_4: 0.7001  loss_ce_5: 1.388  loss_mask_5: 1.351  loss_dice_5: 2.351  loss_bbox_5: 2.592  loss_giou_5: 1.476  loss_ce_dn_5: 1.826  loss_mask_dn_5: 1.227  loss_dice_dn_5: 2.058  loss_bbox_dn_5: 0.8389  loss_giou_dn_5: 0.6956  loss_ce_6: 1.487  loss_mask_6: 1.366  loss_dice_6: 2.304  loss_bbox_6: 2.603  loss_giou_6: 1.431  loss_ce_dn_6: 1.878  loss_mask_dn_6: 1.204  loss_dice_dn_6: 2.022  loss_bbox_dn_6: 0.8347  loss_giou_dn_6: 0.6965  loss_ce_7: 1.527  loss_mask_7: 1.367  loss_dice_7: 2.376  loss_bbox_7: 2.578  loss_giou_7: 1.433  loss_ce_dn_7: 1.847  loss_mask_dn_7: 1.224  loss_dice_dn_7: 1.991  loss_bbox_dn_7: 0.8261  loss_giou_dn_7: 0.6906  loss_ce_8: 1.523  loss_mask_8: 1.379  loss_dice_8: 2.364  loss_bbox_8: 2.587  loss_giou_8: 1.425  loss_ce_dn_8: 1.821  loss_mask_dn_8: 1.223  loss_dice_dn_8: 1.982  loss_bbox_dn_8: 0.8243  loss_giou_dn_8: 0.6901    time: 2.2733  last_time: 2.2460  data_time: 0.0109  last_data_time: 0.0043   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:30:20 d2.utils.events]: \u001b[0m eta: 8 days, 6:08:58  iter: 5259  total_loss: 174  loss_ce: 1.6  loss_mask: 1.542  loss_dice: 2.31  loss_bbox: 2.594  loss_giou: 1.511  loss_ce_dn: 1.984  loss_mask_dn: 1.347  loss_dice_dn: 1.988  loss_bbox_dn: 0.7923  loss_giou_dn: 0.6562  loss_ce_0: 4.789  loss_mask_0: 1.417  loss_dice_0: 2.467  loss_bbox_0: 4.254  loss_giou_0: 1.967  loss_ce_1: 1.58  loss_mask_1: 1.607  loss_dice_1: 2.265  loss_bbox_1: 3.222  loss_giou_1: 1.513  loss_ce_dn_1: 2.357  loss_mask_dn_1: 1.481  loss_dice_dn_1: 2.196  loss_bbox_dn_1: 0.9308  loss_giou_dn_1: 0.7363  loss_ce_2: 1.758  loss_mask_2: 1.507  loss_dice_2: 2.221  loss_bbox_2: 2.94  loss_giou_2: 1.524  loss_ce_dn_2: 1.965  loss_mask_dn_2: 1.455  loss_dice_dn_2: 2.036  loss_bbox_dn_2: 0.8783  loss_giou_dn_2: 0.7107  loss_ce_3: 1.536  loss_mask_3: 1.601  loss_dice_3: 2.263  loss_bbox_3: 2.852  loss_giou_3: 1.528  loss_ce_dn_3: 1.802  loss_mask_dn_3: 1.421  loss_dice_dn_3: 1.982  loss_bbox_dn_3: 0.8303  loss_giou_dn_3: 0.6814  loss_ce_4: 1.616  loss_mask_4: 1.588  loss_dice_4: 2.238  loss_bbox_4: 2.769  loss_giou_4: 1.537  loss_ce_dn_4: 1.8  loss_mask_dn_4: 1.425  loss_dice_dn_4: 1.948  loss_bbox_dn_4: 0.8193  loss_giou_dn_4: 0.6675  loss_ce_5: 1.7  loss_mask_5: 1.61  loss_dice_5: 2.262  loss_bbox_5: 2.761  loss_giou_5: 1.538  loss_ce_dn_5: 1.794  loss_mask_dn_5: 1.377  loss_dice_dn_5: 1.911  loss_bbox_dn_5: 0.7841  loss_giou_dn_5: 0.657  loss_ce_6: 1.681  loss_mask_6: 1.584  loss_dice_6: 2.21  loss_bbox_6: 2.609  loss_giou_6: 1.531  loss_ce_dn_6: 1.763  loss_mask_dn_6: 1.367  loss_dice_dn_6: 1.95  loss_bbox_dn_6: 0.7857  loss_giou_dn_6: 0.6594  loss_ce_7: 1.521  loss_mask_7: 1.571  loss_dice_7: 2.223  loss_bbox_7: 2.599  loss_giou_7: 1.519  loss_ce_dn_7: 1.812  loss_mask_dn_7: 1.35  loss_dice_dn_7: 1.955  loss_bbox_dn_7: 0.7811  loss_giou_dn_7: 0.6522  loss_ce_8: 1.584  loss_mask_8: 1.553  loss_dice_8: 2.24  loss_bbox_8: 2.599  loss_giou_8: 1.51  loss_ce_dn_8: 1.887  loss_mask_dn_8: 1.368  loss_dice_dn_8: 1.97  loss_bbox_dn_8: 0.7896  loss_giou_dn_8: 0.654    time: 2.2733  last_time: 2.2396  data_time: 0.0125  last_data_time: 0.0087   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:31:05 d2.utils.events]: \u001b[0m eta: 8 days, 6:09:14  iter: 5279  total_loss: 168.5  loss_ce: 1.73  loss_mask: 1.397  loss_dice: 2.454  loss_bbox: 2.612  loss_giou: 1.507  loss_ce_dn: 2.285  loss_mask_dn: 1.285  loss_dice_dn: 2.089  loss_bbox_dn: 0.7739  loss_giou_dn: 0.6722  loss_ce_0: 5.423  loss_mask_0: 1.438  loss_dice_0: 2.639  loss_bbox_0: 4.166  loss_giou_0: 1.99  loss_ce_1: 1.922  loss_mask_1: 1.457  loss_dice_1: 2.357  loss_bbox_1: 3.336  loss_giou_1: 1.716  loss_ce_dn_1: 2.533  loss_mask_dn_1: 1.316  loss_dice_dn_1: 2.171  loss_bbox_dn_1: 0.9443  loss_giou_dn_1: 0.7471  loss_ce_2: 1.734  loss_mask_2: 1.409  loss_dice_2: 2.414  loss_bbox_2: 2.949  loss_giou_2: 1.622  loss_ce_dn_2: 2.151  loss_mask_dn_2: 1.282  loss_dice_dn_2: 2.023  loss_bbox_dn_2: 0.8833  loss_giou_dn_2: 0.7254  loss_ce_3: 1.678  loss_mask_3: 1.39  loss_dice_3: 2.456  loss_bbox_3: 2.914  loss_giou_3: 1.57  loss_ce_dn_3: 2.167  loss_mask_dn_3: 1.288  loss_dice_dn_3: 2.026  loss_bbox_dn_3: 0.8412  loss_giou_dn_3: 0.6998  loss_ce_4: 1.667  loss_mask_4: 1.374  loss_dice_4: 2.425  loss_bbox_4: 2.834  loss_giou_4: 1.58  loss_ce_dn_4: 2.145  loss_mask_dn_4: 1.279  loss_dice_dn_4: 2.05  loss_bbox_dn_4: 0.8185  loss_giou_dn_4: 0.692  loss_ce_5: 1.628  loss_mask_5: 1.356  loss_dice_5: 2.476  loss_bbox_5: 2.621  loss_giou_5: 1.576  loss_ce_dn_5: 2.177  loss_mask_dn_5: 1.229  loss_dice_dn_5: 2.034  loss_bbox_dn_5: 0.7989  loss_giou_dn_5: 0.682  loss_ce_6: 1.64  loss_mask_6: 1.361  loss_dice_6: 2.461  loss_bbox_6: 2.745  loss_giou_6: 1.538  loss_ce_dn_6: 2.202  loss_mask_dn_6: 1.246  loss_dice_dn_6: 2.07  loss_bbox_dn_6: 0.7923  loss_giou_dn_6: 0.6744  loss_ce_7: 1.63  loss_mask_7: 1.409  loss_dice_7: 2.435  loss_bbox_7: 2.75  loss_giou_7: 1.555  loss_ce_dn_7: 2.187  loss_mask_dn_7: 1.258  loss_dice_dn_7: 2.065  loss_bbox_dn_7: 0.7787  loss_giou_dn_7: 0.6652  loss_ce_8: 1.712  loss_mask_8: 1.424  loss_dice_8: 2.438  loss_bbox_8: 2.736  loss_giou_8: 1.508  loss_ce_dn_8: 2.192  loss_mask_dn_8: 1.281  loss_dice_dn_8: 2.055  loss_bbox_dn_8: 0.7736  loss_giou_dn_8: 0.6687    time: 2.2733  last_time: 2.2700  data_time: 0.0081  last_data_time: 0.0040   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:31:51 d2.utils.events]: \u001b[0m eta: 8 days, 6:08:49  iter: 5299  total_loss: 168.4  loss_ce: 1.56  loss_mask: 1.471  loss_dice: 2.44  loss_bbox: 2.712  loss_giou: 1.551  loss_ce_dn: 1.814  loss_mask_dn: 1.221  loss_dice_dn: 2.109  loss_bbox_dn: 0.7725  loss_giou_dn: 0.7026  loss_ce_0: 5.349  loss_mask_0: 1.354  loss_dice_0: 2.623  loss_bbox_0: 4.283  loss_giou_0: 1.949  loss_ce_1: 1.949  loss_mask_1: 1.344  loss_dice_1: 2.499  loss_bbox_1: 3.008  loss_giou_1: 1.708  loss_ce_dn_1: 2.488  loss_mask_dn_1: 1.298  loss_dice_dn_1: 2.336  loss_bbox_dn_1: 0.9387  loss_giou_dn_1: 0.764  loss_ce_2: 1.744  loss_mask_2: 1.357  loss_dice_2: 2.417  loss_bbox_2: 2.832  loss_giou_2: 1.635  loss_ce_dn_2: 2.12  loss_mask_dn_2: 1.299  loss_dice_dn_2: 2.176  loss_bbox_dn_2: 0.8764  loss_giou_dn_2: 0.7406  loss_ce_3: 1.611  loss_mask_3: 1.445  loss_dice_3: 2.472  loss_bbox_3: 2.801  loss_giou_3: 1.648  loss_ce_dn_3: 1.945  loss_mask_dn_3: 1.259  loss_dice_dn_3: 2.154  loss_bbox_dn_3: 0.8194  loss_giou_dn_3: 0.7252  loss_ce_4: 1.536  loss_mask_4: 1.502  loss_dice_4: 2.431  loss_bbox_4: 2.745  loss_giou_4: 1.619  loss_ce_dn_4: 1.856  loss_mask_dn_4: 1.246  loss_dice_dn_4: 2.113  loss_bbox_dn_4: 0.803  loss_giou_dn_4: 0.7139  loss_ce_5: 1.571  loss_mask_5: 1.466  loss_dice_5: 2.44  loss_bbox_5: 2.763  loss_giou_5: 1.567  loss_ce_dn_5: 1.86  loss_mask_dn_5: 1.212  loss_dice_dn_5: 2.069  loss_bbox_dn_5: 0.7947  loss_giou_dn_5: 0.7034  loss_ce_6: 1.532  loss_mask_6: 1.542  loss_dice_6: 2.452  loss_bbox_6: 2.76  loss_giou_6: 1.557  loss_ce_dn_6: 1.797  loss_mask_dn_6: 1.229  loss_dice_dn_6: 2.057  loss_bbox_dn_6: 0.7914  loss_giou_dn_6: 0.7003  loss_ce_7: 1.539  loss_mask_7: 1.413  loss_dice_7: 2.455  loss_bbox_7: 2.767  loss_giou_7: 1.544  loss_ce_dn_7: 1.795  loss_mask_dn_7: 1.198  loss_dice_dn_7: 2.092  loss_bbox_dn_7: 0.7721  loss_giou_dn_7: 0.6946  loss_ce_8: 1.539  loss_mask_8: 1.427  loss_dice_8: 2.438  loss_bbox_8: 2.782  loss_giou_8: 1.547  loss_ce_dn_8: 1.813  loss_mask_dn_8: 1.251  loss_dice_dn_8: 2.095  loss_bbox_dn_8: 0.772  loss_giou_dn_8: 0.7    time: 2.2733  last_time: 2.3609  data_time: 0.0115  last_data_time: 0.0072   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:32:36 d2.utils.events]: \u001b[0m eta: 8 days, 6:07:45  iter: 5319  total_loss: 148.3  loss_ce: 1.338  loss_mask: 1.197  loss_dice: 2.136  loss_bbox: 2.298  loss_giou: 1.4  loss_ce_dn: 1.747  loss_mask_dn: 1.269  loss_dice_dn: 1.764  loss_bbox_dn: 0.8034  loss_giou_dn: 0.6427  loss_ce_0: 4.727  loss_mask_0: 1.292  loss_dice_0: 2.268  loss_bbox_0: 4.109  loss_giou_0: 1.917  loss_ce_1: 1.678  loss_mask_1: 1.188  loss_dice_1: 2.147  loss_bbox_1: 2.428  loss_giou_1: 1.601  loss_ce_dn_1: 2.219  loss_mask_dn_1: 1.395  loss_dice_dn_1: 1.955  loss_bbox_dn_1: 0.896  loss_giou_dn_1: 0.7299  loss_ce_2: 1.477  loss_mask_2: 1.188  loss_dice_2: 2.129  loss_bbox_2: 2.358  loss_giou_2: 1.449  loss_ce_dn_2: 1.958  loss_mask_dn_2: 1.349  loss_dice_dn_2: 1.812  loss_bbox_dn_2: 0.8594  loss_giou_dn_2: 0.7001  loss_ce_3: 1.362  loss_mask_3: 1.135  loss_dice_3: 2.116  loss_bbox_3: 2.26  loss_giou_3: 1.459  loss_ce_dn_3: 1.865  loss_mask_dn_3: 1.286  loss_dice_dn_3: 1.797  loss_bbox_dn_3: 0.8476  loss_giou_dn_3: 0.6731  loss_ce_4: 1.325  loss_mask_4: 1.157  loss_dice_4: 2.148  loss_bbox_4: 2.265  loss_giou_4: 1.442  loss_ce_dn_4: 1.766  loss_mask_dn_4: 1.28  loss_dice_dn_4: 1.805  loss_bbox_dn_4: 0.8348  loss_giou_dn_4: 0.6669  loss_ce_5: 1.3  loss_mask_5: 1.177  loss_dice_5: 2.092  loss_bbox_5: 2.284  loss_giou_5: 1.415  loss_ce_dn_5: 1.732  loss_mask_dn_5: 1.275  loss_dice_dn_5: 1.805  loss_bbox_dn_5: 0.8086  loss_giou_dn_5: 0.6512  loss_ce_6: 1.264  loss_mask_6: 1.177  loss_dice_6: 2.094  loss_bbox_6: 2.286  loss_giou_6: 1.413  loss_ce_dn_6: 1.768  loss_mask_dn_6: 1.268  loss_dice_dn_6: 1.786  loss_bbox_dn_6: 0.8011  loss_giou_dn_6: 0.6465  loss_ce_7: 1.327  loss_mask_7: 1.176  loss_dice_7: 2.098  loss_bbox_7: 2.313  loss_giou_7: 1.393  loss_ce_dn_7: 1.76  loss_mask_dn_7: 1.278  loss_dice_dn_7: 1.806  loss_bbox_dn_7: 0.7963  loss_giou_dn_7: 0.6377  loss_ce_8: 1.368  loss_mask_8: 1.188  loss_dice_8: 2.089  loss_bbox_8: 2.302  loss_giou_8: 1.389  loss_ce_dn_8: 1.778  loss_mask_dn_8: 1.296  loss_dice_dn_8: 1.838  loss_bbox_dn_8: 0.7964  loss_giou_dn_8: 0.6392    time: 2.2733  last_time: 2.2683  data_time: 0.0108  last_data_time: 0.0012   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:33:22 d2.utils.events]: \u001b[0m eta: 8 days, 6:07:00  iter: 5339  total_loss: 167  loss_ce: 1.728  loss_mask: 1.224  loss_dice: 2.367  loss_bbox: 2.7  loss_giou: 1.595  loss_ce_dn: 1.984  loss_mask_dn: 1.019  loss_dice_dn: 2.059  loss_bbox_dn: 0.7934  loss_giou_dn: 0.7051  loss_ce_0: 5.555  loss_mask_0: 1.11  loss_dice_0: 2.579  loss_bbox_0: 4.136  loss_giou_0: 2.085  loss_ce_1: 1.997  loss_mask_1: 1.251  loss_dice_1: 2.522  loss_bbox_1: 2.921  loss_giou_1: 1.8  loss_ce_dn_1: 2.297  loss_mask_dn_1: 1.047  loss_dice_dn_1: 2.177  loss_bbox_dn_1: 0.8655  loss_giou_dn_1: 0.7715  loss_ce_2: 1.706  loss_mask_2: 1.171  loss_dice_2: 2.41  loss_bbox_2: 2.71  loss_giou_2: 1.784  loss_ce_dn_2: 1.876  loss_mask_dn_2: 1.017  loss_dice_dn_2: 2.006  loss_bbox_dn_2: 0.8321  loss_giou_dn_2: 0.7482  loss_ce_3: 1.644  loss_mask_3: 1.141  loss_dice_3: 2.415  loss_bbox_3: 2.734  loss_giou_3: 1.697  loss_ce_dn_3: 1.798  loss_mask_dn_3: 0.9851  loss_dice_dn_3: 2.022  loss_bbox_dn_3: 0.8216  loss_giou_dn_3: 0.7318  loss_ce_4: 1.729  loss_mask_4: 1.158  loss_dice_4: 2.422  loss_bbox_4: 2.718  loss_giou_4: 1.631  loss_ce_dn_4: 1.765  loss_mask_dn_4: 0.9982  loss_dice_dn_4: 2.003  loss_bbox_dn_4: 0.8063  loss_giou_dn_4: 0.7243  loss_ce_5: 1.766  loss_mask_5: 1.173  loss_dice_5: 2.403  loss_bbox_5: 2.693  loss_giou_5: 1.612  loss_ce_dn_5: 1.806  loss_mask_dn_5: 0.9767  loss_dice_dn_5: 1.998  loss_bbox_dn_5: 0.7918  loss_giou_dn_5: 0.7152  loss_ce_6: 1.759  loss_mask_6: 1.158  loss_dice_6: 2.405  loss_bbox_6: 2.692  loss_giou_6: 1.601  loss_ce_dn_6: 1.876  loss_mask_dn_6: 0.981  loss_dice_dn_6: 2.017  loss_bbox_dn_6: 0.7943  loss_giou_dn_6: 0.7107  loss_ce_7: 1.748  loss_mask_7: 1.196  loss_dice_7: 2.392  loss_bbox_7: 2.687  loss_giou_7: 1.592  loss_ce_dn_7: 1.874  loss_mask_dn_7: 0.9864  loss_dice_dn_7: 2.046  loss_bbox_dn_7: 0.7908  loss_giou_dn_7: 0.6971  loss_ce_8: 1.771  loss_mask_8: 1.175  loss_dice_8: 2.391  loss_bbox_8: 2.692  loss_giou_8: 1.594  loss_ce_dn_8: 1.879  loss_mask_dn_8: 1.025  loss_dice_dn_8: 2.038  loss_bbox_dn_8: 0.7912  loss_giou_dn_8: 0.6996    time: 2.2733  last_time: 2.2237  data_time: 0.0101  last_data_time: 0.0049   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:34:07 d2.utils.events]: \u001b[0m eta: 8 days, 6:04:24  iter: 5359  total_loss: 159.4  loss_ce: 1.267  loss_mask: 1.456  loss_dice: 2.098  loss_bbox: 2.645  loss_giou: 1.556  loss_ce_dn: 1.648  loss_mask_dn: 1.263  loss_dice_dn: 1.974  loss_bbox_dn: 0.7943  loss_giou_dn: 0.6501  loss_ce_0: 4.775  loss_mask_0: 1.504  loss_dice_0: 2.496  loss_bbox_0: 4  loss_giou_0: 1.919  loss_ce_1: 1.621  loss_mask_1: 1.374  loss_dice_1: 2.204  loss_bbox_1: 2.772  loss_giou_1: 1.665  loss_ce_dn_1: 2.504  loss_mask_dn_1: 1.395  loss_dice_dn_1: 2.115  loss_bbox_dn_1: 0.9605  loss_giou_dn_1: 0.7388  loss_ce_2: 1.454  loss_mask_2: 1.38  loss_dice_2: 2.118  loss_bbox_2: 2.646  loss_giou_2: 1.641  loss_ce_dn_2: 1.925  loss_mask_dn_2: 1.354  loss_dice_dn_2: 2.02  loss_bbox_dn_2: 0.9211  loss_giou_dn_2: 0.7136  loss_ce_3: 1.327  loss_mask_3: 1.428  loss_dice_3: 2.131  loss_bbox_3: 2.637  loss_giou_3: 1.628  loss_ce_dn_3: 1.763  loss_mask_dn_3: 1.34  loss_dice_dn_3: 2.033  loss_bbox_dn_3: 0.8629  loss_giou_dn_3: 0.6894  loss_ce_4: 1.332  loss_mask_4: 1.425  loss_dice_4: 2.119  loss_bbox_4: 2.644  loss_giou_4: 1.589  loss_ce_dn_4: 1.652  loss_mask_dn_4: 1.285  loss_dice_dn_4: 1.986  loss_bbox_dn_4: 0.8262  loss_giou_dn_4: 0.667  loss_ce_5: 1.353  loss_mask_5: 1.448  loss_dice_5: 2.096  loss_bbox_5: 2.655  loss_giou_5: 1.595  loss_ce_dn_5: 1.517  loss_mask_dn_5: 1.273  loss_dice_dn_5: 1.941  loss_bbox_dn_5: 0.8104  loss_giou_dn_5: 0.6607  loss_ce_6: 1.303  loss_mask_6: 1.458  loss_dice_6: 2.055  loss_bbox_6: 2.642  loss_giou_6: 1.591  loss_ce_dn_6: 1.549  loss_mask_dn_6: 1.278  loss_dice_dn_6: 1.941  loss_bbox_dn_6: 0.8131  loss_giou_dn_6: 0.658  loss_ce_7: 1.308  loss_mask_7: 1.464  loss_dice_7: 2.065  loss_bbox_7: 2.62  loss_giou_7: 1.562  loss_ce_dn_7: 1.541  loss_mask_dn_7: 1.246  loss_dice_dn_7: 1.955  loss_bbox_dn_7: 0.798  loss_giou_dn_7: 0.6499  loss_ce_8: 1.276  loss_mask_8: 1.461  loss_dice_8: 2.089  loss_bbox_8: 2.643  loss_giou_8: 1.558  loss_ce_dn_8: 1.521  loss_mask_dn_8: 1.245  loss_dice_dn_8: 1.957  loss_bbox_dn_8: 0.7957  loss_giou_dn_8: 0.6515    time: 2.2733  last_time: 2.2657  data_time: 0.0127  last_data_time: 0.0108   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:34:52 d2.utils.events]: \u001b[0m eta: 8 days, 6:02:31  iter: 5379  total_loss: 159.3  loss_ce: 1.45  loss_mask: 1.445  loss_dice: 2.33  loss_bbox: 2.585  loss_giou: 1.396  loss_ce_dn: 1.514  loss_mask_dn: 1.119  loss_dice_dn: 1.989  loss_bbox_dn: 0.7499  loss_giou_dn: 0.6832  loss_ce_0: 4.538  loss_mask_0: 1.421  loss_dice_0: 2.583  loss_bbox_0: 4.377  loss_giou_0: 2.005  loss_ce_1: 1.573  loss_mask_1: 1.463  loss_dice_1: 2.405  loss_bbox_1: 2.892  loss_giou_1: 1.546  loss_ce_dn_1: 2.079  loss_mask_dn_1: 1.173  loss_dice_dn_1: 2.151  loss_bbox_dn_1: 0.9173  loss_giou_dn_1: 0.76  loss_ce_2: 1.403  loss_mask_2: 1.424  loss_dice_2: 2.331  loss_bbox_2: 2.875  loss_giou_2: 1.505  loss_ce_dn_2: 1.77  loss_mask_dn_2: 1.189  loss_dice_dn_2: 1.984  loss_bbox_dn_2: 0.8704  loss_giou_dn_2: 0.7398  loss_ce_3: 1.293  loss_mask_3: 1.459  loss_dice_3: 2.344  loss_bbox_3: 2.751  loss_giou_3: 1.494  loss_ce_dn_3: 1.627  loss_mask_dn_3: 1.131  loss_dice_dn_3: 1.986  loss_bbox_dn_3: 0.8062  loss_giou_dn_3: 0.7053  loss_ce_4: 1.241  loss_mask_4: 1.403  loss_dice_4: 2.352  loss_bbox_4: 2.66  loss_giou_4: 1.467  loss_ce_dn_4: 1.538  loss_mask_dn_4: 1.131  loss_dice_dn_4: 1.985  loss_bbox_dn_4: 0.7694  loss_giou_dn_4: 0.7009  loss_ce_5: 1.311  loss_mask_5: 1.498  loss_dice_5: 2.364  loss_bbox_5: 2.587  loss_giou_5: 1.43  loss_ce_dn_5: 1.446  loss_mask_dn_5: 1.111  loss_dice_dn_5: 1.988  loss_bbox_dn_5: 0.7498  loss_giou_dn_5: 0.6964  loss_ce_6: 1.288  loss_mask_6: 1.451  loss_dice_6: 2.346  loss_bbox_6: 2.601  loss_giou_6: 1.431  loss_ce_dn_6: 1.458  loss_mask_dn_6: 1.134  loss_dice_dn_6: 1.97  loss_bbox_dn_6: 0.7535  loss_giou_dn_6: 0.6972  loss_ce_7: 1.358  loss_mask_7: 1.477  loss_dice_7: 2.352  loss_bbox_7: 2.571  loss_giou_7: 1.399  loss_ce_dn_7: 1.495  loss_mask_dn_7: 1.154  loss_dice_dn_7: 1.983  loss_bbox_dn_7: 0.743  loss_giou_dn_7: 0.6784  loss_ce_8: 1.395  loss_mask_8: 1.44  loss_dice_8: 2.347  loss_bbox_8: 2.585  loss_giou_8: 1.398  loss_ce_dn_8: 1.492  loss_mask_dn_8: 1.133  loss_dice_dn_8: 1.996  loss_bbox_dn_8: 0.7467  loss_giou_dn_8: 0.6832    time: 2.2732  last_time: 2.2711  data_time: 0.0107  last_data_time: 0.0109   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:35:38 d2.utils.events]: \u001b[0m eta: 8 days, 5:58:48  iter: 5399  total_loss: 170.6  loss_ce: 1.647  loss_mask: 1.702  loss_dice: 2.18  loss_bbox: 2.492  loss_giou: 1.439  loss_ce_dn: 2.453  loss_mask_dn: 1.496  loss_dice_dn: 1.895  loss_bbox_dn: 0.8565  loss_giou_dn: 0.6775  loss_ce_0: 5.104  loss_mask_0: 1.445  loss_dice_0: 2.317  loss_bbox_0: 3.933  loss_giou_0: 1.932  loss_ce_1: 2.073  loss_mask_1: 1.595  loss_dice_1: 2.159  loss_bbox_1: 2.523  loss_giou_1: 1.643  loss_ce_dn_1: 2.804  loss_mask_dn_1: 1.496  loss_dice_dn_1: 2.013  loss_bbox_dn_1: 1.04  loss_giou_dn_1: 0.7505  loss_ce_2: 1.647  loss_mask_2: 1.602  loss_dice_2: 2.132  loss_bbox_2: 2.593  loss_giou_2: 1.557  loss_ce_dn_2: 2.282  loss_mask_dn_2: 1.4  loss_dice_dn_2: 1.91  loss_bbox_dn_2: 0.9728  loss_giou_dn_2: 0.721  loss_ce_3: 1.658  loss_mask_3: 1.615  loss_dice_3: 2.165  loss_bbox_3: 2.562  loss_giou_3: 1.539  loss_ce_dn_3: 2.182  loss_mask_dn_3: 1.436  loss_dice_dn_3: 1.9  loss_bbox_dn_3: 0.9263  loss_giou_dn_3: 0.6997  loss_ce_4: 1.648  loss_mask_4: 1.636  loss_dice_4: 2.169  loss_bbox_4: 2.487  loss_giou_4: 1.495  loss_ce_dn_4: 2.139  loss_mask_dn_4: 1.441  loss_dice_dn_4: 1.895  loss_bbox_dn_4: 0.898  loss_giou_dn_4: 0.6869  loss_ce_5: 1.605  loss_mask_5: 1.659  loss_dice_5: 2.12  loss_bbox_5: 2.479  loss_giou_5: 1.459  loss_ce_dn_5: 2.201  loss_mask_dn_5: 1.475  loss_dice_dn_5: 1.889  loss_bbox_dn_5: 0.8832  loss_giou_dn_5: 0.675  loss_ce_6: 1.617  loss_mask_6: 1.635  loss_dice_6: 2.13  loss_bbox_6: 2.472  loss_giou_6: 1.444  loss_ce_dn_6: 2.361  loss_mask_dn_6: 1.489  loss_dice_dn_6: 1.878  loss_bbox_dn_6: 0.8812  loss_giou_dn_6: 0.6774  loss_ce_7: 1.694  loss_mask_7: 1.657  loss_dice_7: 2.141  loss_bbox_7: 2.461  loss_giou_7: 1.437  loss_ce_dn_7: 2.262  loss_mask_dn_7: 1.475  loss_dice_dn_7: 1.908  loss_bbox_dn_7: 0.8554  loss_giou_dn_7: 0.6728  loss_ce_8: 1.683  loss_mask_8: 1.692  loss_dice_8: 2.159  loss_bbox_8: 2.465  loss_giou_8: 1.435  loss_ce_dn_8: 2.421  loss_mask_dn_8: 1.474  loss_dice_dn_8: 1.899  loss_bbox_dn_8: 0.8562  loss_giou_dn_8: 0.6762    time: 2.2732  last_time: 2.2491  data_time: 0.0105  last_data_time: 0.0062   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:36:22 d2.utils.events]: \u001b[0m eta: 8 days, 5:56:23  iter: 5419  total_loss: 167.6  loss_ce: 1.492  loss_mask: 1.601  loss_dice: 2.328  loss_bbox: 2.689  loss_giou: 1.456  loss_ce_dn: 2.548  loss_mask_dn: 1.424  loss_dice_dn: 1.83  loss_bbox_dn: 0.7798  loss_giou_dn: 0.6257  loss_ce_0: 5.355  loss_mask_0: 1.488  loss_dice_0: 2.579  loss_bbox_0: 3.867  loss_giou_0: 1.863  loss_ce_1: 1.784  loss_mask_1: 1.594  loss_dice_1: 2.463  loss_bbox_1: 3.202  loss_giou_1: 1.593  loss_ce_dn_1: 2.769  loss_mask_dn_1: 1.527  loss_dice_dn_1: 1.894  loss_bbox_dn_1: 1.007  loss_giou_dn_1: 0.7309  loss_ce_2: 1.57  loss_mask_2: 1.562  loss_dice_2: 2.333  loss_bbox_2: 2.913  loss_giou_2: 1.574  loss_ce_dn_2: 2.485  loss_mask_dn_2: 1.477  loss_dice_dn_2: 1.786  loss_bbox_dn_2: 0.9209  loss_giou_dn_2: 0.7041  loss_ce_3: 1.38  loss_mask_3: 1.566  loss_dice_3: 2.325  loss_bbox_3: 2.867  loss_giou_3: 1.544  loss_ce_dn_3: 2.334  loss_mask_dn_3: 1.375  loss_dice_dn_3: 1.785  loss_bbox_dn_3: 0.8602  loss_giou_dn_3: 0.6724  loss_ce_4: 1.342  loss_mask_4: 1.583  loss_dice_4: 2.328  loss_bbox_4: 2.771  loss_giou_4: 1.554  loss_ce_dn_4: 2.225  loss_mask_dn_4: 1.382  loss_dice_dn_4: 1.759  loss_bbox_dn_4: 0.8272  loss_giou_dn_4: 0.6617  loss_ce_5: 1.341  loss_mask_5: 1.661  loss_dice_5: 2.288  loss_bbox_5: 2.666  loss_giou_5: 1.515  loss_ce_dn_5: 2.238  loss_mask_dn_5: 1.403  loss_dice_dn_5: 1.785  loss_bbox_dn_5: 0.8046  loss_giou_dn_5: 0.6469  loss_ce_6: 1.403  loss_mask_6: 1.593  loss_dice_6: 2.258  loss_bbox_6: 2.648  loss_giou_6: 1.508  loss_ce_dn_6: 2.296  loss_mask_dn_6: 1.424  loss_dice_dn_6: 1.772  loss_bbox_dn_6: 0.7983  loss_giou_dn_6: 0.6414  loss_ce_7: 1.417  loss_mask_7: 1.629  loss_dice_7: 2.273  loss_bbox_7: 2.684  loss_giou_7: 1.45  loss_ce_dn_7: 2.307  loss_mask_dn_7: 1.411  loss_dice_dn_7: 1.791  loss_bbox_dn_7: 0.7781  loss_giou_dn_7: 0.6235  loss_ce_8: 1.473  loss_mask_8: 1.595  loss_dice_8: 2.258  loss_bbox_8: 2.693  loss_giou_8: 1.449  loss_ce_dn_8: 2.392  loss_mask_dn_8: 1.431  loss_dice_dn_8: 1.809  loss_bbox_dn_8: 0.7774  loss_giou_dn_8: 0.6228    time: 2.2731  last_time: 2.2487  data_time: 0.0097  last_data_time: 0.0122   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:37:07 d2.utils.events]: \u001b[0m eta: 8 days, 5:53:12  iter: 5439  total_loss: 161  loss_ce: 1.487  loss_mask: 1.574  loss_dice: 2.363  loss_bbox: 2.533  loss_giou: 1.645  loss_ce_dn: 1.818  loss_mask_dn: 1.287  loss_dice_dn: 2.052  loss_bbox_dn: 0.7787  loss_giou_dn: 0.7122  loss_ce_0: 4.659  loss_mask_0: 1.475  loss_dice_0: 2.589  loss_bbox_0: 4.261  loss_giou_0: 2.004  loss_ce_1: 1.764  loss_mask_1: 1.56  loss_dice_1: 2.431  loss_bbox_1: 2.973  loss_giou_1: 1.77  loss_ce_dn_1: 2.188  loss_mask_dn_1: 1.328  loss_dice_dn_1: 2.278  loss_bbox_dn_1: 0.8931  loss_giou_dn_1: 0.7613  loss_ce_2: 1.706  loss_mask_2: 1.561  loss_dice_2: 2.419  loss_bbox_2: 2.718  loss_giou_2: 1.681  loss_ce_dn_2: 1.771  loss_mask_dn_2: 1.28  loss_dice_dn_2: 2.13  loss_bbox_dn_2: 0.8164  loss_giou_dn_2: 0.7353  loss_ce_3: 1.691  loss_mask_3: 1.555  loss_dice_3: 2.342  loss_bbox_3: 2.531  loss_giou_3: 1.662  loss_ce_dn_3: 1.619  loss_mask_dn_3: 1.303  loss_dice_dn_3: 2.097  loss_bbox_dn_3: 0.7929  loss_giou_dn_3: 0.7237  loss_ce_4: 1.641  loss_mask_4: 1.543  loss_dice_4: 2.328  loss_bbox_4: 2.47  loss_giou_4: 1.66  loss_ce_dn_4: 1.68  loss_mask_dn_4: 1.253  loss_dice_dn_4: 2.097  loss_bbox_dn_4: 0.7877  loss_giou_dn_4: 0.7096  loss_ce_5: 1.584  loss_mask_5: 1.529  loss_dice_5: 2.352  loss_bbox_5: 2.437  loss_giou_5: 1.707  loss_ce_dn_5: 1.607  loss_mask_dn_5: 1.282  loss_dice_dn_5: 2.051  loss_bbox_dn_5: 0.7828  loss_giou_dn_5: 0.7098  loss_ce_6: 1.437  loss_mask_6: 1.569  loss_dice_6: 2.386  loss_bbox_6: 2.476  loss_giou_6: 1.683  loss_ce_dn_6: 1.619  loss_mask_dn_6: 1.288  loss_dice_dn_6: 2.07  loss_bbox_dn_6: 0.781  loss_giou_dn_6: 0.7137  loss_ce_7: 1.46  loss_mask_7: 1.547  loss_dice_7: 2.358  loss_bbox_7: 2.534  loss_giou_7: 1.682  loss_ce_dn_7: 1.672  loss_mask_dn_7: 1.286  loss_dice_dn_7: 2.042  loss_bbox_dn_7: 0.7719  loss_giou_dn_7: 0.7089  loss_ce_8: 1.462  loss_mask_8: 1.55  loss_dice_8: 2.382  loss_bbox_8: 2.536  loss_giou_8: 1.671  loss_ce_dn_8: 1.788  loss_mask_dn_8: 1.29  loss_dice_dn_8: 2.058  loss_bbox_dn_8: 0.7745  loss_giou_dn_8: 0.7096    time: 2.2730  last_time: 2.2312  data_time: 0.0091  last_data_time: 0.0081   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:37:52 d2.utils.events]: \u001b[0m eta: 8 days, 5:51:43  iter: 5459  total_loss: 161.1  loss_ce: 1.577  loss_mask: 1.57  loss_dice: 2.217  loss_bbox: 2.645  loss_giou: 1.51  loss_ce_dn: 1.804  loss_mask_dn: 1.389  loss_dice_dn: 1.896  loss_bbox_dn: 0.875  loss_giou_dn: 0.7045  loss_ce_0: 4.677  loss_mask_0: 1.359  loss_dice_0: 2.341  loss_bbox_0: 4.151  loss_giou_0: 1.964  loss_ce_1: 1.751  loss_mask_1: 1.475  loss_dice_1: 2.209  loss_bbox_1: 2.931  loss_giou_1: 1.677  loss_ce_dn_1: 2.238  loss_mask_dn_1: 1.481  loss_dice_dn_1: 2.045  loss_bbox_dn_1: 0.9503  loss_giou_dn_1: 0.7525  loss_ce_2: 1.531  loss_mask_2: 1.448  loss_dice_2: 2.169  loss_bbox_2: 2.847  loss_giou_2: 1.637  loss_ce_dn_2: 1.837  loss_mask_dn_2: 1.435  loss_dice_dn_2: 1.938  loss_bbox_dn_2: 0.9015  loss_giou_dn_2: 0.7385  loss_ce_3: 1.526  loss_mask_3: 1.429  loss_dice_3: 2.195  loss_bbox_3: 2.715  loss_giou_3: 1.574  loss_ce_dn_3: 1.769  loss_mask_dn_3: 1.415  loss_dice_dn_3: 1.89  loss_bbox_dn_3: 0.8898  loss_giou_dn_3: 0.7205  loss_ce_4: 1.492  loss_mask_4: 1.477  loss_dice_4: 2.206  loss_bbox_4: 2.703  loss_giou_4: 1.547  loss_ce_dn_4: 1.744  loss_mask_dn_4: 1.385  loss_dice_dn_4: 1.915  loss_bbox_dn_4: 0.8697  loss_giou_dn_4: 0.7097  loss_ce_5: 1.606  loss_mask_5: 1.475  loss_dice_5: 2.157  loss_bbox_5: 2.669  loss_giou_5: 1.535  loss_ce_dn_5: 1.756  loss_mask_dn_5: 1.351  loss_dice_dn_5: 1.875  loss_bbox_dn_5: 0.8575  loss_giou_dn_5: 0.704  loss_ce_6: 1.524  loss_mask_6: 1.515  loss_dice_6: 2.166  loss_bbox_6: 2.646  loss_giou_6: 1.541  loss_ce_dn_6: 1.674  loss_mask_dn_6: 1.378  loss_dice_dn_6: 1.871  loss_bbox_dn_6: 0.8615  loss_giou_dn_6: 0.7037  loss_ce_7: 1.512  loss_mask_7: 1.54  loss_dice_7: 2.183  loss_bbox_7: 2.634  loss_giou_7: 1.498  loss_ce_dn_7: 1.682  loss_mask_dn_7: 1.358  loss_dice_dn_7: 1.892  loss_bbox_dn_7: 0.8631  loss_giou_dn_7: 0.6957  loss_ce_8: 1.548  loss_mask_8: 1.539  loss_dice_8: 2.177  loss_bbox_8: 2.636  loss_giou_8: 1.5  loss_ce_dn_8: 1.771  loss_mask_dn_8: 1.348  loss_dice_dn_8: 1.888  loss_bbox_dn_8: 0.8653  loss_giou_dn_8: 0.7011    time: 2.2729  last_time: 2.2247  data_time: 0.0130  last_data_time: 0.0210   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:38:37 d2.utils.events]: \u001b[0m eta: 8 days, 5:46:16  iter: 5479  total_loss: 172.5  loss_ce: 1.635  loss_mask: 1.469  loss_dice: 2.238  loss_bbox: 2.518  loss_giou: 1.424  loss_ce_dn: 2.339  loss_mask_dn: 1.297  loss_dice_dn: 2.096  loss_bbox_dn: 0.7997  loss_giou_dn: 0.6379  loss_ce_0: 5.376  loss_mask_0: 1.444  loss_dice_0: 2.483  loss_bbox_0: 4.188  loss_giou_0: 1.99  loss_ce_1: 1.948  loss_mask_1: 1.426  loss_dice_1: 2.368  loss_bbox_1: 2.746  loss_giou_1: 1.602  loss_ce_dn_1: 2.597  loss_mask_dn_1: 1.321  loss_dice_dn_1: 2.241  loss_bbox_dn_1: 0.9459  loss_giou_dn_1: 0.7314  loss_ce_2: 1.723  loss_mask_2: 1.463  loss_dice_2: 2.272  loss_bbox_2: 2.549  loss_giou_2: 1.576  loss_ce_dn_2: 2.134  loss_mask_dn_2: 1.295  loss_dice_dn_2: 2.096  loss_bbox_dn_2: 0.9004  loss_giou_dn_2: 0.6987  loss_ce_3: 1.76  loss_mask_3: 1.463  loss_dice_3: 2.225  loss_bbox_3: 2.59  loss_giou_3: 1.547  loss_ce_dn_3: 2.036  loss_mask_dn_3: 1.247  loss_dice_dn_3: 2.068  loss_bbox_dn_3: 0.8578  loss_giou_dn_3: 0.6693  loss_ce_4: 1.724  loss_mask_4: 1.544  loss_dice_4: 2.217  loss_bbox_4: 2.556  loss_giou_4: 1.518  loss_ce_dn_4: 2.021  loss_mask_dn_4: 1.266  loss_dice_dn_4: 2.068  loss_bbox_dn_4: 0.8319  loss_giou_dn_4: 0.66  loss_ce_5: 1.706  loss_mask_5: 1.577  loss_dice_5: 2.234  loss_bbox_5: 2.549  loss_giou_5: 1.464  loss_ce_dn_5: 2.127  loss_mask_dn_5: 1.278  loss_dice_dn_5: 2.049  loss_bbox_dn_5: 0.8165  loss_giou_dn_5: 0.6482  loss_ce_6: 1.744  loss_mask_6: 1.543  loss_dice_6: 2.251  loss_bbox_6: 2.537  loss_giou_6: 1.455  loss_ce_dn_6: 2.142  loss_mask_dn_6: 1.296  loss_dice_dn_6: 2.079  loss_bbox_dn_6: 0.8107  loss_giou_dn_6: 0.6422  loss_ce_7: 1.795  loss_mask_7: 1.551  loss_dice_7: 2.217  loss_bbox_7: 2.526  loss_giou_7: 1.429  loss_ce_dn_7: 2.225  loss_mask_dn_7: 1.31  loss_dice_dn_7: 2.036  loss_bbox_dn_7: 0.8027  loss_giou_dn_7: 0.6354  loss_ce_8: 1.672  loss_mask_8: 1.497  loss_dice_8: 2.235  loss_bbox_8: 2.514  loss_giou_8: 1.43  loss_ce_dn_8: 2.315  loss_mask_dn_8: 1.304  loss_dice_dn_8: 2.094  loss_bbox_dn_8: 0.8012  loss_giou_dn_8: 0.6369    time: 2.2727  last_time: 2.1865  data_time: 0.0086  last_data_time: 0.0097   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:39:22 d2.utils.events]: \u001b[0m eta: 8 days, 5:43:33  iter: 5499  total_loss: 168.2  loss_ce: 1.45  loss_mask: 1.543  loss_dice: 2.502  loss_bbox: 2.439  loss_giou: 1.539  loss_ce_dn: 2.285  loss_mask_dn: 1.368  loss_dice_dn: 2.355  loss_bbox_dn: 0.855  loss_giou_dn: 0.6809  loss_ce_0: 4.918  loss_mask_0: 1.424  loss_dice_0: 2.783  loss_bbox_0: 3.922  loss_giou_0: 1.969  loss_ce_1: 1.968  loss_mask_1: 1.544  loss_dice_1: 2.544  loss_bbox_1: 2.667  loss_giou_1: 1.728  loss_ce_dn_1: 2.427  loss_mask_dn_1: 1.436  loss_dice_dn_1: 2.517  loss_bbox_dn_1: 0.9345  loss_giou_dn_1: 0.7422  loss_ce_2: 1.793  loss_mask_2: 1.573  loss_dice_2: 2.499  loss_bbox_2: 2.594  loss_giou_2: 1.689  loss_ce_dn_2: 1.898  loss_mask_dn_2: 1.382  loss_dice_dn_2: 2.378  loss_bbox_dn_2: 0.8992  loss_giou_dn_2: 0.7181  loss_ce_3: 1.624  loss_mask_3: 1.69  loss_dice_3: 2.489  loss_bbox_3: 2.55  loss_giou_3: 1.608  loss_ce_dn_3: 1.892  loss_mask_dn_3: 1.409  loss_dice_dn_3: 2.343  loss_bbox_dn_3: 0.8718  loss_giou_dn_3: 0.6927  loss_ce_4: 1.609  loss_mask_4: 1.554  loss_dice_4: 2.446  loss_bbox_4: 2.554  loss_giou_4: 1.645  loss_ce_dn_4: 1.79  loss_mask_dn_4: 1.419  loss_dice_dn_4: 2.337  loss_bbox_dn_4: 0.8648  loss_giou_dn_4: 0.6897  loss_ce_5: 1.455  loss_mask_5: 1.486  loss_dice_5: 2.473  loss_bbox_5: 2.507  loss_giou_5: 1.568  loss_ce_dn_5: 1.795  loss_mask_dn_5: 1.364  loss_dice_dn_5: 2.355  loss_bbox_dn_5: 0.8591  loss_giou_dn_5: 0.6887  loss_ce_6: 1.382  loss_mask_6: 1.488  loss_dice_6: 2.526  loss_bbox_6: 2.493  loss_giou_6: 1.557  loss_ce_dn_6: 1.811  loss_mask_dn_6: 1.364  loss_dice_dn_6: 2.359  loss_bbox_dn_6: 0.8601  loss_giou_dn_6: 0.6911  loss_ce_7: 1.417  loss_mask_7: 1.521  loss_dice_7: 2.533  loss_bbox_7: 2.439  loss_giou_7: 1.552  loss_ce_dn_7: 1.787  loss_mask_dn_7: 1.372  loss_dice_dn_7: 2.348  loss_bbox_dn_7: 0.8482  loss_giou_dn_7: 0.6793  loss_ce_8: 1.427  loss_mask_8: 1.508  loss_dice_8: 2.548  loss_bbox_8: 2.442  loss_giou_8: 1.548  loss_ce_dn_8: 1.857  loss_mask_dn_8: 1.383  loss_dice_dn_8: 2.315  loss_bbox_dn_8: 0.8525  loss_giou_dn_8: 0.6781    time: 2.2726  last_time: 2.2496  data_time: 0.0108  last_data_time: 0.0043   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:40:06 d2.utils.events]: \u001b[0m eta: 8 days, 5:39:46  iter: 5519  total_loss: 163.5  loss_ce: 1.648  loss_mask: 1.317  loss_dice: 2.413  loss_bbox: 2.515  loss_giou: 1.53  loss_ce_dn: 1.838  loss_mask_dn: 1.206  loss_dice_dn: 1.976  loss_bbox_dn: 0.6904  loss_giou_dn: 0.658  loss_ce_0: 4.578  loss_mask_0: 1.276  loss_dice_0: 2.602  loss_bbox_0: 4.269  loss_giou_0: 2.071  loss_ce_1: 1.786  loss_mask_1: 1.262  loss_dice_1: 2.318  loss_bbox_1: 2.629  loss_giou_1: 1.686  loss_ce_dn_1: 2.164  loss_mask_dn_1: 1.259  loss_dice_dn_1: 2.127  loss_bbox_dn_1: 0.8252  loss_giou_dn_1: 0.7387  loss_ce_2: 1.776  loss_mask_2: 1.259  loss_dice_2: 2.274  loss_bbox_2: 2.542  loss_giou_2: 1.621  loss_ce_dn_2: 1.853  loss_mask_dn_2: 1.197  loss_dice_dn_2: 1.98  loss_bbox_dn_2: 0.7911  loss_giou_dn_2: 0.7151  loss_ce_3: 1.666  loss_mask_3: 1.261  loss_dice_3: 2.263  loss_bbox_3: 2.476  loss_giou_3: 1.616  loss_ce_dn_3: 1.791  loss_mask_dn_3: 1.184  loss_dice_dn_3: 1.944  loss_bbox_dn_3: 0.7545  loss_giou_dn_3: 0.6938  loss_ce_4: 1.53  loss_mask_4: 1.272  loss_dice_4: 2.291  loss_bbox_4: 2.476  loss_giou_4: 1.583  loss_ce_dn_4: 1.709  loss_mask_dn_4: 1.179  loss_dice_dn_4: 1.958  loss_bbox_dn_4: 0.7263  loss_giou_dn_4: 0.6786  loss_ce_5: 1.507  loss_mask_5: 1.277  loss_dice_5: 2.325  loss_bbox_5: 2.457  loss_giou_5: 1.561  loss_ce_dn_5: 1.694  loss_mask_dn_5: 1.13  loss_dice_dn_5: 1.947  loss_bbox_dn_5: 0.7075  loss_giou_dn_5: 0.668  loss_ce_6: 1.564  loss_mask_6: 1.31  loss_dice_6: 2.325  loss_bbox_6: 2.484  loss_giou_6: 1.554  loss_ce_dn_6: 1.695  loss_mask_dn_6: 1.196  loss_dice_dn_6: 1.916  loss_bbox_dn_6: 0.7047  loss_giou_dn_6: 0.6647  loss_ce_7: 1.596  loss_mask_7: 1.307  loss_dice_7: 2.367  loss_bbox_7: 2.52  loss_giou_7: 1.537  loss_ce_dn_7: 1.668  loss_mask_dn_7: 1.171  loss_dice_dn_7: 1.957  loss_bbox_dn_7: 0.6896  loss_giou_dn_7: 0.6579  loss_ce_8: 1.608  loss_mask_8: 1.262  loss_dice_8: 2.346  loss_bbox_8: 2.534  loss_giou_8: 1.536  loss_ce_dn_8: 1.798  loss_mask_dn_8: 1.184  loss_dice_dn_8: 1.965  loss_bbox_dn_8: 0.6895  loss_giou_dn_8: 0.6571    time: 2.2724  last_time: 2.2075  data_time: 0.0101  last_data_time: 0.0111   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:40:51 d2.utils.events]: \u001b[0m eta: 8 days, 5:31:56  iter: 5539  total_loss: 179.6  loss_ce: 1.737  loss_mask: 1.77  loss_dice: 2.621  loss_bbox: 2.751  loss_giou: 1.582  loss_ce_dn: 2.479  loss_mask_dn: 1.56  loss_dice_dn: 2.152  loss_bbox_dn: 0.8263  loss_giou_dn: 0.6767  loss_ce_0: 4.865  loss_mask_0: 1.743  loss_dice_0: 2.639  loss_bbox_0: 4.158  loss_giou_0: 1.971  loss_ce_1: 1.927  loss_mask_1: 1.754  loss_dice_1: 2.532  loss_bbox_1: 3.048  loss_giou_1: 1.663  loss_ce_dn_1: 2.732  loss_mask_dn_1: 1.585  loss_dice_dn_1: 2.348  loss_bbox_dn_1: 0.9503  loss_giou_dn_1: 0.7579  loss_ce_2: 1.803  loss_mask_2: 1.81  loss_dice_2: 2.572  loss_bbox_2: 2.773  loss_giou_2: 1.652  loss_ce_dn_2: 2.241  loss_mask_dn_2: 1.48  loss_dice_dn_2: 2.257  loss_bbox_dn_2: 0.8945  loss_giou_dn_2: 0.7283  loss_ce_3: 1.579  loss_mask_3: 1.833  loss_dice_3: 2.578  loss_bbox_3: 2.757  loss_giou_3: 1.659  loss_ce_dn_3: 2.115  loss_mask_dn_3: 1.512  loss_dice_dn_3: 2.209  loss_bbox_dn_3: 0.8607  loss_giou_dn_3: 0.7145  loss_ce_4: 1.716  loss_mask_4: 1.779  loss_dice_4: 2.555  loss_bbox_4: 2.727  loss_giou_4: 1.662  loss_ce_dn_4: 2.143  loss_mask_dn_4: 1.506  loss_dice_dn_4: 2.197  loss_bbox_dn_4: 0.8466  loss_giou_dn_4: 0.7065  loss_ce_5: 1.661  loss_mask_5: 1.768  loss_dice_5: 2.555  loss_bbox_5: 2.672  loss_giou_5: 1.621  loss_ce_dn_5: 2.124  loss_mask_dn_5: 1.51  loss_dice_dn_5: 2.188  loss_bbox_dn_5: 0.8355  loss_giou_dn_5: 0.6942  loss_ce_6: 1.666  loss_mask_6: 1.689  loss_dice_6: 2.552  loss_bbox_6: 2.686  loss_giou_6: 1.605  loss_ce_dn_6: 2.222  loss_mask_dn_6: 1.498  loss_dice_dn_6: 2.157  loss_bbox_dn_6: 0.8305  loss_giou_dn_6: 0.6885  loss_ce_7: 1.699  loss_mask_7: 1.764  loss_dice_7: 2.549  loss_bbox_7: 2.745  loss_giou_7: 1.581  loss_ce_dn_7: 2.357  loss_mask_dn_7: 1.482  loss_dice_dn_7: 2.132  loss_bbox_dn_7: 0.827  loss_giou_dn_7: 0.676  loss_ce_8: 1.758  loss_mask_8: 1.745  loss_dice_8: 2.575  loss_bbox_8: 2.751  loss_giou_8: 1.58  loss_ce_dn_8: 2.45  loss_mask_dn_8: 1.531  loss_dice_dn_8: 2.143  loss_bbox_dn_8: 0.8284  loss_giou_dn_8: 0.6734    time: 2.2722  last_time: 2.1877  data_time: 0.0100  last_data_time: 0.0070   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:41:35 d2.utils.events]: \u001b[0m eta: 8 days, 5:23:17  iter: 5559  total_loss: 176.8  loss_ce: 1.431  loss_mask: 1.521  loss_dice: 2.346  loss_bbox: 2.936  loss_giou: 1.431  loss_ce_dn: 2.109  loss_mask_dn: 1.458  loss_dice_dn: 2.213  loss_bbox_dn: 0.8124  loss_giou_dn: 0.6474  loss_ce_0: 4.609  loss_mask_0: 1.51  loss_dice_0: 2.456  loss_bbox_0: 4.222  loss_giou_0: 1.994  loss_ce_1: 1.592  loss_mask_1: 1.511  loss_dice_1: 2.46  loss_bbox_1: 3.256  loss_giou_1: 1.536  loss_ce_dn_1: 2.471  loss_mask_dn_1: 1.442  loss_dice_dn_1: 2.255  loss_bbox_dn_1: 0.9526  loss_giou_dn_1: 0.7194  loss_ce_2: 1.481  loss_mask_2: 1.562  loss_dice_2: 2.391  loss_bbox_2: 3.12  loss_giou_2: 1.472  loss_ce_dn_2: 2.125  loss_mask_dn_2: 1.382  loss_dice_dn_2: 2.122  loss_bbox_dn_2: 0.9192  loss_giou_dn_2: 0.6919  loss_ce_3: 1.465  loss_mask_3: 1.501  loss_dice_3: 2.322  loss_bbox_3: 3.024  loss_giou_3: 1.445  loss_ce_dn_3: 2.027  loss_mask_dn_3: 1.455  loss_dice_dn_3: 2.165  loss_bbox_dn_3: 0.8771  loss_giou_dn_3: 0.6666  loss_ce_4: 1.386  loss_mask_4: 1.57  loss_dice_4: 2.325  loss_bbox_4: 3.033  loss_giou_4: 1.443  loss_ce_dn_4: 1.998  loss_mask_dn_4: 1.455  loss_dice_dn_4: 2.135  loss_bbox_dn_4: 0.8452  loss_giou_dn_4: 0.6545  loss_ce_5: 1.387  loss_mask_5: 1.595  loss_dice_5: 2.358  loss_bbox_5: 3.058  loss_giou_5: 1.445  loss_ce_dn_5: 1.982  loss_mask_dn_5: 1.461  loss_dice_dn_5: 2.17  loss_bbox_dn_5: 0.8241  loss_giou_dn_5: 0.6469  loss_ce_6: 1.403  loss_mask_6: 1.568  loss_dice_6: 2.331  loss_bbox_6: 3.056  loss_giou_6: 1.444  loss_ce_dn_6: 2.04  loss_mask_dn_6: 1.458  loss_dice_dn_6: 2.201  loss_bbox_dn_6: 0.8206  loss_giou_dn_6: 0.6402  loss_ce_7: 1.377  loss_mask_7: 1.516  loss_dice_7: 2.327  loss_bbox_7: 2.933  loss_giou_7: 1.421  loss_ce_dn_7: 2.062  loss_mask_dn_7: 1.426  loss_dice_dn_7: 2.175  loss_bbox_dn_7: 0.8096  loss_giou_dn_7: 0.6367  loss_ce_8: 1.412  loss_mask_8: 1.5  loss_dice_8: 2.297  loss_bbox_8: 2.928  loss_giou_8: 1.429  loss_ce_dn_8: 2.119  loss_mask_dn_8: 1.443  loss_dice_dn_8: 2.212  loss_bbox_dn_8: 0.8098  loss_giou_dn_8: 0.6421    time: 2.2721  last_time: 2.2591  data_time: 0.0101  last_data_time: 0.0224   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:42:20 d2.utils.events]: \u001b[0m eta: 8 days, 5:18:41  iter: 5579  total_loss: 166  loss_ce: 1.314  loss_mask: 1.597  loss_dice: 2.373  loss_bbox: 2.686  loss_giou: 1.476  loss_ce_dn: 1.525  loss_mask_dn: 1.372  loss_dice_dn: 1.978  loss_bbox_dn: 0.8654  loss_giou_dn: 0.6605  loss_ce_0: 4.469  loss_mask_0: 1.451  loss_dice_0: 2.484  loss_bbox_0: 4.047  loss_giou_0: 1.892  loss_ce_1: 1.699  loss_mask_1: 1.605  loss_dice_1: 2.336  loss_bbox_1: 3.021  loss_giou_1: 1.684  loss_ce_dn_1: 2.041  loss_mask_dn_1: 1.426  loss_dice_dn_1: 2.152  loss_bbox_dn_1: 0.9672  loss_giou_dn_1: 0.7294  loss_ce_2: 1.381  loss_mask_2: 1.596  loss_dice_2: 2.324  loss_bbox_2: 2.9  loss_giou_2: 1.599  loss_ce_dn_2: 1.685  loss_mask_dn_2: 1.457  loss_dice_dn_2: 2.062  loss_bbox_dn_2: 0.9303  loss_giou_dn_2: 0.7063  loss_ce_3: 1.291  loss_mask_3: 1.612  loss_dice_3: 2.315  loss_bbox_3: 2.815  loss_giou_3: 1.548  loss_ce_dn_3: 1.576  loss_mask_dn_3: 1.352  loss_dice_dn_3: 2.008  loss_bbox_dn_3: 0.8976  loss_giou_dn_3: 0.6801  loss_ce_4: 1.292  loss_mask_4: 1.645  loss_dice_4: 2.346  loss_bbox_4: 2.753  loss_giou_4: 1.513  loss_ce_dn_4: 1.507  loss_mask_dn_4: 1.348  loss_dice_dn_4: 1.997  loss_bbox_dn_4: 0.8786  loss_giou_dn_4: 0.6725  loss_ce_5: 1.355  loss_mask_5: 1.594  loss_dice_5: 2.36  loss_bbox_5: 2.68  loss_giou_5: 1.485  loss_ce_dn_5: 1.542  loss_mask_dn_5: 1.341  loss_dice_dn_5: 2.032  loss_bbox_dn_5: 0.8623  loss_giou_dn_5: 0.6649  loss_ce_6: 1.348  loss_mask_6: 1.693  loss_dice_6: 2.381  loss_bbox_6: 2.667  loss_giou_6: 1.47  loss_ce_dn_6: 1.508  loss_mask_dn_6: 1.359  loss_dice_dn_6: 1.972  loss_bbox_dn_6: 0.8613  loss_giou_dn_6: 0.6638  loss_ce_7: 1.298  loss_mask_7: 1.693  loss_dice_7: 2.395  loss_bbox_7: 2.681  loss_giou_7: 1.461  loss_ce_dn_7: 1.535  loss_mask_dn_7: 1.395  loss_dice_dn_7: 1.996  loss_bbox_dn_7: 0.8618  loss_giou_dn_7: 0.6581  loss_ce_8: 1.297  loss_mask_8: 1.658  loss_dice_8: 2.381  loss_bbox_8: 2.686  loss_giou_8: 1.461  loss_ce_dn_8: 1.525  loss_mask_dn_8: 1.354  loss_dice_dn_8: 2.022  loss_bbox_dn_8: 0.8648  loss_giou_dn_8: 0.6591    time: 2.2719  last_time: 2.2093  data_time: 0.0112  last_data_time: 0.0040   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:43:04 d2.utils.events]: \u001b[0m eta: 8 days, 5:15:43  iter: 5599  total_loss: 172.3  loss_ce: 1.381  loss_mask: 1.305  loss_dice: 2.402  loss_bbox: 2.47  loss_giou: 1.52  loss_ce_dn: 1.765  loss_mask_dn: 1.255  loss_dice_dn: 2.262  loss_bbox_dn: 0.8282  loss_giou_dn: 0.6757  loss_ce_0: 4.833  loss_mask_0: 1.264  loss_dice_0: 2.715  loss_bbox_0: 4.129  loss_giou_0: 2.071  loss_ce_1: 1.546  loss_mask_1: 1.231  loss_dice_1: 2.454  loss_bbox_1: 3.102  loss_giou_1: 1.577  loss_ce_dn_1: 2.193  loss_mask_dn_1: 1.346  loss_dice_dn_1: 2.417  loss_bbox_dn_1: 0.9688  loss_giou_dn_1: 0.758  loss_ce_2: 1.331  loss_mask_2: 1.268  loss_dice_2: 2.385  loss_bbox_2: 2.834  loss_giou_2: 1.54  loss_ce_dn_2: 1.876  loss_mask_dn_2: 1.289  loss_dice_dn_2: 2.311  loss_bbox_dn_2: 0.9274  loss_giou_dn_2: 0.729  loss_ce_3: 1.255  loss_mask_3: 1.265  loss_dice_3: 2.369  loss_bbox_3: 2.703  loss_giou_3: 1.527  loss_ce_dn_3: 1.751  loss_mask_dn_3: 1.238  loss_dice_dn_3: 2.281  loss_bbox_dn_3: 0.8692  loss_giou_dn_3: 0.7064  loss_ce_4: 1.356  loss_mask_4: 1.258  loss_dice_4: 2.4  loss_bbox_4: 2.672  loss_giou_4: 1.571  loss_ce_dn_4: 1.756  loss_mask_dn_4: 1.232  loss_dice_dn_4: 2.273  loss_bbox_dn_4: 0.8346  loss_giou_dn_4: 0.6903  loss_ce_5: 1.392  loss_mask_5: 1.264  loss_dice_5: 2.421  loss_bbox_5: 2.541  loss_giou_5: 1.527  loss_ce_dn_5: 1.741  loss_mask_dn_5: 1.215  loss_dice_dn_5: 2.192  loss_bbox_dn_5: 0.8254  loss_giou_dn_5: 0.6823  loss_ce_6: 1.394  loss_mask_6: 1.273  loss_dice_6: 2.393  loss_bbox_6: 2.513  loss_giou_6: 1.534  loss_ce_dn_6: 1.763  loss_mask_dn_6: 1.227  loss_dice_dn_6: 2.225  loss_bbox_dn_6: 0.8254  loss_giou_dn_6: 0.6808  loss_ce_7: 1.383  loss_mask_7: 1.27  loss_dice_7: 2.353  loss_bbox_7: 2.475  loss_giou_7: 1.521  loss_ce_dn_7: 1.744  loss_mask_dn_7: 1.224  loss_dice_dn_7: 2.265  loss_bbox_dn_7: 0.827  loss_giou_dn_7: 0.6724  loss_ce_8: 1.418  loss_mask_8: 1.289  loss_dice_8: 2.426  loss_bbox_8: 2.472  loss_giou_8: 1.521  loss_ce_dn_8: 1.753  loss_mask_dn_8: 1.241  loss_dice_dn_8: 2.248  loss_bbox_dn_8: 0.8285  loss_giou_dn_8: 0.6748    time: 2.2717  last_time: 2.2232  data_time: 0.0112  last_data_time: 0.0085   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:43:49 d2.utils.events]: \u001b[0m eta: 8 days, 5:08:46  iter: 5619  total_loss: 151.7  loss_ce: 1.265  loss_mask: 1.336  loss_dice: 2.036  loss_bbox: 2.757  loss_giou: 1.528  loss_ce_dn: 1.877  loss_mask_dn: 1.099  loss_dice_dn: 1.735  loss_bbox_dn: 0.7991  loss_giou_dn: 0.6845  loss_ce_0: 4.182  loss_mask_0: 1.285  loss_dice_0: 2.2  loss_bbox_0: 4.163  loss_giou_0: 1.99  loss_ce_1: 1.453  loss_mask_1: 1.36  loss_dice_1: 2.058  loss_bbox_1: 2.862  loss_giou_1: 1.754  loss_ce_dn_1: 2.374  loss_mask_dn_1: 1.158  loss_dice_dn_1: 2.005  loss_bbox_dn_1: 0.9311  loss_giou_dn_1: 0.7503  loss_ce_2: 1.244  loss_mask_2: 1.345  loss_dice_2: 2.04  loss_bbox_2: 2.727  loss_giou_2: 1.666  loss_ce_dn_2: 2.048  loss_mask_dn_2: 1.045  loss_dice_dn_2: 1.852  loss_bbox_dn_2: 0.869  loss_giou_dn_2: 0.709  loss_ce_3: 1.288  loss_mask_3: 1.319  loss_dice_3: 2.043  loss_bbox_3: 2.641  loss_giou_3: 1.518  loss_ce_dn_3: 1.914  loss_mask_dn_3: 1.032  loss_dice_dn_3: 1.793  loss_bbox_dn_3: 0.8184  loss_giou_dn_3: 0.6836  loss_ce_4: 1.256  loss_mask_4: 1.311  loss_dice_4: 2.018  loss_bbox_4: 2.673  loss_giou_4: 1.505  loss_ce_dn_4: 1.81  loss_mask_dn_4: 1.008  loss_dice_dn_4: 1.779  loss_bbox_dn_4: 0.7935  loss_giou_dn_4: 0.674  loss_ce_5: 1.288  loss_mask_5: 1.35  loss_dice_5: 2.017  loss_bbox_5: 2.691  loss_giou_5: 1.524  loss_ce_dn_5: 1.811  loss_mask_dn_5: 1.055  loss_dice_dn_5: 1.759  loss_bbox_dn_5: 0.7887  loss_giou_dn_5: 0.6756  loss_ce_6: 1.28  loss_mask_6: 1.342  loss_dice_6: 1.987  loss_bbox_6: 2.702  loss_giou_6: 1.52  loss_ce_dn_6: 1.81  loss_mask_dn_6: 1.073  loss_dice_dn_6: 1.73  loss_bbox_dn_6: 0.7889  loss_giou_dn_6: 0.6728  loss_ce_7: 1.303  loss_mask_7: 1.378  loss_dice_7: 1.978  loss_bbox_7: 2.703  loss_giou_7: 1.526  loss_ce_dn_7: 1.806  loss_mask_dn_7: 1.051  loss_dice_dn_7: 1.721  loss_bbox_dn_7: 0.7999  loss_giou_dn_7: 0.6777  loss_ce_8: 1.312  loss_mask_8: 1.325  loss_dice_8: 2.02  loss_bbox_8: 2.746  loss_giou_8: 1.522  loss_ce_dn_8: 1.823  loss_mask_dn_8: 1.087  loss_dice_dn_8: 1.735  loss_bbox_dn_8: 0.7932  loss_giou_dn_8: 0.6751    time: 2.2715  last_time: 2.2280  data_time: 0.0102  last_data_time: 0.0041   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:44:33 d2.utils.events]: \u001b[0m eta: 8 days, 5:01:25  iter: 5639  total_loss: 166.5  loss_ce: 1.531  loss_mask: 1.284  loss_dice: 2.413  loss_bbox: 2.845  loss_giou: 1.482  loss_ce_dn: 2.324  loss_mask_dn: 1.215  loss_dice_dn: 2.061  loss_bbox_dn: 0.7665  loss_giou_dn: 0.6919  loss_ce_0: 4.978  loss_mask_0: 1.276  loss_dice_0: 2.628  loss_bbox_0: 4.125  loss_giou_0: 2.042  loss_ce_1: 1.944  loss_mask_1: 1.275  loss_dice_1: 2.455  loss_bbox_1: 3.099  loss_giou_1: 1.63  loss_ce_dn_1: 2.517  loss_mask_dn_1: 1.18  loss_dice_dn_1: 2.171  loss_bbox_dn_1: 0.9376  loss_giou_dn_1: 0.7608  loss_ce_2: 1.613  loss_mask_2: 1.227  loss_dice_2: 2.436  loss_bbox_2: 3.022  loss_giou_2: 1.579  loss_ce_dn_2: 2.138  loss_mask_dn_2: 1.158  loss_dice_dn_2: 2.065  loss_bbox_dn_2: 0.8666  loss_giou_dn_2: 0.7382  loss_ce_3: 1.467  loss_mask_3: 1.239  loss_dice_3: 2.446  loss_bbox_3: 2.951  loss_giou_3: 1.567  loss_ce_dn_3: 1.962  loss_mask_dn_3: 1.151  loss_dice_dn_3: 2.062  loss_bbox_dn_3: 0.8132  loss_giou_dn_3: 0.7225  loss_ce_4: 1.491  loss_mask_4: 1.237  loss_dice_4: 2.418  loss_bbox_4: 2.956  loss_giou_4: 1.532  loss_ce_dn_4: 1.929  loss_mask_dn_4: 1.156  loss_dice_dn_4: 2.027  loss_bbox_dn_4: 0.7885  loss_giou_dn_4: 0.703  loss_ce_5: 1.436  loss_mask_5: 1.249  loss_dice_5: 2.414  loss_bbox_5: 2.938  loss_giou_5: 1.51  loss_ce_dn_5: 1.922  loss_mask_dn_5: 1.17  loss_dice_dn_5: 2.015  loss_bbox_dn_5: 0.7766  loss_giou_dn_5: 0.6951  loss_ce_6: 1.454  loss_mask_6: 1.264  loss_dice_6: 2.382  loss_bbox_6: 2.94  loss_giou_6: 1.496  loss_ce_dn_6: 2.039  loss_mask_dn_6: 1.212  loss_dice_dn_6: 2.02  loss_bbox_dn_6: 0.7793  loss_giou_dn_6: 0.6934  loss_ce_7: 1.495  loss_mask_7: 1.289  loss_dice_7: 2.431  loss_bbox_7: 2.904  loss_giou_7: 1.482  loss_ce_dn_7: 2.135  loss_mask_dn_7: 1.211  loss_dice_dn_7: 2.033  loss_bbox_dn_7: 0.7613  loss_giou_dn_7: 0.6885  loss_ce_8: 1.53  loss_mask_8: 1.277  loss_dice_8: 2.433  loss_bbox_8: 2.849  loss_giou_8: 1.482  loss_ce_dn_8: 2.267  loss_mask_dn_8: 1.215  loss_dice_dn_8: 2.043  loss_bbox_dn_8: 0.7642  loss_giou_dn_8: 0.6894    time: 2.2714  last_time: 2.2123  data_time: 0.0130  last_data_time: 0.0070   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:45:18 d2.utils.events]: \u001b[0m eta: 8 days, 4:55:54  iter: 5659  total_loss: 164.6  loss_ce: 1.619  loss_mask: 1.473  loss_dice: 2.464  loss_bbox: 2.6  loss_giou: 1.654  loss_ce_dn: 1.751  loss_mask_dn: 1.095  loss_dice_dn: 1.998  loss_bbox_dn: 0.7644  loss_giou_dn: 0.6882  loss_ce_0: 5.104  loss_mask_0: 1.439  loss_dice_0: 2.643  loss_bbox_0: 4.037  loss_giou_0: 1.967  loss_ce_1: 1.923  loss_mask_1: 1.45  loss_dice_1: 2.471  loss_bbox_1: 2.847  loss_giou_1: 1.75  loss_ce_dn_1: 2.238  loss_mask_dn_1: 1.167  loss_dice_dn_1: 2.262  loss_bbox_dn_1: 0.8633  loss_giou_dn_1: 0.7398  loss_ce_2: 1.691  loss_mask_2: 1.45  loss_dice_2: 2.431  loss_bbox_2: 2.64  loss_giou_2: 1.729  loss_ce_dn_2: 1.911  loss_mask_dn_2: 1.144  loss_dice_dn_2: 2.101  loss_bbox_dn_2: 0.8171  loss_giou_dn_2: 0.7169  loss_ce_3: 1.698  loss_mask_3: 1.467  loss_dice_3: 2.456  loss_bbox_3: 2.628  loss_giou_3: 1.736  loss_ce_dn_3: 1.828  loss_mask_dn_3: 1.107  loss_dice_dn_3: 2.03  loss_bbox_dn_3: 0.7719  loss_giou_dn_3: 0.7058  loss_ce_4: 1.657  loss_mask_4: 1.475  loss_dice_4: 2.512  loss_bbox_4: 2.598  loss_giou_4: 1.713  loss_ce_dn_4: 1.831  loss_mask_dn_4: 1.094  loss_dice_dn_4: 2.008  loss_bbox_dn_4: 0.7608  loss_giou_dn_4: 0.6983  loss_ce_5: 1.596  loss_mask_5: 1.529  loss_dice_5: 2.461  loss_bbox_5: 2.631  loss_giou_5: 1.702  loss_ce_dn_5: 1.767  loss_mask_dn_5: 1.084  loss_dice_dn_5: 1.935  loss_bbox_dn_5: 0.763  loss_giou_dn_5: 0.6942  loss_ce_6: 1.563  loss_mask_6: 1.485  loss_dice_6: 2.455  loss_bbox_6: 2.632  loss_giou_6: 1.684  loss_ce_dn_6: 1.768  loss_mask_dn_6: 1.102  loss_dice_dn_6: 1.952  loss_bbox_dn_6: 0.7653  loss_giou_dn_6: 0.6953  loss_ce_7: 1.575  loss_mask_7: 1.49  loss_dice_7: 2.46  loss_bbox_7: 2.601  loss_giou_7: 1.663  loss_ce_dn_7: 1.775  loss_mask_dn_7: 1.106  loss_dice_dn_7: 1.945  loss_bbox_dn_7: 0.7692  loss_giou_dn_7: 0.6838  loss_ce_8: 1.565  loss_mask_8: 1.48  loss_dice_8: 2.448  loss_bbox_8: 2.604  loss_giou_8: 1.657  loss_ce_dn_8: 1.766  loss_mask_dn_8: 1.074  loss_dice_dn_8: 1.973  loss_bbox_dn_8: 0.7661  loss_giou_dn_8: 0.6876    time: 2.2712  last_time: 2.2512  data_time: 0.0099  last_data_time: 0.0100   lr: 0.0001  max_mem: 13317M\n",
            "\u001b[32m[07/26 18:46:03 d2.utils.events]: \u001b[0m eta: 8 days, 4:49:06  iter: 5679  total_loss: 174.8  loss_ce: 1.632  loss_mask: 1.527  loss_dice: 2.505  loss_bbox: 2.966  loss_giou: 1.514  loss_ce_dn: 2.085  loss_mask_dn: 1.331  loss_dice_dn: 2.18  loss_bbox_dn: 0.867  loss_giou_dn: 0.6846  loss_ce_0: 4.903  loss_mask_0: 1.517  loss_dice_0: 2.681  loss_bbox_0: 4.071  loss_giou_0: 2.021  loss_ce_1: 1.76  loss_mask_1: 1.468  loss_dice_1: 2.488  loss_bbox_1: 3.256  loss_giou_1: 1.628  loss_ce_dn_1: 2.542  loss_mask_dn_1: 1.383  loss_dice_dn_1: 2.303  loss_bbox_dn_1: 0.9768  loss_giou_dn_1: 0.7434  loss_ce_2: 1.545  loss_mask_2: 1.506  loss_dice_2: 2.518  loss_bbox_2: 3.18  loss_giou_2: 1.607  loss_ce_dn_2: 2.185  loss_mask_dn_2: 1.384  loss_dice_dn_2: 2.163  loss_bbox_dn_2: 0.9345  loss_giou_dn_2: 0.7166  loss_ce_3: 1.483  loss_mask_3: 1.54  loss_dice_3: 2.519  loss_bbox_3: 3.137  loss_giou_3: 1.58  loss_ce_dn_3: 1.882  loss_mask_dn_3: 1.367  loss_dice_dn_3: 2.181  loss_bbox_dn_3: 0.9049  loss_giou_dn_3: 0.6975  loss_ce_4: 1.568  loss_mask_4: 1.539  loss_dice_4: 2.504  loss_bbox_4: 3.053  loss_giou_4: 1.555  loss_ce_dn_4: 1.94  loss_mask_dn_4: 1.348  loss_dice_dn_4: 2.151  loss_bbox_dn_4: 0.8837  loss_giou_dn_4: 0.6909  loss_ce_5: 1.586  loss_mask_5: 1.501  loss_dice_5: 2.536  loss_bbox_5: 2.987  loss_giou_5: 1.52  loss_ce_dn_5: 1.931  loss_mask_dn_5: 1.334  loss_dice_dn_5: 2.183  loss_bbox_dn_5: 0.8647  loss_giou_dn_5: 0.6874  loss_ce_6: 1.518  loss_mask_6: 1.52  loss_dice_6: 2.524  loss_bbox_6: 2.986  loss_giou_6: 1.517  loss_ce_dn_6: 1.948  loss_mask_dn_6: 1.339  loss_dice_dn_6: 2.196  loss_bbox_dn_6: 0.8674  loss_giou_dn_6: 0.6855  loss_ce_7: 1.553  loss_mask_7: 1.525  loss_dice_7: 2.54  loss_bbox_7: 2.973  loss_giou_7: 1.517  loss_ce_dn_7: 1.964  loss_mask_dn_7: 1.338  loss_dice_dn_7: 2.202  loss_bbox_dn_7: 0.8578  loss_giou_dn_7: 0.6842  loss_ce_8: 1.601  loss_mask_8: 1.526  loss_dice_8: 2.519  loss_bbox_8: 2.96  loss_giou_8: 1.517  loss_ce_dn_8: 2.018  loss_mask_dn_8: 1.353  loss_dice_dn_8: 2.166  loss_bbox_dn_8: 0.8622  loss_giou_dn_8: 0.6854    time: 2.2711  last_time: 2.2063  data_time: 0.0111  last_data_time: 0.0046   lr: 0.0001  max_mem: 13317M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run demo"
      ],
      "metadata": {
        "id": "f1i7qgMYjuC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MaskDINO/demo\n",
        "!python demo.py --config-file /content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_R50_bs16_160k_steplr.yaml \\\n",
        "  --input /content/MaskDINO/datasets/ADEChallengeData2016/images/validation/*.jpg \\\n",
        "  --output results \\\n",
        "  --opts MODEL.WEIGHTS /content/MaskDINO/weights/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth  MODEL.DEVICE 'cpu'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g2J_TVkYjuYx",
        "outputId": "df0259e8-0750-4f2a-8cbc-92ad939ad474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO/demo\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "\u001b[32m[07/26 14:07:43 detectron2]: \u001b[0mArguments: Namespace(config_file='/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_R50_bs16_160k_steplr.yaml', webcam=False, video_input=None, input=['/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000001.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000002.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000003.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000004.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000005.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000006.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000007.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000008.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000009.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000010.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000011.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000012.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000013.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000014.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000015.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000016.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000017.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000018.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000019.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000020.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000021.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000022.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000023.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000024.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000025.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000026.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000027.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000028.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000029.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000030.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000031.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000032.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000033.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000034.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000035.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000036.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000037.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000038.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000039.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000040.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000041.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000042.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000043.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000044.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000045.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000046.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000047.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000048.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000049.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000050.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000051.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000052.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000053.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000054.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000055.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000056.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000057.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000058.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000059.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000060.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000061.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000062.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000063.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000064.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000065.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000066.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000067.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000068.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000069.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000070.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000071.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000072.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000073.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000074.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000075.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000076.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000077.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000078.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000079.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000080.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000081.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000082.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000083.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000084.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000085.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000086.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000087.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000088.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000089.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000090.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000091.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000092.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000093.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000094.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000095.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000096.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000097.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000098.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000099.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000100.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000101.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000102.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000103.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000104.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000105.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000106.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000107.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000108.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000109.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000110.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000111.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000112.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000113.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000114.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000115.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000116.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000117.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000118.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000119.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000120.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000121.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000122.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000123.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000124.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000125.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000126.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000127.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000128.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000129.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000130.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000131.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000132.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000133.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000134.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000135.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000136.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000137.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000138.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000139.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000140.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000141.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000142.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000143.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000144.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000145.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000146.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000147.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000148.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000149.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000150.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000151.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000152.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000153.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000154.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000155.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000156.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000157.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000158.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000159.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000160.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000161.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000162.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000163.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000164.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000165.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000166.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000167.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000168.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000169.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000170.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000171.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000172.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000173.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000174.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000175.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000176.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000177.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000178.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000179.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000180.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000181.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000182.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000183.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000184.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000185.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000186.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000187.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000188.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000189.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000190.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000191.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000192.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000193.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000194.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000195.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000196.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000197.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000198.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000199.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000200.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000201.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000202.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000203.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000204.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000205.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000206.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000207.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000208.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000209.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000210.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000211.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000212.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000213.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000214.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000215.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000216.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000217.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000218.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000219.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000220.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000221.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000222.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000223.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000224.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000225.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000226.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000227.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000228.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000229.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000230.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000231.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000232.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000233.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000234.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000235.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000236.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000237.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000238.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000239.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000240.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000241.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000242.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000243.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000244.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000245.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000246.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000247.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000248.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000249.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000250.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000251.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000252.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000253.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000254.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000255.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000256.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000257.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000258.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000259.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000260.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000261.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000262.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000263.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000264.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000265.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000266.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000267.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000268.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000269.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000270.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000271.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000272.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000273.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000274.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000275.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000276.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000277.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000278.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000279.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000280.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000281.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000282.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000283.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000284.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000285.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000286.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000287.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000288.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000289.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000290.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000291.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000292.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000293.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000294.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000295.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000296.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000297.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000298.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000299.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000300.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000301.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000302.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000303.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000304.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000305.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000306.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000307.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000308.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000309.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000310.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000311.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000312.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000313.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000314.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000315.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000316.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000317.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000318.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000319.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000320.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000321.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000322.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000323.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000324.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000325.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000326.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000327.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000328.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000329.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000330.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000331.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000332.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000333.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000334.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000335.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000336.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000337.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000338.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000339.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000340.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000341.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000342.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000343.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000344.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000345.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000346.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000347.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000348.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000349.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000350.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000351.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000352.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000353.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000354.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000355.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000356.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000357.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000358.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000359.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000360.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000361.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000362.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000363.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000364.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000365.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000366.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000367.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000368.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000369.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000370.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000371.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000372.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000373.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000374.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000375.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000376.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000377.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000378.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000379.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000380.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000381.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000382.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000383.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000384.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000385.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000386.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000387.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000388.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000389.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000390.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000391.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000392.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000393.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000394.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000395.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000396.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000397.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000398.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000399.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000400.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000401.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000402.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000403.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000404.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000405.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000406.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000407.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000408.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000409.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000410.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000411.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000412.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000413.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000414.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000415.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000416.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000417.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000418.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000419.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000420.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000421.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000422.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000423.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000424.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000425.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000426.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000427.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000428.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000429.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000430.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000431.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000432.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000433.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000434.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000435.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000436.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000437.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000438.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000439.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000440.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000441.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000442.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000443.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000444.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000445.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000446.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000447.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000448.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000449.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000450.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000451.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000452.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000453.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000454.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000455.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000456.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000457.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000458.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000459.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000460.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000461.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000462.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000463.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000464.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000465.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000466.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000467.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000468.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000469.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000470.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000471.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000472.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000473.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000474.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000475.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000476.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000477.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000478.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000479.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000480.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000481.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000482.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000483.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000484.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000485.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000486.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000487.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000488.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000489.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000490.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000491.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000492.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000493.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000494.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000495.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000496.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000497.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000498.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000499.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000500.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000501.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000502.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000503.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000504.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000505.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000506.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000507.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000508.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000509.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000510.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000511.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000512.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000513.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000514.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000515.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000516.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000517.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000518.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000519.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000520.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000521.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000522.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000523.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000524.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000525.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000526.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000527.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000528.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000529.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000530.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000531.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000532.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000533.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000534.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000535.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000536.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000537.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000538.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000539.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000540.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000541.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000542.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000543.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000544.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000545.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000546.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000547.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000548.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000549.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000550.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000551.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000552.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000553.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000554.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000555.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000556.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000557.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000558.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000559.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000560.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000561.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000562.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000563.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000564.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000565.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000566.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000567.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000568.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000569.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000570.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000571.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000572.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000573.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000574.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000575.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000576.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000577.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000578.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000579.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000580.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000581.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000582.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000583.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000584.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000585.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000586.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000587.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000588.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000589.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000590.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000591.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000592.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000593.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000594.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000595.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000596.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000597.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000598.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000599.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000600.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000601.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000602.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000603.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000604.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000605.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000606.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000607.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000608.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000609.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000610.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000611.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000612.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000613.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000614.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000615.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000616.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000617.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000618.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000619.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000620.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000621.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000622.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000623.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000624.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000625.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000626.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000627.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000628.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000629.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000630.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000631.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000632.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000633.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000634.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000635.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000636.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000637.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000638.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000639.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000640.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000641.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000642.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000643.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000644.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000645.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000646.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000647.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000648.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000649.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000650.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000651.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000652.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000653.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000654.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000655.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000656.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000657.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000658.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000659.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000660.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000661.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000662.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000663.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000664.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000665.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000666.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000667.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000668.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000669.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000670.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000671.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000672.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000673.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000674.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000675.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000676.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000677.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000678.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000679.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000680.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000681.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000682.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000683.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000684.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000685.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000686.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000687.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000688.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000689.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000690.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000691.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000692.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000693.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000694.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000695.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000696.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000697.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000698.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000699.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000700.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000701.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000702.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000703.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000704.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000705.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000706.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000707.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000708.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000709.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000710.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000711.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000712.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000713.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000714.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000715.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000716.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000717.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000718.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000719.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000720.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000721.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000722.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000723.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000724.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000725.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000726.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000727.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000728.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000729.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000730.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000731.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000732.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000733.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000734.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000735.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000736.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000737.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000738.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000739.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000740.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000741.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000742.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000743.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000744.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000745.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000746.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000747.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000748.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000749.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000750.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000751.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000752.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000753.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000754.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000755.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000756.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000757.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000758.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000759.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000760.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000761.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000762.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000763.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000764.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000765.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000766.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000767.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000768.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000769.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000770.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000771.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000772.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000773.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000774.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000775.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000776.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000777.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000778.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000779.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000780.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000781.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000782.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000783.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000784.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000785.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000786.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000787.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000788.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000789.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000790.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000791.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000792.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000793.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000794.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000795.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000796.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000797.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000798.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000799.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000800.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000801.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000802.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000803.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000804.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000805.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000806.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000807.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000808.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000809.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000810.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000811.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000812.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000813.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000814.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000815.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000816.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000817.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000818.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000819.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000820.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000821.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000822.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000823.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000824.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000825.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000826.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000827.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000828.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000829.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000830.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000831.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000832.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000833.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000834.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000835.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000836.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000837.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000838.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000839.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000840.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000841.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000842.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000843.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000844.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000845.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000846.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000847.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000848.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000849.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000850.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000851.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000852.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000853.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000854.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000855.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000856.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000857.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000858.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000859.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000860.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000861.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000862.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000863.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000864.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000865.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000866.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000867.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000868.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000869.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000870.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000871.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000872.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000873.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000874.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000875.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000876.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000877.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000878.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000879.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000880.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000881.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000882.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000883.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000884.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000885.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000886.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000887.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000888.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000889.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000890.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000891.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000892.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000893.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000894.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000895.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000896.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000897.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000898.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000899.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000900.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000901.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000902.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000903.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000904.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000905.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000906.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000907.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000908.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000909.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000910.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000911.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000912.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000913.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000914.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000915.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000916.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000917.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000918.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000919.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000920.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000921.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000922.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000923.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000924.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000925.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000926.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000927.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000928.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000929.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000930.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000931.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000932.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000933.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000934.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000935.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000936.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000937.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000938.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000939.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000940.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000941.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000942.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000943.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000944.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000945.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000946.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000947.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000948.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000949.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000950.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000951.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000952.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000953.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000954.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000955.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000956.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000957.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000958.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000959.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000960.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000961.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000962.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000963.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000964.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000965.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000966.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000967.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000968.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000969.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000970.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000971.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000972.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000973.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000974.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000975.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000976.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000977.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000978.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000979.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000980.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000981.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000982.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000983.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000984.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000985.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000986.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000987.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000988.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000989.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000990.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000991.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000992.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000993.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000994.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000995.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000996.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000997.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000998.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000999.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001000.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001001.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001002.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001003.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001004.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001005.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001006.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001007.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001008.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001009.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001010.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001011.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001012.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001013.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001014.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001015.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001016.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001017.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001018.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001019.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001020.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001021.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001022.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001023.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001024.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001025.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001026.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001027.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001028.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001029.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001030.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001031.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001032.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001033.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001034.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001035.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001036.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001037.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001038.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001039.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001040.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001041.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001042.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001043.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001044.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001045.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001046.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001047.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001048.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001049.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001050.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001051.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001052.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001053.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001054.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001055.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001056.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001057.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001058.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001059.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001060.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001061.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001062.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001063.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001064.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001065.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001066.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001067.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001068.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001069.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001070.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001071.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001072.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001073.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001074.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001075.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001076.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001077.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001078.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001079.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001080.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001081.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001082.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001083.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001084.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001085.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001086.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001087.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001088.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001089.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001090.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001091.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001092.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001093.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001094.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001095.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001096.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001097.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001098.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001099.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001100.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001101.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001102.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001103.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001104.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001105.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001106.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001107.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001108.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001109.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001110.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001111.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001112.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001113.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001114.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001115.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001116.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001117.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001118.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001119.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001120.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001121.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001122.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001123.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001124.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001125.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001126.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001127.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001128.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001129.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001130.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001131.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001132.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001133.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001134.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001135.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001136.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001137.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001138.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001139.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001140.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001141.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001142.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001143.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001144.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001145.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001146.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001147.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001148.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001149.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001150.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001151.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001152.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001153.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001154.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001155.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001156.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001157.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001158.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001159.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001160.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001161.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001162.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001163.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001164.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001165.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001166.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001167.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001168.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001169.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001170.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001171.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001172.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001173.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001174.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001175.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001176.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001177.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001178.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001179.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001180.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001181.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001182.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001183.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001184.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001185.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001186.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001187.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001188.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001189.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001190.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001191.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001192.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001193.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001194.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001195.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001196.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001197.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001198.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001199.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001200.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001201.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001202.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001203.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001204.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001205.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001206.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001207.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001208.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001209.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001210.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001211.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001212.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001213.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001214.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001215.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001216.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001217.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001218.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001219.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001220.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001221.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001222.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001223.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001224.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001225.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001226.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001227.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001228.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001229.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001230.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001231.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001232.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001233.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001234.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001235.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001236.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001237.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001238.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001239.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001240.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001241.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001242.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001243.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001244.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001245.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001246.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001247.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001248.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001249.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001250.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001251.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001252.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001253.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001254.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001255.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001256.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001257.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001258.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001259.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001260.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001261.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001262.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001263.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001264.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001265.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001266.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001267.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001268.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001269.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001270.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001271.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001272.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001273.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001274.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001275.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001276.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001277.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001278.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001279.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001280.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001281.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001282.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001283.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001284.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001285.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001286.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001287.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001288.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001289.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001290.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001291.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001292.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001293.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001294.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001295.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001296.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001297.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001298.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001299.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001300.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001301.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001302.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001303.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001304.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001305.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001306.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001307.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001308.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001309.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001310.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001311.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001312.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001313.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001314.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001315.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001316.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001317.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001318.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001319.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001320.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001321.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001322.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001323.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001324.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001325.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001326.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001327.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001328.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001329.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001330.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001331.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001332.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001333.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001334.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001335.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001336.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001337.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001338.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001339.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001340.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001341.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001342.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001343.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001344.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001345.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001346.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001347.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001348.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001349.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001350.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001351.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001352.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001353.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001354.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001355.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001356.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001357.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001358.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001359.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001360.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001361.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001362.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001363.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001364.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001365.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001366.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001367.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001368.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001369.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001370.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001371.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001372.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001373.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001374.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001375.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001376.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001377.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001378.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001379.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001380.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001381.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001382.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001383.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001384.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001385.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001386.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001387.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001388.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001389.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001390.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001391.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001392.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001393.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001394.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001395.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001396.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001397.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001398.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001399.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001400.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001401.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001402.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001403.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001404.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001405.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001406.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001407.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001408.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001409.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001410.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001411.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001412.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001413.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001414.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001415.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001416.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001417.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001418.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001419.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001420.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001421.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001422.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001423.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001424.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001425.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001426.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001427.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001428.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001429.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001430.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001431.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001432.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001433.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001434.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001435.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001436.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001437.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001438.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001439.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001440.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001441.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001442.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001443.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001444.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001445.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001446.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001447.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001448.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001449.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001450.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001451.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001452.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001453.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001454.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001455.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001456.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001457.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001458.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001459.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001460.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001461.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001462.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001463.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001464.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001465.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001466.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001467.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001468.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001469.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001470.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001471.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001472.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001473.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001474.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001475.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001476.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001477.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001478.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001479.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001480.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001481.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001482.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001483.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001484.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001485.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001486.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001487.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001488.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001489.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001490.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001491.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001492.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001493.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001494.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001495.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001496.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001497.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001498.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001499.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001500.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001501.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001502.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001503.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001504.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001505.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001506.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001507.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001508.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001509.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001510.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001511.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001512.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001513.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001514.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001515.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001516.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001517.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001518.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001519.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001520.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001521.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001522.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001523.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001524.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001525.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001526.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001527.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001528.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001529.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001530.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001531.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001532.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001533.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001534.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001535.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001536.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001537.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001538.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001539.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001540.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001541.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001542.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001543.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001544.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001545.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001546.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001547.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001548.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001549.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001550.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001551.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001552.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001553.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001554.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001555.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001556.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001557.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001558.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001559.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001560.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001561.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001562.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001563.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001564.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001565.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001566.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001567.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001568.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001569.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001570.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001571.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001572.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001573.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001574.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001575.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001576.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001577.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001578.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001579.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001580.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001581.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001582.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001583.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001584.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001585.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001586.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001587.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001588.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001589.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001590.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001591.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001592.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001593.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001594.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001595.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001596.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001597.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001598.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001599.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001600.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001601.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001602.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001603.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001604.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001605.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001606.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001607.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001608.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001609.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001610.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001611.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001612.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001613.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001614.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001615.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001616.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001617.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001618.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001619.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001620.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001621.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001622.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001623.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001624.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001625.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001626.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001627.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001628.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001629.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001630.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001631.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001632.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001633.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001634.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001635.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001636.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001637.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001638.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001639.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001640.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001641.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001642.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001643.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001644.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001645.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001646.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001647.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001648.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001649.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001650.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001651.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001652.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001653.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001654.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001655.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001656.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001657.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001658.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001659.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001660.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001661.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001662.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001663.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001664.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001665.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001666.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001667.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001668.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001669.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001670.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001671.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001672.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001673.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001674.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001675.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001676.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001677.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001678.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001679.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001680.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001681.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001682.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001683.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001684.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001685.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001686.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001687.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001688.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001689.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001690.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001691.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001692.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001693.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001694.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001695.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001696.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001697.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001698.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001699.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001700.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001701.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001702.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001703.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001704.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001705.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001706.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001707.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001708.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001709.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001710.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001711.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001712.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001713.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001714.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001715.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001716.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001717.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001718.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001719.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001720.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001721.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001722.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001723.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001724.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001725.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001726.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001727.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001728.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001729.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001730.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001731.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001732.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001733.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001734.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001735.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001736.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001737.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001738.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001739.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001740.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001741.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001742.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001743.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001744.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001745.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001746.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001747.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001748.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001749.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001750.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001751.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001752.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001753.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001754.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001755.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001756.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001757.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001758.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001759.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001760.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001761.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001762.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001763.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001764.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001765.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001766.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001767.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001768.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001769.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001770.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001771.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001772.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001773.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001774.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001775.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001776.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001777.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001778.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001779.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001780.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001781.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001782.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001783.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001784.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001785.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001786.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001787.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001788.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001789.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001790.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001791.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001792.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001793.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001794.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001795.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001796.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001797.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001798.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001799.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001800.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001801.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001802.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001803.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001804.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001805.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001806.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001807.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001808.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001809.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001810.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001811.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001812.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001813.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001814.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001815.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001816.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001817.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001818.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001819.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001820.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001821.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001822.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001823.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001824.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001825.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001826.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001827.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001828.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001829.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001830.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001831.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001832.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001833.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001834.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001835.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001836.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001837.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001838.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001839.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001840.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001841.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001842.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001843.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001844.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001845.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001846.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001847.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001848.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001849.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001850.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001851.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001852.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001853.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001854.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001855.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001856.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001857.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001858.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001859.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001860.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001861.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001862.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001863.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001864.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001865.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001866.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001867.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001868.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001869.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001870.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001871.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001872.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001873.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001874.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001875.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001876.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001877.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001878.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001879.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001880.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001881.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001882.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001883.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001884.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001885.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001886.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001887.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001888.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001889.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001890.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001891.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001892.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001893.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001894.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001895.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001896.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001897.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001898.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001899.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001900.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001901.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001902.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001903.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001904.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001905.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001906.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001907.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001908.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001909.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001910.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001911.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001912.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001913.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001914.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001915.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001916.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001917.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001918.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001919.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001920.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001921.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001922.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001923.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001924.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001925.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001926.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001927.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001928.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001929.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001930.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001931.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001932.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001933.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001934.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001935.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001936.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001937.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001938.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001939.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001940.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001941.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001942.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001943.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001944.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001945.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001946.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001947.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001948.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001949.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001950.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001951.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001952.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001953.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001954.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001955.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001956.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001957.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001958.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001959.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001960.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001961.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001962.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001963.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001964.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001965.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001966.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001967.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001968.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001969.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001970.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001971.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001972.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001973.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001974.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001975.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001976.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001977.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001978.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001979.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001980.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001981.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001982.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001983.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001984.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001985.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001986.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001987.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001988.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001989.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001990.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001991.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001992.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001993.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001994.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001995.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001996.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001997.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001998.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00001999.jpg', '/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00002000.jpg'], output='results', confidence_threshold=0.5, opts=['MODEL.WEIGHTS', '/content/MaskDINO/weights/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth', 'MODEL.DEVICE', 'cpu'])\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/26 14:07:43 fvcore.common.config]: \u001b[0mLoading config /content/MaskDINO/configs/ade20k/semantic-segmentation/Base-ADE20K-SemanticSegmentation.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "criterion.weight_dict  {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0}\n",
            "\u001b[32m[07/26 14:07:45 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /content/MaskDINO/weights/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth ...\n",
            "\u001b[32m[07/26 14:07:45 fvcore.common.checkpoint]: \u001b[0m[Checkpointer] Loading from /content/MaskDINO/weights/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth ...\n",
            "  0% 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\u001b[32m[07/26 14:07:59 detectron2]: \u001b[0m/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000001.jpg: finished in 11.22s\n",
            "  0% 1/2000 [00:11<6:15:38, 11.28s/it]\u001b[32m[07/26 14:08:08 detectron2]: \u001b[0m/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000002.jpg: finished in 9.39s\n",
            "  0% 2/2000 [00:20<5:39:23, 10.19s/it]\u001b[32m[07/26 14:08:18 detectron2]: \u001b[0m/content/MaskDINO/datasets/ADEChallengeData2016/images/validation/ADE_val_00000003.jpg: finished in 9.58s\n",
            "  0% 3/2000 [00:36<6:48:00, 12.26s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MaskDINO/demo/../maskdino/modeling/pixel_decoder/ops/modules/ms_deform_attn.py\", line 117, in forward\n",
            "    output = MSDeformAttnFunction.apply(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\", line 539, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/MaskDINO/demo/../maskdino/modeling/pixel_decoder/ops/functions/ms_deform_attn_func.py\", line 36, in forward\n",
            "    output = MSDA.ms_deform_attn_forward(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Not implemented on the CPU\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MaskDINO/demo/demo.py\", line 118, in <module>\n",
            "    predictions, visualized_output = demo.run_on_image(img)\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/MaskDINO/demo/predictor.py\", line 49, in run_on_image\n",
            "    predictions = self.predictor(image)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/detectron2/detectron2/engine/defaults.py\", line 351, in __call__\n",
            "    predictions = self.model([inputs])[0]\n",
            "                  ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/MaskDINO/demo/../maskdino/maskdino.py\", line 277, in forward\n",
            "    outputs, _ = self.sem_seg_head(features)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/MaskDINO/demo/../maskdino/modeling/meta_arch/maskdino_head.py\", line 75, in forward\n",
            "    return self.layers(features, mask,targets=targets)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/MaskDINO/demo/../maskdino/modeling/meta_arch/maskdino_head.py\", line 78, in layers\n",
            "    mask_features, transformer_encoder_features, multi_scale_features = self.pixel_decoder.forward_features(features, mask)\n",
            "                                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\", line 16, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/MaskDINO/demo/../maskdino/modeling/pixel_decoder/maskdino_encoder.py\", line 396, in forward_features\n",
            "    y, spatial_shapes, level_start_index = self.transformer(srcs, masks, pos)\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/MaskDINO/demo/../maskdino/modeling/pixel_decoder/maskdino_encoder.py\", line 113, in forward\n",
            "    memory = self.encoder(src_flatten, spatial_shapes, level_start_index, valid_ratios, lvl_pos_embed_flatten, mask_flatten)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/MaskDINO/demo/../maskdino/modeling/pixel_decoder/maskdino_encoder.py\", line 185, in forward\n",
            "    output = layer(output, pos, reference_points, spatial_shapes, level_start_index, padding_mask)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/MaskDINO/demo/../maskdino/modeling/pixel_decoder/maskdino_encoder.py\", line 150, in forward\n",
            "    src2 = self.self_attn(self.with_pos_embed(src, pos), reference_points, src, spatial_shapes, level_start_index, padding_mask)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/MaskDINO/demo/../maskdino/modeling/pixel_decoder/ops/modules/ms_deform_attn.py\", line 121, in forward\n",
            "    output = ms_deform_attn_core_pytorch(value, input_spatial_shapes, sampling_locations, attention_weights)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/MaskDINO/demo/../maskdino/modeling/pixel_decoder/ops/functions/ms_deform_attn_func.py\", line 66, in ms_deform_attn_core_pytorch\n",
            "    sampling_value_l_ = F.grid_sample(value_l_, sampling_grid_l_,\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 4304, in grid_sample\n",
            "    return torch.grid_sampler(input, grid, mode_enum, padding_mode_enum, align_corners)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### start export model to onnx"
      ],
      "metadata": {
        "id": "P4BrOcbBDOaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime-gpu==1.22.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DwVmDAzOASm",
        "outputId": "d80c1d47-dda9-4d6e-e6ed-52cb2b36bcaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.1)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n",
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: onnxruntime-gpu==1.22.0 in /usr/local/lib/python3.11/dist-packages (1.22.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.22.0) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.22.0) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.22.0) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.22.0) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.22.0) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.22.0) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime-gpu==1.22.0) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu==1.22.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MaskDINO/export_model.py\n",
        "#!/usr/bin/env python\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "import argparse\n",
        "import os\n",
        "from typing import Dict, List, Tuple\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "\n",
        "import detectron2.data.transforms as T\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import build_detection_test_loader, detection_utils\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset, print_csv_format\n",
        "from detectron2.evaluation import SemSegEvaluator\n",
        "\n",
        "from detectron2.export import (\n",
        "    STABLE_ONNX_OPSET_VERSION,\n",
        "    TracingAdapter,\n",
        "    dump_torchscript_IR,\n",
        "    scripting_with_instances,\n",
        ")\n",
        "from detectron2.modeling import GeneralizedRCNN, RetinaNet, build_model\n",
        "from detectron2.modeling.postprocessing import detector_postprocess\n",
        "from detectron2.projects.point_rend import add_pointrend_config\n",
        "from detectron2.structures import Boxes\n",
        "from detectron2.utils.env import TORCH_VERSION\n",
        "from detectron2.utils.file_io import PathManager\n",
        "from detectron2.utils.logger import setup_logger\n",
        "\n",
        "\n",
        "# new import\n",
        "import detectron2.utils.comm as comm\n",
        "\n",
        "\n",
        "from maskdino import add_maskdino_config\n",
        "from detectron2.engine import default_setup\n",
        "from detectron2.projects.deeplab import add_deeplab_config\n",
        "\n",
        "# def setup_cfg(args):\n",
        "#     cfg = get_cfg()\n",
        "#     # cuda context is initialized before creating dataloader, so we don't fork anymore\n",
        "#     cfg.DATALOADER.NUM_WORKERS = 0\n",
        "#     add_pointrend_config(cfg)\n",
        "#     cfg.merge_from_file(args.config_file)\n",
        "#     cfg.merge_from_list(args.opts)\n",
        "#     cfg.freeze()\n",
        "#     return cfg\n",
        "\n",
        "\n",
        "def setup_cfg(args):\n",
        "    \"\"\"\n",
        "    Create configs and perform basic setups.\n",
        "    \"\"\"\n",
        "    cfg = get_cfg()\n",
        "    # for poly lr schedule\n",
        "    add_deeplab_config(cfg)\n",
        "    add_maskdino_config(cfg)\n",
        "    cfg.merge_from_file(args.config_file)\n",
        "    cfg.merge_from_list(args.opts)\n",
        "    cfg.freeze()\n",
        "    default_setup(cfg, args)\n",
        "    setup_logger(output=cfg.OUTPUT_DIR, distributed_rank=comm.get_rank(), name=\"maskdino\")\n",
        "    return cfg\n",
        "\n",
        "def export_caffe2_tracing(cfg, torch_model, inputs):\n",
        "    from detectron2.export import Caffe2Tracer\n",
        "\n",
        "    tracer = Caffe2Tracer(cfg, torch_model, inputs)\n",
        "    if args.format == \"caffe2\":\n",
        "        caffe2_model = tracer.export_caffe2()\n",
        "        caffe2_model.save_protobuf(args.output)\n",
        "        # draw the caffe2 graph\n",
        "        caffe2_model.save_graph(os.path.join(args.output, \"model.svg\"), inputs=inputs)\n",
        "        return caffe2_model\n",
        "    elif args.format == \"onnx\":\n",
        "        import onnx\n",
        "\n",
        "        onnx_model = tracer.export_onnx()\n",
        "        onnx.save(onnx_model, os.path.join(args.output, \"model.onnx\"))\n",
        "    elif args.format == \"torchscript\":\n",
        "        ts_model = tracer.export_torchscript()\n",
        "        with PathManager.open(os.path.join(args.output, \"model.ts\"), \"wb\") as f:\n",
        "            torch.jit.save(ts_model, f)\n",
        "        dump_torchscript_IR(ts_model, args.output)\n",
        "\n",
        "\n",
        "# experimental. API not yet final\n",
        "def export_scripting(torch_model):\n",
        "    assert TORCH_VERSION >= (1, 8)\n",
        "    fields = {\n",
        "        \"proposal_boxes\": Boxes,\n",
        "        \"objectness_logits\": Tensor,\n",
        "        \"pred_boxes\": Boxes,\n",
        "        \"scores\": Tensor,\n",
        "        \"pred_classes\": Tensor,\n",
        "        \"pred_masks\": Tensor,\n",
        "        \"pred_keypoints\": torch.Tensor,\n",
        "        \"pred_keypoint_heatmaps\": torch.Tensor,\n",
        "    }\n",
        "    assert args.format == \"torchscript\", \"Scripting only supports torchscript format.\"\n",
        "\n",
        "    class ScriptableAdapterBase(nn.Module):\n",
        "        # Use this adapter to workaround https://github.com/pytorch/pytorch/issues/46944\n",
        "        # by not retuning instances but dicts. Otherwise the exported model is not deployable\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.model = torch_model\n",
        "            self.eval()\n",
        "\n",
        "    if isinstance(torch_model, GeneralizedRCNN):\n",
        "\n",
        "        class ScriptableAdapter(ScriptableAdapterBase):\n",
        "            def forward(self, inputs: Tuple[Dict[str, torch.Tensor]]) -> List[Dict[str, Tensor]]:\n",
        "                instances = self.model.inference(inputs, do_postprocess=False)\n",
        "                return [i.get_fields() for i in instances]\n",
        "\n",
        "    else:\n",
        "\n",
        "        class ScriptableAdapter(ScriptableAdapterBase):\n",
        "            def forward(self, inputs: Tuple[Dict[str, torch.Tensor]]) -> List[Dict[str, Tensor]]:\n",
        "                instances = self.model(inputs)\n",
        "                return [i.get_fields() for i in instances]\n",
        "\n",
        "    ts_model = scripting_with_instances(ScriptableAdapter(), fields)\n",
        "    with PathManager.open(os.path.join(args.output, \"model.ts\"), \"wb\") as f:\n",
        "        torch.jit.save(ts_model, f)\n",
        "    dump_torchscript_IR(ts_model, args.output)\n",
        "    # TODO inference in Python now missing postprocessing glue code\n",
        "    return None\n",
        "\n",
        "\n",
        "# experimental. API not yet final\n",
        "def export_tracing(torch_model, inputs):\n",
        "    assert TORCH_VERSION >= (1, 8)\n",
        "    image = inputs[0][\"image\"]\n",
        "    inputs = [{\"image\": image}]  # remove other unused keys\n",
        "\n",
        "    if isinstance(torch_model, GeneralizedRCNN):\n",
        "\n",
        "        def inference(model, inputs):\n",
        "            # use do_postprocess=False so it returns ROI mask\n",
        "            inst = model.inference(inputs, do_postprocess=False)[0]\n",
        "            return [{\"instances\": inst}]\n",
        "\n",
        "    else:\n",
        "        inference = None  # assume that we just call the model directly\n",
        "\n",
        "    traceable_model = TracingAdapter(torch_model, inputs, inference)\n",
        "\n",
        "    if args.format == \"torchscript\":\n",
        "        ts_model = torch.jit.trace(traceable_model, (image,))\n",
        "        with PathManager.open(os.path.join(args.output, \"model.ts\"), \"wb\") as f:\n",
        "            torch.jit.save(ts_model, f)\n",
        "        dump_torchscript_IR(ts_model, args.output)\n",
        "    elif args.format == \"onnx\":\n",
        "        with PathManager.open(os.path.join(args.output, \"model.onnx\"), \"wb\") as f:\n",
        "            # torch.onnx.export(traceable_model, (image,), f, opset_version=STABLE_ONNX_OPSET_VERSION)\n",
        "            torch.onnx.export(traceable_model, (image,), f, opset_version=12, dynamic_axes={\"input\": {1: \"height\", 2: \"width\"}})\n",
        "\n",
        "    logger.info(\"Inputs schema: \" + str(traceable_model.inputs_schema))\n",
        "    logger.info(\"Outputs schema: \" + str(traceable_model.outputs_schema))\n",
        "\n",
        "    if args.format != \"torchscript\":\n",
        "        return None\n",
        "    if not isinstance(torch_model, (GeneralizedRCNN, RetinaNet)):\n",
        "        return None\n",
        "\n",
        "    def eval_wrapper(inputs):\n",
        "        \"\"\"\n",
        "        The exported model does not contain the final resize step, which is typically\n",
        "        unused in deployment but needed for evaluation. We add it manually here.\n",
        "        \"\"\"\n",
        "        input = inputs[0]\n",
        "        instances = traceable_model.outputs_schema(ts_model(input[\"image\"]))[0][\"instances\"]\n",
        "        postprocessed = detector_postprocess(instances, input[\"height\"], input[\"width\"])\n",
        "        return [{\"instances\": postprocessed}]\n",
        "\n",
        "    return eval_wrapper\n",
        "\n",
        "\n",
        "\n",
        "def get_sample_inputs(args):\n",
        "\n",
        "    if args.sample_image is None:\n",
        "        # get a first batch from dataset\n",
        "        data_loader = build_detection_test_loader(cfg, cfg.DATASETS.TEST[0])\n",
        "        first_batch = next(iter(data_loader))\n",
        "        return first_batch\n",
        "    else:\n",
        "        # get a sample data\n",
        "        original_image = detection_utils.read_image(args.sample_image, format=cfg.INPUT.FORMAT)\n",
        "        # Do same preprocessing as DefaultPredictor\n",
        "        aug = T.ResizeShortestEdge(\n",
        "            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
        "        )\n",
        "        height, width = original_image.shape[:2]\n",
        "        image = aug.get_transform(original_image).apply_image(original_image)\n",
        "        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
        "\n",
        "        inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
        "\n",
        "        # Sample ready\n",
        "        sample_inputs = [inputs]\n",
        "        return sample_inputs\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    global logger, cfg, args\n",
        "    parser = argparse.ArgumentParser(description=\"Export a model for deployment.\")\n",
        "    parser.add_argument(\n",
        "        \"--format\",\n",
        "        choices=[\"caffe2\", \"onnx\", \"torchscript\"],\n",
        "        help=\"output format\",\n",
        "        default=\"torchscript\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--export-method\",\n",
        "        choices=[\"caffe2_tracing\", \"tracing\", \"scripting\"],\n",
        "        help=\"Method to export models\",\n",
        "        default=\"tracing\",\n",
        "    )\n",
        "    parser.add_argument(\"--config-file\", default=\"\", metavar=\"FILE\", help=\"path to config file\")\n",
        "    parser.add_argument(\"--sample-image\", default=None, type=str, help=\"sample image for input\")\n",
        "    parser.add_argument(\"--run-eval\", action=\"store_true\")\n",
        "    parser.add_argument(\"--output\", help=\"output directory for the converted model\")\n",
        "    parser.add_argument(\n",
        "        \"opts\",\n",
        "        help=\"Modify config options using the command-line\",\n",
        "        default=None,\n",
        "        nargs=argparse.REMAINDER,\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "    logger = setup_logger()\n",
        "    logger.info(\"Command line arguments: \" + str(args))\n",
        "    PathManager.mkdirs(args.output)\n",
        "    # Disable re-specialization on new shapes. Otherwise --run-eval will be slow\n",
        "    torch._C._jit_set_bailout_depth(1)\n",
        "\n",
        "    cfg = setup_cfg(args)\n",
        "\n",
        "    # create a torch model\n",
        "    torch_model = build_model(cfg)\n",
        "    DetectionCheckpointer(torch_model).resume_or_load(cfg.MODEL.WEIGHTS)\n",
        "    torch_model.eval()\n",
        "\n",
        "    # convert and save model\n",
        "    if args.export_method == \"caffe2_tracing\":\n",
        "        sample_inputs = get_sample_inputs(args)\n",
        "        exported_model = export_caffe2_tracing(cfg, torch_model, sample_inputs)\n",
        "    elif args.export_method == \"scripting\":\n",
        "        exported_model = export_scripting(torch_model)\n",
        "    elif args.export_method == \"tracing\":\n",
        "        sample_inputs = get_sample_inputs(args)\n",
        "        exported_model = export_tracing(torch_model, sample_inputs)\n",
        "\n",
        "    # run evaluation with the converted model\n",
        "    if args.run_eval:\n",
        "        assert exported_model is not None, (\n",
        "            \"Python inference is not yet implemented for \"\n",
        "            f\"export_method={args.export_method}, format={args.format}.\"\n",
        "        )\n",
        "        logger.info(\"Running evaluation ... this takes a long time if you export to CPU.\")\n",
        "        dataset = cfg.DATASETS.TEST[0]\n",
        "        data_loader = build_detection_test_loader(cfg, dataset)\n",
        "        # NOTE: hard-coded evaluator. change to the evaluator for your dataset\n",
        "        # evaluator = COCOEvaluator(dataset, output_dir=args.output)\n",
        "        evaluator = SemSegEvaluator(\n",
        "            dataset_name=dataset,\n",
        "            distributed=True,\n",
        "            output_dir=args.output,\n",
        "        )\n",
        "        metrics = inference_on_dataset(exported_model, data_loader, evaluator)\n",
        "        print_csv_format(metrics)\n",
        "    logger.info(\"Success.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()  # pragma: no cover\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz9khraaDRjI",
        "outputId": "3edc62fd-6a3b-4417-a169-261332b34fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MaskDINO/export_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/MaskDINO/r50_onnx_models\n",
        "%cd /content/MaskDINO\n",
        "!python export_model.py \\\n",
        "  --config-file /content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_R50_bs16_160k_steplr.yaml \\\n",
        "  --output output/exported_model \\\n",
        "  --format onnx \\\n",
        "  --export-method tracing \\\n",
        "  --output r50_onnx_models \\\n",
        "  --sample-image datasets/ADEChallengeData2016/images/validation/ADE_val_00000001.jpg \\\n",
        "  MODEL.WEIGHTS weights/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hpU00-WrojSB",
        "outputId": "efe457f8-db73-4758-c7ee-f38da05655d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "\u001b[32m[07/27 14:22:37 detectron2]: \u001b[0mCommand line arguments: Namespace(format='onnx', export_method='tracing', config_file='/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_R50_bs16_160k_steplr.yaml', sample_image='datasets/ADEChallengeData2016/images/validation/ADE_val_00000001.jpg', run_eval=False, output='r50_onnx_models', opts=['MODEL.WEIGHTS', 'weights/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth'])\n",
            "[W init.cpp:842] Warning: Use _jit_set_fusion_strategy, bailout depth is deprecated. Setting to (STATIC, 1) (function operator())\n",
            "Loading config /content/MaskDINO/configs/ade20k/semantic-segmentation/Base-ADE20K-SemanticSegmentation.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "\u001b[32m[07/27 14:22:37 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[07/27 14:22:37 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[07/27 14:22:38 detectron2]: \u001b[0mEnvironment info:\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "sys.platform                     linux\n",
            "Python                           3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "numpy                            1.26.4\n",
            "detectron2                       0.6 @/content/detectron2/detectron2\n",
            "Compiler                         GCC 11.4\n",
            "CUDA compiler                    CUDA 12.5\n",
            "detectron2 arch flags            7.5\n",
            "DETECTRON2_ENV_MODULE            <not set>\n",
            "PyTorch                          2.1.0+cu121 @/usr/local/lib/python3.11/dist-packages/torch\n",
            "PyTorch debug build              False\n",
            "torch._C._GLIBCXX_USE_CXX11_ABI  False\n",
            "GPU available                    Yes\n",
            "GPU 0                            Tesla T4 (arch=7.5)\n",
            "Driver version                   550.54.15\n",
            "CUDA_HOME                        /usr/local/cuda\n",
            "Pillow                           11.3.0\n",
            "torchvision                      0.16.0+cu121 @/usr/local/lib/python3.11/dist-packages/torchvision\n",
            "torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0\n",
            "fvcore                           0.1.5.post20221221\n",
            "iopath                           0.1.9\n",
            "cv2                              4.12.0\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX512\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "\u001b[32m[07/27 14:22:38 detectron2]: \u001b[0mEnvironment info:\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "sys.platform                     linux\n",
            "Python                           3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "numpy                            1.26.4\n",
            "detectron2                       0.6 @/content/detectron2/detectron2\n",
            "Compiler                         GCC 11.4\n",
            "CUDA compiler                    CUDA 12.5\n",
            "detectron2 arch flags            7.5\n",
            "DETECTRON2_ENV_MODULE            <not set>\n",
            "PyTorch                          2.1.0+cu121 @/usr/local/lib/python3.11/dist-packages/torch\n",
            "PyTorch debug build              False\n",
            "torch._C._GLIBCXX_USE_CXX11_ABI  False\n",
            "GPU available                    Yes\n",
            "GPU 0                            Tesla T4 (arch=7.5)\n",
            "Driver version                   550.54.15\n",
            "CUDA_HOME                        /usr/local/cuda\n",
            "Pillow                           11.3.0\n",
            "torchvision                      0.16.0+cu121 @/usr/local/lib/python3.11/dist-packages/torchvision\n",
            "torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0\n",
            "fvcore                           0.1.5.post20221221\n",
            "iopath                           0.1.9\n",
            "cv2                              4.12.0\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX512\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "\u001b[32m[07/27 14:22:38 detectron2]: \u001b[0mCommand line arguments: Namespace(format='onnx', export_method='tracing', config_file='/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_R50_bs16_160k_steplr.yaml', sample_image='datasets/ADEChallengeData2016/images/validation/ADE_val_00000001.jpg', run_eval=False, output='r50_onnx_models', opts=['MODEL.WEIGHTS', 'weights/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth'])\n",
            "\u001b[32m[07/27 14:22:38 detectron2]: \u001b[0mCommand line arguments: Namespace(format='onnx', export_method='tracing', config_file='/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_R50_bs16_160k_steplr.yaml', sample_image='datasets/ADEChallengeData2016/images/validation/ADE_val_00000001.jpg', run_eval=False, output='r50_onnx_models', opts=['MODEL.WEIGHTS', 'weights/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth'])\n",
            "\u001b[32m[07/27 14:22:38 detectron2]: \u001b[0mContents of args.config_file=/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_R50_bs16_160k_steplr.yaml:\n",
            "\u001b[38;5;204m_BASE_\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mBase-ADE20K-SemanticSegmentation.yaml\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINO\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOHead\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mGN\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;245m# pixel decoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOEncoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres2\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINODecoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\u001b[38;5;15m  \u001b[39m\u001b[38;5;245m# 9 decoder layers, add one for the loss on learnable query\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mseg\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m16\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m160000\u001b[39m\n",
            "\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m(135000,150000)\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\n",
            "\u001b[32m[07/27 14:22:38 detectron2]: \u001b[0mContents of args.config_file=/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_R50_bs16_160k_steplr.yaml:\n",
            "\u001b[38;5;204m_BASE_\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mBase-ADE20K-SemanticSegmentation.yaml\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINO\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOHead\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mGN\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;245m# pixel decoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOEncoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres2\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINODecoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\u001b[38;5;15m  \u001b[39m\u001b[38;5;245m# 9 decoder layers, add one for the loss on learnable query\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mseg\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m16\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m160000\u001b[39m\n",
            "\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m(135000,150000)\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\n",
            "\u001b[32m[07/27 14:22:38 detectron2]: \u001b[0mRunning with full config:\n",
            "\u001b[38;5;204mCUDNN_BENCHMARK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;204mDATALOADER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mASPECT_RATIO_GROUPING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFILTER_EMPTY_ANNOTATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_WORKERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_SQRT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSAMPLER_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrainingSampler\u001b[39m\n",
            "\u001b[38;5;204mDATASETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141made20k_sem_seg_val\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141made20k_sem_seg_train\u001b[39m\n",
            "\u001b[38;5;204mDefault_loading\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;204mFLOAT32_PRECISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;204mGLOBAL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mHACK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;204mINPUT\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCOLOR_AUG_SSD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCROP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSINGLE_CATEGORY_MAX_AREA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mabsolute\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDATASET_MAPPER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mmask_former_semantic\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRGB\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMAGE_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_FORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mpolygon\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m307\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m358\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m409\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m460\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m563\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m614\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m665\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m716\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m819\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m870\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m921\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m972\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN_SAMPLING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mchoice\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRANDOM_FLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhorizontal\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mANCHOR_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mANGLES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-90\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m90\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPECT_RATIOS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mDefaultAnchorGenerator\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOFFSET\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m128\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFREEZE_AT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mbuild_resnet_backbone\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDEVICE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcuda\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFUSE_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msum\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLOAD_PROPOSALS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINO\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_BOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_CLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_DICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_GIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_MASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mseg\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NOISE_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEVAL_FLAG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mGIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIAL_PRED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLEARN_TGT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPANO_BOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRED_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TEMPERATURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.06\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TRANSFORM_EVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mTEST_FOUCUS_ON_BOX\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINODecoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPANOPTIC_FPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMBINE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCES_CONFIDENCE_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSTUFF_AREA_LIMIT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4096\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINSTANCE_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m123.675\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m116.28\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m103.53\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m58.395\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.375\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRPN\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESNETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_MODULATED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_NUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_ON_PER_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFrozenBN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES2_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES4_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_MULTI_GRID\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mbasic\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTRIDE_IN_1X1\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWIDTH_PER_GROUP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRETINANET\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_ALPHA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_GAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONVS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRIOR_PROB\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_LOSS_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOPK_CANDIDATES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_CASCADE_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id001\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOUS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id001\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_BBOX_REG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFC_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_FREQ_WEIGHT_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_NUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FC\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_ON_PRED_BOXES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_FED_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_SIGMOID_CE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRes5ROIHeads\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROPOSAL_APPEND_GT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_KEYPOINT_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_KEYPOINTS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mKRCNNConvDeconvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m17\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_MASK_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_MASK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskRCNNConvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOUNDARY_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHEAD_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mStandardRPNHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DILATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFEATURE_ORDER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhigh2low\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhard_pixel_mining\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mGN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOEncoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_DEPTHWISE_SEPARABLE_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mATTN_DROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m96\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMLP_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m24\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m224\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQKV_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQK_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_CHECKPOINT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m7\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mweights/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth\u001b[39m\n",
            "\u001b[38;5;204mOUTPUT_DIR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m./output\u001b[39m\n",
            "\u001b[38;5;204mSEED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR_END\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCLIP_GRADIENTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfull_model\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mGAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m16\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m160000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMOMENTUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNESTEROV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_DECAYS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mOPTIMIZER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mADAMW\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_CONSTANT_ENDING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREFERENCE_WORLD_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESCALE_INTERVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m135000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_EMBED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAUG\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMAX_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3584\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m896\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDETECTIONS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEXPECTED_RESULTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_OKS_SIGMAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECISE_BN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m200\u001b[39m\n",
            "\u001b[38;5;204mVERSION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;204mVIS_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\n",
            "\u001b[32m[07/27 14:22:38 detectron2]: \u001b[0mRunning with full config:\n",
            "\u001b[38;5;204mCUDNN_BENCHMARK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;204mDATALOADER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mASPECT_RATIO_GROUPING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFILTER_EMPTY_ANNOTATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_WORKERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_SQRT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSAMPLER_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrainingSampler\u001b[39m\n",
            "\u001b[38;5;204mDATASETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141made20k_sem_seg_val\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141made20k_sem_seg_train\u001b[39m\n",
            "\u001b[38;5;204mDefault_loading\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;204mFLOAT32_PRECISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;204mGLOBAL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mHACK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;204mINPUT\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCOLOR_AUG_SSD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCROP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSINGLE_CATEGORY_MAX_AREA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mabsolute\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDATASET_MAPPER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mmask_former_semantic\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRGB\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMAGE_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_FORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mpolygon\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m307\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m358\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m409\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m460\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m563\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m614\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m665\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m716\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m819\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m870\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m921\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m972\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN_SAMPLING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mchoice\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRANDOM_FLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhorizontal\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mANCHOR_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mANGLES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-90\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m90\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPECT_RATIOS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mDefaultAnchorGenerator\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOFFSET\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m128\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFREEZE_AT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mbuild_resnet_backbone\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDEVICE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcuda\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFUSE_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msum\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLOAD_PROPOSALS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINO\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_BOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_CLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_DICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_GIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_MASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mseg\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NOISE_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEVAL_FLAG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mGIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIAL_PRED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLEARN_TGT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPANO_BOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRED_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TEMPERATURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.06\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TRANSFORM_EVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mTEST_FOUCUS_ON_BOX\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINODecoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPANOPTIC_FPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMBINE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCES_CONFIDENCE_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSTUFF_AREA_LIMIT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4096\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINSTANCE_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m123.675\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m116.28\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m103.53\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m58.395\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.375\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRPN\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESNETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_MODULATED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_NUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_ON_PER_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFrozenBN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES2_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES4_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_MULTI_GRID\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mbasic\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTRIDE_IN_1X1\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWIDTH_PER_GROUP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRETINANET\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_ALPHA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_GAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONVS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRIOR_PROB\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_LOSS_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOPK_CANDIDATES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_CASCADE_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id001\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOUS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id001\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_BBOX_REG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFC_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_FREQ_WEIGHT_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_NUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FC\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_ON_PRED_BOXES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_FED_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_SIGMOID_CE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRes5ROIHeads\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROPOSAL_APPEND_GT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_KEYPOINT_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_KEYPOINTS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mKRCNNConvDeconvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m17\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_MASK_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_MASK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskRCNNConvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOUNDARY_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHEAD_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mStandardRPNHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DILATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFEATURE_ORDER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhigh2low\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhard_pixel_mining\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mGN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOEncoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_DEPTHWISE_SEPARABLE_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mATTN_DROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m96\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMLP_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m24\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m224\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQKV_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQK_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_CHECKPOINT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m7\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mweights/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth\u001b[39m\n",
            "\u001b[38;5;204mOUTPUT_DIR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m./output\u001b[39m\n",
            "\u001b[38;5;204mSEED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR_END\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCLIP_GRADIENTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfull_model\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mGAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m16\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m160000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMOMENTUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNESTEROV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_DECAYS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mOPTIMIZER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mADAMW\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_CONSTANT_ENDING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREFERENCE_WORLD_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESCALE_INTERVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m135000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_EMBED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAUG\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMAX_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3584\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m896\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDETECTIONS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEXPECTED_RESULTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_OKS_SIGMAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECISE_BN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m200\u001b[39m\n",
            "\u001b[38;5;204mVERSION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;204mVIS_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\n",
            "\u001b[32m[07/27 14:22:38 detectron2]: \u001b[0mFull config saved to ./output/config.yaml\n",
            "\u001b[32m[07/27 14:22:38 detectron2]: \u001b[0mFull config saved to ./output/config.yaml\n",
            "\u001b[32m[07/27 14:22:38 d2.utils.env]: \u001b[0mUsing a generated random seed 39055760\n",
            "\u001b[32m[07/27 14:22:38 d2.utils.env]: \u001b[0mUsing a generated random seed 39055760\n",
            "criterion.weight_dict  {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0}\n",
            "\u001b[32m[07/27 14:22:40 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from weights/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth ...\n",
            "\u001b[32m[07/27 14:22:40 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from weights/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth ...\n",
            "\u001b[32m[07/27 14:22:40 fvcore.common.checkpoint]: \u001b[0m[Checkpointer] Loading from weights/maskdino_r50_50ep_100q_celoss_hid1024_3s_semantic_ade20k_48.7miou.pth ...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py:2078: UserWarning: Provided key input for dynamic axes is not a valid input/output name\n",
            "  warnings.warn(\n",
            "/content/detectron2/detectron2/structures/image_list.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert t.shape[:-2] == tensors[0].shape[:-2], t.shape\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/maskdino_encoder.py:108: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src_flatten.device)\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/maskdino_encoder.py:169: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
            "  for lvl, (H_, W_) in enumerate(spatial_shapes):\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/modules/ms_deform_attn.py:96: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/modules/ms_deform_attn.py:106: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if reference_points.shape[-1] == 2:\n",
            "/content/MaskDINO/maskdino/modeling/transformer_decoder/maskdino_decoder.py:394: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src_flatten.device)\n",
            "/content/MaskDINO/maskdino/utils/utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if pos_tensor.size(-1) == 2:\n",
            "/content/MaskDINO/maskdino/utils/utils.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  elif pos_tensor.size(-1) == 4:\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/modules/ms_deform_attn.py:96: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/modules/ms_deform_attn.py:106: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if reference_points.shape[-1] == 2:\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/modules/ms_deform_attn.py:110: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  elif reference_points.shape[-1] == 4:\n",
            "/content/MaskDINO/maskdino/modeling/transformer_decoder/maskdino_decoder.py:528: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
            "  for a, b, c in zip(outputs_class[:-1], outputs_seg_masks[:-1], out_boxes[:-1])\n",
            "/content/MaskDINO/maskdino/maskdino.py:292: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
            "  for mask_cls_result, mask_pred_result, mask_box_result, input_per_image, image_size in zip(\n",
            "\u001b[32m[07/27 14:23:05 detectron2]: \u001b[0mInputs schema: TupleSchema(schemas=[ListSchema(schemas=[DictSchema(schemas=[IdentitySchema()], sizes=[1], keys=['image'])], sizes=[1])], sizes=[1])\n",
            "\u001b[32m[07/27 14:23:05 detectron2]: \u001b[0mInputs schema: TupleSchema(schemas=[ListSchema(schemas=[DictSchema(schemas=[IdentitySchema()], sizes=[1], keys=['image'])], sizes=[1])], sizes=[1])\n",
            "\u001b[32m[07/27 14:23:05 detectron2]: \u001b[0mOutputs schema: ListSchema(schemas=[DictSchema(schemas=[IdentitySchema()], sizes=[1], keys=['sem_seg'])], sizes=[1])\n",
            "\u001b[32m[07/27 14:23:05 detectron2]: \u001b[0mOutputs schema: ListSchema(schemas=[DictSchema(schemas=[IdentitySchema()], sizes=[1], keys=['sem_seg'])], sizes=[1])\n",
            "\u001b[32m[07/27 14:23:05 detectron2]: \u001b[0mSuccess.\n",
            "\u001b[32m[07/27 14:23:05 detectron2]: \u001b[0mSuccess.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MaskDINO\n",
        "!python export_model.py \\\n",
        "  --config-file /content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml \\\n",
        "  --output output/exported_model \\\n",
        "  --format onnx \\\n",
        "  --export-method tracing \\\n",
        "  --sample-image datasets/ADEChallengeData2016/images/validation/ADE_val_00000001.jpg \\\n",
        "  --output swinL_onnx_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zFyoNOWpDYkW",
        "outputId": "f4db653b-84f6-4978-fbeb-c862538ba335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "\u001b[32m[07/27 04:06:37 detectron2]: \u001b[0mCommand line arguments: Namespace(format='onnx', export_method='tracing', config_file='/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml', sample_image='datasets/ADEChallengeData2016/images/validation/ADE_val_00000001.jpg', run_eval=False, output='swinL_onnx_models', opts=[])\n",
            "[W init.cpp:842] Warning: Use _jit_set_fusion_strategy, bailout depth is deprecated. Setting to (STATIC, 1) (function operator())\n",
            "Loading config /content/MaskDINO/configs/ade20k/semantic-segmentation/Base-ADE20K-SemanticSegmentation.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "\u001b[32m[07/27 04:06:37 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[07/27 04:06:37 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[07/27 04:06:37 detectron2]: \u001b[0mEnvironment info:\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "sys.platform                     linux\n",
            "Python                           3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "numpy                            1.26.4\n",
            "detectron2                       0.6 @/content/detectron2/detectron2\n",
            "Compiler                         GCC 11.4\n",
            "CUDA compiler                    CUDA 12.5\n",
            "detectron2 arch flags            7.5\n",
            "DETECTRON2_ENV_MODULE            <not set>\n",
            "PyTorch                          2.1.0+cu121 @/usr/local/lib/python3.11/dist-packages/torch\n",
            "PyTorch debug build              False\n",
            "torch._C._GLIBCXX_USE_CXX11_ABI  False\n",
            "GPU available                    Yes\n",
            "GPU 0                            Tesla T4 (arch=7.5)\n",
            "Driver version                   550.54.15\n",
            "CUDA_HOME                        /usr/local/cuda\n",
            "Pillow                           11.3.0\n",
            "torchvision                      0.16.0+cu121 @/usr/local/lib/python3.11/dist-packages/torchvision\n",
            "torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0\n",
            "fvcore                           0.1.5.post20221221\n",
            "iopath                           0.1.9\n",
            "cv2                              4.12.0\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX512\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "\u001b[32m[07/27 04:06:37 detectron2]: \u001b[0mEnvironment info:\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "sys.platform                     linux\n",
            "Python                           3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "numpy                            1.26.4\n",
            "detectron2                       0.6 @/content/detectron2/detectron2\n",
            "Compiler                         GCC 11.4\n",
            "CUDA compiler                    CUDA 12.5\n",
            "detectron2 arch flags            7.5\n",
            "DETECTRON2_ENV_MODULE            <not set>\n",
            "PyTorch                          2.1.0+cu121 @/usr/local/lib/python3.11/dist-packages/torch\n",
            "PyTorch debug build              False\n",
            "torch._C._GLIBCXX_USE_CXX11_ABI  False\n",
            "GPU available                    Yes\n",
            "GPU 0                            Tesla T4 (arch=7.5)\n",
            "Driver version                   550.54.15\n",
            "CUDA_HOME                        /usr/local/cuda\n",
            "Pillow                           11.3.0\n",
            "torchvision                      0.16.0+cu121 @/usr/local/lib/python3.11/dist-packages/torchvision\n",
            "torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0\n",
            "fvcore                           0.1.5.post20221221\n",
            "iopath                           0.1.9\n",
            "cv2                              4.12.0\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX512\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "\u001b[32m[07/27 04:06:37 detectron2]: \u001b[0mCommand line arguments: Namespace(format='onnx', export_method='tracing', config_file='/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml', sample_image='datasets/ADEChallengeData2016/images/validation/ADE_val_00000001.jpg', run_eval=False, output='swinL_onnx_models', opts=[])\n",
            "\u001b[32m[07/27 04:06:37 detectron2]: \u001b[0mCommand line arguments: Namespace(format='onnx', export_method='tracing', config_file='/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml', sample_image='datasets/ADEChallengeData2016/images/validation/ADE_val_00000001.jpg', run_eval=False, output='swinL_onnx_models', opts=[])\n",
            "\u001b[32m[07/27 04:06:37 detectron2]: \u001b[0mContents of args.config_file=/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml:\n",
            "\u001b[38;5;204m_BASE_\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mBase-ADE20K-SemanticSegmentation.yaml\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINO\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mD2SwinTransformer\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m192\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m18\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m6\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m12\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m24\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m48\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mpretrained/swin_large_patch4_window12_384_22k.pth\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m123.675\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m116.280\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m103.530\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m58.395\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m57.120\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m57.375\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOHead\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mGN\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;245m# pixel decoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOEncoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres2\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres2\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINODecoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;245m# 9 decoder layers, add one for the loss on learnable query\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mseg\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR_END\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m320000\u001b[39m\n",
            "\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m(270000,300000)\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\n",
            "\u001b[32m[07/27 04:06:37 detectron2]: \u001b[0mContents of args.config_file=/content/MaskDINO/configs/ade20k/semantic-segmentation/maskdino_SwinL_bs16_160k_steplr.yaml:\n",
            "\u001b[38;5;204m_BASE_\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mBase-ADE20K-SemanticSegmentation.yaml\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINO\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mD2SwinTransformer\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m192\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m18\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m6\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m12\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m24\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m48\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mpretrained/swin_large_patch4_window12_384_22k.pth\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m123.675\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m116.280\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m103.530\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m58.395\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m57.120\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m57.375\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOHead\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mGN\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;245m# pixel decoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOEncoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres2\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres2\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINODecoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;245m# 9 decoder layers, add one for the loss on learnable query\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mseg\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR_END\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m320000\u001b[39m\n",
            "\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m(270000,300000)\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\n",
            "\u001b[32m[07/27 04:06:37 detectron2]: \u001b[0mRunning with full config:\n",
            "\u001b[38;5;204mCUDNN_BENCHMARK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;204mDATALOADER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mASPECT_RATIO_GROUPING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFILTER_EMPTY_ANNOTATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_WORKERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_SQRT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSAMPLER_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrainingSampler\u001b[39m\n",
            "\u001b[38;5;204mDATASETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141made20k_sem_seg_val\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141made20k_sem_seg_train\u001b[39m\n",
            "\u001b[38;5;204mDefault_loading\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;204mFLOAT32_PRECISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;204mGLOBAL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mHACK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;204mINPUT\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCOLOR_AUG_SSD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCROP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSINGLE_CATEGORY_MAX_AREA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mabsolute\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDATASET_MAPPER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mmask_former_semantic\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRGB\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMAGE_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_FORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mpolygon\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m307\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m358\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m409\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m460\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m563\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m614\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m665\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m716\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m819\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m870\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m921\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m972\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN_SAMPLING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mchoice\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRANDOM_FLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhorizontal\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mANCHOR_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mANGLES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-90\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m90\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPECT_RATIOS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mDefaultAnchorGenerator\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOFFSET\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m128\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFREEZE_AT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mD2SwinTransformer\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDEVICE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcuda\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFUSE_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msum\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLOAD_PROPOSALS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINO\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_BOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_CLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_DICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_GIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_MASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mseg\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NOISE_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEVAL_FLAG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mGIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIAL_PRED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLEARN_TGT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPANO_BOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRED_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TEMPERATURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.06\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TRANSFORM_EVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mTEST_FOUCUS_ON_BOX\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINODecoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPANOPTIC_FPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMBINE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCES_CONFIDENCE_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSTUFF_AREA_LIMIT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4096\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINSTANCE_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m123.675\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m116.28\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m103.53\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m58.395\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.375\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRPN\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESNETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_MODULATED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_NUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_ON_PER_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFrozenBN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES2_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES4_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_MULTI_GRID\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mbasic\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTRIDE_IN_1X1\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWIDTH_PER_GROUP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRETINANET\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_ALPHA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_GAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONVS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRIOR_PROB\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_LOSS_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOPK_CANDIDATES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_CASCADE_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id001\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOUS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id001\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_BBOX_REG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFC_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_FREQ_WEIGHT_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_NUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FC\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_ON_PRED_BOXES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_FED_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_SIGMOID_CE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRes5ROIHeads\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROPOSAL_APPEND_GT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_KEYPOINT_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_KEYPOINTS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mKRCNNConvDeconvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m17\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_MASK_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_MASK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskRCNNConvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOUNDARY_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHEAD_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mStandardRPNHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DILATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFEATURE_ORDER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhigh2low\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhard_pixel_mining\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mGN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOEncoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_DEPTHWISE_SEPARABLE_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mATTN_DROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m192\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMLP_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m24\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQKV_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQK_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_CHECKPOINT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mpretrained/swin_large_patch4_window12_384_22k.pth\u001b[39m\n",
            "\u001b[38;5;204mOUTPUT_DIR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m./output\u001b[39m\n",
            "\u001b[38;5;204mSEED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR_END\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCLIP_GRADIENTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfull_model\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mGAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m320000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMOMENTUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNESTEROV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_DECAYS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mOPTIMIZER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mADAMW\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_CONSTANT_ENDING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREFERENCE_WORLD_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESCALE_INTERVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m270000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m300000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_EMBED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAUG\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMAX_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3584\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m896\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDETECTIONS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEXPECTED_RESULTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_OKS_SIGMAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECISE_BN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m200\u001b[39m\n",
            "\u001b[38;5;204mVERSION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;204mVIS_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\n",
            "\u001b[32m[07/27 04:06:37 detectron2]: \u001b[0mRunning with full config:\n",
            "\u001b[38;5;204mCUDNN_BENCHMARK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;204mDATALOADER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mASPECT_RATIO_GROUPING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFILTER_EMPTY_ANNOTATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_WORKERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_SQRT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSAMPLER_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrainingSampler\u001b[39m\n",
            "\u001b[38;5;204mDATASETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141made20k_sem_seg_val\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141made20k_sem_seg_train\u001b[39m\n",
            "\u001b[38;5;204mDefault_loading\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;204mFLOAT32_PRECISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;204mGLOBAL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mHACK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;204mINPUT\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCOLOR_AUG_SSD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCROP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSINGLE_CATEGORY_MAX_AREA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mabsolute\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDATASET_MAPPER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mmask_former_semantic\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRGB\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMAGE_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_FORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mpolygon\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m307\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m358\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m409\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m460\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m563\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m614\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m665\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m716\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m819\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m870\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m921\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m972\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN_SAMPLING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mchoice\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRANDOM_FLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhorizontal\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mANCHOR_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mANGLES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-90\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m90\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPECT_RATIOS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mDefaultAnchorGenerator\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOFFSET\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m128\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFREEZE_AT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mD2SwinTransformer\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDEVICE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcuda\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFUSE_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msum\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLOAD_PROPOSALS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINO\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_BOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_CLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_DICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_GIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_MASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mseg\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NOISE_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEVAL_FLAG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mGIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIAL_PRED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLEARN_TGT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPANO_BOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRED_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TEMPERATURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.06\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TRANSFORM_EVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mTEST_FOUCUS_ON_BOX\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINODecoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPANOPTIC_FPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMBINE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCES_CONFIDENCE_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSTUFF_AREA_LIMIT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4096\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINSTANCE_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m123.675\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m116.28\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m103.53\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m58.395\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.375\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRPN\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESNETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_MODULATED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_NUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_ON_PER_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFrozenBN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES2_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES4_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_MULTI_GRID\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mbasic\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTRIDE_IN_1X1\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWIDTH_PER_GROUP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRETINANET\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_ALPHA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_GAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONVS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRIOR_PROB\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_LOSS_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOPK_CANDIDATES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_CASCADE_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id001\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOUS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id001\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_BBOX_REG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFC_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_FREQ_WEIGHT_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_NUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FC\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_ON_PRED_BOXES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_FED_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_SIGMOID_CE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRes5ROIHeads\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROPOSAL_APPEND_GT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_KEYPOINT_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_KEYPOINTS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mKRCNNConvDeconvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m17\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_MASK_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_MASK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskRCNNConvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOUNDARY_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHEAD_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mStandardRPNHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DILATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFEATURE_ORDER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhigh2low\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhard_pixel_mining\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mGN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m150\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOEncoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_DEPTHWISE_SEPARABLE_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mATTN_DROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m192\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMLP_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m24\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQKV_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQK_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_CHECKPOINT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mpretrained/swin_large_patch4_window12_384_22k.pth\u001b[39m\n",
            "\u001b[38;5;204mOUTPUT_DIR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m./output\u001b[39m\n",
            "\u001b[38;5;204mSEED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR_END\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCLIP_GRADIENTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfull_model\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mGAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m320000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMOMENTUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNESTEROV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_DECAYS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mOPTIMIZER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mADAMW\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_CONSTANT_ENDING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREFERENCE_WORLD_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESCALE_INTERVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m270000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m300000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_EMBED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAUG\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMAX_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3584\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m896\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDETECTIONS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEXPECTED_RESULTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_OKS_SIGMAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECISE_BN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m200\u001b[39m\n",
            "\u001b[38;5;204mVERSION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;204mVIS_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\n",
            "\u001b[32m[07/27 04:06:38 detectron2]: \u001b[0mFull config saved to ./output/config.yaml\n",
            "\u001b[32m[07/27 04:06:38 detectron2]: \u001b[0mFull config saved to ./output/config.yaml\n",
            "\u001b[32m[07/27 04:06:38 d2.utils.env]: \u001b[0mUsing a generated random seed 38303017\n",
            "\u001b[32m[07/27 04:06:38 d2.utils.env]: \u001b[0mUsing a generated random seed 38303017\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "criterion.weight_dict  {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0}\n",
            "\u001b[32m[07/27 04:06:40 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from pretrained/swin_large_patch4_window12_384_22k.pth ...\n",
            "\u001b[32m[07/27 04:06:40 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from pretrained/swin_large_patch4_window12_384_22k.pth ...\n",
            "\u001b[32m[07/27 04:06:40 fvcore.common.checkpoint]: \u001b[0m[Checkpointer] Loading from pretrained/swin_large_patch4_window12_384_22k.pth ...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 04:06:41 fvcore.common.checkpoint]: \u001b[0mSome model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mbackbone.layers.0.blocks.0.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.blocks.0.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.blocks.0.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.blocks.0.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.blocks.0.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.blocks.0.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.blocks.0.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.blocks.1.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.blocks.1.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.blocks.1.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.blocks.1.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.blocks.1.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.blocks.1.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.blocks.1.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.downsample.norm.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.0.downsample.reduction.weight\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.0.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.0.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.0.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.0.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.0.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.0.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.0.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.1.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.1.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.1.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.1.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.1.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.1.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.blocks.1.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.downsample.norm.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.1.downsample.reduction.weight\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.0.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.0.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.0.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.0.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.0.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.0.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.0.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.1.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.1.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.1.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.1.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.1.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.1.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.1.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.10.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.10.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.10.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.10.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.10.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.10.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.10.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.11.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.11.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.11.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.11.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.11.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.11.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.11.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.12.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.12.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.12.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.12.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.12.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.12.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.12.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.13.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.13.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.13.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.13.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.13.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.13.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.13.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.14.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.14.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.14.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.14.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.14.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.14.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.14.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.15.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.15.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.15.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.15.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.15.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.15.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.15.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.16.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.16.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.16.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.16.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.16.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.16.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.16.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.17.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.17.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.17.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.17.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.17.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.17.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.17.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.2.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.2.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.2.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.2.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.2.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.2.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.2.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.3.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.3.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.3.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.3.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.3.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.3.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.3.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.4.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.4.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.4.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.4.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.4.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.4.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.4.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.5.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.5.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.5.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.5.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.5.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.5.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.5.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.6.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.6.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.6.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.6.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.6.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.6.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.6.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.7.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.7.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.7.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.7.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.7.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.7.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.7.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.8.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.8.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.8.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.8.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.8.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.8.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.8.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.9.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.9.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.9.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.9.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.9.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.9.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.blocks.9.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.downsample.norm.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.2.downsample.reduction.weight\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.0.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.0.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.0.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.0.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.0.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.0.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.0.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.1.attn.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.1.attn.qkv.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.1.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.1.mlp.fc1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.1.mlp.fc2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.1.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.layers.3.blocks.1.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.norm0.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.patch_embed.norm.{bias, weight}\u001b[0m\n",
            "\u001b[34mbackbone.patch_embed.proj.{bias, weight}\u001b[0m\n",
            "\u001b[34mcriterion.empty_weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.adapter_1.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.3.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.3.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.4.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.4.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.layer_1.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.level_embed\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor._bbox_embed.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor._bbox_embed.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor._bbox_embed.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.0.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.0.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.0.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.1.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.1.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.1.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.2.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.2.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.2.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.3.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.3.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.3.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.4.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.4.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.4.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.5.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.5.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.5.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.6.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.6.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.6.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.7.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.7.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.7.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.8.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.8.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.8.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.class_embed.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.0.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.0.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.0.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.1.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.1.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.1.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.2.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.2.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.2.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.3.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.3.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.3.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.4.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.4.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.4.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.5.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.5.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.5.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.6.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.6.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.6.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.7.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.7.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.7.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.8.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.8.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.8.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.norm.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.ref_point_head.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.ref_point_head.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder_norm.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.label_enc.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.query_embed.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.query_feat.weight\u001b[0m\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/27 04:06:41 fvcore.common.checkpoint]: \u001b[0mThe checkpoint state_dict contains keys that are not used by the model:\n",
            "  \u001b[35mpatch_embed.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mpatch_embed.norm.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.0.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.0.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.0.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.0.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.0.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.0.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.1.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.1.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.1.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.1.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.1.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.1.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.downsample.norm.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.0.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.0.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.0.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.0.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.0.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.0.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.1.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.1.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.1.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.1.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.1.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.1.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.1.downsample.norm.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.0.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.0.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.0.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.0.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.0.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.0.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.1.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.1.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.1.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.1.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.1.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.1.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.2.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.2.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.2.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.2.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.2.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.2.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.3.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.3.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.3.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.3.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.3.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.3.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.4.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.4.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.4.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.4.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.4.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.4.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.5.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.5.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.5.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.5.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.5.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.5.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.6.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.6.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.6.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.6.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.6.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.6.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.7.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.7.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.7.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.7.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.7.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.7.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.8.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.8.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.8.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.8.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.8.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.8.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.9.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.9.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.9.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.9.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.9.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.9.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.10.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.10.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.10.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.10.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.10.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.10.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.11.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.11.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.11.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.11.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.11.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.11.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.12.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.12.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.12.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.12.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.12.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.12.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.13.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.13.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.13.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.13.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.13.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.13.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.14.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.14.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.14.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.14.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.14.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.14.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.15.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.15.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.15.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.15.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.15.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.15.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.16.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.16.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.16.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.16.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.16.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.16.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.17.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.17.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.17.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.17.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.17.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.17.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.2.downsample.norm.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.0.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.0.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.0.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.0.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.0.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.0.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.1.norm1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.1.attn.qkv.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.1.attn.proj.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.1.norm2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.1.mlp.fc1.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.1.mlp.fc2.{bias, weight}\u001b[0m\n",
            "  \u001b[35mnorm.{bias, weight}\u001b[0m\n",
            "  \u001b[35mhead.{bias, weight}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.0.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.1.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.0.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.1.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.0.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.1.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.2.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.3.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.4.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.5.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.6.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.7.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.8.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.9.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.10.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.11.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.12.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.13.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.14.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.15.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.16.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.17.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.0.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.3.blocks.1.attn.{relative_position_bias_table, relative_position_index}\u001b[0m\n",
            "  \u001b[35mlayers.0.blocks.1.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.1.blocks.1.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.1.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.3.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.5.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.7.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.9.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.11.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.13.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.15.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.2.blocks.17.attn_mask\u001b[0m\n",
            "  \u001b[35mlayers.0.downsample.reduction.weight\u001b[0m\n",
            "  \u001b[35mlayers.1.downsample.reduction.weight\u001b[0m\n",
            "  \u001b[35mlayers.2.downsample.reduction.weight\u001b[0m\n",
            "/content/detectron2/detectron2/structures/image_list.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert t.shape[:-2] == tensors[0].shape[:-2], t.shape\n",
            "/content/MaskDINO/maskdino/modeling/backbone/swin.py:483: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if W % self.patch_size[1] != 0:\n",
            "/content/MaskDINO/maskdino/modeling/backbone/swin.py:485: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if H % self.patch_size[0] != 0:\n",
            "/content/MaskDINO/maskdino/modeling/backbone/swin.py:414: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
            "/content/MaskDINO/maskdino/modeling/backbone/swin.py:415: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
            "/content/MaskDINO/maskdino/modeling/backbone/swin.py:244: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert L == H * W, \"input feature has wrong size\"\n",
            "/content/MaskDINO/maskdino/modeling/backbone/swin.py:68: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
            "/content/MaskDINO/maskdino/modeling/backbone/swin.py:286: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if pad_r > 0 or pad_b > 0:\n",
            "/content/MaskDINO/maskdino/modeling/backbone/swin.py:318: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert L == H * W, \"input feature has wrong size\"\n",
            "/content/MaskDINO/maskdino/modeling/backbone/swin.py:323: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
            "/content/MaskDINO/maskdino/modeling/backbone/swin.py:324: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if pad_input:\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/maskdino_encoder.py:108: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src_flatten.device)\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/maskdino_encoder.py:169: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
            "  for lvl, (H_, W_) in enumerate(spatial_shapes):\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/modules/ms_deform_attn.py:96: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/modules/ms_deform_attn.py:106: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if reference_points.shape[-1] == 2:\n",
            "/content/MaskDINO/maskdino/modeling/transformer_decoder/maskdino_decoder.py:394: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src_flatten.device)\n",
            "/content/MaskDINO/maskdino/utils/utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if pos_tensor.size(-1) == 2:\n",
            "/content/MaskDINO/maskdino/utils/utils.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  elif pos_tensor.size(-1) == 4:\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/modules/ms_deform_attn.py:96: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/modules/ms_deform_attn.py:106: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if reference_points.shape[-1] == 2:\n",
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/modules/ms_deform_attn.py:110: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  elif reference_points.shape[-1] == 4:\n",
            "/content/MaskDINO/maskdino/modeling/transformer_decoder/maskdino_decoder.py:528: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
            "  for a, b, c in zip(outputs_class[:-1], outputs_seg_masks[:-1], out_boxes[:-1])\n",
            "/content/MaskDINO/maskdino/maskdino.py:292: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
            "  for mask_cls_result, mask_pred_result, mask_box_result, input_per_image, image_size in zip(\n",
            "\u001b[32m[07/27 04:08:38 detectron2]: \u001b[0mInputs schema: TupleSchema(schemas=[ListSchema(schemas=[DictSchema(schemas=[IdentitySchema()], sizes=[1], keys=['image'])], sizes=[1])], sizes=[1])\n",
            "\u001b[32m[07/27 04:08:38 detectron2]: \u001b[0mInputs schema: TupleSchema(schemas=[ListSchema(schemas=[DictSchema(schemas=[IdentitySchema()], sizes=[1], keys=['image'])], sizes=[1])], sizes=[1])\n",
            "\u001b[32m[07/27 04:08:38 detectron2]: \u001b[0mOutputs schema: ListSchema(schemas=[DictSchema(schemas=[IdentitySchema()], sizes=[1], keys=['sem_seg'])], sizes=[1])\n",
            "\u001b[32m[07/27 04:08:38 detectron2]: \u001b[0mOutputs schema: ListSchema(schemas=[DictSchema(schemas=[IdentitySchema()], sizes=[1], keys=['sem_seg'])], sizes=[1])\n",
            "\u001b[32m[07/27 04:08:38 detectron2]: \u001b[0mSuccess.\n",
            "\u001b[32m[07/27 04:08:38 detectron2]: \u001b[0mSuccess.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluation onnx model"
      ],
      "metadata": {
        "id": "d6oqXXq3VFOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MaskDINO/eval_onnx_model.py\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import detectron2.data.transforms as T\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.evaluation.sem_seg_evaluation import SemSegEvaluator\n",
        "import cv2\n",
        "\n",
        "def get_preprocessing(cfg):\n",
        "    target_size = (683, 512)  # (width, height)\n",
        "\n",
        "    def preprocess_func(inputs):\n",
        "        image = utils.read_image(inputs[\"file_name\"], format=cfg.INPUT.FORMAT)\n",
        "        image = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n",
        "        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))  # [C, H, W]\n",
        "        inputs[\"image\"] = image\n",
        "        return inputs\n",
        "\n",
        "    return preprocess_func\n",
        "\n",
        "def compute_iou_incremental(confusion, pred, gt, num_classes):\n",
        "    mask = (gt >= 0) & (gt < num_classes)\n",
        "    label = num_classes * gt[mask].astype(int) + pred[mask]\n",
        "    count = np.bincount(label, minlength=num_classes**2)\n",
        "    confusion += count.reshape(num_classes, num_classes)\n",
        "    return confusion\n",
        "\n",
        "def normalize_and_colorize(pred, colormap):\n",
        "    color_mask = np.zeros((pred.shape[0], pred.shape[1], 3), dtype=np.uint8)\n",
        "    for label_id, color in enumerate(colormap):\n",
        "        color_mask[pred == label_id] = color\n",
        "    return color_mask\n",
        "\n",
        "def load_and_evaluate(session, dataset_dicts, preprocess_func, metadata, save_dir=None):\n",
        "    input_name = session.get_inputs()[0].name\n",
        "    num_classes = len(metadata.stuff_classes)\n",
        "    confusion = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for idx, data in enumerate(tqdm(dataset_dicts)):\n",
        "        data = preprocess_func(data)\n",
        "        image = data[\"image\"].numpy()  # [3, H, W]\n",
        "\n",
        "        output = session.run(None, {input_name: image})[0]\n",
        "        if output.ndim == 4:\n",
        "            output = output[0]\n",
        "\n",
        "        pred = np.argmax(output, axis=0).astype(np.uint8)\n",
        "\n",
        "        gt = utils.read_image(data[\"sem_seg_file_name\"], \"L\")\n",
        "        gt = cv2.resize(gt, (pred.shape[1], pred.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        confusion = compute_iou_incremental(confusion, pred, gt, num_classes)\n",
        "\n",
        "        # Save colorized prediction mask\n",
        "        if save_dir:\n",
        "            file_name = os.path.basename(data[\"file_name\"])\n",
        "            name, _ = os.path.splitext(file_name)\n",
        "            color_pred = normalize_and_colorize(pred, metadata.stuff_colors)\n",
        "            save_path = os.path.join(save_dir, f\"{name}_pred.png\")\n",
        "            cv2.imwrite(save_path, cv2.cvtColor(color_pred, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    iou = np.diag(confusion) / (\n",
        "        np.sum(confusion, axis=1) + np.sum(confusion, axis=0) - np.diag(confusion) + 1e-10\n",
        "    )\n",
        "    mIoU = np.nanmean(iou)\n",
        "    return mIoU, iou\n",
        "\n",
        "def main(args):\n",
        "    from detectron2.config import get_cfg\n",
        "    from maskdino import add_maskdino_config\n",
        "    from detectron2.projects.deeplab import add_deeplab_config\n",
        "\n",
        "    cfg = get_cfg()\n",
        "    add_deeplab_config(cfg)\n",
        "    add_maskdino_config(cfg)\n",
        "    cfg.merge_from_file(args.config_file)\n",
        "    cfg.freeze()\n",
        "\n",
        "    dataset_name = cfg.DATASETS.TEST[0]\n",
        "    dataset_dicts = DatasetCatalog.get(dataset_name)\n",
        "    metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "    preprocess_func = get_preprocessing(cfg)\n",
        "\n",
        "    providers = ['CUDAExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']\n",
        "    session = ort.InferenceSession(args.onnx_model_path, providers=providers)\n",
        "\n",
        "    mIoU, per_class_iou = load_and_evaluate(\n",
        "        session, dataset_dicts, preprocess_func, metadata, save_dir=args.output_dir\n",
        "    )\n",
        "\n",
        "    print(\"Mean IoU: {:.4f}\".format(mIoU))\n",
        "    for i, iou in enumerate(per_class_iou):\n",
        "        print(f\"Class {i}: IoU = {iou:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--onnx-model-path\", type=str, required=True, help=\"Path to ONNX model file\")\n",
        "    parser.add_argument(\"--config-file\", type=str, required=True, help=\"Path to Detectron2 config file\")\n",
        "    parser.add_argument(\"--output-dir\", type=str, default=\"pred_masks\", help=\"Folder to save predicted masks\")\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY1NhEg-PPjZ",
        "outputId": "ec0f615c-78b5-4b47-d9e6-cc1c1fd250e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MaskDINO/eval_onnx_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MaskDINO\n",
        "!python eval_onnx_model.py \\\n",
        "  --config-file configs/ade20k/semantic-segmentation/maskdino_R50_bs16_160k_steplr.yaml \\\n",
        "  --onnx-model-path r50_onnx_models/model.onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0zuEB_2PaEe",
        "outputId": "5edbc38b-9722-4e9b-e38b-e4925751af1f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Loading config configs/ade20k/semantic-segmentation/Base-ADE20K-SemanticSegmentation.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "  3% 53/2000 [00:17<10:34,  3.07it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MaskDINO/eval_onnx_model.py\", line 109, in <module>\n",
            "    main(args)\n",
            "  File \"/content/MaskDINO/eval_onnx_model.py\", line 95, in main\n",
            "    mIoU, per_class_iou = load_and_evaluate(\n",
            "                          ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/MaskDINO/eval_onnx_model.py\", line 54, in load_and_evaluate\n",
            "    pred = np.argmax(output, axis=0).astype(np.uint8)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\", line 1229, in argmax\n",
            "    return _wrapfunc(a, 'argmax', axis=axis, out=out, **kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n",
            "    return bound(*args, **kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}
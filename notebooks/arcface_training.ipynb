{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11785377,"sourceType":"datasetVersion","datasetId":7399584},{"sourceId":12034218,"sourceType":"datasetVersion","datasetId":7572107}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load the model and requirements","metadata":{}},{"cell_type":"code","source":"!pip install gdown -q\n!pip install -q scipy==1.8.1 --force-reinstall","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip show scipy\n!python --version","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/Arcface_torch/requirement.txt\ntensorboard\neasydict\nmxnet==1.8.0\nonnx\nscikit-learn\nopencv-python\nnumpy==1.23.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/Arcface_torch\n!pip install -q -r requirement.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load checkpoint","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/Arcface_torch/dataset.py\nimport numbers\nimport os\nimport queue as Queue\nimport threading\nfrom typing import Iterable\n\nimport mxnet as mx\nimport numpy as np\nimport torch\nfrom functools import partial\nfrom torch import distributed\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom utils.utils_distributed_sampler import DistributedSampler\nfrom utils.utils_distributed_sampler import get_dist_info, worker_init_fn\n\n\ndef get_dataloader(\n    root_dir,\n    local_rank,\n    batch_size,\n    dali = False,\n    dali_aug = False,\n    seed = 2048,\n    num_workers = 2,\n    image_size = 112,\n  \n    ) -> Iterable:\n\n    rec = os.path.join(root_dir, 'train.rec')\n    idx = os.path.join(root_dir, 'train.idx')\n    train_set = None\n\n    # Synthetic\n    if root_dir == \"synthetic\":\n        train_set = SyntheticDataset()\n        dali = False\n\n    # Mxnet RecordIO\n    elif os.path.exists(rec) and os.path.exists(idx):\n        train_set = MXFaceDataset(root_dir=root_dir, local_rank=local_rank, image_size = image_size)\n\n    # Image Folder\n    else:\n        transform = transforms.Compose([\n             transforms.RandomHorizontalFlip(),\n             transforms.ToTensor(),\n             transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n             ])\n        train_set = ImageFolder(root_dir, transform)\n\n    # DALI\n    if dali:\n        return dali_data_iter(\n            batch_size=batch_size, rec_file=rec, idx_file=idx,\n            num_threads=2, local_rank=local_rank, dali_aug=dali_aug)\n\n    rank, world_size = get_dist_info()\n    train_sampler = DistributedSampler(\n        train_set, num_replicas=world_size, rank=rank, shuffle=True, seed=seed)\n\n    if seed is None:\n        init_fn = None\n    else:\n        init_fn = partial(worker_init_fn, num_workers=num_workers, rank=rank, seed=seed)\n\n    train_loader = DataLoaderX(\n        local_rank=local_rank,\n        dataset=train_set,\n        batch_size=batch_size,\n        sampler=train_sampler,\n        num_workers=num_workers,\n        pin_memory=True,\n        drop_last=True,\n        worker_init_fn=init_fn,\n    )\n\n    return train_loader\n\nclass BackgroundGenerator(threading.Thread):\n    def __init__(self, generator, local_rank, max_prefetch=6):\n        super(BackgroundGenerator, self).__init__()\n        self.queue = Queue.Queue(max_prefetch)\n        self.generator = generator\n        self.local_rank = local_rank\n        self.daemon = True\n        self.start()\n\n    def run(self):\n        torch.cuda.set_device(self.local_rank)\n        for item in self.generator:\n            self.queue.put(item)\n        self.queue.put(None)\n\n    def next(self):\n        next_item = self.queue.get()\n        if next_item is None:\n            raise StopIteration\n        return next_item\n\n    def __next__(self):\n        return self.next()\n\n    def __iter__(self):\n        return self\n\n\nclass DataLoaderX(DataLoader):\n\n    def __init__(self, local_rank, **kwargs):\n        super(DataLoaderX, self).__init__(**kwargs)\n        self.stream = torch.cuda.Stream(local_rank)\n        self.local_rank = local_rank\n\n    def __iter__(self):\n        self.iter = super(DataLoaderX, self).__iter__()\n        self.iter = BackgroundGenerator(self.iter, self.local_rank)\n        self.preload()\n        return self\n\n    def preload(self):\n        self.batch = next(self.iter, None)\n        if self.batch is None:\n            return None\n        with torch.cuda.stream(self.stream):\n            for k in range(len(self.batch)):\n                self.batch[k] = self.batch[k].to(device=self.local_rank, non_blocking=True)\n\n    def __next__(self):\n        torch.cuda.current_stream().wait_stream(self.stream)\n        batch = self.batch\n        if batch is None:\n            raise StopIteration\n        self.preload()\n        return batch\n\n\nclass MXFaceDataset(Dataset):\n    def __init__(self, root_dir, local_rank, image_size):\n        super(MXFaceDataset, self).__init__()\n        self.transform = transforms.Compose(\n            [transforms.ToPILImage(),\n             transforms.Resize((image_size, image_size)),\n             transforms.RandomHorizontalFlip(),\n             transforms.ToTensor(),\n             transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n             ])\n        self.root_dir = root_dir\n        self.local_rank = local_rank\n        path_imgrec = os.path.join(root_dir, 'train.rec')\n        path_imgidx = os.path.join(root_dir, 'train.idx')\n        self.imgrec = mx.recordio.MXIndexedRecordIO(path_imgidx, path_imgrec, 'r')\n        s = self.imgrec.read_idx(0)\n        header, _ = mx.recordio.unpack(s)\n        if header.flag > 0:\n            self.header0 = (int(header.label[0]), int(header.label[1]))\n            self.imgidx = np.array(range(1, int(header.label[0])))\n        else:\n            self.imgidx = np.array(list(self.imgrec.keys))\n\n    def __getitem__(self, index):\n        idx = self.imgidx[index]\n        s = self.imgrec.read_idx(idx)\n        header, img = mx.recordio.unpack(s)\n        label = header.label\n        if not isinstance(label, numbers.Number):\n            label = label[0]\n        # label = torch.tensor(label, dtype=torch.long)\n        label = torch.tensor(int(label), dtype=torch.long)\n\n        sample = mx.image.imdecode(img).asnumpy()\n        if self.transform is not None:\n            sample = self.transform(sample)\n        return sample, label\n\n    def __len__(self):\n        return len(self.imgidx)\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self):\n        super(SyntheticDataset, self).__init__()\n        img = np.random.randint(0, 255, size=(112, 112, 3), dtype=np.int32)\n        img = np.transpose(img, (2, 0, 1))\n        img = torch.from_numpy(img).squeeze(0).float()\n        img = ((img / 255) - 0.5) / 0.5\n        self.img = img\n        self.label = 1\n\n    def __getitem__(self, index):\n        return self.img, self.label\n\n    def __len__(self):\n        return 1000000\n\n\ndef dali_data_iter(\n    batch_size: int, rec_file: str, idx_file: str, num_threads: int,\n    initial_fill=32768, random_shuffle=True,\n    prefetch_queue_depth=1, local_rank=0, name=\"reader\",\n    mean=(127.5, 127.5, 127.5), \n    std=(127.5, 127.5, 127.5),\n    dali_aug=False\n    ):\n    \"\"\"\n    Parameters:\n    ----------\n    initial_fill: int\n        Size of the buffer that is used for shuffling. If random_shuffle is False, this parameter is ignored.\n\n    \"\"\"\n    rank: int = distributed.get_rank()\n    world_size: int = distributed.get_world_size()\n    import nvidia.dali.fn as fn\n    import nvidia.dali.types as types\n    from nvidia.dali.pipeline import Pipeline\n    from nvidia.dali.plugin.pytorch import DALIClassificationIterator\n\n    def dali_random_resize(img, resize_size, image_size=112):\n        img = fn.resize(img, resize_x=resize_size, resize_y=resize_size)\n        img = fn.resize(img, size=(image_size, image_size))\n        return img\n    def dali_random_gaussian_blur(img, window_size):\n        img = fn.gaussian_blur(img, window_size=window_size * 2 + 1)\n        return img\n    def dali_random_gray(img, prob_gray):\n        saturate = fn.random.coin_flip(probability=1 - prob_gray)\n        saturate = fn.cast(saturate, dtype=types.FLOAT)\n        img = fn.hsv(img, saturation=saturate)\n        return img\n    def dali_random_hsv(img, hue, saturation):\n        img = fn.hsv(img, hue=hue, saturation=saturation)\n        return img\n    def multiplexing(condition, true_case, false_case):\n        neg_condition = condition ^ True\n        return condition * true_case + neg_condition * false_case\n\n    condition_resize = fn.random.coin_flip(probability=0.1)\n    size_resize = fn.random.uniform(range=(int(112 * 0.5), int(112 * 0.8)), dtype=types.FLOAT)\n    condition_blur = fn.random.coin_flip(probability=0.2)\n    window_size_blur = fn.random.uniform(range=(1, 2), dtype=types.INT32)\n    condition_flip = fn.random.coin_flip(probability=0.5)\n    condition_hsv = fn.random.coin_flip(probability=0.2)\n    hsv_hue = fn.random.uniform(range=(0., 20.), dtype=types.FLOAT)\n    hsv_saturation = fn.random.uniform(range=(1., 1.2), dtype=types.FLOAT)\n\n    pipe = Pipeline(\n        batch_size=batch_size, num_threads=num_threads,\n        device_id=local_rank, prefetch_queue_depth=prefetch_queue_depth, )\n    condition_flip = fn.random.coin_flip(probability=0.5)\n    with pipe:\n        jpegs, labels = fn.readers.mxnet(\n            path=rec_file, index_path=idx_file, initial_fill=initial_fill, \n            num_shards=world_size, shard_id=rank,\n            random_shuffle=random_shuffle, pad_last_batch=False, name=name)\n        images = fn.decoders.image(jpegs, device=\"mixed\", output_type=types.RGB)\n        if dali_aug:\n            images = fn.cast(images, dtype=types.UINT8)\n            images = multiplexing(condition_resize, dali_random_resize(images, size_resize, image_size=112), images)\n            images = multiplexing(condition_blur, dali_random_gaussian_blur(images, window_size_blur), images)\n            images = multiplexing(condition_hsv, dali_random_hsv(images, hsv_hue, hsv_saturation), images)\n            images = dali_random_gray(images, 0.1)\n\n        images = fn.crop_mirror_normalize(\n            images, dtype=types.FLOAT, mean=mean, std=std, mirror=condition_flip)\n        pipe.set_outputs(images, labels)\n    pipe.build()\n    return DALIWarper(DALIClassificationIterator(pipelines=[pipe], reader_name=name, ))\n\n\n@torch.no_grad()\nclass DALIWarper(object):\n    def __init__(self, dali_iter):\n        self.iter = dali_iter\n\n    def __next__(self):\n        data_dict = self.iter.__next__()[0]\n        tensor_data = data_dict['data'].cuda()\n        tensor_label: torch.Tensor = data_dict['label'].cuda().long()\n        tensor_label.squeeze_()\n        return tensor_data, tensor_label\n\n    def __iter__(self):\n        return self\n\n    def reset(self):\n        self.iter.reset()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/Arcface_torch/backbones/iresnet_plus.py\nimport torch\nfrom torch import nn\nfrom torch.utils.checkpoint import checkpoint\nimport math\n\n__all__ = ['iresnet18_plus', 'iresnet34_plus', 'iresnet50_plus', 'iresnet100_plus', 'iresnet200_plus']\nusing_ckpt = False\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=3,\n                     stride=stride,\n                     padding=dilation,\n                     groups=groups,\n                     bias=False,\n                     dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=1,\n                     stride=stride,\n                     bias=False)\n\n\nclass IBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 groups=1, base_width=64, dilation=1):\n        super(IBasicBlock, self).__init__()\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        \n        # Add dilated convolution support\n        self.dilation = dilation\n        self.stride = stride\n        \n        # First conv layer with dilation\n        self.bn1 = nn.BatchNorm2d(inplanes, eps=1e-05)\n        self.conv1 = conv3x3(inplanes, planes, stride=stride, dilation=dilation)\n        \n        # Second conv layer with dilation\n        self.bn2 = nn.BatchNorm2d(planes, eps=1e-05)\n        self.prelu = nn.PReLU(planes)\n        self.conv2 = conv3x3(planes, planes, dilation=dilation)\n        self.bn3 = nn.BatchNorm2d(planes, eps=1e-05)\n        \n        # Downsample layer\n        self.downsample = downsample\n        \n        # Initialize weights using Kaiming initialization\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward_impl(self, x):\n        identity = x\n        \n        # First conv block\n        out = self.bn1(x)\n        out = self.conv1(out)\n        out = self.bn2(out)\n        out = self.prelu(out)\n        \n        # Second conv block\n        out = self.conv2(out)\n        out = self.bn3(out)\n        \n        # Add skip connection\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        \n        # Add residual connection\n        out += identity\n        return out\n\n    def forward(self, x):\n        if self.training and using_ckpt:\n            return checkpoint(self.forward_impl, x)\n        else:\n            return self.forward_impl(x)\n\n\nclass IResNet(nn.Module):\n    def __init__(self,\n                 block, layers, dropout=0, num_features=512, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None, fp16=False, image_size=112):\n        super(IResNet, self).__init__()\n        self.device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n        self.extra_gflops = 0.0\n        self.fp16 = fp16\n        self.inplanes = 64\n        self.dilation = 1\n        \n        # Calculate feature map size after all stride operations\n        self.fc_scale = (image_size // 16)**2  # 4 stride=2 operations: 2^4 = 16\n        \n        if replace_stride_with_dilation is None:\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        \n        self.groups = groups\n        self.base_width = width_per_group\n        \n        # Initial conv layer\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplanes, eps=1e-05)\n        self.prelu = nn.PReLU(self.inplanes)\n        \n        # Create layers with skip connections\n        self.layer1 = self._make_layer(block, 96, layers[0], stride=2)\n        self.layer2 = self._make_layer(block, 160, layers[1], stride=2,\n                                     dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 320, layers[2], stride=2,\n                                     dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                     dilate=replace_stride_with_dilation[2])\n        \n        # Final layers\n        self.bn2 = nn.BatchNorm2d(512 * block.expansion, eps=1e-05)\n        self.dropout = nn.Dropout(p=dropout, inplace=True)\n        \n        # Calculate input size for fc layer\n        fc_input_size = 512 * block.expansion * self.fc_scale\n        self.fc = nn.Linear(fc_input_size, num_features)\n        self.features = nn.BatchNorm1d(num_features, eps=1e-05)\n        \n        # Initialize weights\n        self._initialize_weights()\n        \n        # Set feature weights to 1 and freeze\n        nn.init.constant_(self.features.weight, 1.0)\n        self.features.weight.requires_grad = False\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        downsample = None\n        previous_dilation = self.dilation\n        \n        if dilate:\n            self.dilation *= stride\n            stride = 1\n            \n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion, eps=1e-05),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                          self.base_width, previous_dilation))\n        self.inplanes = planes * block.expansion\n        \n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                              base_width=self.base_width, dilation=self.dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.prelu(x)\n        \n        # Forward through layers with skip connections\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = self.bn2(x)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        x = self.fc(x.float() if self.fp16 else x)\n        x = self.features(x)\n        return x\n\n\n\ndef _iresnet_plus(arch, block, layers, pretrained, progress, **kwargs):\n    model = IResNet(block, layers, **kwargs)\n    if pretrained:\n        raise ValueError()\n    return model\n\n\ndef iresnet18_plus(pretrained=False, progress=True, **kwargs):\n    return _iresnet_plus('iresnet18_plus', IBasicBlock, [2, 2, 2, 2], pretrained,\n                    progress, **kwargs)\n\n\ndef iresnet34_plus(pretrained=False, progress=True, **kwargs):\n    return _iresnet_plus('iresnet34_plus', IBasicBlock, [3, 4, 6, 3], pretrained,\n                    progress, **kwargs)\n\n\ndef iresnet50_plus(pretrained=False, progress=True, **kwargs):\n    return _iresnet_plus('iresnet50_plus', IBasicBlock, [3, 4, 8, 3], pretrained,\n                    progress, **kwargs)\n\n\ndef iresnet100_plus(pretrained=False, progress=True, **kwargs):\n    return _iresnet_plus('iresnet100_plus', IBasicBlock, [4, 8, 16, 3], pretrained,\n                    progress, **kwargs)\n\n\ndef iresnet200_plus(pretrained=False, progress=True, **kwargs):\n    return _iresnet_plus('iresnet200_plus', IBasicBlock, [6, 26, 60, 6], pretrained,\n                    progress, **kwargs)\n\n\n\nif __name__ == \"__main__\":\n    import cv2\n    import time\n    model = iresnet100_plus(image_size=112)  # Specify image size explicitly\n    model.eval()\n\n    img = cv2.imread(\"data/img1.jpg\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (112, 112))\n    img = img.transpose(2, 0, 1)  # [H,W,C] -> [C,H,W]\n    img = torch.from_numpy(img).float().unsqueeze(0)  # [1,C,H,W]\n\n    # Test forward pass\n    with torch.no_grad():\n        start = time.time()\n        for _ in range(10):\n            y = model(img)\n        end = time.time()\n    print(y.shape)\n    infer_time = (end - start) / 10\n\n    # Results\n    print(f'Average inference time: {infer_time:.6f} seconds')\n    print('Output shape:', y.shape)  # [1, 512]\n    print(\"Vector norm:\", torch.norm(y, p=2, dim=1).item())\n    num_params = sum(p.numel() for p in model.parameters())\n    print(f\"Total number of parameters: {num_params}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/Arcface_torch/losses.py\nimport torch\nimport math\n\nclass CombinedMarginLoss(torch.nn.Module):\n    def __init__(self, \n                 s, \n                 m1,\n                 m2,\n                 m3,\n                 interclass_filtering_threshold=0):\n        super().__init__()\n        self.s = s\n        self.m1 = m1\n        self.m2 = m2\n        self.m3 = m3\n        self.interclass_filtering_threshold = interclass_filtering_threshold\n        \n        # For ArcFace\n        self.cos_m = math.cos(self.m2)\n        self.sin_m = math.sin(self.m2)\n        self.theta = math.cos(math.pi - self.m2)\n        self.sinmm = math.sin(math.pi - self.m2) * self.m2\n        self.easy_margin = False\n\n\n    def forward(self, logits, labels):\n        index_positive = torch.where(labels != -1)[0]\n\n        if self.interclass_filtering_threshold > 0:\n            with torch.no_grad():\n                dirty = logits > self.interclass_filtering_threshold\n                dirty = dirty.float()\n                mask = torch.ones([index_positive.size(0), logits.size(1)], device=logits.device)\n                mask.scatter_(1, labels[index_positive], 0)\n                dirty[index_positive] *= mask\n                tensor_mul = 1 - dirty    \n            logits = tensor_mul * logits\n\n        target_logit = logits[index_positive, labels[index_positive].view(-1)]\n\n        if self.m1 == 1.0 and self.m3 == 0.0:\n            with torch.no_grad():\n                target_logit.arccos_()\n                logits.arccos_()\n                final_target_logit = target_logit + self.m2\n                logits[index_positive, labels[index_positive].view(-1)] = final_target_logit\n                logits.cos_()\n            logits = logits * self.s        \n\n        elif self.m3 > 0:\n            final_target_logit = target_logit - self.m3\n            logits[index_positive, labels[index_positive].view(-1)] = final_target_logit\n            logits = logits * self.s\n        else:\n            raise\n\n        return logits\n\n\nclass CombinedDynamicMarginLoss(torch.nn.Module):\n    def __init__(self,\n                 s: float = 64.0,\n                 m1: float = 1.0,\n                 m2: float = 0.5,\n                 m3: float = 0,\n                 interclass_filtering_threshold: float = 0.0,\n                 alpha: float = 0.1):\n        super().__init__()\n        self.s = s\n        self.m1 = m1\n        self.m2 = m2\n        self.m3 = m3\n        self.alpha = alpha\n        self.interclass_filtering_threshold = interclass_filtering_threshold\n\n    def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n        original_logits = logits.clone()\n        adjusted_logits = original_logits.clone()\n        index_positive = torch.where(labels != -1)[0]\n\n        if self.interclass_filtering_threshold > 0:\n            with torch.no_grad():\n                dirty = logits > self.interclass_filtering_threshold\n                dirty = dirty.float()\n                mask = torch.ones([index_positive.size(0), logits.size(1)], device=logits.device)\n                mask.scatter_(1, labels[index_positive].unsqueeze(1), 0)\n                dirty[index_positive] *= mask\n                tensor_mul = 1 - dirty\n            logits = logits * tensor_mul\n\n        if index_positive.numel() == 0:\n            return logits * self.s\n\n        pos_labels = labels[index_positive]\n        cos_y = logits[index_positive, pos_labels]\n\n        logits_clone = logits[index_positive].clone()\n        logits_clone[torch.arange(index_positive.size(0)), pos_labels] = -1e9\n        max_other, _ = logits_clone.max(dim=1)\n\n        h = 1.0 - (cos_y - max_other)\n        m_i = self.m2 + self.alpha * h\n        theta_y = torch.acos(cos_y.clamp(-1.0, 1.0))\n        phi_y = torch.cos(self.m1 * theta_y + m_i) - self.m3\n\n        # đảm bảo đồng biến: nếu phi_y < cos_y thì update\n        # (tùy bạn muốn chắc chắn giữ thứ tự)\n        mask_update = phi_y < cos_y\n        final_phi = torch.where(mask_update, phi_y, cos_y)\n\n        adjusted_logits[index_positive, pos_labels] = final_phi\n        return adjusted_logits * self.s\n\n\nclass ArcFace(torch.nn.Module):\n    \"\"\" ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf):\n    \"\"\"\n    def __init__(self, s=64.0, margin=0.5):\n        super(ArcFace, self).__init__()\n        self.s = s\n        self.margin = margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.theta = math.cos(math.pi - margin)\n        self.sinmm = math.sin(math.pi - margin) * margin\n        self.easy_margin = False\n\n\n    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n        index = torch.where(labels != -1)[0]\n        target_logit = logits[index, labels[index].view(-1)]\n\n        with torch.no_grad():\n            target_logit.arccos_()\n            logits.arccos_()\n            final_target_logit = target_logit + self.margin\n            logits[index, labels[index].view(-1)] = final_target_logit\n            logits.cos_()\n        logits = logits * self.s   \n        return logits\n\n\nclass CosFace(torch.nn.Module):\n    def __init__(self, s=64.0, m=0.40):\n        super(CosFace, self).__init__()\n        self.s = s\n        self.m = m\n\n    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n        index = torch.where(labels != -1)[0]\n        target_logit = logits[index, labels[index].view(-1)]\n        final_target_logit = target_logit - self.m\n        logits[index, labels[index].view(-1)] = final_target_logit\n        logits = logits * self.s\n        return logits","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/Arcface_torch/train_v2.py\nimport argparse\nimport logging\nimport os\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\nfrom backbones import get_model\nfrom dataset import get_dataloader\nfrom losses import CombinedMarginLoss, CombinedDynamicMarginLoss\nfrom lr_scheduler import PolynomialLRWarmup\nfrom partial_fc_v2 import PartialFC_V2\nfrom torch import distributed\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom utils.utils_callbacks import CallBackLogging, CallBackVerification\nfrom utils.utils_config import get_config\nfrom utils.utils_distributed_sampler import setup_seed\nfrom utils.utils_logging import AverageMeter, init_logging\nfrom torch.distributed.algorithms.ddp_comm_hooks.default_hooks import fp16_compress_hook\n\nassert torch.__version__ >= \"1.12.0\", \"In order to enjoy the features of the new torch, \\\nwe have upgraded the torch to 1.12.0. torch before than 1.12.0 may not work in the future.\"\n\ntry:\n    rank = int(os.environ[\"RANK\"])\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    world_size = int(os.environ[\"WORLD_SIZE\"])\n    distributed.init_process_group(\"nccl\")\nexcept KeyError:\n    rank = 0\n    local_rank = 0\n    world_size = 1\n    distributed.init_process_group(\n        backend=\"nccl\",\n        init_method=\"tcp://127.0.0.1:12584\",\n        rank=rank,\n        world_size=world_size,\n    )\n\n\ndef main(args):\n\n    # get config\n    cfg = get_config(args.config)\n    # global control random seed\n    setup_seed(seed=cfg.seed, cuda_deterministic=False)\n\n    torch.cuda.set_device(local_rank)\n\n    os.makedirs(cfg.output, exist_ok=True)\n    init_logging(rank, cfg.output)\n\n    summary_writer = (\n        SummaryWriter(log_dir=os.path.join(cfg.output, \"tensorboard\"))\n        if rank == 0\n        else None\n    )\n    \n    wandb_logger = None\n    if cfg.using_wandb:\n        import wandb\n        # Sign in to wandb\n        try:\n            wandb.login(key=cfg.wandb_key)\n        except Exception as e:\n            print(\"WandB Key must be provided in config file (base.py).\")\n            print(f\"Config Error: {e}\")\n        # Initialize wandb\n        run_name = datetime.now().strftime(\"%y%m%d_%H%M\") + f\"_GPU{rank}\"\n        run_name = run_name if cfg.suffix_run_name is None else run_name + f\"_{cfg.suffix_run_name}\"\n        try:\n            wandb_logger = wandb.init(\n                entity = cfg.wandb_entity, \n                project = cfg.wandb_project, \n                sync_tensorboard = True,\n                resume=cfg.wandb_resume,\n                name = run_name, \n                notes = cfg.notes) if rank == 0 or cfg.wandb_log_all else None\n            if wandb_logger:\n                wandb_logger.config.update(cfg)\n        except Exception as e:\n            print(\"WandB Data (Entity and Project name) must be provided in config file (base.py).\")\n            print(f\"Config Error: {e}\")\n    train_loader = get_dataloader(\n        cfg.rec,\n        local_rank,\n        cfg.batch_size,\n        cfg.dali,\n        cfg.dali_aug,\n        cfg.seed,\n        cfg.num_workers,\n        cfg.image_size,\n    )\n\n    backbone = get_model(\n        cfg.network, dropout=0.0, fp16=cfg.fp16, num_features=cfg.embedding_size, image_size = cfg.image_size).cuda()\n\n    backbone = torch.nn.parallel.DistributedDataParallel(\n        module=backbone, broadcast_buffers=False, device_ids=[local_rank], bucket_cap_mb=16,\n        find_unused_parameters=True)\n    backbone.register_comm_hook(None, fp16_compress_hook)\n\n    backbone.train()\n    # FIXME using gradient checkpoint if there are some unused parameters will cause error\n    backbone._set_static_graph()\n\n    margin_loss = CombinedDynamicMarginLoss(\n        64,\n        cfg.margin_list[0],\n        cfg.margin_list[1],\n        cfg.margin_list[2],\n        cfg.interclass_filtering_threshold\n    )\n\n    if cfg.optimizer == \"sgd\":\n        module_partial_fc = PartialFC_V2(\n            margin_loss, cfg.embedding_size, cfg.num_classes,\n            cfg.sample_rate, False)\n        module_partial_fc.train().cuda()\n        # TODO the params of partial fc must be last in the params list\n        opt = torch.optim.SGD(\n            params=[{\"params\": backbone.parameters()}, {\"params\": module_partial_fc.parameters()}],\n            lr=cfg.lr, momentum=0.9, weight_decay=cfg.weight_decay)\n\n    elif cfg.optimizer == \"adamw\":\n        module_partial_fc = PartialFC_V2(\n            margin_loss, cfg.embedding_size, cfg.num_classes,\n            cfg.sample_rate, False)\n        module_partial_fc.train().cuda()\n        opt = torch.optim.AdamW(\n            params=[{\"params\": backbone.parameters()}, {\"params\": module_partial_fc.parameters()}],\n            lr=cfg.lr, weight_decay=cfg.weight_decay)\n    else:\n        raise\n\n    cfg.total_batch_size = cfg.batch_size * world_size\n    cfg.warmup_step = cfg.num_image // cfg.total_batch_size * cfg.warmup_epoch\n    cfg.total_step = cfg.num_image // cfg.total_batch_size * cfg.num_epoch\n\n    lr_scheduler = PolynomialLRWarmup(\n        optimizer=opt,\n        warmup_iters=cfg.warmup_step,\n        total_iters=cfg.total_step)\n\n    start_epoch = 0\n    global_step = 0\n    if cfg.resume:\n        dict_checkpoint = torch.load(os.path.join(cfg.output, f\"checkpoint_gpu_{rank}.pt\"))\n        start_epoch = dict_checkpoint[\"epoch\"]\n        global_step = dict_checkpoint[\"global_step\"]\n        backbone.module.load_state_dict(dict_checkpoint[\"state_dict_backbone\"])\n        module_partial_fc.load_state_dict(dict_checkpoint[\"state_dict_softmax_fc\"])\n        opt.load_state_dict(dict_checkpoint[\"state_optimizer\"])\n        lr_scheduler.load_state_dict(dict_checkpoint[\"state_lr_scheduler\"])\n        del dict_checkpoint\n\n    for key, value in cfg.items():\n        num_space = 25 - len(key)\n        logging.info(\": \" + key + \" \" * num_space + str(value))\n\n    callback_verification = CallBackVerification(\n        val_targets=cfg.val_targets, rec_prefix=cfg.rec, \n        summary_writer=summary_writer, wandb_logger = wandb_logger\n    )\n    callback_logging = CallBackLogging(\n        frequent=cfg.frequent,\n        total_step=cfg.total_step,\n        batch_size=cfg.batch_size,\n        start_step = global_step,\n        writer=summary_writer\n    )\n\n    loss_am = AverageMeter()\n    amp = torch.cuda.amp.grad_scaler.GradScaler(growth_interval=100)\n\n    for epoch in range(start_epoch, cfg.num_epoch):\n\n        if isinstance(train_loader, DataLoader):\n            train_loader.sampler.set_epoch(epoch)\n        for _, (img, local_labels) in enumerate(train_loader):\n            global_step += 1\n            local_embeddings = backbone(img)\n            loss: torch.Tensor = module_partial_fc(local_embeddings, local_labels)\n\n            if cfg.fp16:\n                amp.scale(loss).backward()\n                if global_step % cfg.gradient_acc == 0:\n                    amp.unscale_(opt)\n                    torch.nn.utils.clip_grad_norm_(backbone.parameters(), 5)\n                    amp.step(opt)\n                    amp.update()\n                    opt.zero_grad()\n            else:\n                loss.backward()\n                if global_step % cfg.gradient_acc == 0:\n                    torch.nn.utils.clip_grad_norm_(backbone.parameters(), 5)\n                    opt.step()\n                    opt.zero_grad()\n            lr_scheduler.step()\n\n            with torch.no_grad():\n                if wandb_logger:\n                    wandb_logger.log({\n                        'Loss/Step Loss': loss.item(),\n                        'Loss/Train Loss': loss_am.avg,\n                        'Process/Step': global_step,\n                        'Process/Epoch': epoch\n                    })\n                    \n                loss_am.update(loss.item(), 1)\n                callback_logging(global_step, loss_am, epoch, cfg.fp16, lr_scheduler.get_last_lr()[0], amp)\n\n                if global_step % cfg.verbose == 0 and global_step > 0:\n                    callback_verification(global_step, backbone)\n                    if cfg.save_all_states:\n                        checkpoint = {\n                            \"epoch\": epoch,\n                            \"global_step\": global_step,\n                            \"state_dict_backbone\": backbone.module.state_dict(),\n                            \"state_dict_softmax_fc\": module_partial_fc.state_dict(),\n                            \"state_optimizer\": opt.state_dict(),\n                            \"state_lr_scheduler\": lr_scheduler.state_dict()\n                        }\n                        # Lưu checkpoint cho resume (luôn ghi đè)\n                        torch.save(checkpoint, os.path.join(cfg.output, f\"checkpoint_gpu_{rank}.pt\"))\n                        # Lưu checkpoint lịch sử (chỉ lưu state_dict backbone, không lưu trạng thái khác)\n                        torch.save(backbone.module.state_dict(), os.path.join(cfg.output, f\"checkpoint_step_{global_step}_gpu_{rank}.pt\"))\n\n        if rank == 0:\n            path_module = os.path.join(cfg.output, \"model.pt\")\n            torch.save(backbone.module.state_dict(), path_module)\n\n            if wandb_logger and cfg.save_artifacts:\n                artifact_name = f\"{run_name}_E{epoch}\"\n                model = wandb.Artifact(artifact_name, type='model')\n                model.add_file(path_module)\n                wandb_logger.log_artifact(model)\n                \n        if cfg.dali:\n            train_loader.reset()\n\n    if rank == 0:\n        path_module = os.path.join(cfg.output, \"model.pt\")\n        torch.save(backbone.module.state_dict(), path_module)\n        \n        if wandb_logger and cfg.save_artifacts:\n            artifact_name = f\"{run_name}_Final\"\n            model = wandb.Artifact(artifact_name, type='model')\n            model.add_file(path_module)\n            wandb_logger.log_artifact(model)\n\n\n\nif __name__ == \"__main__\":\n    torch.backends.cudnn.benchmark = True\n    parser = argparse.ArgumentParser(\n        description=\"Distributed Arcface Training in Pytorch\")\n    parser.add_argument(\"config\", type=str, help=\"py config file\")\n    main(parser.parse_args())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/Arcface_torch/backbones/__init__.py\nfrom .iresnet import iresnet18, iresnet34, iresnet50, iresnet100, iresnet200\nfrom .iresnet_lite import iresnet18_lite, iresnet34_lite, iresnet50_lite, iresnet100_lite, iresnet200_lite\nfrom .iresnet_plus import iresnet18_plus, iresnet34_plus, iresnet50_plus, iresnet100_plus, iresnet200_plus\ndef get_model(name, **kwargs):\n    # resnet\n    if name == \"r18\":\n        return iresnet18(False, **kwargs)\n    elif name == \"r34\":\n        return iresnet34(False, **kwargs)\n    elif name == \"r50\":\n        return iresnet50(False, **kwargs)\n    elif name == \"r100\":\n        return iresnet100(False, **kwargs)\n    elif name == \"r200\":\n        return iresnet200(False, **kwargs)\n    elif name == \"r18_lite\":\n        return iresnet18_lite(False, **kwargs)\n    elif name == \"r34_lite\":\n        return iresnet34_lite(False, **kwargs)\n    elif name == \"r50_lite\":\n        return iresnet50_lite(False, **kwargs)\n    elif name == \"r100_lite\":\n        return iresnet100_lite(False, **kwargs)\n    elif name == \"r200_lite\":\n        return iresnet200_lite(False, **kwargs)\n    elif name == \"r18_plus\":\n        return iresnet18_plus(False, **kwargs)\n    elif name == \"r34_plus\":\n        return iresnet34_plus(False, **kwargs)\n    elif name == \"r50_plus\":\n        return iresnet50_plus(False, **kwargs)\n    elif name == \"r100_plus\":\n        return iresnet100_plus(False, **kwargs)\n    elif name == \"r200_plus\":\n        return iresnet200_plus(False, **kwargs)\n    else:\n        raise ValueError()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_lst_file(lst_file_path):\n    \"\"\"\n    Đọc file .lst và đếm số lượng ảnh cùng số lượng class.\n    \n    Args:\n        lst_file_path (str): Đường dẫn đến file .lst\n        \n    Returns:\n        tuple: (total_images, num_classes)\n            - total_images: Tổng số ảnh (số dòng trong file)\n            - num_classes: Số lượng class (số nhãn duy nhất)\n    \"\"\"\n    # Khởi tạo biến\n    total_images = 0\n    labels = set()  # Sử dụng set để lưu các nhãn duy nhất\n    \n    try:\n        # Mở và đọc file\n        with open(lst_file_path, 'r') as f:\n            for line in f:\n                # Tách dòng thành các cột bằng dấu tab\n                columns = line.strip().split('\\t')\n                if len(columns) >= 2:  # Đảm bảo dòng có ít nhất 2 cột (ID và label)\n                    total_images += 1\n                    label = int(float(columns[2]))  # Chuyển nhãn thành số nguyên\n                    labels.add(label)\n        \n        num_classes = len(labels)\n        return total_images, num_classes\n    \n    except FileNotFoundError:\n        print(f\"Không tìm thấy file: {lst_file_path}\")\n        return 0, 0\n    except Exception as e:\n        print(f\"Lỗi khi đọc file: {e}\")\n        return 0, 0\n\nlst_file = '/kaggle/input/ms1m-retinaface-t1/ms1m-retinaface-t1/train.lst'\ntotal_images, num_classes = analyze_lst_file(lst_file)\nprint(f\"Num-image: {total_images}\")\nprint(f\"Num-class: {num_classes}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/Arcface_torch/configs/ms1mv3_r100_plus.py\nfrom easydict import EasyDict as edict\n# make training faster\n# our RAM is 256G\n# mount -t tmpfs -o size=140G  tmpfs /train_tmp\n\nconfig = edict()\nconfig.margin_list = (1.0, 0.5, 0.0)\nconfig.network = \"r100_plus\"\nconfig.resume = True\nconfig.output = 'ms1mv3_112x112_r100_plus_workdirs'\nconfig.embedding_size = 512\nconfig.sample_rate = 1.0\nconfig.fp16 = True\nconfig.momentum = 0.9\nconfig.weight_decay = 5e-4\nconfig.batch_size = 128\nconfig.lr = 0.1\nconfig.verbose = 4000\nconfig.dali = False\nconfig.save_all_states = True\nconfig.image_size = 112\n\nconfig.rec = \"/kaggle/input/ms1m-retinaface-t1/ms1m-retinaface-t1\"\nconfig.num_classes = 93431\nconfig.num_image = 5179510\nconfig.num_epoch = 30\nconfig.warmup_epoch = 0\nconfig.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:25:49.103737Z","iopub.execute_input":"2025-06-03T16:25:49.104084Z","iopub.status.idle":"2025-06-03T16:25:49.110190Z","shell.execute_reply.started":"2025-06-03T16:25:49.104037Z","shell.execute_reply":"2025-06-03T16:25:49.109254Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/Arcface_torch/configs/ms1mv3_r100_plus.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd /kaggle/working/Arcface_torch\n!torchrun --nproc_per_node=2 train_v2.py configs/ms1mv3_r100_plus.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:25:53.035288Z","iopub.execute_input":"2025-06-03T16:25:53.035587Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/Arcface_torch\nW0603 16:25:56.631000 122 torch/distributed/run.py:793] \nW0603 16:25:56.631000 122 torch/distributed/run.py:793] *****************************************\nW0603 16:25:56.631000 122 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0603 16:25:56.631000 122 torch/distributed/run.py:793] *****************************************\n/kaggle/working/Arcface_torch/dataset.py:273: FutureWarning: Decorating classes is deprecated and will be disabled in future versions. You should only decorate functions or methods. To preserve the current behavior of class decoration, you can directly decorate the `__init__` method and nothing else.\n  class DALIWarper(object):\n/kaggle/working/Arcface_torch/dataset.py:273: FutureWarning: Decorating classes is deprecated and will be disabled in future versions. You should only decorate functions or methods. To preserve the current behavior of class decoration, you can directly decorate the `__init__` method and nothing else.\n  class DALIWarper(object):\n2025-06-03 16:26:05.732220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-03 16:26:05.732219: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-03 16:26:05.959529: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-06-03 16:26:05.959535: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-06-03 16:26:06.024855: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-03 16:26:06.024879: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/kaggle/working/Arcface_torch/utils/utils_callbacks.py:22: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if self.rank is 0:\n/kaggle/working/Arcface_torch/utils/utils_callbacks.py:62: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if self.rank is 0 and num_update > 0:\nTraining: 2025-06-03 16:26:16,482-rank_id: 0\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py:2351: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py:2351: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n/kaggle/working/Arcface_torch/train_v2.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  dict_checkpoint = torch.load(os.path.join(cfg.output, f\"checkpoint_gpu_{rank}.pt\"))\n/kaggle/working/Arcface_torch/train_v2.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  dict_checkpoint = torch.load(os.path.join(cfg.output, f\"checkpoint_gpu_{rank}.pt\"))\n/kaggle/working/Arcface_torch/train_v2.py:175: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  amp = torch.cuda.amp.grad_scaler.GradScaler(growth_interval=100)\nTraining: 2025-06-03 16:26:25,794-: margin_list              (1.0, 0.5, 0.0)\nTraining: 2025-06-03 16:26:25,794-: network                  r100_plus\nTraining: 2025-06-03 16:26:25,794-: resume                   True\nTraining: 2025-06-03 16:26:25,794-: save_all_states          True\nTraining: 2025-06-03 16:26:25,794-: output                   ms1mv3_112x112_r100_plus_workdirs\nTraining: 2025-06-03 16:26:25,794-: embedding_size           512\nTraining: 2025-06-03 16:26:25,795-: sample_rate              1.0\nTraining: 2025-06-03 16:26:25,795-: interclass_filtering_threshold0\nTraining: 2025-06-03 16:26:25,795-: fp16                     True\nTraining: 2025-06-03 16:26:25,795-: batch_size               128\nTraining: 2025-06-03 16:26:25,795-: optimizer                sgd\nTraining: 2025-06-03 16:26:25,795-: lr                       0.1\nTraining: 2025-06-03 16:26:25,795-: momentum                 0.9\nTraining: 2025-06-03 16:26:25,795-: weight_decay             0.0005\nTraining: 2025-06-03 16:26:25,795-: verbose                  4000\nTraining: 2025-06-03 16:26:25,795-: frequent                 10\nTraining: 2025-06-03 16:26:25,795-: dali                     False\nTraining: 2025-06-03 16:26:25,796-: dali_aug                 False\nTraining: 2025-06-03 16:26:25,796-: gradient_acc             1\nTraining: 2025-06-03 16:26:25,796-: seed                     2048\nTraining: 2025-06-03 16:26:25,796-: num_workers              2\nTraining: 2025-06-03 16:26:25,796-: wandb_key                XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nTraining: 2025-06-03 16:26:25,796-: suffix_run_name          None\nTraining: 2025-06-03 16:26:25,796-: using_wandb              False\nTraining: 2025-06-03 16:26:25,796-: wandb_entity             entity\nTraining: 2025-06-03 16:26:25,796-: wandb_project            project\nTraining: 2025-06-03 16:26:25,796-: wandb_log_all            True\nTraining: 2025-06-03 16:26:25,796-: save_artifacts           False\nTraining: 2025-06-03 16:26:25,796-: wandb_resume             False\nTraining: 2025-06-03 16:26:25,797-: image_size               112\nTraining: 2025-06-03 16:26:25,797-: rec                      /kaggle/input/ms1m-retinaface-t1/ms1m-retinaface-t1\nTraining: 2025-06-03 16:26:25,797-: num_classes              93431\nTraining: 2025-06-03 16:26:25,797-: num_image                5179510\nTraining: 2025-06-03 16:26:25,797-: num_epoch                30\nTraining: 2025-06-03 16:26:25,797-: warmup_epoch             0\nTraining: 2025-06-03 16:26:25,797-: val_targets              ['lfw', 'cfp_fp', 'agedb_30']\nTraining: 2025-06-03 16:26:25,797-: total_batch_size         256\nTraining: 2025-06-03 16:26:25,797-: warmup_step              0\nTraining: 2025-06-03 16:26:25,797-: total_step               606960\nloading bin 0\nloading bin 1000\nloading bin 2000\nloading bin 3000\nloading bin 4000\nloading bin 5000\nloading bin 6000\nloading bin 7000\nloading bin 8000\nloading bin 9000\nloading bin 10000\nloading bin 11000\ntorch.Size([12000, 3, 112, 112])\nloading bin 0\nloading bin 1000\nloading bin 2000\nloading bin 3000\nloading bin 4000\nloading bin 5000\nloading bin 6000\nloading bin 7000\nloading bin 8000\nloading bin 9000\nloading bin 10000\nloading bin 11000\nloading bin 12000\nloading bin 13000\ntorch.Size([14000, 3, 112, 112])\nloading bin 0\nloading bin 1000\nloading bin 2000\nloading bin 3000\nloading bin 4000\nloading bin 5000\nloading bin 6000\nloading bin 7000\nloading bin 8000\nloading bin 9000\nloading bin 10000\nloading bin 11000\ntorch.Size([12000, 3, 112, 112])\n/kaggle/working/Arcface_torch/train_v2.py:175: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  amp = torch.cuda.amp.grad_scaler.GradScaler(growth_interval=100)\n/kaggle/working/Arcface_torch/partial_fc_v2.py:157: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(self.fp16):\n/kaggle/working/Arcface_torch/partial_fc_v2.py:157: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(self.fp16):\nTraining: 2025-06-03 16:28:42,810-Speed 124.07 samples/sec   Loss 22.5519   LearningRate 0.090111   Epoch: 2   Global Step: 60020   Fp16 Grad Scale: 65536   Required: 449 hours\nTraining: 2025-06-03 16:29:04,853-Speed 116.15 samples/sec   Loss 23.0523   LearningRate 0.090110   Epoch: 2   Global Step: 60030   Fp16 Grad Scale: 65536   Required: 412 hours\nTraining: 2025-06-03 16:29:25,741-Speed 122.56 samples/sec   Loss 22.0900   LearningRate 0.090108   Epoch: 2   Global Step: 60040   Fp16 Grad Scale: 65536   Required: 389 hours\nTraining: 2025-06-03 16:29:45,966-Speed 126.58 samples/sec   Loss 23.0979   LearningRate 0.090106   Epoch: 2   Global Step: 60050   Fp16 Grad Scale: 65536   Required: 372 hours\nTraining: 2025-06-03 16:30:06,369-Speed 125.48 samples/sec   Loss 22.5441   LearningRate 0.090105   Epoch: 2   Global Step: 60060   Fp16 Grad Scale: 65536   Required: 361 hours\nTraining: 2025-06-03 16:30:27,531-Speed 120.98 samples/sec   Loss 22.7755   LearningRate 0.090103   Epoch: 2   Global Step: 60070   Fp16 Grad Scale: 65536   Required: 357 hours\nTraining: 2025-06-03 16:30:48,668-Speed 121.12 samples/sec   Loss 22.5371   LearningRate 0.090101   Epoch: 2   Global Step: 60080   Fp16 Grad Scale: 65536   Required: 353 hours\nTraining: 2025-06-03 16:31:09,333-Speed 123.89 samples/sec   Loss 22.5187   LearningRate 0.090100   Epoch: 2   Global Step: 60090   Fp16 Grad Scale: 65536   Required: 347 hours\nTraining: 2025-06-03 16:31:29,909-Speed 124.42 samples/sec   Loss 22.4940   LearningRate 0.090098   Epoch: 2   Global Step: 60100   Fp16 Grad Scale: 131072   Required: 344 hours\nTraining: 2025-06-03 16:31:50,782-Speed 122.66 samples/sec   Loss 22.8574   LearningRate 0.090097   Epoch: 2   Global Step: 60110   Fp16 Grad Scale: 131072   Required: 342 hours\nTraining: 2025-06-03 16:32:11,781-Speed 121.92 samples/sec   Loss 22.7377   LearningRate 0.090095   Epoch: 2   Global Step: 60120   Fp16 Grad Scale: 131072   Required: 340 hours\nTraining: 2025-06-03 16:32:32,558-Speed 123.21 samples/sec   Loss 22.7675   LearningRate 0.090093   Epoch: 2   Global Step: 60130   Fp16 Grad Scale: 131072   Required: 339 hours\nTraining: 2025-06-03 16:32:53,291-Speed 123.48 samples/sec   Loss 22.3627   LearningRate 0.090092   Epoch: 2   Global Step: 60140   Fp16 Grad Scale: 131072   Required: 336 hours\nTraining: 2025-06-03 16:33:14,017-Speed 123.52 samples/sec   Loss 22.4308   LearningRate 0.090090   Epoch: 2   Global Step: 60150   Fp16 Grad Scale: 131072   Required: 335 hours\nTraining: 2025-06-03 16:33:34,824-Speed 123.04 samples/sec   Loss 22.8743   LearningRate 0.090088   Epoch: 2   Global Step: 60160   Fp16 Grad Scale: 131072   Required: 334 hours\nTraining: 2025-06-03 16:33:55,645-Speed 122.96 samples/sec   Loss 22.4923   LearningRate 0.090087   Epoch: 2   Global Step: 60170   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 16:34:16,605-Speed 122.15 samples/sec   Loss 22.8221   LearningRate 0.090085   Epoch: 2   Global Step: 60180   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 16:34:37,606-Speed 121.90 samples/sec   Loss 22.6725   LearningRate 0.090083   Epoch: 2   Global Step: 60190   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 16:34:58,580-Speed 122.06 samples/sec   Loss 22.3189   LearningRate 0.090082   Epoch: 2   Global Step: 60200   Fp16 Grad Scale: 262144   Required: 331 hours\nTraining: 2025-06-03 16:35:19,431-Speed 122.78 samples/sec   Loss 22.6619   LearningRate 0.090080   Epoch: 2   Global Step: 60210   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 16:35:40,177-Speed 123.40 samples/sec   Loss 22.6647   LearningRate 0.090078   Epoch: 2   Global Step: 60220   Fp16 Grad Scale: 262144   Required: 329 hours\nTraining: 2025-06-03 16:36:00,793-Speed 124.18 samples/sec   Loss 23.0429   LearningRate 0.090077   Epoch: 2   Global Step: 60230   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 16:36:21,554-Speed 123.31 samples/sec   Loss 22.6110   LearningRate 0.090075   Epoch: 2   Global Step: 60240   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 16:36:42,491-Speed 122.28 samples/sec   Loss 22.8112   LearningRate 0.090073   Epoch: 2   Global Step: 60250   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 16:37:03,553-Speed 121.55 samples/sec   Loss 22.3997   LearningRate 0.090072   Epoch: 2   Global Step: 60260   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 16:37:24,394-Speed 122.84 samples/sec   Loss 22.6044   LearningRate 0.090070   Epoch: 2   Global Step: 60270   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 16:37:44,996-Speed 124.26 samples/sec   Loss 22.6132   LearningRate 0.090069   Epoch: 2   Global Step: 60280   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 16:38:05,939-Speed 122.24 samples/sec   Loss 23.3795   LearningRate 0.090067   Epoch: 2   Global Step: 60290   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 16:38:27,013-Speed 121.48 samples/sec   Loss 22.5302   LearningRate 0.090065   Epoch: 2   Global Step: 60300   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 16:38:47,710-Speed 123.69 samples/sec   Loss 22.6721   LearningRate 0.090064   Epoch: 2   Global Step: 60310   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 16:39:08,536-Speed 122.93 samples/sec   Loss 22.5350   LearningRate 0.090062   Epoch: 2   Global Step: 60320   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 16:39:29,654-Speed 121.22 samples/sec   Loss 22.6748   LearningRate 0.090060   Epoch: 2   Global Step: 60330   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 16:39:50,562-Speed 122.44 samples/sec   Loss 22.5568   LearningRate 0.090059   Epoch: 2   Global Step: 60340   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 16:40:11,127-Speed 124.49 samples/sec   Loss 22.7239   LearningRate 0.090057   Epoch: 2   Global Step: 60350   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 16:40:31,980-Speed 122.77 samples/sec   Loss 22.6908   LearningRate 0.090055   Epoch: 2   Global Step: 60360   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 16:40:53,161-Speed 120.87 samples/sec   Loss 22.5865   LearningRate 0.090054   Epoch: 2   Global Step: 60370   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 16:41:14,027-Speed 122.69 samples/sec   Loss 22.5808   LearningRate 0.090052   Epoch: 2   Global Step: 60380   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 16:41:34,608-Speed 124.39 samples/sec   Loss 22.6610   LearningRate 0.090050   Epoch: 2   Global Step: 60390   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 16:41:55,581-Speed 122.07 samples/sec   Loss 22.6747   LearningRate 0.090049   Epoch: 2   Global Step: 60400   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 16:42:16,635-Speed 121.59 samples/sec   Loss 22.4903   LearningRate 0.090047   Epoch: 2   Global Step: 60410   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 16:42:37,295-Speed 123.92 samples/sec   Loss 22.6553   LearningRate 0.090045   Epoch: 2   Global Step: 60420   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 16:42:58,060-Speed 123.29 samples/sec   Loss 22.1699   LearningRate 0.090044   Epoch: 2   Global Step: 60430   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 16:43:18,880-Speed 122.96 samples/sec   Loss 22.9778   LearningRate 0.090042   Epoch: 2   Global Step: 60440   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 16:43:39,693-Speed 123.00 samples/sec   Loss 22.9340   LearningRate 0.090041   Epoch: 2   Global Step: 60450   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 16:44:00,533-Speed 122.85 samples/sec   Loss 22.3851   LearningRate 0.090039   Epoch: 2   Global Step: 60460   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 16:44:21,500-Speed 122.10 samples/sec   Loss 23.0656   LearningRate 0.090037   Epoch: 2   Global Step: 60470   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 16:44:42,477-Speed 122.05 samples/sec   Loss 22.5705   LearningRate 0.090036   Epoch: 2   Global Step: 60480   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 16:45:03,431-Speed 122.17 samples/sec   Loss 22.5750   LearningRate 0.090034   Epoch: 2   Global Step: 60490   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 16:45:24,405-Speed 122.06 samples/sec   Loss 22.6944   LearningRate 0.090032   Epoch: 2   Global Step: 60500   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 16:45:45,408-Speed 121.89 samples/sec   Loss 22.7601   LearningRate 0.090031   Epoch: 2   Global Step: 60510   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 16:46:06,333-Speed 122.34 samples/sec   Loss 22.4512   LearningRate 0.090029   Epoch: 2   Global Step: 60520   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 16:46:27,024-Speed 123.73 samples/sec   Loss 22.9721   LearningRate 0.090027   Epoch: 2   Global Step: 60530   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 16:46:47,772-Speed 123.39 samples/sec   Loss 22.7305   LearningRate 0.090026   Epoch: 2   Global Step: 60540   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 16:47:08,659-Speed 122.57 samples/sec   Loss 22.8576   LearningRate 0.090024   Epoch: 2   Global Step: 60550   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 16:47:29,370-Speed 123.61 samples/sec   Loss 22.6951   LearningRate 0.090022   Epoch: 2   Global Step: 60560   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 16:47:50,070-Speed 123.68 samples/sec   Loss 23.0863   LearningRate 0.090021   Epoch: 2   Global Step: 60570   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 16:48:10,861-Speed 123.13 samples/sec   Loss 22.9851   LearningRate 0.090019   Epoch: 2   Global Step: 60580   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 16:48:31,640-Speed 123.20 samples/sec   Loss 22.1594   LearningRate 0.090017   Epoch: 2   Global Step: 60590   Fp16 Grad Scale: 262144   Required: 321 hours\nTraining: 2025-06-03 16:48:52,438-Speed 123.10 samples/sec   Loss 22.7599   LearningRate 0.090016   Epoch: 2   Global Step: 60600   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 16:49:13,102-Speed 123.89 samples/sec   Loss 22.1181   LearningRate 0.090014   Epoch: 2   Global Step: 60610   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 16:49:34,115-Speed 121.83 samples/sec   Loss 22.6670   LearningRate 0.090013   Epoch: 2   Global Step: 60620   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 16:49:55,041-Speed 122.34 samples/sec   Loss 22.7742   LearningRate 0.090011   Epoch: 2   Global Step: 60630   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 16:50:15,789-Speed 123.39 samples/sec   Loss 22.7598   LearningRate 0.090009   Epoch: 2   Global Step: 60640   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 16:50:36,458-Speed 123.86 samples/sec   Loss 22.6051   LearningRate 0.090008   Epoch: 2   Global Step: 60650   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 16:50:57,303-Speed 122.82 samples/sec   Loss 22.6597   LearningRate 0.090006   Epoch: 2   Global Step: 60660   Fp16 Grad Scale: 131072   Required: 320 hours\nTraining: 2025-06-03 16:51:18,255-Speed 122.19 samples/sec   Loss 23.0509   LearningRate 0.090004   Epoch: 2   Global Step: 60670   Fp16 Grad Scale: 131072   Required: 320 hours\nTraining: 2025-06-03 16:51:39,244-Speed 121.98 samples/sec   Loss 22.8095   LearningRate 0.090003   Epoch: 2   Global Step: 60680   Fp16 Grad Scale: 131072   Required: 320 hours\nTraining: 2025-06-03 16:52:00,219-Speed 122.05 samples/sec   Loss 22.6201   LearningRate 0.090001   Epoch: 2   Global Step: 60690   Fp16 Grad Scale: 131072   Required: 320 hours\nTraining: 2025-06-03 16:52:21,184-Speed 122.11 samples/sec   Loss 22.5720   LearningRate 0.089999   Epoch: 2   Global Step: 60700   Fp16 Grad Scale: 262144   Required: 320 hours\nTraining: 2025-06-03 16:52:42,142-Speed 122.16 samples/sec   Loss 22.6736   LearningRate 0.089998   Epoch: 2   Global Step: 60710   Fp16 Grad Scale: 262144   Required: 320 hours\nTraining: 2025-06-03 16:53:02,969-Speed 122.92 samples/sec   Loss 22.3168   LearningRate 0.089996   Epoch: 2   Global Step: 60720   Fp16 Grad Scale: 131072   Required: 320 hours\nTraining: 2025-06-03 16:53:23,819-Speed 122.79 samples/sec   Loss 22.6325   LearningRate 0.089994   Epoch: 2   Global Step: 60730   Fp16 Grad Scale: 131072   Required: 320 hours\nTraining: 2025-06-03 16:53:44,708-Speed 122.56 samples/sec   Loss 22.7044   LearningRate 0.089993   Epoch: 2   Global Step: 60740   Fp16 Grad Scale: 131072   Required: 320 hours\nTraining: 2025-06-03 16:54:05,400-Speed 123.73 samples/sec   Loss 22.7505   LearningRate 0.089991   Epoch: 2   Global Step: 60750   Fp16 Grad Scale: 131072   Required: 320 hours\nTraining: 2025-06-03 16:54:26,102-Speed 123.66 samples/sec   Loss 22.7328   LearningRate 0.089989   Epoch: 2   Global Step: 60760   Fp16 Grad Scale: 131072   Required: 320 hours\nTraining: 2025-06-03 16:54:46,984-Speed 122.60 samples/sec   Loss 22.7887   LearningRate 0.089988   Epoch: 2   Global Step: 60770   Fp16 Grad Scale: 131072   Required: 320 hours\nTraining: 2025-06-03 16:55:07,966-Speed 122.01 samples/sec   Loss 22.4478   LearningRate 0.089986   Epoch: 2   Global Step: 60780   Fp16 Grad Scale: 131072   Required: 320 hours\nTraining: 2025-06-03 16:55:28,948-Speed 122.02 samples/sec   Loss 22.8710   LearningRate 0.089985   Epoch: 2   Global Step: 60790   Fp16 Grad Scale: 131072   Required: 320 hours\nTraining: 2025-06-03 16:55:49,868-Speed 122.38 samples/sec   Loss 22.8019   LearningRate 0.089983   Epoch: 2   Global Step: 60800   Fp16 Grad Scale: 131072   Required: 320 hours\nTraining: 2025-06-03 16:56:10,728-Speed 122.72 samples/sec   Loss 22.7590   LearningRate 0.089981   Epoch: 2   Global Step: 60810   Fp16 Grad Scale: 65536   Required: 320 hours\nTraining: 2025-06-03 16:56:31,482-Speed 123.36 samples/sec   Loss 22.8349   LearningRate 0.089980   Epoch: 2   Global Step: 60820   Fp16 Grad Scale: 65536   Required: 320 hours\nTraining: 2025-06-03 16:56:52,271-Speed 123.15 samples/sec   Loss 22.6716   LearningRate 0.089978   Epoch: 2   Global Step: 60830   Fp16 Grad Scale: 65536   Required: 320 hours\nTraining: 2025-06-03 16:57:13,097-Speed 122.93 samples/sec   Loss 22.6039   LearningRate 0.089976   Epoch: 2   Global Step: 60840   Fp16 Grad Scale: 65536   Required: 320 hours\nTraining: 2025-06-03 16:57:33,805-Speed 123.62 samples/sec   Loss 22.8968   LearningRate 0.089975   Epoch: 2   Global Step: 60850   Fp16 Grad Scale: 65536   Required: 320 hours\nTraining: 2025-06-03 16:57:54,450-Speed 124.01 samples/sec   Loss 22.2214   LearningRate 0.089973   Epoch: 2   Global Step: 60860   Fp16 Grad Scale: 65536   Required: 320 hours\nTraining: 2025-06-03 16:58:15,373-Speed 122.36 samples/sec   Loss 23.4242   LearningRate 0.089971   Epoch: 2   Global Step: 60870   Fp16 Grad Scale: 65536   Required: 319 hours\nTraining: 2025-06-03 16:58:36,347-Speed 122.06 samples/sec   Loss 22.6340   LearningRate 0.089970   Epoch: 2   Global Step: 60880   Fp16 Grad Scale: 65536   Required: 319 hours\nTraining: 2025-06-03 16:58:57,208-Speed 122.72 samples/sec   Loss 22.6494   LearningRate 0.089968   Epoch: 2   Global Step: 60890   Fp16 Grad Scale: 65536   Required: 319 hours\nTraining: 2025-06-03 16:59:17,817-Speed 124.22 samples/sec   Loss 22.1954   LearningRate 0.089966   Epoch: 2   Global Step: 60900   Fp16 Grad Scale: 65536   Required: 319 hours\nTraining: 2025-06-03 16:59:38,578-Speed 123.31 samples/sec   Loss 22.7533   LearningRate 0.089965   Epoch: 2   Global Step: 60910   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 16:59:59,526-Speed 122.22 samples/sec   Loss 22.8283   LearningRate 0.089963   Epoch: 2   Global Step: 60920   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:00:20,515-Speed 121.97 samples/sec   Loss 22.8327   LearningRate 0.089961   Epoch: 2   Global Step: 60930   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:00:41,281-Speed 123.28 samples/sec   Loss 22.4867   LearningRate 0.089960   Epoch: 2   Global Step: 60940   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:01:01,955-Speed 123.84 samples/sec   Loss 22.3963   LearningRate 0.089958   Epoch: 2   Global Step: 60950   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:01:22,923-Speed 122.10 samples/sec   Loss 22.3464   LearningRate 0.089957   Epoch: 2   Global Step: 60960   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:01:43,893-Speed 122.08 samples/sec   Loss 22.5475   LearningRate 0.089955   Epoch: 2   Global Step: 60970   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:02:04,721-Speed 122.92 samples/sec   Loss 22.6020   LearningRate 0.089953   Epoch: 2   Global Step: 60980   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:02:25,365-Speed 124.01 samples/sec   Loss 22.9407   LearningRate 0.089952   Epoch: 2   Global Step: 60990   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:02:46,281-Speed 122.40 samples/sec   Loss 22.8718   LearningRate 0.089950   Epoch: 2   Global Step: 61000   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:03:07,261-Speed 122.02 samples/sec   Loss 22.9060   LearningRate 0.089948   Epoch: 2   Global Step: 61010   Fp16 Grad Scale: 262144   Required: 319 hours\nTraining: 2025-06-03 17:03:28,140-Speed 122.62 samples/sec   Loss 22.5674   LearningRate 0.089947   Epoch: 2   Global Step: 61020   Fp16 Grad Scale: 262144   Required: 319 hours\nTraining: 2025-06-03 17:03:49,077-Speed 122.27 samples/sec   Loss 22.6973   LearningRate 0.089945   Epoch: 2   Global Step: 61030   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:04:10,054-Speed 122.04 samples/sec   Loss 22.3325   LearningRate 0.089943   Epoch: 2   Global Step: 61040   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:04:30,800-Speed 123.40 samples/sec   Loss 22.5325   LearningRate 0.089942   Epoch: 2   Global Step: 61050   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:04:51,387-Speed 124.36 samples/sec   Loss 22.8767   LearningRate 0.089940   Epoch: 2   Global Step: 61060   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:05:12,366-Speed 122.03 samples/sec   Loss 22.2900   LearningRate 0.089938   Epoch: 2   Global Step: 61070   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:05:33,437-Speed 121.50 samples/sec   Loss 22.8533   LearningRate 0.089937   Epoch: 2   Global Step: 61080   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:05:54,239-Speed 123.07 samples/sec   Loss 22.9445   LearningRate 0.089935   Epoch: 2   Global Step: 61090   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:06:14,848-Speed 124.22 samples/sec   Loss 22.4576   LearningRate 0.089933   Epoch: 2   Global Step: 61100   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:06:35,595-Speed 123.40 samples/sec   Loss 22.1539   LearningRate 0.089932   Epoch: 2   Global Step: 61110   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:06:56,397-Speed 123.07 samples/sec   Loss 22.8075   LearningRate 0.089930   Epoch: 2   Global Step: 61120   Fp16 Grad Scale: 131072   Required: 319 hours\nTraining: 2025-06-03 17:07:17,204-Speed 123.04 samples/sec   Loss 22.7610   LearningRate 0.089928   Epoch: 2   Global Step: 61130   Fp16 Grad Scale: 262144   Required: 319 hours\nTraining: 2025-06-03 17:07:38,120-Speed 122.40 samples/sec   Loss 22.5016   LearningRate 0.089927   Epoch: 2   Global Step: 61140   Fp16 Grad Scale: 262144   Required: 319 hours\nTraining: 2025-06-03 17:07:59,044-Speed 122.35 samples/sec   Loss 22.7122   LearningRate 0.089925   Epoch: 2   Global Step: 61150   Fp16 Grad Scale: 262144   Required: 319 hours\nTraining: 2025-06-03 17:08:19,975-Speed 122.31 samples/sec   Loss 22.3348   LearningRate 0.089924   Epoch: 2   Global Step: 61160   Fp16 Grad Scale: 262144   Required: 319 hours\nTraining: 2025-06-03 17:08:40,911-Speed 122.29 samples/sec   Loss 22.2676   LearningRate 0.089922   Epoch: 2   Global Step: 61170   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:09:01,863-Speed 122.19 samples/sec   Loss 22.9202   LearningRate 0.089920   Epoch: 2   Global Step: 61180   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:09:22,608-Speed 123.41 samples/sec   Loss 22.2984   LearningRate 0.089919   Epoch: 2   Global Step: 61190   Fp16 Grad Scale: 65536   Required: 318 hours\nTraining: 2025-06-03 17:09:43,408-Speed 123.08 samples/sec   Loss 22.8032   LearningRate 0.089917   Epoch: 2   Global Step: 61200   Fp16 Grad Scale: 65536   Required: 318 hours\nTraining: 2025-06-03 17:10:04,223-Speed 123.00 samples/sec   Loss 22.3160   LearningRate 0.089915   Epoch: 2   Global Step: 61210   Fp16 Grad Scale: 65536   Required: 318 hours\nTraining: 2025-06-03 17:10:25,029-Speed 123.04 samples/sec   Loss 22.7637   LearningRate 0.089914   Epoch: 2   Global Step: 61220   Fp16 Grad Scale: 65536   Required: 318 hours\nTraining: 2025-06-03 17:10:45,641-Speed 124.21 samples/sec   Loss 22.8027   LearningRate 0.089912   Epoch: 2   Global Step: 61230   Fp16 Grad Scale: 65536   Required: 318 hours\nTraining: 2025-06-03 17:11:06,425-Speed 123.17 samples/sec   Loss 22.4316   LearningRate 0.089910   Epoch: 2   Global Step: 61240   Fp16 Grad Scale: 65536   Required: 318 hours\nTraining: 2025-06-03 17:11:27,416-Speed 121.96 samples/sec   Loss 22.7252   LearningRate 0.089909   Epoch: 2   Global Step: 61250   Fp16 Grad Scale: 65536   Required: 318 hours\nTraining: 2025-06-03 17:11:48,352-Speed 122.28 samples/sec   Loss 22.7172   LearningRate 0.089907   Epoch: 2   Global Step: 61260   Fp16 Grad Scale: 65536   Required: 318 hours\nTraining: 2025-06-03 17:12:08,981-Speed 124.10 samples/sec   Loss 22.4067   LearningRate 0.089905   Epoch: 2   Global Step: 61270   Fp16 Grad Scale: 65536   Required: 318 hours\nTraining: 2025-06-03 17:12:29,728-Speed 123.40 samples/sec   Loss 22.9168   LearningRate 0.089904   Epoch: 2   Global Step: 61280   Fp16 Grad Scale: 65536   Required: 318 hours\nTraining: 2025-06-03 17:12:50,673-Speed 122.23 samples/sec   Loss 22.4853   LearningRate 0.089902   Epoch: 2   Global Step: 61290   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:13:11,727-Speed 121.60 samples/sec   Loss 22.0891   LearningRate 0.089900   Epoch: 2   Global Step: 61300   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:13:32,515-Speed 123.15 samples/sec   Loss 22.7174   LearningRate 0.089899   Epoch: 2   Global Step: 61310   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:13:53,000-Speed 124.98 samples/sec   Loss 23.0656   LearningRate 0.089897   Epoch: 2   Global Step: 61320   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:14:13,845-Speed 122.81 samples/sec   Loss 22.7353   LearningRate 0.089896   Epoch: 2   Global Step: 61330   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:14:34,848-Speed 121.89 samples/sec   Loss 23.1022   LearningRate 0.089894   Epoch: 2   Global Step: 61340   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:14:55,842-Speed 121.94 samples/sec   Loss 22.7539   LearningRate 0.089892   Epoch: 2   Global Step: 61350   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:15:16,708-Speed 122.69 samples/sec   Loss 22.7523   LearningRate 0.089891   Epoch: 2   Global Step: 61360   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:15:37,322-Speed 124.20 samples/sec   Loss 22.6797   LearningRate 0.089889   Epoch: 2   Global Step: 61370   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:15:58,137-Speed 122.99 samples/sec   Loss 22.9475   LearningRate 0.089887   Epoch: 2   Global Step: 61380   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:16:19,170-Speed 121.72 samples/sec   Loss 22.6150   LearningRate 0.089886   Epoch: 2   Global Step: 61390   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:16:40,148-Speed 122.04 samples/sec   Loss 22.6801   LearningRate 0.089884   Epoch: 2   Global Step: 61400   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:17:01,058-Speed 122.43 samples/sec   Loss 22.4239   LearningRate 0.089882   Epoch: 2   Global Step: 61410   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:17:21,680-Speed 124.15 samples/sec   Loss 22.9852   LearningRate 0.089881   Epoch: 2   Global Step: 61420   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:17:42,547-Speed 122.68 samples/sec   Loss 22.4807   LearningRate 0.089879   Epoch: 2   Global Step: 61430   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:18:03,533-Speed 121.99 samples/sec   Loss 22.1365   LearningRate 0.089877   Epoch: 2   Global Step: 61440   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:18:24,536-Speed 121.89 samples/sec   Loss 22.2650   LearningRate 0.089876   Epoch: 2   Global Step: 61450   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:18:45,323-Speed 123.16 samples/sec   Loss 22.6331   LearningRate 0.089874   Epoch: 2   Global Step: 61460   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:19:06,039-Speed 123.59 samples/sec   Loss 23.1577   LearningRate 0.089872   Epoch: 2   Global Step: 61470   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:19:26,832-Speed 123.12 samples/sec   Loss 22.6943   LearningRate 0.089871   Epoch: 2   Global Step: 61480   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:19:47,607-Speed 123.23 samples/sec   Loss 22.8475   LearningRate 0.089869   Epoch: 2   Global Step: 61490   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:20:08,419-Speed 123.01 samples/sec   Loss 22.5666   LearningRate 0.089868   Epoch: 2   Global Step: 61500   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:20:29,167-Speed 123.39 samples/sec   Loss 22.1990   LearningRate 0.089866   Epoch: 2   Global Step: 61510   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:20:50,119-Speed 122.19 samples/sec   Loss 22.3150   LearningRate 0.089864   Epoch: 2   Global Step: 61520   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:21:11,055-Speed 122.28 samples/sec   Loss 22.5980   LearningRate 0.089863   Epoch: 2   Global Step: 61530   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:21:31,855-Speed 123.08 samples/sec   Loss 22.4248   LearningRate 0.089861   Epoch: 2   Global Step: 61540   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:21:52,665-Speed 123.02 samples/sec   Loss 22.7509   LearningRate 0.089859   Epoch: 2   Global Step: 61550   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:22:13,643-Speed 122.04 samples/sec   Loss 22.6496   LearningRate 0.089858   Epoch: 2   Global Step: 61560   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:22:34,600-Speed 122.16 samples/sec   Loss 22.5835   LearningRate 0.089856   Epoch: 2   Global Step: 61570   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:22:55,261-Speed 123.91 samples/sec   Loss 22.7195   LearningRate 0.089854   Epoch: 2   Global Step: 61580   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:23:16,026-Speed 123.29 samples/sec   Loss 22.6307   LearningRate 0.089853   Epoch: 2   Global Step: 61590   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:23:37,072-Speed 121.64 samples/sec   Loss 22.5365   LearningRate 0.089851   Epoch: 2   Global Step: 61600   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:23:57,955-Speed 122.60 samples/sec   Loss 22.5190   LearningRate 0.089849   Epoch: 2   Global Step: 61610   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:24:18,606-Speed 123.97 samples/sec   Loss 22.8884   LearningRate 0.089848   Epoch: 2   Global Step: 61620   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:24:39,543-Speed 122.28 samples/sec   Loss 23.2542   LearningRate 0.089846   Epoch: 2   Global Step: 61630   Fp16 Grad Scale: 262144   Required: 318 hours\nTraining: 2025-06-03 17:25:00,567-Speed 121.77 samples/sec   Loss 22.6897   LearningRate 0.089844   Epoch: 2   Global Step: 61640   Fp16 Grad Scale: 131072   Required: 318 hours\nTraining: 2025-06-03 17:25:21,312-Speed 123.41 samples/sec   Loss 22.2252   LearningRate 0.089843   Epoch: 2   Global Step: 61650   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:25:41,901-Speed 124.34 samples/sec   Loss 22.4347   LearningRate 0.089841   Epoch: 2   Global Step: 61660   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:26:02,736-Speed 122.88 samples/sec   Loss 22.6648   LearningRate 0.089840   Epoch: 2   Global Step: 61670   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:26:23,805-Speed 121.51 samples/sec   Loss 22.7765   LearningRate 0.089838   Epoch: 2   Global Step: 61680   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:26:44,559-Speed 123.35 samples/sec   Loss 22.3394   LearningRate 0.089836   Epoch: 2   Global Step: 61690   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:27:05,247-Speed 123.75 samples/sec   Loss 22.9455   LearningRate 0.089835   Epoch: 2   Global Step: 61700   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:27:26,080-Speed 122.88 samples/sec   Loss 22.0293   LearningRate 0.089833   Epoch: 2   Global Step: 61710   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:27:47,087-Speed 121.87 samples/sec   Loss 22.3834   LearningRate 0.089831   Epoch: 2   Global Step: 61720   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:28:07,923-Speed 122.87 samples/sec   Loss 22.4688   LearningRate 0.089830   Epoch: 2   Global Step: 61730   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:28:28,413-Speed 124.94 samples/sec   Loss 22.2017   LearningRate 0.089828   Epoch: 2   Global Step: 61740   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:28:49,413-Speed 121.91 samples/sec   Loss 22.6116   LearningRate 0.089826   Epoch: 2   Global Step: 61750   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:29:10,478-Speed 121.53 samples/sec   Loss 22.6714   LearningRate 0.089825   Epoch: 2   Global Step: 61760   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:29:31,108-Speed 124.10 samples/sec   Loss 22.4770   LearningRate 0.089823   Epoch: 2   Global Step: 61770   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:29:51,848-Speed 123.44 samples/sec   Loss 22.1271   LearningRate 0.089821   Epoch: 2   Global Step: 61780   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:30:12,797-Speed 122.20 samples/sec   Loss 22.7786   LearningRate 0.089820   Epoch: 2   Global Step: 61790   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:30:33,781-Speed 122.01 samples/sec   Loss 22.4299   LearningRate 0.089818   Epoch: 2   Global Step: 61800   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:30:54,512-Speed 123.49 samples/sec   Loss 22.3454   LearningRate 0.089816   Epoch: 2   Global Step: 61810   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:31:15,228-Speed 123.58 samples/sec   Loss 22.8820   LearningRate 0.089815   Epoch: 2   Global Step: 61820   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:31:36,109-Speed 122.61 samples/sec   Loss 22.7216   LearningRate 0.089813   Epoch: 2   Global Step: 61830   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:31:57,095-Speed 121.99 samples/sec   Loss 22.6999   LearningRate 0.089812   Epoch: 2   Global Step: 61840   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:32:17,960-Speed 122.70 samples/sec   Loss 22.8475   LearningRate 0.089810   Epoch: 2   Global Step: 61850   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:32:38,611-Speed 123.97 samples/sec   Loss 22.8052   LearningRate 0.089808   Epoch: 2   Global Step: 61860   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:32:59,337-Speed 123.52 samples/sec   Loss 22.6091   LearningRate 0.089807   Epoch: 2   Global Step: 61870   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:33:20,338-Speed 121.90 samples/sec   Loss 22.1991   LearningRate 0.089805   Epoch: 2   Global Step: 61880   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:33:41,140-Speed 123.07 samples/sec   Loss 22.7434   LearningRate 0.089803   Epoch: 2   Global Step: 61890   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:34:01,768-Speed 124.11 samples/sec   Loss 22.5600   LearningRate 0.089802   Epoch: 2   Global Step: 61900   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:34:22,442-Speed 123.83 samples/sec   Loss 22.2849   LearningRate 0.089800   Epoch: 2   Global Step: 61910   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:34:43,161-Speed 123.56 samples/sec   Loss 22.5653   LearningRate 0.089798   Epoch: 2   Global Step: 61920   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:35:04,105-Speed 122.23 samples/sec   Loss 22.5368   LearningRate 0.089797   Epoch: 2   Global Step: 61930   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:35:25,080-Speed 122.05 samples/sec   Loss 22.3688   LearningRate 0.089795   Epoch: 2   Global Step: 61940   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:35:46,063-Speed 122.01 samples/sec   Loss 22.6346   LearningRate 0.089793   Epoch: 2   Global Step: 61950   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:36:07,070-Speed 121.87 samples/sec   Loss 22.6745   LearningRate 0.089792   Epoch: 2   Global Step: 61960   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:36:27,777-Speed 123.63 samples/sec   Loss 22.7023   LearningRate 0.089790   Epoch: 2   Global Step: 61970   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:36:48,511-Speed 123.47 samples/sec   Loss 22.0297   LearningRate 0.089788   Epoch: 2   Global Step: 61980   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:37:09,602-Speed 121.38 samples/sec   Loss 22.4617   LearningRate 0.089787   Epoch: 2   Global Step: 61990   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:37:30,568-Speed 122.11 samples/sec   Loss 22.9306   LearningRate 0.089785   Epoch: 2   Global Step: 62000   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:37:51,334-Speed 123.28 samples/sec   Loss 22.6512   LearningRate 0.089784   Epoch: 2   Global Step: 62010   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:38:12,093-Speed 123.33 samples/sec   Loss 22.3609   LearningRate 0.089782   Epoch: 2   Global Step: 62020   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:38:32,658-Speed 124.49 samples/sec   Loss 22.9296   LearningRate 0.089780   Epoch: 2   Global Step: 62030   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:38:53,447-Speed 123.15 samples/sec   Loss 22.6768   LearningRate 0.089779   Epoch: 2   Global Step: 62040   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:39:14,432-Speed 121.99 samples/sec   Loss 22.5716   LearningRate 0.089777   Epoch: 2   Global Step: 62050   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:39:35,385-Speed 122.18 samples/sec   Loss 22.3210   LearningRate 0.089775   Epoch: 2   Global Step: 62060   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:39:56,196-Speed 123.02 samples/sec   Loss 22.2943   LearningRate 0.089774   Epoch: 2   Global Step: 62070   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:40:16,913-Speed 123.57 samples/sec   Loss 22.3390   LearningRate 0.089772   Epoch: 2   Global Step: 62080   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:40:37,582-Speed 123.86 samples/sec   Loss 22.9263   LearningRate 0.089770   Epoch: 2   Global Step: 62090   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:40:58,294-Speed 123.60 samples/sec   Loss 22.6770   LearningRate 0.089769   Epoch: 2   Global Step: 62100   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:41:19,277-Speed 122.01 samples/sec   Loss 22.7302   LearningRate 0.089767   Epoch: 2   Global Step: 62110   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:41:40,161-Speed 122.59 samples/sec   Loss 22.4316   LearningRate 0.089765   Epoch: 2   Global Step: 62120   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:42:00,969-Speed 123.03 samples/sec   Loss 22.8013   LearningRate 0.089764   Epoch: 2   Global Step: 62130   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:42:21,659-Speed 123.74 samples/sec   Loss 22.2857   LearningRate 0.089762   Epoch: 2   Global Step: 62140   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:42:42,298-Speed 124.04 samples/sec   Loss 22.6008   LearningRate 0.089760   Epoch: 2   Global Step: 62150   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:43:03,050-Speed 123.36 samples/sec   Loss 22.8725   LearningRate 0.089759   Epoch: 2   Global Step: 62160   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:43:23,850-Speed 123.08 samples/sec   Loss 22.8730   LearningRate 0.089757   Epoch: 2   Global Step: 62170   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:43:44,773-Speed 122.36 samples/sec   Loss 22.8916   LearningRate 0.089756   Epoch: 2   Global Step: 62180   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:44:05,584-Speed 123.02 samples/sec   Loss 22.5441   LearningRate 0.089754   Epoch: 2   Global Step: 62190   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:44:26,315-Speed 123.49 samples/sec   Loss 22.7462   LearningRate 0.089752   Epoch: 2   Global Step: 62200   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:44:47,156-Speed 122.84 samples/sec   Loss 22.8271   LearningRate 0.089751   Epoch: 2   Global Step: 62210   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:45:08,029-Speed 122.65 samples/sec   Loss 22.3972   LearningRate 0.089749   Epoch: 2   Global Step: 62220   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:45:28,952-Speed 122.36 samples/sec   Loss 22.7113   LearningRate 0.089747   Epoch: 2   Global Step: 62230   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:45:49,704-Speed 123.36 samples/sec   Loss 22.3260   LearningRate 0.089746   Epoch: 2   Global Step: 62240   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:46:10,450-Speed 123.40 samples/sec   Loss 22.8857   LearningRate 0.089744   Epoch: 2   Global Step: 62250   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:46:31,414-Speed 122.12 samples/sec   Loss 22.3651   LearningRate 0.089742   Epoch: 2   Global Step: 62260   Fp16 Grad Scale: 262144   Required: 317 hours\nTraining: 2025-06-03 17:46:52,251-Speed 122.86 samples/sec   Loss 22.7447   LearningRate 0.089741   Epoch: 2   Global Step: 62270   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:47:13,037-Speed 123.17 samples/sec   Loss 22.4308   LearningRate 0.089739   Epoch: 2   Global Step: 62280   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:47:33,979-Speed 122.25 samples/sec   Loss 22.2879   LearningRate 0.089737   Epoch: 2   Global Step: 62290   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:47:54,719-Speed 123.44 samples/sec   Loss 22.2574   LearningRate 0.089736   Epoch: 2   Global Step: 62300   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:48:15,432-Speed 123.60 samples/sec   Loss 22.5717   LearningRate 0.089734   Epoch: 2   Global Step: 62310   Fp16 Grad Scale: 131072   Required: 317 hours\nTraining: 2025-06-03 17:48:36,244-Speed 123.01 samples/sec   Loss 22.4430   LearningRate 0.089732   Epoch: 2   Global Step: 62320   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:48:57,210-Speed 122.11 samples/sec   Loss 22.4135   LearningRate 0.089731   Epoch: 2   Global Step: 62330   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:49:18,046-Speed 122.87 samples/sec   Loss 22.2512   LearningRate 0.089729   Epoch: 2   Global Step: 62340   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:49:38,817-Speed 123.26 samples/sec   Loss 22.5006   LearningRate 0.089727   Epoch: 2   Global Step: 62350   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 17:49:59,453-Speed 124.06 samples/sec   Loss 22.7880   LearningRate 0.089726   Epoch: 2   Global Step: 62360   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 17:50:20,113-Speed 123.92 samples/sec   Loss 22.4914   LearningRate 0.089724   Epoch: 2   Global Step: 62370   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 17:50:41,088-Speed 122.06 samples/sec   Loss 22.8675   LearningRate 0.089723   Epoch: 2   Global Step: 62380   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 17:51:01,990-Speed 122.48 samples/sec   Loss 22.5418   LearningRate 0.089721   Epoch: 2   Global Step: 62390   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 17:51:22,599-Speed 124.22 samples/sec   Loss 22.3294   LearningRate 0.089719   Epoch: 2   Global Step: 62400   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 17:51:43,348-Speed 123.39 samples/sec   Loss 22.5288   LearningRate 0.089718   Epoch: 2   Global Step: 62410   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 17:52:04,346-Speed 121.92 samples/sec   Loss 22.7514   LearningRate 0.089716   Epoch: 2   Global Step: 62420   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 17:52:25,127-Speed 123.19 samples/sec   Loss 22.5669   LearningRate 0.089714   Epoch: 2   Global Step: 62430   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 17:52:45,596-Speed 125.07 samples/sec   Loss 22.7864   LearningRate 0.089713   Epoch: 2   Global Step: 62440   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 17:53:06,548-Speed 122.19 samples/sec   Loss 22.4604   LearningRate 0.089711   Epoch: 2   Global Step: 62450   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:53:27,345-Speed 123.10 samples/sec   Loss 22.9392   LearningRate 0.089709   Epoch: 2   Global Step: 62460   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:53:47,944-Speed 124.28 samples/sec   Loss 22.6416   LearningRate 0.089708   Epoch: 2   Global Step: 62470   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:54:08,933-Speed 121.98 samples/sec   Loss 22.6809   LearningRate 0.089706   Epoch: 2   Global Step: 62480   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:54:30,069-Speed 121.13 samples/sec   Loss 22.8942   LearningRate 0.089704   Epoch: 2   Global Step: 62490   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:54:50,684-Speed 124.18 samples/sec   Loss 22.4583   LearningRate 0.089703   Epoch: 2   Global Step: 62500   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:55:11,193-Speed 124.83 samples/sec   Loss 22.6764   LearningRate 0.089701   Epoch: 2   Global Step: 62510   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:55:32,194-Speed 121.90 samples/sec   Loss 22.8787   LearningRate 0.089699   Epoch: 2   Global Step: 62520   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:55:53,257-Speed 121.54 samples/sec   Loss 22.5493   LearningRate 0.089698   Epoch: 2   Global Step: 62530   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:56:13,915-Speed 123.93 samples/sec   Loss 22.7893   LearningRate 0.089696   Epoch: 2   Global Step: 62540   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:56:34,560-Speed 124.00 samples/sec   Loss 22.8599   LearningRate 0.089695   Epoch: 2   Global Step: 62550   Fp16 Grad Scale: 262144   Required: 316 hours\nTraining: 2025-06-03 17:56:55,492-Speed 122.30 samples/sec   Loss 22.1711   LearningRate 0.089693   Epoch: 2   Global Step: 62560   Fp16 Grad Scale: 262144   Required: 316 hours\nTraining: 2025-06-03 17:57:16,453-Speed 122.14 samples/sec   Loss 22.3799   LearningRate 0.089691   Epoch: 2   Global Step: 62570   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:57:37,421-Speed 122.10 samples/sec   Loss 22.9967   LearningRate 0.089690   Epoch: 2   Global Step: 62580   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:57:58,190-Speed 123.27 samples/sec   Loss 22.9360   LearningRate 0.089688   Epoch: 2   Global Step: 62590   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:58:18,846-Speed 123.94 samples/sec   Loss 22.3808   LearningRate 0.089686   Epoch: 2   Global Step: 62600   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:58:39,474-Speed 124.10 samples/sec   Loss 22.4498   LearningRate 0.089685   Epoch: 2   Global Step: 62610   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:59:00,069-Speed 124.31 samples/sec   Loss 22.7266   LearningRate 0.089683   Epoch: 2   Global Step: 62620   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:59:20,721-Speed 123.96 samples/sec   Loss 22.3892   LearningRate 0.089681   Epoch: 2   Global Step: 62630   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 17:59:41,651-Speed 122.32 samples/sec   Loss 22.3223   LearningRate 0.089680   Epoch: 2   Global Step: 62640   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:00:02,580-Speed 122.32 samples/sec   Loss 22.5163   LearningRate 0.089678   Epoch: 2   Global Step: 62650   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:00:23,189-Speed 124.22 samples/sec   Loss 22.2563   LearningRate 0.089676   Epoch: 2   Global Step: 62660   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:00:43,987-Speed 123.09 samples/sec   Loss 22.3283   LearningRate 0.089675   Epoch: 2   Global Step: 62670   Fp16 Grad Scale: 262144   Required: 316 hours\nTraining: 2025-06-03 18:01:04,975-Speed 121.98 samples/sec   Loss 22.5781   LearningRate 0.089673   Epoch: 2   Global Step: 62680   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:01:25,789-Speed 123.00 samples/sec   Loss 22.0072   LearningRate 0.089671   Epoch: 2   Global Step: 62690   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:01:46,358-Speed 124.47 samples/sec   Loss 22.8367   LearningRate 0.089670   Epoch: 2   Global Step: 62700   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:02:07,170-Speed 123.01 samples/sec   Loss 22.3756   LearningRate 0.089668   Epoch: 2   Global Step: 62710   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:02:28,140-Speed 122.08 samples/sec   Loss 22.3338   LearningRate 0.089667   Epoch: 2   Global Step: 62720   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:02:48,959-Speed 122.97 samples/sec   Loss 22.7562   LearningRate 0.089665   Epoch: 2   Global Step: 62730   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:03:09,750-Speed 123.13 samples/sec   Loss 22.1934   LearningRate 0.089663   Epoch: 2   Global Step: 62740   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:03:30,568-Speed 122.98 samples/sec   Loss 22.8286   LearningRate 0.089662   Epoch: 2   Global Step: 62750   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:03:51,177-Speed 124.22 samples/sec   Loss 22.6704   LearningRate 0.089660   Epoch: 2   Global Step: 62760   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:04:11,865-Speed 123.75 samples/sec   Loss 22.6124   LearningRate 0.089658   Epoch: 2   Global Step: 62770   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:04:32,755-Speed 122.55 samples/sec   Loss 22.5113   LearningRate 0.089657   Epoch: 2   Global Step: 62780   Fp16 Grad Scale: 262144   Required: 316 hours\nTraining: 2025-06-03 18:04:53,742-Speed 121.98 samples/sec   Loss 22.7210   LearningRate 0.089655   Epoch: 2   Global Step: 62790   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:05:14,618-Speed 122.63 samples/sec   Loss 22.1938   LearningRate 0.089653   Epoch: 2   Global Step: 62800   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:05:35,315-Speed 123.69 samples/sec   Loss 22.3893   LearningRate 0.089652   Epoch: 2   Global Step: 62810   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:05:56,052-Speed 123.46 samples/sec   Loss 22.3816   LearningRate 0.089650   Epoch: 2   Global Step: 62820   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:06:16,708-Speed 123.94 samples/sec   Loss 22.6415   LearningRate 0.089648   Epoch: 2   Global Step: 62830   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:06:37,382-Speed 123.83 samples/sec   Loss 22.5643   LearningRate 0.089647   Epoch: 2   Global Step: 62840   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:06:58,267-Speed 122.58 samples/sec   Loss 22.8012   LearningRate 0.089645   Epoch: 2   Global Step: 62850   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:07:19,245-Speed 122.04 samples/sec   Loss 22.5475   LearningRate 0.089643   Epoch: 2   Global Step: 62860   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:07:40,175-Speed 122.32 samples/sec   Loss 22.9513   LearningRate 0.089642   Epoch: 2   Global Step: 62870   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:08:00,968-Speed 123.12 samples/sec   Loss 22.6708   LearningRate 0.089640   Epoch: 2   Global Step: 62880   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:08:21,739-Speed 123.25 samples/sec   Loss 22.6552   LearningRate 0.089639   Epoch: 2   Global Step: 62890   Fp16 Grad Scale: 262144   Required: 316 hours\nTraining: 2025-06-03 18:08:42,554-Speed 122.99 samples/sec   Loss 22.5532   LearningRate 0.089637   Epoch: 2   Global Step: 62900   Fp16 Grad Scale: 262144   Required: 316 hours\nTraining: 2025-06-03 18:09:03,353-Speed 123.09 samples/sec   Loss 22.2216   LearningRate 0.089635   Epoch: 2   Global Step: 62910   Fp16 Grad Scale: 262144   Required: 316 hours\nTraining: 2025-06-03 18:09:24,148-Speed 123.11 samples/sec   Loss 22.3297   LearningRate 0.089634   Epoch: 2   Global Step: 62920   Fp16 Grad Scale: 262144   Required: 316 hours\nTraining: 2025-06-03 18:09:44,976-Speed 122.91 samples/sec   Loss 22.3598   LearningRate 0.089632   Epoch: 2   Global Step: 62930   Fp16 Grad Scale: 262144   Required: 316 hours\nTraining: 2025-06-03 18:10:05,739-Speed 123.30 samples/sec   Loss 22.7751   LearningRate 0.089630   Epoch: 2   Global Step: 62940   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:10:26,481-Speed 123.43 samples/sec   Loss 22.4931   LearningRate 0.089629   Epoch: 2   Global Step: 62950   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:10:47,385-Speed 122.47 samples/sec   Loss 22.5058   LearningRate 0.089627   Epoch: 2   Global Step: 62960   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:11:08,191-Speed 123.04 samples/sec   Loss 22.1504   LearningRate 0.089625   Epoch: 2   Global Step: 62970   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:11:29,003-Speed 123.02 samples/sec   Loss 22.5446   LearningRate 0.089624   Epoch: 2   Global Step: 62980   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:11:49,792-Speed 123.15 samples/sec   Loss 22.4717   LearningRate 0.089622   Epoch: 2   Global Step: 62990   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:12:10,737-Speed 122.23 samples/sec   Loss 22.7182   LearningRate 0.089620   Epoch: 2   Global Step: 63000   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:12:31,692-Speed 122.17 samples/sec   Loss 22.3779   LearningRate 0.089619   Epoch: 2   Global Step: 63010   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:12:52,508-Speed 122.98 samples/sec   Loss 22.2708   LearningRate 0.089617   Epoch: 2   Global Step: 63020   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:13:13,139-Speed 124.09 samples/sec   Loss 22.7976   LearningRate 0.089615   Epoch: 2   Global Step: 63030   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:13:33,947-Speed 123.03 samples/sec   Loss 22.5245   LearningRate 0.089614   Epoch: 2   Global Step: 63040   Fp16 Grad Scale: 262144   Required: 316 hours\nTraining: 2025-06-03 18:13:54,896-Speed 122.21 samples/sec   Loss 22.7918   LearningRate 0.089612   Epoch: 2   Global Step: 63050   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:14:15,716-Speed 122.97 samples/sec   Loss 22.3574   LearningRate 0.089611   Epoch: 2   Global Step: 63060   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:14:36,683-Speed 122.10 samples/sec   Loss 22.5376   LearningRate 0.089609   Epoch: 2   Global Step: 63070   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:14:57,626-Speed 122.24 samples/sec   Loss 23.1302   LearningRate 0.089607   Epoch: 2   Global Step: 63080   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:15:18,605-Speed 122.03 samples/sec   Loss 22.3225   LearningRate 0.089606   Epoch: 2   Global Step: 63090   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:15:39,386-Speed 123.20 samples/sec   Loss 22.3821   LearningRate 0.089604   Epoch: 2   Global Step: 63100   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:15:59,996-Speed 124.21 samples/sec   Loss 22.4890   LearningRate 0.089602   Epoch: 2   Global Step: 63110   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:16:20,896-Speed 122.50 samples/sec   Loss 22.8304   LearningRate 0.089601   Epoch: 2   Global Step: 63120   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:16:41,938-Speed 121.67 samples/sec   Loss 22.6652   LearningRate 0.089599   Epoch: 2   Global Step: 63130   Fp16 Grad Scale: 131072   Required: 316 hours\nTraining: 2025-06-03 18:17:02,477-Speed 124.64 samples/sec   Loss 22.6259   LearningRate 0.089597   Epoch: 2   Global Step: 63140   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 18:17:23,011-Speed 124.68 samples/sec   Loss 22.6207   LearningRate 0.089596   Epoch: 2   Global Step: 63150   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 18:17:44,128-Speed 121.24 samples/sec   Loss 23.1500   LearningRate 0.089594   Epoch: 2   Global Step: 63160   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 18:18:04,962-Speed 122.88 samples/sec   Loss 22.6709   LearningRate 0.089592   Epoch: 2   Global Step: 63170   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 18:18:25,490-Speed 124.71 samples/sec   Loss 22.4094   LearningRate 0.089591   Epoch: 2   Global Step: 63180   Fp16 Grad Scale: 65536   Required: 316 hours\nTraining: 2025-06-03 18:18:46,291-Speed 123.08 samples/sec   Loss 22.5250   LearningRate 0.089589   Epoch: 2   Global Step: 63190   Fp16 Grad Scale: 65536   Required: 315 hours\nTraining: 2025-06-03 18:19:07,274-Speed 122.00 samples/sec   Loss 22.3810   LearningRate 0.089587   Epoch: 2   Global Step: 63200   Fp16 Grad Scale: 65536   Required: 315 hours\nTraining: 2025-06-03 18:19:28,159-Speed 122.58 samples/sec   Loss 22.6091   LearningRate 0.089586   Epoch: 2   Global Step: 63210   Fp16 Grad Scale: 65536   Required: 315 hours\nTraining: 2025-06-03 18:19:48,821-Speed 123.91 samples/sec   Loss 22.4925   LearningRate 0.089584   Epoch: 2   Global Step: 63220   Fp16 Grad Scale: 65536   Required: 315 hours\nTraining: 2025-06-03 18:20:09,509-Speed 123.75 samples/sec   Loss 22.3303   LearningRate 0.089583   Epoch: 2   Global Step: 63230   Fp16 Grad Scale: 65536   Required: 315 hours\nTraining: 2025-06-03 18:20:30,410-Speed 122.49 samples/sec   Loss 22.7857   LearningRate 0.089581   Epoch: 2   Global Step: 63240   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:20:51,189-Speed 123.21 samples/sec   Loss 23.1414   LearningRate 0.089579   Epoch: 2   Global Step: 63250   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:21:11,978-Speed 123.15 samples/sec   Loss 23.0247   LearningRate 0.089578   Epoch: 2   Global Step: 63260   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:21:32,853-Speed 122.64 samples/sec   Loss 22.4870   LearningRate 0.089576   Epoch: 2   Global Step: 63270   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:21:53,653-Speed 123.08 samples/sec   Loss 22.1935   LearningRate 0.089574   Epoch: 2   Global Step: 63280   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:22:14,569-Speed 122.40 samples/sec   Loss 22.4977   LearningRate 0.089573   Epoch: 2   Global Step: 63290   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:22:35,445-Speed 122.64 samples/sec   Loss 22.2352   LearningRate 0.089571   Epoch: 2   Global Step: 63300   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:22:56,264-Speed 122.97 samples/sec   Loss 22.4813   LearningRate 0.089569   Epoch: 2   Global Step: 63310   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:23:17,265-Speed 121.91 samples/sec   Loss 22.3164   LearningRate 0.089568   Epoch: 2   Global Step: 63320   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:23:38,219-Speed 122.17 samples/sec   Loss 22.6285   LearningRate 0.089566   Epoch: 2   Global Step: 63330   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:23:59,083-Speed 122.70 samples/sec   Loss 22.3509   LearningRate 0.089564   Epoch: 2   Global Step: 63340   Fp16 Grad Scale: 262144   Required: 315 hours\nTraining: 2025-06-03 18:24:20,085-Speed 121.90 samples/sec   Loss 22.6394   LearningRate 0.089563   Epoch: 2   Global Step: 63350   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:24:41,023-Speed 122.27 samples/sec   Loss 22.4798   LearningRate 0.089561   Epoch: 2   Global Step: 63360   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:25:01,709-Speed 123.76 samples/sec   Loss 22.0255   LearningRate 0.089559   Epoch: 2   Global Step: 63370   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:25:22,401-Speed 123.73 samples/sec   Loss 22.4147   LearningRate 0.089558   Epoch: 2   Global Step: 63380   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:25:43,413-Speed 121.84 samples/sec   Loss 22.4642   LearningRate 0.089556   Epoch: 2   Global Step: 63390   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:26:04,392-Speed 122.03 samples/sec   Loss 22.7496   LearningRate 0.089555   Epoch: 2   Global Step: 63400   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:26:25,074-Speed 123.78 samples/sec   Loss 22.5703   LearningRate 0.089553   Epoch: 2   Global Step: 63410   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:26:45,705-Speed 124.09 samples/sec   Loss 22.7225   LearningRate 0.089551   Epoch: 2   Global Step: 63420   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:27:06,611-Speed 122.45 samples/sec   Loss 22.5260   LearningRate 0.089550   Epoch: 2   Global Step: 63430   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:27:27,584-Speed 122.07 samples/sec   Loss 22.7200   LearningRate 0.089548   Epoch: 2   Global Step: 63440   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:27:48,385-Speed 123.08 samples/sec   Loss 22.8153   LearningRate 0.089546   Epoch: 2   Global Step: 63450   Fp16 Grad Scale: 262144   Required: 315 hours\nTraining: 2025-06-03 18:28:09,061-Speed 123.82 samples/sec   Loss 22.8113   LearningRate 0.089545   Epoch: 2   Global Step: 63460   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:28:29,634-Speed 124.44 samples/sec   Loss 22.5462   LearningRate 0.089543   Epoch: 2   Global Step: 63470   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:28:50,239-Speed 124.24 samples/sec   Loss 22.2106   LearningRate 0.089541   Epoch: 2   Global Step: 63480   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:29:11,023-Speed 123.18 samples/sec   Loss 22.7341   LearningRate 0.089540   Epoch: 2   Global Step: 63490   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:29:31,969-Speed 122.22 samples/sec   Loss 22.5849   LearningRate 0.089538   Epoch: 2   Global Step: 63500   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:29:52,848-Speed 122.62 samples/sec   Loss 22.5914   LearningRate 0.089536   Epoch: 2   Global Step: 63510   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:30:13,586-Speed 123.45 samples/sec   Loss 22.5340   LearningRate 0.089535   Epoch: 2   Global Step: 63520   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:30:34,370-Speed 123.18 samples/sec   Loss 22.5369   LearningRate 0.089533   Epoch: 2   Global Step: 63530   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:30:55,102-Speed 123.49 samples/sec   Loss 22.8457   LearningRate 0.089531   Epoch: 2   Global Step: 63540   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:31:15,770-Speed 123.87 samples/sec   Loss 22.6350   LearningRate 0.089530   Epoch: 2   Global Step: 63550   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:31:36,544-Speed 123.23 samples/sec   Loss 22.7894   LearningRate 0.089528   Epoch: 2   Global Step: 63560   Fp16 Grad Scale: 262144   Required: 315 hours\nTraining: 2025-06-03 18:31:57,218-Speed 123.83 samples/sec   Loss 22.5892   LearningRate 0.089526   Epoch: 2   Global Step: 63570   Fp16 Grad Scale: 262144   Required: 315 hours\nTraining: 2025-06-03 18:32:17,905-Speed 123.75 samples/sec   Loss 22.3728   LearningRate 0.089525   Epoch: 2   Global Step: 63580   Fp16 Grad Scale: 262144   Required: 315 hours\nTraining: 2025-06-03 18:32:38,769-Speed 122.70 samples/sec   Loss 22.5241   LearningRate 0.089523   Epoch: 2   Global Step: 63590   Fp16 Grad Scale: 262144   Required: 315 hours\nTraining: 2025-06-03 18:32:59,750-Speed 122.02 samples/sec   Loss 22.4381   LearningRate 0.089522   Epoch: 2   Global Step: 63600   Fp16 Grad Scale: 262144   Required: 315 hours\nTraining: 2025-06-03 18:33:20,619-Speed 122.67 samples/sec   Loss 22.7066   LearningRate 0.089520   Epoch: 2   Global Step: 63610   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:33:41,504-Speed 122.58 samples/sec   Loss 22.6296   LearningRate 0.089518   Epoch: 2   Global Step: 63620   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:34:02,490-Speed 122.00 samples/sec   Loss 22.6002   LearningRate 0.089517   Epoch: 2   Global Step: 63630   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:34:23,287-Speed 123.10 samples/sec   Loss 22.2282   LearningRate 0.089515   Epoch: 2   Global Step: 63640   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:34:43,871-Speed 124.37 samples/sec   Loss 22.1403   LearningRate 0.089513   Epoch: 2   Global Step: 63650   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:35:04,526-Speed 123.95 samples/sec   Loss 22.6463   LearningRate 0.089512   Epoch: 2   Global Step: 63660   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:35:25,412-Speed 122.57 samples/sec   Loss 22.1380   LearningRate 0.089510   Epoch: 2   Global Step: 63670   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:35:46,288-Speed 122.63 samples/sec   Loss 22.5940   LearningRate 0.089508   Epoch: 2   Global Step: 63680   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:36:07,141-Speed 122.77 samples/sec   Loss 22.4312   LearningRate 0.089507   Epoch: 2   Global Step: 63690   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:36:28,049-Speed 122.44 samples/sec   Loss 22.9280   LearningRate 0.089505   Epoch: 2   Global Step: 63700   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:36:48,865-Speed 122.99 samples/sec   Loss 22.7605   LearningRate 0.089503   Epoch: 2   Global Step: 63710   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:37:09,677-Speed 123.01 samples/sec   Loss 22.5486   LearningRate 0.089502   Epoch: 2   Global Step: 63720   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:37:30,604-Speed 122.33 samples/sec   Loss 22.4187   LearningRate 0.089500   Epoch: 2   Global Step: 63730   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:37:51,589-Speed 121.99 samples/sec   Loss 22.4478   LearningRate 0.089498   Epoch: 2   Global Step: 63740   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:38:12,383-Speed 123.12 samples/sec   Loss 22.9392   LearningRate 0.089497   Epoch: 2   Global Step: 63750   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:38:33,098-Speed 123.59 samples/sec   Loss 22.1491   LearningRate 0.089495   Epoch: 2   Global Step: 63760   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:38:53,955-Speed 122.75 samples/sec   Loss 22.9601   LearningRate 0.089494   Epoch: 2   Global Step: 63770   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:39:14,893-Speed 122.27 samples/sec   Loss 22.0359   LearningRate 0.089492   Epoch: 2   Global Step: 63780   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:39:35,670-Speed 123.22 samples/sec   Loss 22.7872   LearningRate 0.089490   Epoch: 2   Global Step: 63790   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:39:56,306-Speed 124.06 samples/sec   Loss 22.6406   LearningRate 0.089489   Epoch: 2   Global Step: 63800   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:40:16,957-Speed 123.97 samples/sec   Loss 22.8311   LearningRate 0.089487   Epoch: 2   Global Step: 63810   Fp16 Grad Scale: 262144   Required: 315 hours\nTraining: 2025-06-03 18:40:37,811-Speed 122.77 samples/sec   Loss 22.3860   LearningRate 0.089485   Epoch: 2   Global Step: 63820   Fp16 Grad Scale: 262144   Required: 315 hours\nTraining: 2025-06-03 18:40:58,508-Speed 123.69 samples/sec   Loss 22.7482   LearningRate 0.089484   Epoch: 2   Global Step: 63830   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:41:19,244-Speed 123.47 samples/sec   Loss 22.8410   LearningRate 0.089482   Epoch: 2   Global Step: 63840   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:41:40,207-Speed 122.12 samples/sec   Loss 22.7592   LearningRate 0.089480   Epoch: 2   Global Step: 63850   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:42:01,105-Speed 122.50 samples/sec   Loss 22.7029   LearningRate 0.089479   Epoch: 2   Global Step: 63860   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:42:21,715-Speed 124.22 samples/sec   Loss 22.5090   LearningRate 0.089477   Epoch: 2   Global Step: 63870   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:42:42,521-Speed 123.05 samples/sec   Loss 22.4719   LearningRate 0.089475   Epoch: 2   Global Step: 63880   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:43:03,333-Speed 123.01 samples/sec   Loss 22.7745   LearningRate 0.089474   Epoch: 2   Global Step: 63890   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:43:24,123-Speed 123.14 samples/sec   Loss 22.4941   LearningRate 0.089472   Epoch: 2   Global Step: 63900   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:43:45,109-Speed 121.99 samples/sec   Loss 22.7851   LearningRate 0.089470   Epoch: 2   Global Step: 63910   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:44:06,061-Speed 122.19 samples/sec   Loss 22.8931   LearningRate 0.089469   Epoch: 2   Global Step: 63920   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:44:26,983-Speed 122.36 samples/sec   Loss 22.8859   LearningRate 0.089467   Epoch: 2   Global Step: 63930   Fp16 Grad Scale: 262144   Required: 315 hours\nTraining: 2025-06-03 18:44:47,919-Speed 122.28 samples/sec   Loss 22.4213   LearningRate 0.089466   Epoch: 2   Global Step: 63940   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:45:08,894-Speed 122.05 samples/sec   Loss 22.4212   LearningRate 0.089464   Epoch: 2   Global Step: 63950   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:45:29,852-Speed 122.16 samples/sec   Loss 22.8389   LearningRate 0.089462   Epoch: 2   Global Step: 63960   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:45:50,663-Speed 123.02 samples/sec   Loss 22.6691   LearningRate 0.089461   Epoch: 2   Global Step: 63970   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:46:11,409-Speed 123.40 samples/sec   Loss 22.5058   LearningRate 0.089459   Epoch: 2   Global Step: 63980   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:46:32,123-Speed 123.59 samples/sec   Loss 22.4788   LearningRate 0.089457   Epoch: 2   Global Step: 63990   Fp16 Grad Scale: 131072   Required: 315 hours\nTraining: 2025-06-03 18:46:52,757-Speed 124.08 samples/sec   Loss 22.4219   LearningRate 0.089456   Epoch: 2   Global Step: 64000   Fp16 Grad Scale: 131072   Required: 315 hours\ntesting verification..\n(12000, 512)\ninfer time 153.38767999999965\nTraining: 2025-06-03 18:49:29,487-[lfw][64000]XNorm: 22.786081\nTraining: 2025-06-03 18:49:29,487-[lfw][64000]Accuracy-Flip: 0.99133+-0.00458\nTraining: 2025-06-03 18:49:29,488-[lfw][64000]Accuracy-Highest: 0.99133\ntesting verification..\n(14000, 512)\ninfer time 177.5615499999999\nTraining: 2025-06-03 18:52:30,824-[cfp_fp][64000]XNorm: 19.703165\nTraining: 2025-06-03 18:52:30,824-[cfp_fp][64000]Accuracy-Flip: 0.92986+-0.01204\nTraining: 2025-06-03 18:52:30,824-[cfp_fp][64000]Accuracy-Highest: 0.92986\ntesting verification..\n(12000, 512)\ninfer time 152.88668099999975\nTraining: 2025-06-03 18:55:06,644-[agedb_30][64000]XNorm: 21.971892\nTraining: 2025-06-03 18:55:06,644-[agedb_30][64000]Accuracy-Flip: 0.94633+-0.00891\nTraining: 2025-06-03 18:55:06,644-[agedb_30][64000]Accuracy-Highest: 0.94633\n/kaggle/working/Arcface_torch/partial_fc_v2.py:157: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(self.fp16):\nTraining: 2025-06-03 18:55:29,230-Speed 4.96 samples/sec   Loss 22.5193   LearningRate 0.089454   Epoch: 2   Global Step: 64010   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 18:55:49,538-Speed 126.06 samples/sec   Loss 22.1014   LearningRate 0.089452   Epoch: 2   Global Step: 64020   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 18:56:09,747-Speed 126.68 samples/sec   Loss 22.4814   LearningRate 0.089451   Epoch: 2   Global Step: 64030   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 18:56:29,929-Speed 126.85 samples/sec   Loss 22.4204   LearningRate 0.089449   Epoch: 2   Global Step: 64040   Fp16 Grad Scale: 262144   Required: 333 hours\nTraining: 2025-06-03 18:56:50,097-Speed 126.94 samples/sec   Loss 22.1861   LearningRate 0.089447   Epoch: 2   Global Step: 64050   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 18:57:10,301-Speed 126.72 samples/sec   Loss 22.3582   LearningRate 0.089446   Epoch: 2   Global Step: 64060   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 18:57:30,443-Speed 127.10 samples/sec   Loss 22.0092   LearningRate 0.089444   Epoch: 2   Global Step: 64070   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 18:57:50,631-Speed 126.81 samples/sec   Loss 22.6185   LearningRate 0.089442   Epoch: 2   Global Step: 64080   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 18:58:10,956-Speed 125.96 samples/sec   Loss 23.1528   LearningRate 0.089441   Epoch: 2   Global Step: 64090   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 18:58:31,350-Speed 125.54 samples/sec   Loss 22.2982   LearningRate 0.089439   Epoch: 2   Global Step: 64100   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 18:58:51,571-Speed 126.60 samples/sec   Loss 22.9130   LearningRate 0.089438   Epoch: 2   Global Step: 64110   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 18:59:11,922-Speed 125.80 samples/sec   Loss 22.8287   LearningRate 0.089436   Epoch: 2   Global Step: 64120   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 18:59:32,417-Speed 124.91 samples/sec   Loss 22.7856   LearningRate 0.089434   Epoch: 2   Global Step: 64130   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 18:59:53,133-Speed 123.59 samples/sec   Loss 22.1743   LearningRate 0.089433   Epoch: 2   Global Step: 64140   Fp16 Grad Scale: 131072   Required: 333 hours\nTraining: 2025-06-03 19:00:13,945-Speed 123.01 samples/sec   Loss 22.7159   LearningRate 0.089431   Epoch: 2   Global Step: 64150   Fp16 Grad Scale: 262144   Required: 332 hours\nTraining: 2025-06-03 19:00:34,897-Speed 122.19 samples/sec   Loss 22.6151   LearningRate 0.089429   Epoch: 2   Global Step: 64160   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:00:55,781-Speed 122.58 samples/sec   Loss 22.4218   LearningRate 0.089428   Epoch: 2   Global Step: 64170   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:01:16,509-Speed 123.51 samples/sec   Loss 22.5599   LearningRate 0.089426   Epoch: 2   Global Step: 64180   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:01:37,336-Speed 122.92 samples/sec   Loss 22.8541   LearningRate 0.089424   Epoch: 2   Global Step: 64190   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:01:58,162-Speed 122.93 samples/sec   Loss 22.3898   LearningRate 0.089423   Epoch: 2   Global Step: 64200   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:02:18,996-Speed 122.88 samples/sec   Loss 22.2449   LearningRate 0.089421   Epoch: 2   Global Step: 64210   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:02:39,789-Speed 123.12 samples/sec   Loss 22.5191   LearningRate 0.089419   Epoch: 2   Global Step: 64220   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:03:00,601-Speed 123.01 samples/sec   Loss 22.4629   LearningRate 0.089418   Epoch: 2   Global Step: 64230   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:03:21,382-Speed 123.19 samples/sec   Loss 22.4176   LearningRate 0.089416   Epoch: 2   Global Step: 64240   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:03:42,197-Speed 123.00 samples/sec   Loss 22.2279   LearningRate 0.089414   Epoch: 2   Global Step: 64250   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:04:02,972-Speed 123.23 samples/sec   Loss 22.4312   LearningRate 0.089413   Epoch: 2   Global Step: 64260   Fp16 Grad Scale: 262144   Required: 332 hours\nTraining: 2025-06-03 19:04:23,663-Speed 123.73 samples/sec   Loss 22.1606   LearningRate 0.089411   Epoch: 2   Global Step: 64270   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:04:44,359-Speed 123.71 samples/sec   Loss 21.8859   LearningRate 0.089410   Epoch: 2   Global Step: 64280   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:05:05,161-Speed 123.07 samples/sec   Loss 22.4049   LearningRate 0.089408   Epoch: 2   Global Step: 64290   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:05:25,935-Speed 123.23 samples/sec   Loss 22.4007   LearningRate 0.089406   Epoch: 2   Global Step: 64300   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:05:46,871-Speed 122.28 samples/sec   Loss 22.4654   LearningRate 0.089405   Epoch: 2   Global Step: 64310   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:06:07,645-Speed 123.23 samples/sec   Loss 22.7642   LearningRate 0.089403   Epoch: 2   Global Step: 64320   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:06:28,260-Speed 124.19 samples/sec   Loss 22.4269   LearningRate 0.089401   Epoch: 2   Global Step: 64330   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:06:49,036-Speed 123.22 samples/sec   Loss 22.6222   LearningRate 0.089400   Epoch: 2   Global Step: 64340   Fp16 Grad Scale: 131072   Required: 332 hours\nTraining: 2025-06-03 19:07:09,893-Speed 122.75 samples/sec   Loss 22.5979   LearningRate 0.089398   Epoch: 2   Global Step: 64350   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:07:30,763-Speed 122.67 samples/sec   Loss 22.6393   LearningRate 0.089396   Epoch: 2   Global Step: 64360   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:07:51,439-Speed 123.82 samples/sec   Loss 22.3254   LearningRate 0.089395   Epoch: 2   Global Step: 64370   Fp16 Grad Scale: 262144   Required: 331 hours\nTraining: 2025-06-03 19:08:12,259-Speed 122.97 samples/sec   Loss 22.5487   LearningRate 0.089393   Epoch: 2   Global Step: 64380   Fp16 Grad Scale: 262144   Required: 331 hours\nTraining: 2025-06-03 19:08:33,050-Speed 123.13 samples/sec   Loss 22.6497   LearningRate 0.089391   Epoch: 2   Global Step: 64390   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:08:53,956-Speed 122.46 samples/sec   Loss 23.2027   LearningRate 0.089390   Epoch: 2   Global Step: 64400   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:09:14,954-Speed 121.92 samples/sec   Loss 22.3955   LearningRate 0.089388   Epoch: 2   Global Step: 64410   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:09:35,726-Speed 123.25 samples/sec   Loss 22.2722   LearningRate 0.089386   Epoch: 2   Global Step: 64420   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:09:56,361-Speed 124.07 samples/sec   Loss 22.6855   LearningRate 0.089385   Epoch: 2   Global Step: 64430   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:10:17,311-Speed 122.20 samples/sec   Loss 22.8570   LearningRate 0.089383   Epoch: 2   Global Step: 64440   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:10:38,283-Speed 122.08 samples/sec   Loss 22.3558   LearningRate 0.089382   Epoch: 2   Global Step: 64450   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:10:58,930-Speed 123.99 samples/sec   Loss 22.8774   LearningRate 0.089380   Epoch: 2   Global Step: 64460   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:11:19,607-Speed 123.81 samples/sec   Loss 22.4737   LearningRate 0.089378   Epoch: 2   Global Step: 64470   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:11:40,188-Speed 124.39 samples/sec   Loss 22.6717   LearningRate 0.089377   Epoch: 2   Global Step: 64480   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:12:00,924-Speed 123.47 samples/sec   Loss 22.6561   LearningRate 0.089375   Epoch: 2   Global Step: 64490   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:12:21,819-Speed 122.52 samples/sec   Loss 22.3534   LearningRate 0.089373   Epoch: 2   Global Step: 64500   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:12:42,758-Speed 122.26 samples/sec   Loss 22.0735   LearningRate 0.089372   Epoch: 2   Global Step: 64510   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:13:03,732-Speed 122.06 samples/sec   Loss 22.3338   LearningRate 0.089370   Epoch: 2   Global Step: 64520   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:13:24,575-Speed 122.83 samples/sec   Loss 22.5767   LearningRate 0.089368   Epoch: 2   Global Step: 64530   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:13:45,202-Speed 124.12 samples/sec   Loss 22.1124   LearningRate 0.089367   Epoch: 2   Global Step: 64540   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:14:05,912-Speed 123.62 samples/sec   Loss 22.2062   LearningRate 0.089365   Epoch: 2   Global Step: 64550   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:14:26,820-Speed 122.44 samples/sec   Loss 22.7685   LearningRate 0.089363   Epoch: 2   Global Step: 64560   Fp16 Grad Scale: 131072   Required: 331 hours\nTraining: 2025-06-03 19:14:47,806-Speed 121.99 samples/sec   Loss 22.5450   LearningRate 0.089362   Epoch: 2   Global Step: 64570   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:15:08,554-Speed 123.39 samples/sec   Loss 22.0721   LearningRate 0.089360   Epoch: 2   Global Step: 64580   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:15:29,178-Speed 124.13 samples/sec   Loss 22.4071   LearningRate 0.089358   Epoch: 2   Global Step: 64590   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 19:15:50,166-Speed 121.98 samples/sec   Loss 22.6308   LearningRate 0.089357   Epoch: 2   Global Step: 64600   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:16:10,901-Speed 123.47 samples/sec   Loss 22.4626   LearningRate 0.089355   Epoch: 2   Global Step: 64610   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:16:31,432-Speed 124.69 samples/sec   Loss 21.9387   LearningRate 0.089353   Epoch: 2   Global Step: 64620   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:16:52,354-Speed 122.37 samples/sec   Loss 22.8307   LearningRate 0.089352   Epoch: 2   Global Step: 64630   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:17:13,338-Speed 122.00 samples/sec   Loss 22.1785   LearningRate 0.089350   Epoch: 2   Global Step: 64640   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:17:34,001-Speed 123.90 samples/sec   Loss 22.8124   LearningRate 0.089349   Epoch: 2   Global Step: 64650   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:17:54,777-Speed 123.23 samples/sec   Loss 22.5956   LearningRate 0.089347   Epoch: 2   Global Step: 64660   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:18:15,367-Speed 124.33 samples/sec   Loss 22.8643   LearningRate 0.089345   Epoch: 2   Global Step: 64670   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:18:36,098-Speed 123.49 samples/sec   Loss 22.6372   LearningRate 0.089344   Epoch: 2   Global Step: 64680   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:18:56,781-Speed 123.78 samples/sec   Loss 22.4581   LearningRate 0.089342   Epoch: 2   Global Step: 64690   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:19:17,731-Speed 122.20 samples/sec   Loss 22.2580   LearningRate 0.089340   Epoch: 2   Global Step: 64700   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 19:19:38,638-Speed 122.46 samples/sec   Loss 22.2828   LearningRate 0.089339   Epoch: 2   Global Step: 64710   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 19:19:59,564-Speed 122.34 samples/sec   Loss 22.7759   LearningRate 0.089337   Epoch: 2   Global Step: 64720   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 19:20:20,552-Speed 121.98 samples/sec   Loss 22.3116   LearningRate 0.089335   Epoch: 2   Global Step: 64730   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 19:20:41,434-Speed 122.60 samples/sec   Loss 22.3646   LearningRate 0.089334   Epoch: 2   Global Step: 64740   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:21:02,213-Speed 123.21 samples/sec   Loss 22.6839   LearningRate 0.089332   Epoch: 2   Global Step: 64750   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:21:23,129-Speed 122.40 samples/sec   Loss 23.1863   LearningRate 0.089330   Epoch: 2   Global Step: 64760   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:21:44,049-Speed 122.38 samples/sec   Loss 22.7048   LearningRate 0.089329   Epoch: 2   Global Step: 64770   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:22:04,661-Speed 124.20 samples/sec   Loss 22.4899   LearningRate 0.089327   Epoch: 2   Global Step: 64780   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:22:25,493-Speed 122.89 samples/sec   Loss 22.4897   LearningRate 0.089325   Epoch: 2   Global Step: 64790   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:22:46,512-Speed 121.80 samples/sec   Loss 22.1147   LearningRate 0.089324   Epoch: 2   Global Step: 64800   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 19:23:07,185-Speed 123.84 samples/sec   Loss 22.3762   LearningRate 0.089322   Epoch: 2   Global Step: 64810   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:23:27,814-Speed 124.10 samples/sec   Loss 22.5738   LearningRate 0.089321   Epoch: 2   Global Step: 64820   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:23:48,618-Speed 123.06 samples/sec   Loss 22.7255   LearningRate 0.089319   Epoch: 2   Global Step: 64830   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:24:09,581-Speed 122.12 samples/sec   Loss 22.1766   LearningRate 0.089317   Epoch: 2   Global Step: 64840   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:24:30,260-Speed 123.80 samples/sec   Loss 22.3591   LearningRate 0.089316   Epoch: 2   Global Step: 64850   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:24:50,884-Speed 124.13 samples/sec   Loss 22.7289   LearningRate 0.089314   Epoch: 2   Global Step: 64860   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:25:11,782-Speed 122.51 samples/sec   Loss 22.3166   LearningRate 0.089312   Epoch: 2   Global Step: 64870   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:25:32,756-Speed 122.06 samples/sec   Loss 22.3096   LearningRate 0.089311   Epoch: 2   Global Step: 64880   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:25:53,450-Speed 123.71 samples/sec   Loss 22.8539   LearningRate 0.089309   Epoch: 2   Global Step: 64890   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 19:26:14,088-Speed 124.05 samples/sec   Loss 22.1005   LearningRate 0.089307   Epoch: 2   Global Step: 64900   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 19:26:34,805-Speed 123.57 samples/sec   Loss 22.3219   LearningRate 0.089306   Epoch: 2   Global Step: 64910   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 19:26:55,684-Speed 122.62 samples/sec   Loss 22.2642   LearningRate 0.089304   Epoch: 2   Global Step: 64920   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 19:27:16,565-Speed 122.60 samples/sec   Loss 22.7150   LearningRate 0.089302   Epoch: 2   Global Step: 64930   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 19:27:37,514-Speed 122.21 samples/sec   Loss 22.6161   LearningRate 0.089301   Epoch: 2   Global Step: 64940   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 19:27:58,143-Speed 124.10 samples/sec   Loss 22.2139   LearningRate 0.089299   Epoch: 2   Global Step: 64950   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 19:28:18,816-Speed 123.84 samples/sec   Loss 22.8172   LearningRate 0.089297   Epoch: 2   Global Step: 64960   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 19:28:39,762-Speed 122.22 samples/sec   Loss 21.9105   LearningRate 0.089296   Epoch: 2   Global Step: 64970   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 19:29:00,601-Speed 122.85 samples/sec   Loss 22.4579   LearningRate 0.089294   Epoch: 2   Global Step: 64980   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 19:29:21,124-Speed 124.74 samples/sec   Loss 22.6106   LearningRate 0.089293   Epoch: 2   Global Step: 64990   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:29:42,060-Speed 122.28 samples/sec   Loss 22.2279   LearningRate 0.089291   Epoch: 2   Global Step: 65000   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:30:03,085-Speed 121.77 samples/sec   Loss 22.3023   LearningRate 0.089289   Epoch: 2   Global Step: 65010   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:30:23,783-Speed 123.69 samples/sec   Loss 22.3469   LearningRate 0.089288   Epoch: 2   Global Step: 65020   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:30:44,471-Speed 123.75 samples/sec   Loss 22.4430   LearningRate 0.089286   Epoch: 2   Global Step: 65030   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:31:05,441-Speed 122.08 samples/sec   Loss 22.4008   LearningRate 0.089284   Epoch: 2   Global Step: 65040   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:31:26,400-Speed 122.14 samples/sec   Loss 22.5856   LearningRate 0.089283   Epoch: 2   Global Step: 65050   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:31:47,357-Speed 122.16 samples/sec   Loss 22.6878   LearningRate 0.089281   Epoch: 2   Global Step: 65060   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:32:08,343-Speed 121.99 samples/sec   Loss 22.5720   LearningRate 0.089279   Epoch: 2   Global Step: 65070   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 19:32:29,324-Speed 122.02 samples/sec   Loss 22.4506   LearningRate 0.089278   Epoch: 2   Global Step: 65080   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:32:50,134-Speed 123.02 samples/sec   Loss 22.8225   LearningRate 0.089276   Epoch: 2   Global Step: 65090   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 19:33:10,967-Speed 122.88 samples/sec   Loss 22.3277   LearningRate 0.089274   Epoch: 2   Global Step: 65100   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:33:31,740-Speed 123.24 samples/sec   Loss 22.4685   LearningRate 0.089273   Epoch: 2   Global Step: 65110   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:33:52,515-Speed 123.23 samples/sec   Loss 22.3897   LearningRate 0.089271   Epoch: 2   Global Step: 65120   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:34:13,481-Speed 122.10 samples/sec   Loss 22.7895   LearningRate 0.089269   Epoch: 2   Global Step: 65130   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:34:34,371-Speed 122.55 samples/sec   Loss 22.5170   LearningRate 0.089268   Epoch: 2   Global Step: 65140   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:34:55,093-Speed 123.54 samples/sec   Loss 22.8533   LearningRate 0.089266   Epoch: 2   Global Step: 65150   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:35:15,940-Speed 122.80 samples/sec   Loss 22.6631   LearningRate 0.089265   Epoch: 2   Global Step: 65160   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:35:36,921-Speed 122.02 samples/sec   Loss 22.5067   LearningRate 0.089263   Epoch: 2   Global Step: 65170   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:35:57,859-Speed 122.27 samples/sec   Loss 22.3485   LearningRate 0.089261   Epoch: 2   Global Step: 65180   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:36:18,671-Speed 123.01 samples/sec   Loss 22.4254   LearningRate 0.089260   Epoch: 2   Global Step: 65190   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:36:39,552-Speed 122.60 samples/sec   Loss 22.3055   LearningRate 0.089258   Epoch: 2   Global Step: 65200   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 19:37:00,533-Speed 122.02 samples/sec   Loss 22.0291   LearningRate 0.089256   Epoch: 2   Global Step: 65210   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 19:37:21,278-Speed 123.41 samples/sec   Loss 22.4244   LearningRate 0.089255   Epoch: 2   Global Step: 65220   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 19:37:42,046-Speed 123.27 samples/sec   Loss 22.1569   LearningRate 0.089253   Epoch: 2   Global Step: 65230   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 19:38:03,006-Speed 122.14 samples/sec   Loss 22.6569   LearningRate 0.089251   Epoch: 2   Global Step: 65240   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 19:38:23,998-Speed 121.95 samples/sec   Loss 23.0135   LearningRate 0.089250   Epoch: 2   Global Step: 65250   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 19:38:44,854-Speed 122.76 samples/sec   Loss 22.6678   LearningRate 0.089248   Epoch: 2   Global Step: 65260   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 19:39:05,608-Speed 123.35 samples/sec   Loss 22.8453   LearningRate 0.089246   Epoch: 2   Global Step: 65270   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 19:39:26,457-Speed 122.79 samples/sec   Loss 22.7942   LearningRate 0.089245   Epoch: 2   Global Step: 65280   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 19:39:47,129-Speed 123.85 samples/sec   Loss 22.3002   LearningRate 0.089243   Epoch: 2   Global Step: 65290   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:40:07,826-Speed 123.69 samples/sec   Loss 22.5615   LearningRate 0.089241   Epoch: 2   Global Step: 65300   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:40:28,780-Speed 122.17 samples/sec   Loss 22.3812   LearningRate 0.089240   Epoch: 2   Global Step: 65310   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:40:49,728-Speed 122.21 samples/sec   Loss 22.6941   LearningRate 0.089238   Epoch: 2   Global Step: 65320   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:41:10,561-Speed 122.89 samples/sec   Loss 22.3256   LearningRate 0.089237   Epoch: 2   Global Step: 65330   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:41:31,535-Speed 122.06 samples/sec   Loss 22.2631   LearningRate 0.089235   Epoch: 2   Global Step: 65340   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:41:52,384-Speed 122.79 samples/sec   Loss 22.9064   LearningRate 0.089233   Epoch: 2   Global Step: 65350   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:42:13,219-Speed 122.88 samples/sec   Loss 22.1114   LearningRate 0.089232   Epoch: 2   Global Step: 65360   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:42:33,940-Speed 123.55 samples/sec   Loss 22.3890   LearningRate 0.089230   Epoch: 2   Global Step: 65370   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 19:42:54,739-Speed 123.09 samples/sec   Loss 22.3012   LearningRate 0.089228   Epoch: 2   Global Step: 65380   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:43:15,522-Speed 123.19 samples/sec   Loss 22.3569   LearningRate 0.089227   Epoch: 2   Global Step: 65390   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 19:43:36,433-Speed 122.42 samples/sec   Loss 22.2627   LearningRate 0.089225   Epoch: 2   Global Step: 65400   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 19:43:57,424-Speed 121.96 samples/sec   Loss 22.8682   LearningRate 0.089223   Epoch: 2   Global Step: 65410   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 19:44:18,383-Speed 122.15 samples/sec   Loss 22.6917   LearningRate 0.089222   Epoch: 2   Global Step: 65420   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 19:44:39,266-Speed 122.59 samples/sec   Loss 21.9075   LearningRate 0.089220   Epoch: 2   Global Step: 65430   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 19:45:00,232-Speed 122.10 samples/sec   Loss 22.7572   LearningRate 0.089218   Epoch: 2   Global Step: 65440   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:45:21,066-Speed 122.88 samples/sec   Loss 22.7613   LearningRate 0.089217   Epoch: 2   Global Step: 65450   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:45:41,731-Speed 123.89 samples/sec   Loss 22.4382   LearningRate 0.089215   Epoch: 2   Global Step: 65460   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:46:02,312-Speed 124.39 samples/sec   Loss 22.3221   LearningRate 0.089213   Epoch: 2   Global Step: 65470   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:46:23,215-Speed 122.48 samples/sec   Loss 22.0977   LearningRate 0.089212   Epoch: 2   Global Step: 65480   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:46:44,317-Speed 121.32 samples/sec   Loss 22.4671   LearningRate 0.089210   Epoch: 2   Global Step: 65490   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:47:05,140-Speed 122.94 samples/sec   Loss 22.1229   LearningRate 0.089209   Epoch: 2   Global Step: 65500   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:47:25,649-Speed 124.83 samples/sec   Loss 22.7054   LearningRate 0.089207   Epoch: 2   Global Step: 65510   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:47:46,537-Speed 122.57 samples/sec   Loss 22.1127   LearningRate 0.089205   Epoch: 2   Global Step: 65520   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:48:07,513-Speed 122.05 samples/sec   Loss 22.6729   LearningRate 0.089204   Epoch: 2   Global Step: 65530   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:48:28,272-Speed 123.33 samples/sec   Loss 22.3316   LearningRate 0.089202   Epoch: 2   Global Step: 65540   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 19:48:48,936-Speed 123.89 samples/sec   Loss 22.4266   LearningRate 0.089200   Epoch: 2   Global Step: 65550   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 19:49:10,002-Speed 121.53 samples/sec   Loss 22.5836   LearningRate 0.089199   Epoch: 2   Global Step: 65560   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 19:49:30,852-Speed 122.78 samples/sec   Loss 21.8245   LearningRate 0.089197   Epoch: 2   Global Step: 65570   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 19:49:51,414-Speed 124.51 samples/sec   Loss 22.2471   LearningRate 0.089195   Epoch: 2   Global Step: 65580   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 19:50:12,367-Speed 122.18 samples/sec   Loss 22.4134   LearningRate 0.089194   Epoch: 2   Global Step: 65590   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 19:50:33,405-Speed 121.69 samples/sec   Loss 22.0502   LearningRate 0.089192   Epoch: 2   Global Step: 65600   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 19:50:53,953-Speed 124.59 samples/sec   Loss 22.4217   LearningRate 0.089190   Epoch: 2   Global Step: 65610   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:51:14,644-Speed 123.73 samples/sec   Loss 22.4775   LearningRate 0.089189   Epoch: 2   Global Step: 65620   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:51:35,714-Speed 121.50 samples/sec   Loss 22.1739   LearningRate 0.089187   Epoch: 2   Global Step: 65630   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:51:56,678-Speed 122.12 samples/sec   Loss 22.3392   LearningRate 0.089185   Epoch: 2   Global Step: 65640   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:52:17,179-Speed 124.88 samples/sec   Loss 22.1783   LearningRate 0.089184   Epoch: 2   Global Step: 65650   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:52:37,841-Speed 123.90 samples/sec   Loss 22.7227   LearningRate 0.089182   Epoch: 2   Global Step: 65660   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:52:58,964-Speed 121.20 samples/sec   Loss 22.6345   LearningRate 0.089181   Epoch: 2   Global Step: 65670   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:53:19,902-Speed 122.27 samples/sec   Loss 22.4226   LearningRate 0.089179   Epoch: 2   Global Step: 65680   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:53:40,553-Speed 123.97 samples/sec   Loss 22.1186   LearningRate 0.089177   Epoch: 2   Global Step: 65690   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:54:01,441-Speed 122.57 samples/sec   Loss 22.3499   LearningRate 0.089176   Epoch: 2   Global Step: 65700   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 19:54:22,406-Speed 122.11 samples/sec   Loss 22.2202   LearningRate 0.089174   Epoch: 2   Global Step: 65710   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 19:54:43,372-Speed 122.10 samples/sec   Loss 22.8545   LearningRate 0.089172   Epoch: 2   Global Step: 65720   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 19:55:04,313-Speed 122.25 samples/sec   Loss 22.3884   LearningRate 0.089171   Epoch: 2   Global Step: 65730   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 19:55:24,927-Speed 124.19 samples/sec   Loss 22.7317   LearningRate 0.089169   Epoch: 2   Global Step: 65740   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 19:55:45,524-Speed 124.30 samples/sec   Loss 22.4380   LearningRate 0.089167   Epoch: 2   Global Step: 65750   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 19:56:06,185-Speed 123.91 samples/sec   Loss 22.4673   LearningRate 0.089166   Epoch: 2   Global Step: 65760   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 19:56:27,172-Speed 121.99 samples/sec   Loss 22.6936   LearningRate 0.089164   Epoch: 2   Global Step: 65770   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 19:56:48,139-Speed 122.10 samples/sec   Loss 22.5627   LearningRate 0.089162   Epoch: 2   Global Step: 65780   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 19:57:09,025-Speed 122.58 samples/sec   Loss 22.6948   LearningRate 0.089161   Epoch: 2   Global Step: 65790   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 19:57:30,019-Speed 121.94 samples/sec   Loss 22.8063   LearningRate 0.089159   Epoch: 2   Global Step: 65800   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 19:57:51,003-Speed 122.01 samples/sec   Loss 22.4072   LearningRate 0.089157   Epoch: 2   Global Step: 65810   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 19:58:11,906-Speed 122.47 samples/sec   Loss 22.3948   LearningRate 0.089156   Epoch: 2   Global Step: 65820   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 19:58:32,469-Speed 124.50 samples/sec   Loss 22.3140   LearningRate 0.089154   Epoch: 2   Global Step: 65830   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 19:58:53,258-Speed 123.15 samples/sec   Loss 22.7731   LearningRate 0.089152   Epoch: 2   Global Step: 65840   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 19:59:14,317-Speed 121.57 samples/sec   Loss 22.0941   LearningRate 0.089151   Epoch: 2   Global Step: 65850   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 19:59:35,190-Speed 122.65 samples/sec   Loss 22.4989   LearningRate 0.089149   Epoch: 2   Global Step: 65860   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 19:59:55,847-Speed 123.93 samples/sec   Loss 22.7115   LearningRate 0.089148   Epoch: 2   Global Step: 65870   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:00:16,482-Speed 124.06 samples/sec   Loss 22.4941   LearningRate 0.089146   Epoch: 2   Global Step: 65880   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:00:37,395-Speed 122.42 samples/sec   Loss 21.8988   LearningRate 0.089144   Epoch: 2   Global Step: 65890   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:00:58,424-Speed 121.74 samples/sec   Loss 22.5872   LearningRate 0.089143   Epoch: 2   Global Step: 65900   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:01:19,000-Speed 124.42 samples/sec   Loss 22.1744   LearningRate 0.089141   Epoch: 2   Global Step: 65910   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:01:39,768-Speed 123.27 samples/sec   Loss 22.4453   LearningRate 0.089139   Epoch: 2   Global Step: 65920   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:02:00,808-Speed 121.68 samples/sec   Loss 22.1660   LearningRate 0.089138   Epoch: 2   Global Step: 65930   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:02:21,530-Speed 123.55 samples/sec   Loss 22.3233   LearningRate 0.089136   Epoch: 2   Global Step: 65940   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:02:42,124-Speed 124.31 samples/sec   Loss 22.3610   LearningRate 0.089134   Epoch: 2   Global Step: 65950   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 20:03:03,075-Speed 122.19 samples/sec   Loss 22.1881   LearningRate 0.089133   Epoch: 2   Global Step: 65960   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 20:03:24,043-Speed 122.10 samples/sec   Loss 22.9446   LearningRate 0.089131   Epoch: 2   Global Step: 65970   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:03:45,027-Speed 122.01 samples/sec   Loss 22.0289   LearningRate 0.089129   Epoch: 2   Global Step: 65980   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:04:05,950-Speed 122.36 samples/sec   Loss 22.5935   LearningRate 0.089128   Epoch: 2   Global Step: 65990   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:04:26,709-Speed 123.33 samples/sec   Loss 22.4008   LearningRate 0.089126   Epoch: 2   Global Step: 66000   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:04:47,429-Speed 123.56 samples/sec   Loss 22.6966   LearningRate 0.089124   Epoch: 2   Global Step: 66010   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:05:08,327-Speed 122.51 samples/sec   Loss 22.8464   LearningRate 0.089123   Epoch: 2   Global Step: 66020   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:05:29,292-Speed 122.12 samples/sec   Loss 22.4148   LearningRate 0.089121   Epoch: 2   Global Step: 66030   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:05:50,128-Speed 122.87 samples/sec   Loss 22.5970   LearningRate 0.089120   Epoch: 2   Global Step: 66040   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:06:10,851-Speed 123.54 samples/sec   Loss 22.4286   LearningRate 0.089118   Epoch: 2   Global Step: 66050   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:06:31,641-Speed 123.14 samples/sec   Loss 22.7288   LearningRate 0.089116   Epoch: 2   Global Step: 66060   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 20:06:52,399-Speed 123.33 samples/sec   Loss 22.6451   LearningRate 0.089115   Epoch: 2   Global Step: 66070   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:07:13,168-Speed 123.27 samples/sec   Loss 22.5835   LearningRate 0.089113   Epoch: 2   Global Step: 66080   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:07:33,966-Speed 123.09 samples/sec   Loss 22.2844   LearningRate 0.089111   Epoch: 2   Global Step: 66090   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:07:54,699-Speed 123.48 samples/sec   Loss 22.1522   LearningRate 0.089110   Epoch: 2   Global Step: 66100   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:08:15,412-Speed 123.60 samples/sec   Loss 22.3491   LearningRate 0.089108   Epoch: 2   Global Step: 66110   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:08:36,102-Speed 123.74 samples/sec   Loss 22.5423   LearningRate 0.089106   Epoch: 2   Global Step: 66120   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:08:56,984-Speed 122.60 samples/sec   Loss 22.7054   LearningRate 0.089105   Epoch: 2   Global Step: 66130   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:09:17,800-Speed 122.99 samples/sec   Loss 22.2513   LearningRate 0.089103   Epoch: 2   Global Step: 66140   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:09:38,582-Speed 123.19 samples/sec   Loss 22.1442   LearningRate 0.089101   Epoch: 2   Global Step: 66150   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:09:59,482-Speed 122.50 samples/sec   Loss 22.4272   LearningRate 0.089100   Epoch: 2   Global Step: 66160   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:10:20,394-Speed 122.42 samples/sec   Loss 22.2500   LearningRate 0.089098   Epoch: 2   Global Step: 66170   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:10:41,338-Speed 122.24 samples/sec   Loss 22.3292   LearningRate 0.089096   Epoch: 2   Global Step: 66180   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:11:02,332-Speed 121.94 samples/sec   Loss 22.1551   LearningRate 0.089095   Epoch: 2   Global Step: 66190   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:11:23,311-Speed 122.03 samples/sec   Loss 22.5317   LearningRate 0.089093   Epoch: 2   Global Step: 66200   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:11:44,120-Speed 123.03 samples/sec   Loss 22.4346   LearningRate 0.089092   Epoch: 2   Global Step: 66210   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:12:04,798-Speed 123.81 samples/sec   Loss 22.4482   LearningRate 0.089090   Epoch: 2   Global Step: 66220   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:12:25,402-Speed 124.25 samples/sec   Loss 22.6125   LearningRate 0.089088   Epoch: 2   Global Step: 66230   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:12:46,068-Speed 123.88 samples/sec   Loss 22.0332   LearningRate 0.089087   Epoch: 2   Global Step: 66240   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:13:07,026-Speed 122.16 samples/sec   Loss 22.3911   LearningRate 0.089085   Epoch: 2   Global Step: 66250   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:13:27,918-Speed 122.54 samples/sec   Loss 22.0653   LearningRate 0.089083   Epoch: 2   Global Step: 66260   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:13:48,728-Speed 123.02 samples/sec   Loss 22.5488   LearningRate 0.089082   Epoch: 2   Global Step: 66270   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:14:09,397-Speed 123.86 samples/sec   Loss 22.7051   LearningRate 0.089080   Epoch: 2   Global Step: 66280   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:14:30,311-Speed 122.41 samples/sec   Loss 22.6605   LearningRate 0.089078   Epoch: 2   Global Step: 66290   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:14:51,271-Speed 122.15 samples/sec   Loss 22.6586   LearningRate 0.089077   Epoch: 2   Global Step: 66300   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:15:12,170-Speed 122.50 samples/sec   Loss 22.3387   LearningRate 0.089075   Epoch: 2   Global Step: 66310   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:15:32,916-Speed 123.40 samples/sec   Loss 22.3785   LearningRate 0.089073   Epoch: 2   Global Step: 66320   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:15:53,761-Speed 122.81 samples/sec   Loss 22.5202   LearningRate 0.089072   Epoch: 2   Global Step: 66330   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:16:14,744-Speed 122.01 samples/sec   Loss 22.0372   LearningRate 0.089070   Epoch: 2   Global Step: 66340   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:16:35,512-Speed 123.27 samples/sec   Loss 22.8010   LearningRate 0.089068   Epoch: 2   Global Step: 66350   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:16:56,189-Speed 123.81 samples/sec   Loss 22.2413   LearningRate 0.089067   Epoch: 2   Global Step: 66360   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:17:17,080-Speed 122.55 samples/sec   Loss 22.4114   LearningRate 0.089065   Epoch: 2   Global Step: 66370   Fp16 Grad Scale: 524288   Required: 325 hours\nTraining: 2025-06-03 20:17:38,020-Speed 122.26 samples/sec   Loss 22.6553   LearningRate 0.089064   Epoch: 2   Global Step: 66380   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:17:58,855-Speed 122.88 samples/sec   Loss 22.5711   LearningRate 0.089062   Epoch: 2   Global Step: 66390   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:18:19,613-Speed 123.33 samples/sec   Loss 22.4550   LearningRate 0.089060   Epoch: 2   Global Step: 66400   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:18:40,503-Speed 122.55 samples/sec   Loss 22.1712   LearningRate 0.089059   Epoch: 2   Global Step: 66410   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:19:01,491-Speed 121.98 samples/sec   Loss 22.3756   LearningRate 0.089057   Epoch: 2   Global Step: 66420   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 20:19:22,364-Speed 122.65 samples/sec   Loss 22.6854   LearningRate 0.089055   Epoch: 2   Global Step: 66430   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:19:43,004-Speed 124.04 samples/sec   Loss 22.3444   LearningRate 0.089054   Epoch: 2   Global Step: 66440   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:20:03,876-Speed 122.66 samples/sec   Loss 22.6458   LearningRate 0.089052   Epoch: 2   Global Step: 66450   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 20:20:24,851-Speed 122.05 samples/sec   Loss 22.4695   LearningRate 0.089050   Epoch: 2   Global Step: 66460   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:20:45,666-Speed 122.99 samples/sec   Loss 22.3387   LearningRate 0.089049   Epoch: 2   Global Step: 66470   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:21:06,229-Speed 124.50 samples/sec   Loss 22.4582   LearningRate 0.089047   Epoch: 2   Global Step: 66480   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:21:27,049-Speed 122.97 samples/sec   Loss 22.3512   LearningRate 0.089045   Epoch: 2   Global Step: 66490   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:21:48,044-Speed 121.94 samples/sec   Loss 22.2296   LearningRate 0.089044   Epoch: 2   Global Step: 66500   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:22:09,021-Speed 122.04 samples/sec   Loss 22.4006   LearningRate 0.089042   Epoch: 2   Global Step: 66510   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:22:29,652-Speed 124.09 samples/sec   Loss 22.4167   LearningRate 0.089040   Epoch: 2   Global Step: 66520   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:22:50,326-Speed 123.83 samples/sec   Loss 22.7395   LearningRate 0.089039   Epoch: 2   Global Step: 66530   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 20:23:11,117-Speed 123.14 samples/sec   Loss 22.1701   LearningRate 0.089037   Epoch: 2   Global Step: 66540   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 20:23:31,899-Speed 123.19 samples/sec   Loss 22.5973   LearningRate 0.089036   Epoch: 2   Global Step: 66550   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:23:52,783-Speed 122.59 samples/sec   Loss 22.2830   LearningRate 0.089034   Epoch: 2   Global Step: 66560   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:24:13,746-Speed 122.13 samples/sec   Loss 22.4377   LearningRate 0.089032   Epoch: 2   Global Step: 66570   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:24:34,420-Speed 123.83 samples/sec   Loss 22.2269   LearningRate 0.089031   Epoch: 2   Global Step: 66580   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:24:55,076-Speed 123.94 samples/sec   Loss 22.2934   LearningRate 0.089029   Epoch: 2   Global Step: 66590   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:25:15,885-Speed 123.03 samples/sec   Loss 22.6793   LearningRate 0.089027   Epoch: 2   Global Step: 66600   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:25:36,883-Speed 121.92 samples/sec   Loss 22.3276   LearningRate 0.089026   Epoch: 2   Global Step: 66610   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:25:57,865-Speed 122.01 samples/sec   Loss 22.6401   LearningRate 0.089024   Epoch: 2   Global Step: 66620   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:26:18,813-Speed 122.21 samples/sec   Loss 22.6509   LearningRate 0.089022   Epoch: 2   Global Step: 66630   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:26:39,455-Speed 124.03 samples/sec   Loss 22.6132   LearningRate 0.089021   Epoch: 2   Global Step: 66640   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:27:00,251-Speed 123.11 samples/sec   Loss 22.0577   LearningRate 0.089019   Epoch: 2   Global Step: 66650   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 20:27:21,271-Speed 121.79 samples/sec   Loss 22.1441   LearningRate 0.089017   Epoch: 2   Global Step: 66660   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:27:41,971-Speed 123.68 samples/sec   Loss 22.2652   LearningRate 0.089016   Epoch: 2   Global Step: 66670   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:28:02,653-Speed 123.78 samples/sec   Loss 22.9434   LearningRate 0.089014   Epoch: 2   Global Step: 66680   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:28:23,585-Speed 122.31 samples/sec   Loss 22.8217   LearningRate 0.089012   Epoch: 2   Global Step: 66690   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:28:44,609-Speed 121.77 samples/sec   Loss 22.3512   LearningRate 0.089011   Epoch: 2   Global Step: 66700   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:29:05,298-Speed 123.74 samples/sec   Loss 22.0295   LearningRate 0.089009   Epoch: 2   Global Step: 66710   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:29:25,915-Speed 124.17 samples/sec   Loss 22.2252   LearningRate 0.089008   Epoch: 2   Global Step: 66720   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:29:46,699-Speed 123.18 samples/sec   Loss 22.2920   LearningRate 0.089006   Epoch: 2   Global Step: 66730   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:30:07,658-Speed 122.15 samples/sec   Loss 22.5656   LearningRate 0.089004   Epoch: 2   Global Step: 66740   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:30:28,589-Speed 122.31 samples/sec   Loss 22.0284   LearningRate 0.089003   Epoch: 2   Global Step: 66750   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:30:49,300-Speed 123.61 samples/sec   Loss 22.2600   LearningRate 0.089001   Epoch: 2   Global Step: 66760   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 20:31:09,940-Speed 124.03 samples/sec   Loss 22.6252   LearningRate 0.088999   Epoch: 2   Global Step: 66770   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:31:30,559-Speed 124.17 samples/sec   Loss 22.2055   LearningRate 0.088998   Epoch: 2   Global Step: 66780   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:31:51,344-Speed 123.17 samples/sec   Loss 22.6206   LearningRate 0.088996   Epoch: 2   Global Step: 66790   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:32:12,339-Speed 121.94 samples/sec   Loss 22.3093   LearningRate 0.088994   Epoch: 2   Global Step: 66800   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:32:33,182-Speed 122.83 samples/sec   Loss 22.3115   LearningRate 0.088993   Epoch: 2   Global Step: 66810   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:32:53,753-Speed 124.45 samples/sec   Loss 22.7098   LearningRate 0.088991   Epoch: 2   Global Step: 66820   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:33:14,612-Speed 122.73 samples/sec   Loss 22.3894   LearningRate 0.088989   Epoch: 2   Global Step: 66830   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:33:35,600-Speed 121.98 samples/sec   Loss 22.3152   LearningRate 0.088988   Epoch: 2   Global Step: 66840   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:33:56,532-Speed 122.31 samples/sec   Loss 22.2635   LearningRate 0.088986   Epoch: 2   Global Step: 66850   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:34:17,103-Speed 124.46 samples/sec   Loss 22.7271   LearningRate 0.088984   Epoch: 2   Global Step: 66860   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 20:34:37,765-Speed 123.90 samples/sec   Loss 22.5384   LearningRate 0.088983   Epoch: 2   Global Step: 66870   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 20:34:58,852-Speed 121.41 samples/sec   Loss 22.5244   LearningRate 0.088981   Epoch: 2   Global Step: 66880   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 20:35:19,901-Speed 121.63 samples/sec   Loss 22.4908   LearningRate 0.088980   Epoch: 2   Global Step: 66890   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:35:40,514-Speed 124.20 samples/sec   Loss 22.4150   LearningRate 0.088978   Epoch: 2   Global Step: 66900   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:36:01,141-Speed 124.12 samples/sec   Loss 22.6762   LearningRate 0.088976   Epoch: 2   Global Step: 66910   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:36:22,161-Speed 121.79 samples/sec   Loss 22.0669   LearningRate 0.088975   Epoch: 2   Global Step: 66920   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:36:43,175-Speed 121.83 samples/sec   Loss 22.3275   LearningRate 0.088973   Epoch: 2   Global Step: 66930   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:37:03,958-Speed 123.18 samples/sec   Loss 22.1807   LearningRate 0.088971   Epoch: 2   Global Step: 66940   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:37:24,489-Speed 124.69 samples/sec   Loss 22.5651   LearningRate 0.088970   Epoch: 2   Global Step: 66950   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:37:45,269-Speed 123.20 samples/sec   Loss 22.3042   LearningRate 0.088968   Epoch: 2   Global Step: 66960   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:38:06,246-Speed 122.04 samples/sec   Loss 22.3519   LearningRate 0.088966   Epoch: 2   Global Step: 66970   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:38:27,187-Speed 122.25 samples/sec   Loss 22.3146   LearningRate 0.088965   Epoch: 2   Global Step: 66980   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:38:47,664-Speed 125.02 samples/sec   Loss 22.5516   LearningRate 0.088963   Epoch: 2   Global Step: 66990   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:39:08,421-Speed 123.34 samples/sec   Loss 22.1726   LearningRate 0.088961   Epoch: 2   Global Step: 67000   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:39:29,506-Speed 121.42 samples/sec   Loss 22.4180   LearningRate 0.088960   Epoch: 2   Global Step: 67010   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:39:50,275-Speed 123.26 samples/sec   Loss 22.0687   LearningRate 0.088958   Epoch: 2   Global Step: 67020   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:40:10,678-Speed 125.48 samples/sec   Loss 22.5254   LearningRate 0.088956   Epoch: 2   Global Step: 67030   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:40:31,534-Speed 122.75 samples/sec   Loss 22.5451   LearningRate 0.088955   Epoch: 2   Global Step: 67040   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:40:52,601-Speed 121.52 samples/sec   Loss 22.4857   LearningRate 0.088953   Epoch: 2   Global Step: 67050   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:41:13,465-Speed 122.71 samples/sec   Loss 22.4978   LearningRate 0.088951   Epoch: 2   Global Step: 67060   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:41:34,030-Speed 124.49 samples/sec   Loss 22.0611   LearningRate 0.088950   Epoch: 2   Global Step: 67070   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:41:54,624-Speed 124.32 samples/sec   Loss 22.5086   LearningRate 0.088948   Epoch: 2   Global Step: 67080   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:42:15,378-Speed 123.35 samples/sec   Loss 22.5081   LearningRate 0.088947   Epoch: 2   Global Step: 67090   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:42:36,285-Speed 122.45 samples/sec   Loss 21.9582   LearningRate 0.088945   Epoch: 2   Global Step: 67100   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:42:57,259-Speed 122.06 samples/sec   Loss 22.6719   LearningRate 0.088943   Epoch: 2   Global Step: 67110   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:43:18,075-Speed 122.98 samples/sec   Loss 22.0681   LearningRate 0.088942   Epoch: 2   Global Step: 67120   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:43:38,593-Speed 124.77 samples/sec   Loss 22.1466   LearningRate 0.088940   Epoch: 2   Global Step: 67130   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:43:59,397-Speed 123.06 samples/sec   Loss 22.3480   LearningRate 0.088938   Epoch: 2   Global Step: 67140   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:44:20,422-Speed 121.77 samples/sec   Loss 22.1124   LearningRate 0.088937   Epoch: 2   Global Step: 67150   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:44:41,122-Speed 123.68 samples/sec   Loss 22.2391   LearningRate 0.088935   Epoch: 2   Global Step: 67160   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:45:01,805-Speed 123.78 samples/sec   Loss 22.6998   LearningRate 0.088933   Epoch: 2   Global Step: 67170   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:45:22,749-Speed 122.23 samples/sec   Loss 22.0749   LearningRate 0.088932   Epoch: 2   Global Step: 67180   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:45:43,698-Speed 122.21 samples/sec   Loss 22.0807   LearningRate 0.088930   Epoch: 2   Global Step: 67190   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:46:04,367-Speed 123.86 samples/sec   Loss 22.0447   LearningRate 0.088928   Epoch: 2   Global Step: 67200   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:46:25,167-Speed 123.08 samples/sec   Loss 22.3371   LearningRate 0.088927   Epoch: 2   Global Step: 67210   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:46:45,987-Speed 122.96 samples/sec   Loss 21.9253   LearningRate 0.088925   Epoch: 2   Global Step: 67220   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:47:06,967-Speed 122.03 samples/sec   Loss 22.3563   LearningRate 0.088923   Epoch: 2   Global Step: 67230   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:47:27,732-Speed 123.29 samples/sec   Loss 22.5384   LearningRate 0.088922   Epoch: 2   Global Step: 67240   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:47:48,275-Speed 124.62 samples/sec   Loss 22.1581   LearningRate 0.088920   Epoch: 2   Global Step: 67250   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 20:48:09,002-Speed 123.52 samples/sec   Loss 21.8348   LearningRate 0.088919   Epoch: 2   Global Step: 67260   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:48:29,989-Speed 121.98 samples/sec   Loss 22.7980   LearningRate 0.088917   Epoch: 2   Global Step: 67270   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:48:50,845-Speed 122.75 samples/sec   Loss 22.4964   LearningRate 0.088915   Epoch: 2   Global Step: 67280   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:49:11,483-Speed 124.05 samples/sec   Loss 22.5891   LearningRate 0.088914   Epoch: 2   Global Step: 67290   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:49:32,079-Speed 124.30 samples/sec   Loss 22.4681   LearningRate 0.088912   Epoch: 2   Global Step: 67300   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:49:52,997-Speed 122.39 samples/sec   Loss 22.2683   LearningRate 0.088910   Epoch: 2   Global Step: 67310   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:50:14,107-Speed 121.27 samples/sec   Loss 22.7485   LearningRate 0.088909   Epoch: 2   Global Step: 67320   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:50:34,911-Speed 123.06 samples/sec   Loss 22.4742   LearningRate 0.088907   Epoch: 2   Global Step: 67330   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:50:55,481-Speed 124.46 samples/sec   Loss 22.2472   LearningRate 0.088905   Epoch: 2   Global Step: 67340   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 20:51:16,176-Speed 123.71 samples/sec   Loss 22.2369   LearningRate 0.088904   Epoch: 2   Global Step: 67350   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:51:36,768-Speed 124.33 samples/sec   Loss 22.4213   LearningRate 0.088902   Epoch: 2   Global Step: 67360   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 20:51:57,676-Speed 122.44 samples/sec   Loss 22.4745   LearningRate 0.088900   Epoch: 2   Global Step: 67370   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 20:52:18,674-Speed 121.92 samples/sec   Loss 22.2718   LearningRate 0.088899   Epoch: 2   Global Step: 67380   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 20:52:39,655-Speed 122.02 samples/sec   Loss 22.4415   LearningRate 0.088897   Epoch: 2   Global Step: 67390   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 20:53:00,497-Speed 122.84 samples/sec   Loss 22.3120   LearningRate 0.088895   Epoch: 2   Global Step: 67400   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 20:53:21,095-Speed 124.29 samples/sec   Loss 22.3438   LearningRate 0.088894   Epoch: 2   Global Step: 67410   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 20:53:41,975-Speed 122.61 samples/sec   Loss 22.3126   LearningRate 0.088892   Epoch: 2   Global Step: 67420   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 20:54:03,034-Speed 121.57 samples/sec   Loss 22.4727   LearningRate 0.088891   Epoch: 2   Global Step: 67430   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 20:54:23,632-Speed 124.29 samples/sec   Loss 22.5414   LearningRate 0.088889   Epoch: 2   Global Step: 67440   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 20:54:44,317-Speed 123.76 samples/sec   Loss 22.3626   LearningRate 0.088887   Epoch: 2   Global Step: 67450   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:55:05,298-Speed 122.02 samples/sec   Loss 22.0662   LearningRate 0.088886   Epoch: 2   Global Step: 67460   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:55:26,183-Speed 122.58 samples/sec   Loss 22.9043   LearningRate 0.088884   Epoch: 2   Global Step: 67470   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:55:46,702-Speed 124.77 samples/sec   Loss 22.2820   LearningRate 0.088882   Epoch: 2   Global Step: 67480   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:56:07,512-Speed 123.02 samples/sec   Loss 22.2993   LearningRate 0.088881   Epoch: 2   Global Step: 67490   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:56:28,659-Speed 121.06 samples/sec   Loss 22.2623   LearningRate 0.088879   Epoch: 2   Global Step: 67500   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:56:49,643-Speed 122.00 samples/sec   Loss 22.5721   LearningRate 0.088877   Epoch: 2   Global Step: 67510   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:57:10,265-Speed 124.14 samples/sec   Loss 22.3334   LearningRate 0.088876   Epoch: 2   Global Step: 67520   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:57:30,989-Speed 123.53 samples/sec   Loss 22.8727   LearningRate 0.088874   Epoch: 2   Global Step: 67530   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:57:51,980-Speed 121.96 samples/sec   Loss 22.4288   LearningRate 0.088872   Epoch: 2   Global Step: 67540   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:58:12,943-Speed 122.13 samples/sec   Loss 22.6228   LearningRate 0.088871   Epoch: 2   Global Step: 67550   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 20:58:33,530-Speed 124.35 samples/sec   Loss 22.4921   LearningRate 0.088869   Epoch: 2   Global Step: 67560   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:58:54,191-Speed 123.91 samples/sec   Loss 22.5186   LearningRate 0.088867   Epoch: 2   Global Step: 67570   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:59:15,140-Speed 122.21 samples/sec   Loss 22.2256   LearningRate 0.088866   Epoch: 2   Global Step: 67580   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:59:36,093-Speed 122.18 samples/sec   Loss 22.4067   LearningRate 0.088864   Epoch: 2   Global Step: 67590   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 20:59:56,737-Speed 124.01 samples/sec   Loss 22.4186   LearningRate 0.088863   Epoch: 2   Global Step: 67600   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:00:17,418-Speed 123.79 samples/sec   Loss 22.6341   LearningRate 0.088861   Epoch: 2   Global Step: 67610   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:00:38,497-Speed 121.45 samples/sec   Loss 22.3320   LearningRate 0.088859   Epoch: 2   Global Step: 67620   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:00:59,455-Speed 122.16 samples/sec   Loss 22.7018   LearningRate 0.088858   Epoch: 2   Global Step: 67630   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:01:20,063-Speed 124.23 samples/sec   Loss 21.9365   LearningRate 0.088856   Epoch: 2   Global Step: 67640   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:01:41,002-Speed 122.27 samples/sec   Loss 22.0767   LearningRate 0.088854   Epoch: 2   Global Step: 67650   Fp16 Grad Scale: 65536   Required: 322 hours\nTraining: 2025-06-03 21:02:01,848-Speed 122.81 samples/sec   Loss 22.5366   LearningRate 0.088853   Epoch: 2   Global Step: 67660   Fp16 Grad Scale: 65536   Required: 322 hours\nTraining: 2025-06-03 21:02:22,454-Speed 124.24 samples/sec   Loss 22.2648   LearningRate 0.088851   Epoch: 2   Global Step: 67670   Fp16 Grad Scale: 65536   Required: 322 hours\nTraining: 2025-06-03 21:02:43,371-Speed 122.40 samples/sec   Loss 22.6185   LearningRate 0.088849   Epoch: 2   Global Step: 67680   Fp16 Grad Scale: 65536   Required: 322 hours\nTraining: 2025-06-03 21:03:04,339-Speed 122.10 samples/sec   Loss 22.3223   LearningRate 0.088848   Epoch: 2   Global Step: 67690   Fp16 Grad Scale: 65536   Required: 322 hours\nTraining: 2025-06-03 21:03:25,075-Speed 123.46 samples/sec   Loss 22.0102   LearningRate 0.088846   Epoch: 2   Global Step: 67700   Fp16 Grad Scale: 65536   Required: 322 hours\nTraining: 2025-06-03 21:03:45,679-Speed 124.25 samples/sec   Loss 22.1446   LearningRate 0.088844   Epoch: 2   Global Step: 67710   Fp16 Grad Scale: 65536   Required: 322 hours\nTraining: 2025-06-03 21:04:06,311-Speed 124.08 samples/sec   Loss 22.2862   LearningRate 0.088843   Epoch: 2   Global Step: 67720   Fp16 Grad Scale: 65536   Required: 322 hours\nTraining: 2025-06-03 21:04:27,219-Speed 122.45 samples/sec   Loss 22.4724   LearningRate 0.088841   Epoch: 2   Global Step: 67730   Fp16 Grad Scale: 65536   Required: 322 hours\nTraining: 2025-06-03 21:04:48,188-Speed 122.09 samples/sec   Loss 22.0552   LearningRate 0.088839   Epoch: 2   Global Step: 67740   Fp16 Grad Scale: 65536   Required: 322 hours\nTraining: 2025-06-03 21:05:08,985-Speed 123.10 samples/sec   Loss 22.2265   LearningRate 0.088838   Epoch: 2   Global Step: 67750   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:05:29,734-Speed 123.38 samples/sec   Loss 22.6859   LearningRate 0.088836   Epoch: 2   Global Step: 67760   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:05:50,362-Speed 124.11 samples/sec   Loss 22.5438   LearningRate 0.088835   Epoch: 2   Global Step: 67770   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:06:11,036-Speed 123.83 samples/sec   Loss 22.0078   LearningRate 0.088833   Epoch: 2   Global Step: 67780   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:06:31,790-Speed 123.36 samples/sec   Loss 22.4684   LearningRate 0.088831   Epoch: 2   Global Step: 67790   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:06:52,573-Speed 123.18 samples/sec   Loss 22.4034   LearningRate 0.088830   Epoch: 2   Global Step: 67800   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:07:13,361-Speed 123.16 samples/sec   Loss 22.4009   LearningRate 0.088828   Epoch: 2   Global Step: 67810   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:07:34,220-Speed 122.73 samples/sec   Loss 22.7613   LearningRate 0.088826   Epoch: 2   Global Step: 67820   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:07:55,145-Speed 122.35 samples/sec   Loss 22.2261   LearningRate 0.088825   Epoch: 2   Global Step: 67830   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:08:16,106-Speed 122.14 samples/sec   Loss 22.2057   LearningRate 0.088823   Epoch: 2   Global Step: 67840   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:08:37,076-Speed 122.08 samples/sec   Loss 22.6963   LearningRate 0.088821   Epoch: 2   Global Step: 67850   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 21:08:58,043-Speed 122.10 samples/sec   Loss 22.6351   LearningRate 0.088820   Epoch: 2   Global Step: 67860   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 21:09:19,015-Speed 122.07 samples/sec   Loss 22.1246   LearningRate 0.088818   Epoch: 2   Global Step: 67870   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 21:09:39,846-Speed 122.90 samples/sec   Loss 22.2647   LearningRate 0.088816   Epoch: 2   Global Step: 67880   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 21:10:00,594-Speed 123.39 samples/sec   Loss 22.0938   LearningRate 0.088815   Epoch: 2   Global Step: 67890   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 21:10:21,389-Speed 123.11 samples/sec   Loss 22.2282   LearningRate 0.088813   Epoch: 2   Global Step: 67900   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 21:10:42,164-Speed 123.23 samples/sec   Loss 22.3586   LearningRate 0.088811   Epoch: 2   Global Step: 67910   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 21:11:02,989-Speed 122.93 samples/sec   Loss 22.3463   LearningRate 0.088810   Epoch: 2   Global Step: 67920   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 21:11:23,862-Speed 122.66 samples/sec   Loss 22.5546   LearningRate 0.088808   Epoch: 2   Global Step: 67930   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 21:11:44,809-Speed 122.22 samples/sec   Loss 21.8407   LearningRate 0.088807   Epoch: 2   Global Step: 67940   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 21:12:05,792-Speed 122.01 samples/sec   Loss 22.4119   LearningRate 0.088805   Epoch: 2   Global Step: 67950   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-03 21:12:26,778-Speed 121.99 samples/sec   Loss 22.4492   LearningRate 0.088803   Epoch: 2   Global Step: 67960   Fp16 Grad Scale: 262144   Required: 321 hours\nTraining: 2025-06-03 21:12:47,765-Speed 121.98 samples/sec   Loss 22.3808   LearningRate 0.088802   Epoch: 2   Global Step: 67970   Fp16 Grad Scale: 262144   Required: 321 hours\nTraining: 2025-06-03 21:13:08,744-Speed 122.04 samples/sec   Loss 22.2910   LearningRate 0.088800   Epoch: 2   Global Step: 67980   Fp16 Grad Scale: 262144   Required: 321 hours\nTraining: 2025-06-03 21:13:29,736-Speed 121.95 samples/sec   Loss 22.0568   LearningRate 0.088798   Epoch: 2   Global Step: 67990   Fp16 Grad Scale: 262144   Required: 321 hours\nTraining: 2025-06-03 21:13:50,725-Speed 121.98 samples/sec   Loss 22.3136   LearningRate 0.088797   Epoch: 2   Global Step: 68000   Fp16 Grad Scale: 262144   Required: 321 hours\ntesting verification..\n(12000, 512)\ninfer time 151.76276500000017\nTraining: 2025-06-03 21:16:25,231-[lfw][68000]XNorm: 21.243571\nTraining: 2025-06-03 21:16:25,231-[lfw][68000]Accuracy-Flip: 0.99283+-0.00402\nTraining: 2025-06-03 21:16:25,232-[lfw][68000]Accuracy-Highest: 0.99283\ntesting verification..\n(14000, 512)\ninfer time 177.24833700000002\nTraining: 2025-06-03 21:19:26,020-[cfp_fp][68000]XNorm: 18.129760\nTraining: 2025-06-03 21:19:26,020-[cfp_fp][68000]Accuracy-Flip: 0.92529+-0.01379\nTraining: 2025-06-03 21:19:26,021-[cfp_fp][68000]Accuracy-Highest: 0.92986\ntesting verification..\n(12000, 512)\ninfer time 151.71628200000018\nTraining: 2025-06-03 21:22:00,517-[agedb_30][68000]XNorm: 20.830714\nTraining: 2025-06-03 21:22:00,517-[agedb_30][68000]Accuracy-Flip: 0.94433+-0.01177\nTraining: 2025-06-03 21:22:00,518-[agedb_30][68000]Accuracy-Highest: 0.94633\n/kaggle/working/Arcface_torch/partial_fc_v2.py:157: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(self.fp16):\nTraining: 2025-06-03 21:22:22,985-Speed 5.00 samples/sec   Loss 21.7207   LearningRate 0.088795   Epoch: 2   Global Step: 68010   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 21:22:43,609-Speed 124.14 samples/sec   Loss 22.3254   LearningRate 0.088793   Epoch: 2   Global Step: 68020   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 21:23:03,769-Speed 126.99 samples/sec   Loss 22.4044   LearningRate 0.088792   Epoch: 2   Global Step: 68030   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:23:23,757-Speed 128.08 samples/sec   Loss 21.9181   LearningRate 0.088790   Epoch: 2   Global Step: 68040   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:23:43,948-Speed 126.79 samples/sec   Loss 21.9996   LearningRate 0.088788   Epoch: 2   Global Step: 68050   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:24:04,352-Speed 125.48 samples/sec   Loss 22.6308   LearningRate 0.088787   Epoch: 2   Global Step: 68060   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:24:24,626-Speed 126.28 samples/sec   Loss 22.5112   LearningRate 0.088785   Epoch: 2   Global Step: 68070   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:24:44,855-Speed 126.56 samples/sec   Loss 22.6242   LearningRate 0.088783   Epoch: 2   Global Step: 68080   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:25:05,218-Speed 125.73 samples/sec   Loss 22.4772   LearningRate 0.088782   Epoch: 2   Global Step: 68090   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:25:25,652-Speed 125.29 samples/sec   Loss 22.3156   LearningRate 0.088780   Epoch: 2   Global Step: 68100   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:25:46,269-Speed 124.17 samples/sec   Loss 22.5974   LearningRate 0.088779   Epoch: 2   Global Step: 68110   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:26:07,047-Speed 123.21 samples/sec   Loss 22.3265   LearningRate 0.088777   Epoch: 2   Global Step: 68120   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:26:28,003-Speed 122.17 samples/sec   Loss 22.2638   LearningRate 0.088775   Epoch: 2   Global Step: 68130   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 21:26:48,987-Speed 122.01 samples/sec   Loss 22.0991   LearningRate 0.088774   Epoch: 2   Global Step: 68140   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 21:27:09,838-Speed 122.78 samples/sec   Loss 22.2062   LearningRate 0.088772   Epoch: 2   Global Step: 68150   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 21:27:30,615-Speed 123.22 samples/sec   Loss 22.4304   LearningRate 0.088770   Epoch: 2   Global Step: 68160   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 21:27:51,387-Speed 123.24 samples/sec   Loss 22.4004   LearningRate 0.088769   Epoch: 2   Global Step: 68170   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 21:28:12,103-Speed 123.59 samples/sec   Loss 22.4621   LearningRate 0.088767   Epoch: 2   Global Step: 68180   Fp16 Grad Scale: 262144   Required: 330 hours\nTraining: 2025-06-03 21:28:32,729-Speed 124.12 samples/sec   Loss 22.3477   LearningRate 0.088765   Epoch: 2   Global Step: 68190   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:28:53,324-Speed 124.31 samples/sec   Loss 22.1625   LearningRate 0.088764   Epoch: 2   Global Step: 68200   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:29:14,181-Speed 122.74 samples/sec   Loss 23.1006   LearningRate 0.088762   Epoch: 2   Global Step: 68210   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:29:35,171-Speed 121.97 samples/sec   Loss 22.8461   LearningRate 0.088760   Epoch: 2   Global Step: 68220   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:29:56,037-Speed 122.69 samples/sec   Loss 22.7563   LearningRate 0.088759   Epoch: 2   Global Step: 68230   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:30:16,696-Speed 123.92 samples/sec   Loss 22.3517   LearningRate 0.088757   Epoch: 2   Global Step: 68240   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:30:37,400-Speed 123.65 samples/sec   Loss 22.4338   LearningRate 0.088755   Epoch: 2   Global Step: 68250   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:30:58,205-Speed 123.06 samples/sec   Loss 21.9359   LearningRate 0.088754   Epoch: 2   Global Step: 68260   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:31:19,047-Speed 122.83 samples/sec   Loss 22.4282   LearningRate 0.088752   Epoch: 2   Global Step: 68270   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:31:39,917-Speed 122.67 samples/sec   Loss 22.0294   LearningRate 0.088750   Epoch: 2   Global Step: 68280   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:32:00,824-Speed 122.45 samples/sec   Loss 22.4128   LearningRate 0.088749   Epoch: 2   Global Step: 68290   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:32:21,759-Speed 122.29 samples/sec   Loss 22.2737   LearningRate 0.088747   Epoch: 2   Global Step: 68300   Fp16 Grad Scale: 131072   Required: 330 hours\nTraining: 2025-06-03 21:32:42,680-Speed 122.37 samples/sec   Loss 22.1867   LearningRate 0.088746   Epoch: 2   Global Step: 68310   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:33:03,615-Speed 122.29 samples/sec   Loss 22.3961   LearningRate 0.088744   Epoch: 2   Global Step: 68320   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:33:24,533-Speed 122.39 samples/sec   Loss 22.3510   LearningRate 0.088742   Epoch: 2   Global Step: 68330   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:33:45,442-Speed 122.44 samples/sec   Loss 22.3293   LearningRate 0.088741   Epoch: 2   Global Step: 68340   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:34:06,368-Speed 122.34 samples/sec   Loss 22.4445   LearningRate 0.088739   Epoch: 2   Global Step: 68350   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:34:27,330-Speed 122.13 samples/sec   Loss 22.4348   LearningRate 0.088737   Epoch: 2   Global Step: 68360   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:34:48,300-Speed 122.09 samples/sec   Loss 22.7455   LearningRate 0.088736   Epoch: 2   Global Step: 68370   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:35:09,232-Speed 122.31 samples/sec   Loss 22.5116   LearningRate 0.088734   Epoch: 2   Global Step: 68380   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:35:30,196-Speed 122.12 samples/sec   Loss 22.3996   LearningRate 0.088732   Epoch: 2   Global Step: 68390   Fp16 Grad Scale: 262144   Required: 329 hours\nTraining: 2025-06-03 21:35:51,107-Speed 122.43 samples/sec   Loss 22.3710   LearningRate 0.088731   Epoch: 2   Global Step: 68400   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:36:12,056-Speed 122.20 samples/sec   Loss 22.4546   LearningRate 0.088729   Epoch: 2   Global Step: 68410   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:36:33,019-Speed 122.13 samples/sec   Loss 22.0912   LearningRate 0.088727   Epoch: 2   Global Step: 68420   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:36:54,002-Speed 122.01 samples/sec   Loss 22.4682   LearningRate 0.088726   Epoch: 2   Global Step: 68430   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:37:15,003-Speed 121.90 samples/sec   Loss 22.2586   LearningRate 0.088724   Epoch: 2   Global Step: 68440   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:37:35,956-Speed 122.18 samples/sec   Loss 22.3489   LearningRate 0.088722   Epoch: 2   Global Step: 68450   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 21:37:56,914-Speed 122.16 samples/sec   Loss 22.2194   LearningRate 0.088721   Epoch: 2   Global Step: 68460   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 21:38:17,888-Speed 122.06 samples/sec   Loss 22.2805   LearningRate 0.088719   Epoch: 2   Global Step: 68470   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 21:38:38,866-Speed 122.04 samples/sec   Loss 22.2248   LearningRate 0.088718   Epoch: 2   Global Step: 68480   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 21:38:59,701-Speed 122.88 samples/sec   Loss 22.5050   LearningRate 0.088716   Epoch: 2   Global Step: 68490   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 21:39:20,485-Speed 123.17 samples/sec   Loss 22.0691   LearningRate 0.088714   Epoch: 2   Global Step: 68500   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 21:39:41,245-Speed 123.32 samples/sec   Loss 22.6322   LearningRate 0.088713   Epoch: 2   Global Step: 68510   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 21:40:02,016-Speed 123.25 samples/sec   Loss 22.6008   LearningRate 0.088711   Epoch: 2   Global Step: 68520   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 21:40:22,756-Speed 123.44 samples/sec   Loss 22.5674   LearningRate 0.088709   Epoch: 2   Global Step: 68530   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 21:40:43,418-Speed 123.91 samples/sec   Loss 22.3829   LearningRate 0.088708   Epoch: 2   Global Step: 68540   Fp16 Grad Scale: 65536   Required: 329 hours\nTraining: 2025-06-03 21:41:03,996-Speed 124.41 samples/sec   Loss 22.5861   LearningRate 0.088706   Epoch: 2   Global Step: 68550   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:41:24,607-Speed 124.21 samples/sec   Loss 22.2355   LearningRate 0.088704   Epoch: 2   Global Step: 68560   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:41:45,280-Speed 123.84 samples/sec   Loss 22.3712   LearningRate 0.088703   Epoch: 2   Global Step: 68570   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:42:06,033-Speed 123.36 samples/sec   Loss 22.2615   LearningRate 0.088701   Epoch: 2   Global Step: 68580   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:42:26,814-Speed 123.19 samples/sec   Loss 22.8057   LearningRate 0.088699   Epoch: 2   Global Step: 68590   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:42:47,629-Speed 123.00 samples/sec   Loss 22.2754   LearningRate 0.088698   Epoch: 2   Global Step: 68600   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:43:08,531-Speed 122.48 samples/sec   Loss 22.7801   LearningRate 0.088696   Epoch: 2   Global Step: 68610   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:43:29,492-Speed 122.14 samples/sec   Loss 22.1945   LearningRate 0.088694   Epoch: 2   Global Step: 68620   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:43:50,485-Speed 121.95 samples/sec   Loss 22.2605   LearningRate 0.088693   Epoch: 2   Global Step: 68630   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:44:11,466-Speed 122.02 samples/sec   Loss 22.1789   LearningRate 0.088691   Epoch: 2   Global Step: 68640   Fp16 Grad Scale: 131072   Required: 329 hours\nTraining: 2025-06-03 21:44:32,446-Speed 122.03 samples/sec   Loss 22.5773   LearningRate 0.088690   Epoch: 2   Global Step: 68650   Fp16 Grad Scale: 262144   Required: 329 hours\nTraining: 2025-06-03 21:44:53,388-Speed 122.24 samples/sec   Loss 22.6525   LearningRate 0.088688   Epoch: 2   Global Step: 68660   Fp16 Grad Scale: 262144   Required: 329 hours\nTraining: 2025-06-03 21:45:14,235-Speed 122.81 samples/sec   Loss 22.2376   LearningRate 0.088686   Epoch: 2   Global Step: 68670   Fp16 Grad Scale: 262144   Required: 329 hours\nTraining: 2025-06-03 21:45:34,910-Speed 123.83 samples/sec   Loss 22.4931   LearningRate 0.088685   Epoch: 2   Global Step: 68680   Fp16 Grad Scale: 262144   Required: 329 hours\nTraining: 2025-06-03 21:45:55,590-Speed 123.79 samples/sec   Loss 22.1907   LearningRate 0.088683   Epoch: 2   Global Step: 68690   Fp16 Grad Scale: 262144   Required: 329 hours\nTraining: 2025-06-03 21:46:16,367-Speed 123.22 samples/sec   Loss 22.3876   LearningRate 0.088681   Epoch: 2   Global Step: 68700   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:46:37,298-Speed 122.31 samples/sec   Loss 22.0652   LearningRate 0.088680   Epoch: 2   Global Step: 68710   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:46:58,284-Speed 121.99 samples/sec   Loss 22.6153   LearningRate 0.088678   Epoch: 2   Global Step: 68720   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:47:19,277-Speed 121.95 samples/sec   Loss 22.2897   LearningRate 0.088676   Epoch: 2   Global Step: 68730   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:47:40,213-Speed 122.28 samples/sec   Loss 22.4635   LearningRate 0.088675   Epoch: 2   Global Step: 68740   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:48:01,069-Speed 122.75 samples/sec   Loss 22.2169   LearningRate 0.088673   Epoch: 2   Global Step: 68750   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:48:21,860-Speed 123.14 samples/sec   Loss 22.1545   LearningRate 0.088671   Epoch: 2   Global Step: 68760   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:48:42,643-Speed 123.18 samples/sec   Loss 22.5313   LearningRate 0.088670   Epoch: 2   Global Step: 68770   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:49:03,413-Speed 123.26 samples/sec   Loss 22.2102   LearningRate 0.088668   Epoch: 2   Global Step: 68780   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:49:24,161-Speed 123.39 samples/sec   Loss 22.5387   LearningRate 0.088666   Epoch: 2   Global Step: 68790   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:49:44,920-Speed 123.32 samples/sec   Loss 22.4315   LearningRate 0.088665   Epoch: 2   Global Step: 68800   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 21:50:05,659-Speed 123.45 samples/sec   Loss 22.3417   LearningRate 0.088663   Epoch: 2   Global Step: 68810   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:50:26,356-Speed 123.69 samples/sec   Loss 22.7005   LearningRate 0.088662   Epoch: 2   Global Step: 68820   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:50:47,004-Speed 123.99 samples/sec   Loss 22.3411   LearningRate 0.088660   Epoch: 2   Global Step: 68830   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:51:07,695-Speed 123.73 samples/sec   Loss 21.7323   LearningRate 0.088658   Epoch: 2   Global Step: 68840   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:51:28,433-Speed 123.45 samples/sec   Loss 22.2540   LearningRate 0.088657   Epoch: 2   Global Step: 68850   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:51:49,231-Speed 123.09 samples/sec   Loss 21.8630   LearningRate 0.088655   Epoch: 2   Global Step: 68860   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:52:10,076-Speed 122.82 samples/sec   Loss 22.0108   LearningRate 0.088653   Epoch: 2   Global Step: 68870   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:52:31,012-Speed 122.28 samples/sec   Loss 22.1318   LearningRate 0.088652   Epoch: 2   Global Step: 68880   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:52:51,931-Speed 122.38 samples/sec   Loss 22.5586   LearningRate 0.088650   Epoch: 2   Global Step: 68890   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:53:12,872-Speed 122.25 samples/sec   Loss 22.3493   LearningRate 0.088648   Epoch: 2   Global Step: 68900   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:53:33,857-Speed 122.00 samples/sec   Loss 21.9965   LearningRate 0.088647   Epoch: 2   Global Step: 68910   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 21:53:54,845-Speed 121.98 samples/sec   Loss 22.1748   LearningRate 0.088645   Epoch: 2   Global Step: 68920   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 21:54:15,840-Speed 121.93 samples/sec   Loss 22.4228   LearningRate 0.088643   Epoch: 2   Global Step: 68930   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 21:54:36,797-Speed 122.16 samples/sec   Loss 22.1941   LearningRate 0.088642   Epoch: 2   Global Step: 68940   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:54:57,782-Speed 122.00 samples/sec   Loss 22.0324   LearningRate 0.088640   Epoch: 2   Global Step: 68950   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:55:18,735-Speed 122.19 samples/sec   Loss 22.2527   LearningRate 0.088638   Epoch: 2   Global Step: 68960   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:55:39,584-Speed 122.79 samples/sec   Loss 22.5912   LearningRate 0.088637   Epoch: 2   Global Step: 68970   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:56:00,366-Speed 123.19 samples/sec   Loss 22.3890   LearningRate 0.088635   Epoch: 2   Global Step: 68980   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:56:21,157-Speed 123.13 samples/sec   Loss 22.3811   LearningRate 0.088634   Epoch: 2   Global Step: 68990   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:56:41,917-Speed 123.32 samples/sec   Loss 22.3333   LearningRate 0.088632   Epoch: 2   Global Step: 69000   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:57:02,608-Speed 123.73 samples/sec   Loss 22.6493   LearningRate 0.088630   Epoch: 2   Global Step: 69010   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:57:23,301-Speed 123.72 samples/sec   Loss 21.9896   LearningRate 0.088629   Epoch: 2   Global Step: 69020   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:57:43,969-Speed 123.87 samples/sec   Loss 22.1250   LearningRate 0.088627   Epoch: 2   Global Step: 69030   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 21:58:04,585-Speed 124.18 samples/sec   Loss 22.1939   LearningRate 0.088625   Epoch: 2   Global Step: 69040   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 21:58:25,299-Speed 123.59 samples/sec   Loss 22.1821   LearningRate 0.088624   Epoch: 2   Global Step: 69050   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 21:58:46,188-Speed 122.56 samples/sec   Loss 22.3473   LearningRate 0.088622   Epoch: 2   Global Step: 69060   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 21:59:07,167-Speed 122.03 samples/sec   Loss 22.1659   LearningRate 0.088620   Epoch: 2   Global Step: 69070   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 21:59:28,116-Speed 122.21 samples/sec   Loss 22.5106   LearningRate 0.088619   Epoch: 2   Global Step: 69080   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 21:59:49,051-Speed 122.29 samples/sec   Loss 22.2515   LearningRate 0.088617   Epoch: 2   Global Step: 69090   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 22:00:10,030-Speed 122.03 samples/sec   Loss 22.2317   LearningRate 0.088615   Epoch: 2   Global Step: 69100   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 22:00:31,015-Speed 122.00 samples/sec   Loss 22.1546   LearningRate 0.088614   Epoch: 2   Global Step: 69110   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 22:00:52,007-Speed 121.96 samples/sec   Loss 22.1960   LearningRate 0.088612   Epoch: 2   Global Step: 69120   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 22:01:12,967-Speed 122.14 samples/sec   Loss 22.3783   LearningRate 0.088610   Epoch: 2   Global Step: 69130   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:01:33,937-Speed 122.08 samples/sec   Loss 22.2168   LearningRate 0.088609   Epoch: 2   Global Step: 69140   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:01:54,958-Speed 121.79 samples/sec   Loss 22.3231   LearningRate 0.088607   Epoch: 2   Global Step: 69150   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:02:15,929-Speed 122.08 samples/sec   Loss 21.9624   LearningRate 0.088606   Epoch: 2   Global Step: 69160   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:02:36,719-Speed 123.14 samples/sec   Loss 22.2256   LearningRate 0.088604   Epoch: 2   Global Step: 69170   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:02:57,433-Speed 123.59 samples/sec   Loss 22.2541   LearningRate 0.088602   Epoch: 2   Global Step: 69180   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:03:18,237-Speed 123.06 samples/sec   Loss 22.3399   LearningRate 0.088601   Epoch: 2   Global Step: 69190   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:03:39,178-Speed 122.25 samples/sec   Loss 22.4087   LearningRate 0.088599   Epoch: 2   Global Step: 69200   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:04:00,026-Speed 122.80 samples/sec   Loss 22.5253   LearningRate 0.088597   Epoch: 2   Global Step: 69210   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:04:20,893-Speed 122.68 samples/sec   Loss 22.2315   LearningRate 0.088596   Epoch: 2   Global Step: 69220   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:04:41,860-Speed 122.11 samples/sec   Loss 22.3588   LearningRate 0.088594   Epoch: 2   Global Step: 69230   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 22:05:02,786-Speed 122.34 samples/sec   Loss 22.6481   LearningRate 0.088592   Epoch: 2   Global Step: 69240   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:05:23,631-Speed 122.82 samples/sec   Loss 23.0332   LearningRate 0.088591   Epoch: 2   Global Step: 69250   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:05:44,595-Speed 122.12 samples/sec   Loss 22.0247   LearningRate 0.088589   Epoch: 2   Global Step: 69260   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:06:05,572-Speed 122.05 samples/sec   Loss 22.2954   LearningRate 0.088587   Epoch: 2   Global Step: 69270   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:06:26,427-Speed 122.75 samples/sec   Loss 22.3555   LearningRate 0.088586   Epoch: 2   Global Step: 69280   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:06:47,384-Speed 122.16 samples/sec   Loss 22.2042   LearningRate 0.088584   Epoch: 2   Global Step: 69290   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:07:08,331-Speed 122.22 samples/sec   Loss 22.6179   LearningRate 0.088582   Epoch: 2   Global Step: 69300   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:07:29,269-Speed 122.27 samples/sec   Loss 22.0796   LearningRate 0.088581   Epoch: 2   Global Step: 69310   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:07:50,254-Speed 121.99 samples/sec   Loss 22.0663   LearningRate 0.088579   Epoch: 2   Global Step: 69320   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:08:11,233-Speed 122.03 samples/sec   Loss 22.7287   LearningRate 0.088578   Epoch: 2   Global Step: 69330   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:08:32,207-Speed 122.06 samples/sec   Loss 22.8006   LearningRate 0.088576   Epoch: 2   Global Step: 69340   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 22:08:53,068-Speed 122.72 samples/sec   Loss 22.2333   LearningRate 0.088574   Epoch: 2   Global Step: 69350   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 22:09:13,849-Speed 123.19 samples/sec   Loss 22.5694   LearningRate 0.088573   Epoch: 2   Global Step: 69360   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:09:34,608-Speed 123.33 samples/sec   Loss 22.4842   LearningRate 0.088571   Epoch: 2   Global Step: 69370   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:09:55,289-Speed 123.79 samples/sec   Loss 22.8161   LearningRate 0.088569   Epoch: 2   Global Step: 69380   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:10:15,922-Speed 124.08 samples/sec   Loss 22.2827   LearningRate 0.088568   Epoch: 2   Global Step: 69390   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:10:36,598-Speed 123.82 samples/sec   Loss 22.2128   LearningRate 0.088566   Epoch: 2   Global Step: 69400   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:10:57,396-Speed 123.10 samples/sec   Loss 22.2988   LearningRate 0.088564   Epoch: 2   Global Step: 69410   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:11:18,172-Speed 123.22 samples/sec   Loss 21.9609   LearningRate 0.088563   Epoch: 2   Global Step: 69420   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:11:38,986-Speed 123.00 samples/sec   Loss 22.3078   LearningRate 0.088561   Epoch: 2   Global Step: 69430   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:11:59,777-Speed 123.14 samples/sec   Loss 22.0942   LearningRate 0.088559   Epoch: 2   Global Step: 69440   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:12:20,390-Speed 124.20 samples/sec   Loss 22.7664   LearningRate 0.088558   Epoch: 2   Global Step: 69450   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:12:41,021-Speed 124.09 samples/sec   Loss 22.4419   LearningRate 0.088556   Epoch: 2   Global Step: 69460   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-03 22:13:01,731-Speed 123.62 samples/sec   Loss 22.3213   LearningRate 0.088554   Epoch: 2   Global Step: 69470   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:13:22,428-Speed 123.69 samples/sec   Loss 22.5660   LearningRate 0.088553   Epoch: 2   Global Step: 69480   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:13:43,081-Speed 123.96 samples/sec   Loss 22.1325   LearningRate 0.088551   Epoch: 2   Global Step: 69490   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:14:03,962-Speed 122.61 samples/sec   Loss 22.5731   LearningRate 0.088549   Epoch: 2   Global Step: 69500   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:14:24,950-Speed 121.98 samples/sec   Loss 22.9163   LearningRate 0.088548   Epoch: 2   Global Step: 69510   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:14:45,818-Speed 122.68 samples/sec   Loss 22.6898   LearningRate 0.088546   Epoch: 2   Global Step: 69520   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:15:06,555-Speed 123.46 samples/sec   Loss 22.5408   LearningRate 0.088545   Epoch: 2   Global Step: 69530   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 22:15:27,139-Speed 124.37 samples/sec   Loss 22.6751   LearningRate 0.088543   Epoch: 2   Global Step: 69540   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:15:47,727-Speed 124.35 samples/sec   Loss 22.8822   LearningRate 0.088541   Epoch: 2   Global Step: 69550   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:16:08,323-Speed 124.30 samples/sec   Loss 22.0049   LearningRate 0.088540   Epoch: 2   Global Step: 69560   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:16:28,979-Speed 123.94 samples/sec   Loss 22.7489   LearningRate 0.088538   Epoch: 2   Global Step: 69570   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 22:16:49,882-Speed 122.48 samples/sec   Loss 22.4310   LearningRate 0.088536   Epoch: 2   Global Step: 69580   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:17:10,833-Speed 122.19 samples/sec   Loss 22.6500   LearningRate 0.088535   Epoch: 2   Global Step: 69590   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:17:31,642-Speed 123.03 samples/sec   Loss 22.2164   LearningRate 0.088533   Epoch: 2   Global Step: 69600   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:17:52,313-Speed 123.85 samples/sec   Loss 22.4924   LearningRate 0.088531   Epoch: 2   Global Step: 69610   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:18:12,885-Speed 124.45 samples/sec   Loss 22.0949   LearningRate 0.088530   Epoch: 2   Global Step: 69620   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:18:33,470-Speed 124.37 samples/sec   Loss 22.3747   LearningRate 0.088528   Epoch: 2   Global Step: 69630   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:18:54,155-Speed 123.76 samples/sec   Loss 22.4935   LearningRate 0.088526   Epoch: 2   Global Step: 69640   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:19:14,936-Speed 123.20 samples/sec   Loss 22.3930   LearningRate 0.088525   Epoch: 2   Global Step: 69650   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:19:35,717-Speed 123.19 samples/sec   Loss 22.5509   LearningRate 0.088523   Epoch: 2   Global Step: 69660   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:19:56,541-Speed 122.94 samples/sec   Loss 22.2637   LearningRate 0.088521   Epoch: 2   Global Step: 69670   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:20:17,438-Speed 122.51 samples/sec   Loss 21.9003   LearningRate 0.088520   Epoch: 2   Global Step: 69680   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 22:20:38,363-Speed 122.35 samples/sec   Loss 22.4014   LearningRate 0.088518   Epoch: 2   Global Step: 69690   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 22:20:59,232-Speed 122.67 samples/sec   Loss 22.6487   LearningRate 0.088517   Epoch: 2   Global Step: 69700   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 22:21:20,112-Speed 122.61 samples/sec   Loss 22.3484   LearningRate 0.088515   Epoch: 2   Global Step: 69710   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:21:40,929-Speed 122.98 samples/sec   Loss 22.5135   LearningRate 0.088513   Epoch: 2   Global Step: 69720   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:22:01,741-Speed 123.01 samples/sec   Loss 22.3227   LearningRate 0.088512   Epoch: 2   Global Step: 69730   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:22:22,539-Speed 123.10 samples/sec   Loss 22.3657   LearningRate 0.088510   Epoch: 2   Global Step: 69740   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:22:43,319-Speed 123.20 samples/sec   Loss 22.3597   LearningRate 0.088508   Epoch: 2   Global Step: 69750   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:23:04,125-Speed 123.04 samples/sec   Loss 22.6054   LearningRate 0.088507   Epoch: 2   Global Step: 69760   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:23:24,902-Speed 123.22 samples/sec   Loss 22.3017   LearningRate 0.088505   Epoch: 2   Global Step: 69770   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:23:45,681-Speed 123.21 samples/sec   Loss 22.5916   LearningRate 0.088503   Epoch: 2   Global Step: 69780   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:24:06,469-Speed 123.15 samples/sec   Loss 22.3712   LearningRate 0.088502   Epoch: 2   Global Step: 69790   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:24:27,257-Speed 123.15 samples/sec   Loss 22.4374   LearningRate 0.088500   Epoch: 2   Global Step: 69800   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:24:48,036-Speed 123.21 samples/sec   Loss 22.1127   LearningRate 0.088498   Epoch: 2   Global Step: 69810   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 22:25:08,817-Speed 123.19 samples/sec   Loss 22.3941   LearningRate 0.088497   Epoch: 2   Global Step: 69820   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 22:25:29,605-Speed 123.16 samples/sec   Loss 22.3878   LearningRate 0.088495   Epoch: 2   Global Step: 69830   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 22:25:50,389-Speed 123.17 samples/sec   Loss 22.8933   LearningRate 0.088493   Epoch: 2   Global Step: 69840   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 22:26:11,164-Speed 123.23 samples/sec   Loss 22.4165   LearningRate 0.088492   Epoch: 2   Global Step: 69850   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:26:31,960-Speed 123.10 samples/sec   Loss 22.8025   LearningRate 0.088490   Epoch: 2   Global Step: 69860   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:26:52,732-Speed 123.25 samples/sec   Loss 22.2692   LearningRate 0.088489   Epoch: 2   Global Step: 69870   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:27:13,513-Speed 123.19 samples/sec   Loss 22.5656   LearningRate 0.088487   Epoch: 2   Global Step: 69880   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:27:34,304-Speed 123.14 samples/sec   Loss 22.4327   LearningRate 0.088485   Epoch: 2   Global Step: 69890   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:27:55,104-Speed 123.08 samples/sec   Loss 22.3823   LearningRate 0.088484   Epoch: 2   Global Step: 69900   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:28:15,883-Speed 123.21 samples/sec   Loss 22.3353   LearningRate 0.088482   Epoch: 2   Global Step: 69910   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:28:36,684-Speed 123.08 samples/sec   Loss 22.1133   LearningRate 0.088480   Epoch: 2   Global Step: 69920   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:28:57,473-Speed 123.14 samples/sec   Loss 22.3150   LearningRate 0.088479   Epoch: 2   Global Step: 69930   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:29:18,244-Speed 123.25 samples/sec   Loss 22.7844   LearningRate 0.088477   Epoch: 2   Global Step: 69940   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:29:39,031-Speed 123.16 samples/sec   Loss 22.4958   LearningRate 0.088475   Epoch: 2   Global Step: 69950   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 22:29:59,826-Speed 123.11 samples/sec   Loss 22.5302   LearningRate 0.088474   Epoch: 2   Global Step: 69960   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-03 22:30:20,588-Speed 123.31 samples/sec   Loss 22.3578   LearningRate 0.088472   Epoch: 2   Global Step: 69970   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-03 22:30:41,365-Speed 123.22 samples/sec   Loss 22.0347   LearningRate 0.088470   Epoch: 2   Global Step: 69980   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:31:02,163-Speed 123.10 samples/sec   Loss 22.5844   LearningRate 0.088469   Epoch: 2   Global Step: 69990   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:31:22,938-Speed 123.23 samples/sec   Loss 22.1756   LearningRate 0.088467   Epoch: 2   Global Step: 70000   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:31:43,720-Speed 123.19 samples/sec   Loss 22.5790   LearningRate 0.088465   Epoch: 2   Global Step: 70010   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:32:04,513-Speed 123.12 samples/sec   Loss 22.3556   LearningRate 0.088464   Epoch: 2   Global Step: 70020   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:32:25,300-Speed 123.16 samples/sec   Loss 22.3751   LearningRate 0.088462   Epoch: 2   Global Step: 70030   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:32:46,097-Speed 123.10 samples/sec   Loss 22.0072   LearningRate 0.088461   Epoch: 2   Global Step: 70040   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:33:06,887-Speed 123.14 samples/sec   Loss 22.3807   LearningRate 0.088459   Epoch: 2   Global Step: 70050   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:33:27,690-Speed 123.06 samples/sec   Loss 22.2480   LearningRate 0.088457   Epoch: 2   Global Step: 70060   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:33:48,615-Speed 122.35 samples/sec   Loss 21.9596   LearningRate 0.088456   Epoch: 2   Global Step: 70070   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:34:09,479-Speed 122.71 samples/sec   Loss 22.2135   LearningRate 0.088454   Epoch: 2   Global Step: 70080   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:34:30,407-Speed 122.33 samples/sec   Loss 22.2441   LearningRate 0.088452   Epoch: 2   Global Step: 70090   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:34:51,394-Speed 121.98 samples/sec   Loss 22.8845   LearningRate 0.088451   Epoch: 2   Global Step: 70100   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:35:12,352-Speed 122.16 samples/sec   Loss 22.0572   LearningRate 0.088449   Epoch: 2   Global Step: 70110   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:35:33,329-Speed 122.05 samples/sec   Loss 22.4154   LearningRate 0.088447   Epoch: 2   Global Step: 70120   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:35:54,284-Speed 122.17 samples/sec   Loss 21.9948   LearningRate 0.088446   Epoch: 2   Global Step: 70130   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:36:15,144-Speed 122.73 samples/sec   Loss 21.8112   LearningRate 0.088444   Epoch: 2   Global Step: 70140   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:36:35,813-Speed 123.86 samples/sec   Loss 22.1879   LearningRate 0.088442   Epoch: 2   Global Step: 70150   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:36:56,473-Speed 123.91 samples/sec   Loss 22.0759   LearningRate 0.088441   Epoch: 2   Global Step: 70160   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:37:17,207-Speed 123.48 samples/sec   Loss 21.9226   LearningRate 0.088439   Epoch: 2   Global Step: 70170   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 22:37:37,829-Speed 124.14 samples/sec   Loss 22.6049   LearningRate 0.088437   Epoch: 2   Global Step: 70180   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:37:58,429-Speed 124.28 samples/sec   Loss 22.7273   LearningRate 0.088436   Epoch: 2   Global Step: 70190   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:38:19,105-Speed 123.82 samples/sec   Loss 22.3902   LearningRate 0.088434   Epoch: 2   Global Step: 70200   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:38:39,890-Speed 123.17 samples/sec   Loss 22.3629   LearningRate 0.088433   Epoch: 2   Global Step: 70210   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:39:00,745-Speed 122.76 samples/sec   Loss 22.1544   LearningRate 0.088431   Epoch: 2   Global Step: 70220   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:39:21,518-Speed 123.24 samples/sec   Loss 22.8575   LearningRate 0.088429   Epoch: 2   Global Step: 70230   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:39:42,279-Speed 123.32 samples/sec   Loss 22.5862   LearningRate 0.088428   Epoch: 2   Global Step: 70240   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:40:03,141-Speed 122.71 samples/sec   Loss 22.2752   LearningRate 0.088426   Epoch: 2   Global Step: 70250   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:40:24,035-Speed 122.53 samples/sec   Loss 22.1420   LearningRate 0.088424   Epoch: 2   Global Step: 70260   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:40:44,871-Speed 122.86 samples/sec   Loss 22.3500   LearningRate 0.088423   Epoch: 2   Global Step: 70270   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:41:05,710-Speed 122.85 samples/sec   Loss 22.4206   LearningRate 0.088421   Epoch: 2   Global Step: 70280   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 22:41:26,656-Speed 122.22 samples/sec   Loss 22.1942   LearningRate 0.088419   Epoch: 2   Global Step: 70290   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-03 22:41:47,570-Speed 122.41 samples/sec   Loss 22.6147   LearningRate 0.088418   Epoch: 2   Global Step: 70300   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:42:08,366-Speed 123.11 samples/sec   Loss 22.3217   LearningRate 0.088416   Epoch: 2   Global Step: 70310   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:42:29,255-Speed 122.55 samples/sec   Loss 22.4449   LearningRate 0.088414   Epoch: 2   Global Step: 70320   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:42:50,210-Speed 122.17 samples/sec   Loss 23.0506   LearningRate 0.088413   Epoch: 2   Global Step: 70330   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:43:11,205-Speed 121.94 samples/sec   Loss 22.6156   LearningRate 0.088411   Epoch: 2   Global Step: 70340   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:43:32,146-Speed 122.25 samples/sec   Loss 22.2046   LearningRate 0.088409   Epoch: 2   Global Step: 70350   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:43:53,002-Speed 122.75 samples/sec   Loss 22.2062   LearningRate 0.088408   Epoch: 2   Global Step: 70360   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:44:13,845-Speed 122.83 samples/sec   Loss 22.5128   LearningRate 0.088406   Epoch: 2   Global Step: 70370   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:44:34,694-Speed 122.79 samples/sec   Loss 22.1104   LearningRate 0.088405   Epoch: 2   Global Step: 70380   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:44:55,611-Speed 122.39 samples/sec   Loss 21.9720   LearningRate 0.088403   Epoch: 2   Global Step: 70390   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:45:16,514-Speed 122.47 samples/sec   Loss 22.0450   LearningRate 0.088401   Epoch: 2   Global Step: 70400   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:45:37,386-Speed 122.66 samples/sec   Loss 22.5832   LearningRate 0.088400   Epoch: 2   Global Step: 70410   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:45:58,343-Speed 122.16 samples/sec   Loss 22.2353   LearningRate 0.088398   Epoch: 2   Global Step: 70420   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:46:19,301-Speed 122.16 samples/sec   Loss 22.5741   LearningRate 0.088396   Epoch: 2   Global Step: 70430   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:46:40,222-Speed 122.37 samples/sec   Loss 22.1031   LearningRate 0.088395   Epoch: 2   Global Step: 70440   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:47:01,183-Speed 122.14 samples/sec   Loss 22.4591   LearningRate 0.088393   Epoch: 2   Global Step: 70450   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:47:22,107-Speed 122.36 samples/sec   Loss 22.2853   LearningRate 0.088391   Epoch: 2   Global Step: 70460   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:47:42,991-Speed 122.58 samples/sec   Loss 21.9376   LearningRate 0.088390   Epoch: 2   Global Step: 70470   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:48:03,773-Speed 123.19 samples/sec   Loss 22.5823   LearningRate 0.088388   Epoch: 2   Global Step: 70480   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-03 22:48:24,550-Speed 123.22 samples/sec   Loss 21.9646   LearningRate 0.088386   Epoch: 2   Global Step: 70490   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:48:45,336-Speed 123.16 samples/sec   Loss 22.2003   LearningRate 0.088385   Epoch: 2   Global Step: 70500   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 22:49:06,217-Speed 122.61 samples/sec   Loss 22.2411   LearningRate 0.088383   Epoch: 2   Global Step: 70510   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 22:49:27,130-Speed 122.42 samples/sec   Loss 22.0096   LearningRate 0.088381   Epoch: 2   Global Step: 70520   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 22:49:48,029-Speed 122.50 samples/sec   Loss 22.4390   LearningRate 0.088380   Epoch: 2   Global Step: 70530   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:50:08,962-Speed 122.30 samples/sec   Loss 22.5182   LearningRate 0.088378   Epoch: 2   Global Step: 70540   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 22:50:29,900-Speed 122.27 samples/sec   Loss 22.6228   LearningRate 0.088376   Epoch: 2   Global Step: 70550   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 22:50:50,877-Speed 122.04 samples/sec   Loss 22.1987   LearningRate 0.088375   Epoch: 2   Global Step: 70560   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 22:51:11,736-Speed 122.74 samples/sec   Loss 22.7098   LearningRate 0.088373   Epoch: 2   Global Step: 70570   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 22:51:32,712-Speed 122.05 samples/sec   Loss 22.7164   LearningRate 0.088372   Epoch: 2   Global Step: 70580   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 22:51:53,671-Speed 122.15 samples/sec   Loss 22.0986   LearningRate 0.088370   Epoch: 2   Global Step: 70590   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 22:52:14,586-Speed 122.40 samples/sec   Loss 22.2196   LearningRate 0.088368   Epoch: 2   Global Step: 70600   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 22:52:35,505-Speed 122.38 samples/sec   Loss 22.3921   LearningRate 0.088367   Epoch: 2   Global Step: 70610   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 22:52:56,423-Speed 122.39 samples/sec   Loss 22.0469   LearningRate 0.088365   Epoch: 2   Global Step: 70620   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 22:53:17,373-Speed 122.20 samples/sec   Loss 22.0862   LearningRate 0.088363   Epoch: 2   Global Step: 70630   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 22:53:38,293-Speed 122.38 samples/sec   Loss 22.3849   LearningRate 0.088362   Epoch: 2   Global Step: 70640   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:53:59,251-Speed 122.15 samples/sec   Loss 22.1468   LearningRate 0.088360   Epoch: 2   Global Step: 70650   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:54:20,236-Speed 122.00 samples/sec   Loss 22.2164   LearningRate 0.088358   Epoch: 2   Global Step: 70660   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:54:41,230-Speed 121.94 samples/sec   Loss 22.0878   LearningRate 0.088357   Epoch: 2   Global Step: 70670   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:55:02,213-Speed 122.01 samples/sec   Loss 22.1264   LearningRate 0.088355   Epoch: 2   Global Step: 70680   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:55:23,187-Speed 122.06 samples/sec   Loss 22.4735   LearningRate 0.088353   Epoch: 2   Global Step: 70690   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:55:44,185-Speed 121.92 samples/sec   Loss 22.7299   LearningRate 0.088352   Epoch: 2   Global Step: 70700   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:56:05,160-Speed 122.06 samples/sec   Loss 22.8352   LearningRate 0.088350   Epoch: 2   Global Step: 70710   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:56:26,143-Speed 122.00 samples/sec   Loss 22.4683   LearningRate 0.088348   Epoch: 2   Global Step: 70720   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:56:47,141-Speed 121.92 samples/sec   Loss 22.4377   LearningRate 0.088347   Epoch: 2   Global Step: 70730   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:57:08,120-Speed 122.03 samples/sec   Loss 22.0088   LearningRate 0.088345   Epoch: 2   Global Step: 70740   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 22:57:29,100-Speed 122.02 samples/sec   Loss 22.9710   LearningRate 0.088344   Epoch: 2   Global Step: 70750   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 22:57:50,091-Speed 121.96 samples/sec   Loss 22.4608   LearningRate 0.088342   Epoch: 2   Global Step: 70760   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 22:58:11,079-Speed 121.98 samples/sec   Loss 22.7880   LearningRate 0.088340   Epoch: 2   Global Step: 70770   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 22:58:32,051-Speed 122.07 samples/sec   Loss 22.7035   LearningRate 0.088339   Epoch: 2   Global Step: 70780   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 22:58:53,018-Speed 122.10 samples/sec   Loss 22.8305   LearningRate 0.088337   Epoch: 2   Global Step: 70790   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:59:14,007-Speed 121.97 samples/sec   Loss 22.0351   LearningRate 0.088335   Epoch: 2   Global Step: 70800   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:59:34,993-Speed 121.99 samples/sec   Loss 21.8893   LearningRate 0.088334   Epoch: 2   Global Step: 70810   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 22:59:55,970-Speed 122.04 samples/sec   Loss 21.8551   LearningRate 0.088332   Epoch: 2   Global Step: 70820   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:00:16,972-Speed 121.90 samples/sec   Loss 22.4087   LearningRate 0.088330   Epoch: 2   Global Step: 70830   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:00:37,941-Speed 122.09 samples/sec   Loss 21.9794   LearningRate 0.088329   Epoch: 2   Global Step: 70840   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:00:58,927-Speed 121.99 samples/sec   Loss 22.0844   LearningRate 0.088327   Epoch: 2   Global Step: 70850   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:01:19,919-Speed 121.96 samples/sec   Loss 22.2477   LearningRate 0.088325   Epoch: 2   Global Step: 70860   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:01:40,901-Speed 122.01 samples/sec   Loss 22.0953   LearningRate 0.088324   Epoch: 2   Global Step: 70870   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:02:01,875-Speed 122.06 samples/sec   Loss 21.8053   LearningRate 0.088322   Epoch: 2   Global Step: 70880   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:02:22,872-Speed 121.93 samples/sec   Loss 22.3200   LearningRate 0.088320   Epoch: 2   Global Step: 70890   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 23:02:43,849-Speed 122.05 samples/sec   Loss 22.0819   LearningRate 0.088319   Epoch: 2   Global Step: 70900   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-03 23:03:04,805-Speed 122.16 samples/sec   Loss 22.3333   LearningRate 0.088317   Epoch: 2   Global Step: 70910   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:03:25,798-Speed 121.95 samples/sec   Loss 22.4091   LearningRate 0.088316   Epoch: 2   Global Step: 70920   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:03:46,785-Speed 121.99 samples/sec   Loss 22.4706   LearningRate 0.088314   Epoch: 2   Global Step: 70930   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:04:07,766-Speed 122.02 samples/sec   Loss 22.4112   LearningRate 0.088312   Epoch: 2   Global Step: 70940   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:04:28,752-Speed 121.99 samples/sec   Loss 22.5220   LearningRate 0.088311   Epoch: 2   Global Step: 70950   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:04:49,747-Speed 121.94 samples/sec   Loss 22.4531   LearningRate 0.088309   Epoch: 2   Global Step: 70960   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:05:10,724-Speed 122.04 samples/sec   Loss 22.3691   LearningRate 0.088307   Epoch: 2   Global Step: 70970   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:05:31,706-Speed 122.02 samples/sec   Loss 22.5342   LearningRate 0.088306   Epoch: 2   Global Step: 70980   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:05:52,701-Speed 121.94 samples/sec   Loss 22.6618   LearningRate 0.088304   Epoch: 2   Global Step: 70990   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:06:13,680-Speed 122.03 samples/sec   Loss 22.3206   LearningRate 0.088302   Epoch: 2   Global Step: 71000   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:06:34,640-Speed 122.14 samples/sec   Loss 22.1415   LearningRate 0.088301   Epoch: 2   Global Step: 71010   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-03 23:06:55,614-Speed 122.06 samples/sec   Loss 22.0928   LearningRate 0.088299   Epoch: 2   Global Step: 71020   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 23:07:16,596-Speed 122.01 samples/sec   Loss 22.2571   LearningRate 0.088297   Epoch: 2   Global Step: 71030   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 23:07:37,576-Speed 122.03 samples/sec   Loss 22.5047   LearningRate 0.088296   Epoch: 2   Global Step: 71040   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 23:07:58,567-Speed 121.96 samples/sec   Loss 22.2526   LearningRate 0.088294   Epoch: 2   Global Step: 71050   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 23:08:19,551-Speed 122.00 samples/sec   Loss 22.4989   LearningRate 0.088292   Epoch: 2   Global Step: 71060   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 23:08:40,528-Speed 122.05 samples/sec   Loss 22.3348   LearningRate 0.088291   Epoch: 2   Global Step: 71070   Fp16 Grad Scale: 65536   Required: 324 hours\nTraining: 2025-06-03 23:09:01,516-Speed 121.98 samples/sec   Loss 22.2421   LearningRate 0.088289   Epoch: 2   Global Step: 71080   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:09:22,508-Speed 121.96 samples/sec   Loss 22.3445   LearningRate 0.088288   Epoch: 2   Global Step: 71090   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:09:43,484-Speed 122.05 samples/sec   Loss 22.2068   LearningRate 0.088286   Epoch: 2   Global Step: 71100   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:10:04,471-Speed 121.99 samples/sec   Loss 22.0012   LearningRate 0.088284   Epoch: 2   Global Step: 71110   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:10:25,462-Speed 121.96 samples/sec   Loss 22.1774   LearningRate 0.088283   Epoch: 2   Global Step: 71120   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:10:46,442-Speed 122.03 samples/sec   Loss 22.7114   LearningRate 0.088281   Epoch: 2   Global Step: 71130   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:11:07,422-Speed 122.03 samples/sec   Loss 22.1953   LearningRate 0.088279   Epoch: 2   Global Step: 71140   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:11:28,418-Speed 121.93 samples/sec   Loss 22.0142   LearningRate 0.088278   Epoch: 2   Global Step: 71150   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:11:49,348-Speed 122.32 samples/sec   Loss 22.2561   LearningRate 0.088276   Epoch: 2   Global Step: 71160   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:12:10,158-Speed 123.02 samples/sec   Loss 22.4259   LearningRate 0.088274   Epoch: 2   Global Step: 71170   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:12:30,949-Speed 123.13 samples/sec   Loss 22.6890   LearningRate 0.088273   Epoch: 2   Global Step: 71180   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:12:51,727-Speed 123.21 samples/sec   Loss 22.3051   LearningRate 0.088271   Epoch: 2   Global Step: 71190   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:13:12,484-Speed 123.33 samples/sec   Loss 22.6214   LearningRate 0.088269   Epoch: 2   Global Step: 71200   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:13:33,093-Speed 124.22 samples/sec   Loss 22.5799   LearningRate 0.088268   Epoch: 2   Global Step: 71210   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:13:53,769-Speed 123.82 samples/sec   Loss 22.9644   LearningRate 0.088266   Epoch: 2   Global Step: 71220   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 23:14:14,402-Speed 124.08 samples/sec   Loss 22.3574   LearningRate 0.088264   Epoch: 2   Global Step: 71230   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 23:14:35,049-Speed 124.00 samples/sec   Loss 21.9576   LearningRate 0.088263   Epoch: 2   Global Step: 71240   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 23:14:55,836-Speed 123.15 samples/sec   Loss 22.3205   LearningRate 0.088261   Epoch: 2   Global Step: 71250   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 23:15:16,659-Speed 122.95 samples/sec   Loss 22.2158   LearningRate 0.088260   Epoch: 2   Global Step: 71260   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 23:15:37,541-Speed 122.60 samples/sec   Loss 22.4061   LearningRate 0.088258   Epoch: 2   Global Step: 71270   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 23:15:58,468-Speed 122.34 samples/sec   Loss 22.0715   LearningRate 0.088256   Epoch: 2   Global Step: 71280   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 23:16:19,443-Speed 122.06 samples/sec   Loss 22.0680   LearningRate 0.088255   Epoch: 2   Global Step: 71290   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 23:16:40,408-Speed 122.11 samples/sec   Loss 22.3722   LearningRate 0.088253   Epoch: 2   Global Step: 71300   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:17:01,411-Speed 121.89 samples/sec   Loss 22.6445   LearningRate 0.088251   Epoch: 2   Global Step: 71310   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:17:22,392-Speed 122.02 samples/sec   Loss 22.1615   LearningRate 0.088250   Epoch: 2   Global Step: 71320   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:17:43,373-Speed 122.02 samples/sec   Loss 21.9998   LearningRate 0.088248   Epoch: 2   Global Step: 71330   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:18:04,370-Speed 121.93 samples/sec   Loss 22.0504   LearningRate 0.088246   Epoch: 2   Global Step: 71340   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:18:25,353-Speed 122.01 samples/sec   Loss 22.4921   LearningRate 0.088245   Epoch: 2   Global Step: 71350   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:18:46,326-Speed 122.07 samples/sec   Loss 22.3826   LearningRate 0.088243   Epoch: 2   Global Step: 71360   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:19:07,320-Speed 121.94 samples/sec   Loss 22.5087   LearningRate 0.088241   Epoch: 2   Global Step: 71370   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:19:28,284-Speed 122.12 samples/sec   Loss 22.0625   LearningRate 0.088240   Epoch: 2   Global Step: 71380   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:19:49,207-Speed 122.36 samples/sec   Loss 22.3854   LearningRate 0.088238   Epoch: 2   Global Step: 71390   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:20:10,024-Speed 122.98 samples/sec   Loss 22.3021   LearningRate 0.088236   Epoch: 2   Global Step: 71400   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 23:20:30,768-Speed 123.42 samples/sec   Loss 22.6223   LearningRate 0.088235   Epoch: 2   Global Step: 71410   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:20:51,492-Speed 123.53 samples/sec   Loss 22.2760   LearningRate 0.088233   Epoch: 2   Global Step: 71420   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:21:12,148-Speed 123.94 samples/sec   Loss 22.2160   LearningRate 0.088232   Epoch: 2   Global Step: 71430   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:21:32,755-Speed 124.23 samples/sec   Loss 22.4885   LearningRate 0.088230   Epoch: 2   Global Step: 71440   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:21:53,510-Speed 123.35 samples/sec   Loss 22.1090   LearningRate 0.088228   Epoch: 2   Global Step: 71450   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:22:14,286-Speed 123.22 samples/sec   Loss 22.2342   LearningRate 0.088227   Epoch: 2   Global Step: 71460   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:22:35,090-Speed 123.06 samples/sec   Loss 22.3795   LearningRate 0.088225   Epoch: 2   Global Step: 71470   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:22:55,907-Speed 122.98 samples/sec   Loss 22.5275   LearningRate 0.088223   Epoch: 2   Global Step: 71480   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:23:16,807-Speed 122.49 samples/sec   Loss 22.1417   LearningRate 0.088222   Epoch: 2   Global Step: 71490   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:23:37,699-Speed 122.54 samples/sec   Loss 22.3471   LearningRate 0.088220   Epoch: 2   Global Step: 71500   Fp16 Grad Scale: 65536   Required: 323 hours\nTraining: 2025-06-03 23:23:58,662-Speed 122.13 samples/sec   Loss 22.3925   LearningRate 0.088218   Epoch: 2   Global Step: 71510   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:24:19,585-Speed 122.36 samples/sec   Loss 22.1245   LearningRate 0.088217   Epoch: 2   Global Step: 71520   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:24:40,532-Speed 122.21 samples/sec   Loss 22.8137   LearningRate 0.088215   Epoch: 2   Global Step: 71530   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:25:01,480-Speed 122.21 samples/sec   Loss 22.0844   LearningRate 0.088213   Epoch: 2   Global Step: 71540   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:25:22,460-Speed 122.02 samples/sec   Loss 22.9096   LearningRate 0.088212   Epoch: 2   Global Step: 71550   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:25:43,440-Speed 122.03 samples/sec   Loss 22.5504   LearningRate 0.088210   Epoch: 2   Global Step: 71560   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:26:04,431-Speed 121.96 samples/sec   Loss 22.6280   LearningRate 0.088208   Epoch: 2   Global Step: 71570   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:26:25,412-Speed 122.02 samples/sec   Loss 21.9891   LearningRate 0.088207   Epoch: 2   Global Step: 71580   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:26:46,393-Speed 122.02 samples/sec   Loss 22.1910   LearningRate 0.088205   Epoch: 2   Global Step: 71590   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:27:07,391-Speed 121.92 samples/sec   Loss 22.5677   LearningRate 0.088204   Epoch: 2   Global Step: 71600   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:27:28,370-Speed 122.03 samples/sec   Loss 22.5753   LearningRate 0.088202   Epoch: 2   Global Step: 71610   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 23:27:49,350-Speed 122.03 samples/sec   Loss 22.3765   LearningRate 0.088200   Epoch: 2   Global Step: 71620   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-03 23:28:10,333-Speed 122.01 samples/sec   Loss 22.4322   LearningRate 0.088199   Epoch: 2   Global Step: 71630   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:28:31,316-Speed 122.01 samples/sec   Loss 22.0948   LearningRate 0.088197   Epoch: 2   Global Step: 71640   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:28:52,293-Speed 122.05 samples/sec   Loss 22.4998   LearningRate 0.088195   Epoch: 2   Global Step: 71650   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:29:13,282-Speed 121.97 samples/sec   Loss 22.4613   LearningRate 0.088194   Epoch: 2   Global Step: 71660   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-03 23:29:34,264-Speed 122.01 samples/sec   Loss 22.2771   LearningRate 0.088192   Epoch: 2   Global Step: 71670   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:29:55,246-Speed 122.01 samples/sec   Loss 22.3564   LearningRate 0.088190   Epoch: 2   Global Step: 71680   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:30:16,233-Speed 121.99 samples/sec   Loss 22.6404   LearningRate 0.088189   Epoch: 2   Global Step: 71690   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:30:37,226-Speed 121.95 samples/sec   Loss 22.1918   LearningRate 0.088187   Epoch: 2   Global Step: 71700   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:30:58,210-Speed 122.00 samples/sec   Loss 22.5057   LearningRate 0.088185   Epoch: 2   Global Step: 71710   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:31:19,193-Speed 122.01 samples/sec   Loss 22.4607   LearningRate 0.088184   Epoch: 2   Global Step: 71720   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:31:40,192-Speed 121.91 samples/sec   Loss 22.1338   LearningRate 0.088182   Epoch: 2   Global Step: 71730   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 23:32:01,144-Speed 122.19 samples/sec   Loss 22.3650   LearningRate 0.088180   Epoch: 2   Global Step: 71740   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:32:22,125-Speed 122.02 samples/sec   Loss 22.4794   LearningRate 0.088179   Epoch: 2   Global Step: 71750   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:32:43,119-Speed 121.95 samples/sec   Loss 22.5782   LearningRate 0.088177   Epoch: 2   Global Step: 71760   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:33:04,098-Speed 122.03 samples/sec   Loss 22.4459   LearningRate 0.088175   Epoch: 2   Global Step: 71770   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:33:25,077-Speed 122.03 samples/sec   Loss 22.1689   LearningRate 0.088174   Epoch: 2   Global Step: 71780   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:33:46,074-Speed 121.93 samples/sec   Loss 22.6678   LearningRate 0.088172   Epoch: 2   Global Step: 71790   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:34:07,056-Speed 122.02 samples/sec   Loss 22.2348   LearningRate 0.088171   Epoch: 2   Global Step: 71800   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:34:28,034-Speed 122.04 samples/sec   Loss 21.8688   LearningRate 0.088169   Epoch: 2   Global Step: 71810   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:34:49,024-Speed 121.97 samples/sec   Loss 22.4434   LearningRate 0.088167   Epoch: 2   Global Step: 71820   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:35:10,019-Speed 121.94 samples/sec   Loss 22.2582   LearningRate 0.088166   Epoch: 2   Global Step: 71830   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:35:30,999-Speed 122.02 samples/sec   Loss 22.2257   LearningRate 0.088164   Epoch: 2   Global Step: 71840   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 23:35:51,961-Speed 122.13 samples/sec   Loss 22.2500   LearningRate 0.088162   Epoch: 2   Global Step: 71850   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:36:12,955-Speed 121.94 samples/sec   Loss 22.2225   LearningRate 0.088161   Epoch: 2   Global Step: 71860   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:36:33,908-Speed 122.18 samples/sec   Loss 22.5083   LearningRate 0.088159   Epoch: 2   Global Step: 71870   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:36:54,748-Speed 122.85 samples/sec   Loss 22.3541   LearningRate 0.088157   Epoch: 2   Global Step: 71880   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:37:15,540-Speed 123.13 samples/sec   Loss 22.4305   LearningRate 0.088156   Epoch: 2   Global Step: 71890   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:37:36,320-Speed 123.20 samples/sec   Loss 22.1690   LearningRate 0.088154   Epoch: 2   Global Step: 71900   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:37:57,016-Speed 123.70 samples/sec   Loss 22.4740   LearningRate 0.088152   Epoch: 2   Global Step: 71910   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:38:17,707-Speed 123.73 samples/sec   Loss 22.1185   LearningRate 0.088151   Epoch: 2   Global Step: 71920   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:38:38,441-Speed 123.48 samples/sec   Loss 22.4152   LearningRate 0.088149   Epoch: 2   Global Step: 71930   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:38:59,218-Speed 123.21 samples/sec   Loss 22.4472   LearningRate 0.088147   Epoch: 2   Global Step: 71940   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:39:20,020-Speed 123.07 samples/sec   Loss 22.4164   LearningRate 0.088146   Epoch: 2   Global Step: 71950   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-03 23:39:40,907-Speed 122.57 samples/sec   Loss 22.2243   LearningRate 0.088144   Epoch: 2   Global Step: 71960   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:40:01,756-Speed 122.79 samples/sec   Loss 22.2545   LearningRate 0.088143   Epoch: 2   Global Step: 71970   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:40:22,683-Speed 122.34 samples/sec   Loss 22.0560   LearningRate 0.088141   Epoch: 2   Global Step: 71980   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:40:43,636-Speed 122.18 samples/sec   Loss 22.2582   LearningRate 0.088139   Epoch: 2   Global Step: 71990   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-03 23:41:04,596-Speed 122.15 samples/sec   Loss 21.8876   LearningRate 0.088138   Epoch: 2   Global Step: 72000   Fp16 Grad Scale: 131072   Required: 322 hours\ntesting verification..\n(12000, 512)\ninfer time 151.96598499999988\nTraining: 2025-06-03 23:43:39,915-[lfw][72000]XNorm: 21.003331\nTraining: 2025-06-03 23:43:39,916-[lfw][72000]Accuracy-Flip: 0.99267+-0.00512\nTraining: 2025-06-03 23:43:39,916-[lfw][72000]Accuracy-Highest: 0.99283\ntesting verification..\n(14000, 512)\ninfer time 176.2783500000001\nTraining: 2025-06-03 23:46:39,336-[cfp_fp][72000]XNorm: 18.073828\nTraining: 2025-06-03 23:46:39,336-[cfp_fp][72000]Accuracy-Flip: 0.94071+-0.01362\nTraining: 2025-06-03 23:46:39,337-[cfp_fp][72000]Accuracy-Highest: 0.94071\ntesting verification..\n(12000, 512)\ninfer time 151.8742570000001\nTraining: 2025-06-03 23:49:13,987-[agedb_30][72000]XNorm: 20.709395\nTraining: 2025-06-03 23:49:13,987-[agedb_30][72000]Accuracy-Flip: 0.95100+-0.01111\nTraining: 2025-06-03 23:49:13,987-[agedb_30][72000]Accuracy-Highest: 0.95100\n/kaggle/working/Arcface_torch/partial_fc_v2.py:157: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(self.fp16):\nTraining: 2025-06-03 23:49:36,627-Speed 5.00 samples/sec   Loss 22.6298   LearningRate 0.088136   Epoch: 2   Global Step: 72010   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:49:57,244-Speed 124.18 samples/sec   Loss 22.2381   LearningRate 0.088134   Epoch: 2   Global Step: 72020   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:50:17,435-Speed 126.80 samples/sec   Loss 22.2541   LearningRate 0.088133   Epoch: 2   Global Step: 72030   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:50:37,476-Speed 127.74 samples/sec   Loss 22.4499   LearningRate 0.088131   Epoch: 2   Global Step: 72040   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:50:57,630-Speed 127.02 samples/sec   Loss 22.1320   LearningRate 0.088129   Epoch: 2   Global Step: 72050   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:51:17,832-Speed 126.73 samples/sec   Loss 22.2880   LearningRate 0.088128   Epoch: 2   Global Step: 72060   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 23:51:38,004-Speed 126.91 samples/sec   Loss 22.1508   LearningRate 0.088126   Epoch: 2   Global Step: 72070   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:51:58,228-Speed 126.59 samples/sec   Loss 22.3335   LearningRate 0.088124   Epoch: 2   Global Step: 72080   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:52:18,601-Speed 125.66 samples/sec   Loss 22.2766   LearningRate 0.088123   Epoch: 2   Global Step: 72090   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:52:39,146-Speed 124.61 samples/sec   Loss 23.2937   LearningRate 0.088121   Epoch: 2   Global Step: 72100   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:52:59,896-Speed 123.38 samples/sec   Loss 22.6638   LearningRate 0.088119   Epoch: 2   Global Step: 72110   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:53:20,844-Speed 122.21 samples/sec   Loss 22.2611   LearningRate 0.088118   Epoch: 2   Global Step: 72120   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:53:41,826-Speed 122.01 samples/sec   Loss 22.4167   LearningRate 0.088116   Epoch: 2   Global Step: 72130   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:54:02,760-Speed 122.30 samples/sec   Loss 22.6100   LearningRate 0.088115   Epoch: 2   Global Step: 72140   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:54:23,739-Speed 122.03 samples/sec   Loss 22.3385   LearningRate 0.088113   Epoch: 2   Global Step: 72150   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:54:44,598-Speed 122.73 samples/sec   Loss 22.5437   LearningRate 0.088111   Epoch: 2   Global Step: 72160   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:55:05,412-Speed 123.00 samples/sec   Loss 22.1709   LearningRate 0.088110   Epoch: 2   Global Step: 72170   Fp16 Grad Scale: 262144   Required: 328 hours\nTraining: 2025-06-03 23:55:26,226-Speed 123.00 samples/sec   Loss 22.1181   LearningRate 0.088108   Epoch: 2   Global Step: 72180   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:55:47,039-Speed 123.00 samples/sec   Loss 22.6251   LearningRate 0.088106   Epoch: 2   Global Step: 72190   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:56:07,895-Speed 122.75 samples/sec   Loss 22.3505   LearningRate 0.088105   Epoch: 2   Global Step: 72200   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:56:28,840-Speed 122.23 samples/sec   Loss 22.3972   LearningRate 0.088103   Epoch: 2   Global Step: 72210   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:56:49,819-Speed 122.03 samples/sec   Loss 22.2971   LearningRate 0.088101   Epoch: 2   Global Step: 72220   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:57:10,784-Speed 122.11 samples/sec   Loss 22.1940   LearningRate 0.088100   Epoch: 2   Global Step: 72230   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:57:31,772-Speed 121.98 samples/sec   Loss 22.2363   LearningRate 0.088098   Epoch: 2   Global Step: 72240   Fp16 Grad Scale: 131072   Required: 328 hours\nTraining: 2025-06-03 23:57:52,752-Speed 122.02 samples/sec   Loss 21.8859   LearningRate 0.088096   Epoch: 2   Global Step: 72250   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 23:58:13,705-Speed 122.18 samples/sec   Loss 22.2635   LearningRate 0.088095   Epoch: 2   Global Step: 72260   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 23:58:34,431-Speed 123.52 samples/sec   Loss 22.2725   LearningRate 0.088093   Epoch: 2   Global Step: 72270   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 23:58:55,045-Speed 124.19 samples/sec   Loss 22.7211   LearningRate 0.088091   Epoch: 2   Global Step: 72280   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 23:59:15,971-Speed 122.34 samples/sec   Loss 22.3851   LearningRate 0.088090   Epoch: 2   Global Step: 72290   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 23:59:36,974-Speed 121.89 samples/sec   Loss 22.1168   LearningRate 0.088088   Epoch: 2   Global Step: 72300   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-03 23:59:57,953-Speed 122.03 samples/sec   Loss 22.5917   LearningRate 0.088087   Epoch: 2   Global Step: 72310   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:00:18,940-Speed 121.98 samples/sec   Loss 22.3203   LearningRate 0.088085   Epoch: 2   Global Step: 72320   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:00:39,940-Speed 121.91 samples/sec   Loss 22.4509   LearningRate 0.088083   Epoch: 2   Global Step: 72330   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:01:00,920-Speed 122.03 samples/sec   Loss 22.7798   LearningRate 0.088082   Epoch: 2   Global Step: 72340   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:01:21,863-Speed 122.24 samples/sec   Loss 22.2462   LearningRate 0.088080   Epoch: 2   Global Step: 72350   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:01:42,694-Speed 122.90 samples/sec   Loss 22.2409   LearningRate 0.088078   Epoch: 2   Global Step: 72360   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:02:03,567-Speed 122.65 samples/sec   Loss 22.9301   LearningRate 0.088077   Epoch: 2   Global Step: 72370   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:02:24,532-Speed 122.12 samples/sec   Loss 22.4184   LearningRate 0.088075   Epoch: 2   Global Step: 72380   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:02:45,516-Speed 122.00 samples/sec   Loss 22.3853   LearningRate 0.088073   Epoch: 2   Global Step: 72390   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:03:06,506-Speed 121.97 samples/sec   Loss 21.9834   LearningRate 0.088072   Epoch: 2   Global Step: 72400   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:03:27,461-Speed 122.18 samples/sec   Loss 22.1779   LearningRate 0.088070   Epoch: 2   Global Step: 72410   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:03:48,384-Speed 122.35 samples/sec   Loss 22.2319   LearningRate 0.088068   Epoch: 2   Global Step: 72420   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:04:09,336-Speed 122.19 samples/sec   Loss 22.1376   LearningRate 0.088067   Epoch: 2   Global Step: 72430   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:04:30,267-Speed 122.31 samples/sec   Loss 22.5774   LearningRate 0.088065   Epoch: 2   Global Step: 72440   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:04:51,232-Speed 122.12 samples/sec   Loss 22.3790   LearningRate 0.088063   Epoch: 2   Global Step: 72450   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:05:12,114-Speed 122.60 samples/sec   Loss 22.2860   LearningRate 0.088062   Epoch: 2   Global Step: 72460   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:05:33,050-Speed 122.28 samples/sec   Loss 22.3600   LearningRate 0.088060   Epoch: 2   Global Step: 72470   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:05:53,947-Speed 122.51 samples/sec   Loss 22.3839   LearningRate 0.088059   Epoch: 2   Global Step: 72480   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-04 00:06:14,827-Speed 122.61 samples/sec   Loss 21.9506   LearningRate 0.088057   Epoch: 2   Global Step: 72490   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-04 00:06:35,700-Speed 122.65 samples/sec   Loss 22.1537   LearningRate 0.088055   Epoch: 2   Global Step: 72500   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-04 00:06:56,639-Speed 122.26 samples/sec   Loss 22.0954   LearningRate 0.088054   Epoch: 2   Global Step: 72510   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-04 00:07:17,555-Speed 122.40 samples/sec   Loss 22.2264   LearningRate 0.088052   Epoch: 2   Global Step: 72520   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-04 00:07:38,443-Speed 122.56 samples/sec   Loss 22.1834   LearningRate 0.088050   Epoch: 2   Global Step: 72530   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-04 00:07:59,316-Speed 122.66 samples/sec   Loss 22.3691   LearningRate 0.088049   Epoch: 2   Global Step: 72540   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:08:20,183-Speed 122.68 samples/sec   Loss 22.1931   LearningRate 0.088047   Epoch: 2   Global Step: 72550   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:08:41,068-Speed 122.58 samples/sec   Loss 22.2583   LearningRate 0.088045   Epoch: 2   Global Step: 72560   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:09:01,989-Speed 122.37 samples/sec   Loss 22.2106   LearningRate 0.088044   Epoch: 2   Global Step: 72570   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:09:22,867-Speed 122.62 samples/sec   Loss 22.1668   LearningRate 0.088042   Epoch: 2   Global Step: 72580   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:09:43,708-Speed 122.84 samples/sec   Loss 22.4067   LearningRate 0.088040   Epoch: 2   Global Step: 72590   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:10:04,578-Speed 122.67 samples/sec   Loss 22.0035   LearningRate 0.088039   Epoch: 2   Global Step: 72600   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:10:25,461-Speed 122.59 samples/sec   Loss 22.3706   LearningRate 0.088037   Epoch: 2   Global Step: 72610   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:10:46,359-Speed 122.50 samples/sec   Loss 22.6061   LearningRate 0.088035   Epoch: 2   Global Step: 72620   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:11:07,239-Speed 122.61 samples/sec   Loss 22.3052   LearningRate 0.088034   Epoch: 2   Global Step: 72630   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:11:28,135-Speed 122.52 samples/sec   Loss 22.3986   LearningRate 0.088032   Epoch: 2   Global Step: 72640   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-04 00:11:49,014-Speed 122.62 samples/sec   Loss 22.0986   LearningRate 0.088031   Epoch: 2   Global Step: 72650   Fp16 Grad Scale: 262144   Required: 327 hours\nTraining: 2025-06-04 00:12:09,904-Speed 122.55 samples/sec   Loss 22.2955   LearningRate 0.088029   Epoch: 2   Global Step: 72660   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:12:30,842-Speed 122.27 samples/sec   Loss 22.2385   LearningRate 0.088027   Epoch: 2   Global Step: 72670   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:12:51,697-Speed 122.75 samples/sec   Loss 22.1054   LearningRate 0.088026   Epoch: 2   Global Step: 72680   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:13:12,560-Speed 122.71 samples/sec   Loss 22.3043   LearningRate 0.088024   Epoch: 2   Global Step: 72690   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:13:33,397-Speed 122.87 samples/sec   Loss 22.0086   LearningRate 0.088022   Epoch: 2   Global Step: 72700   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:13:54,221-Speed 122.94 samples/sec   Loss 22.2993   LearningRate 0.088021   Epoch: 2   Global Step: 72710   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:14:15,035-Speed 123.00 samples/sec   Loss 22.3439   LearningRate 0.088019   Epoch: 2   Global Step: 72720   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:14:35,842-Speed 123.04 samples/sec   Loss 21.7645   LearningRate 0.088017   Epoch: 2   Global Step: 72730   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:14:56,668-Speed 122.93 samples/sec   Loss 21.9338   LearningRate 0.088016   Epoch: 2   Global Step: 72740   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:15:17,460-Speed 123.13 samples/sec   Loss 21.8647   LearningRate 0.088014   Epoch: 2   Global Step: 72750   Fp16 Grad Scale: 131072   Required: 327 hours\nTraining: 2025-06-04 00:15:38,241-Speed 123.20 samples/sec   Loss 22.7277   LearningRate 0.088012   Epoch: 2   Global Step: 72760   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-04 00:15:59,060-Speed 122.97 samples/sec   Loss 22.5390   LearningRate 0.088011   Epoch: 2   Global Step: 72770   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:16:19,866-Speed 123.05 samples/sec   Loss 22.2144   LearningRate 0.088009   Epoch: 2   Global Step: 72780   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:16:40,668-Speed 123.07 samples/sec   Loss 22.3857   LearningRate 0.088007   Epoch: 2   Global Step: 72790   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:17:01,461-Speed 123.12 samples/sec   Loss 22.1423   LearningRate 0.088006   Epoch: 2   Global Step: 72800   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:17:22,248-Speed 123.16 samples/sec   Loss 22.0433   LearningRate 0.088004   Epoch: 2   Global Step: 72810   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:17:43,091-Speed 122.83 samples/sec   Loss 22.3328   LearningRate 0.088003   Epoch: 2   Global Step: 72820   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:18:03,920-Speed 122.91 samples/sec   Loss 22.2424   LearningRate 0.088001   Epoch: 2   Global Step: 72830   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:18:24,698-Speed 123.21 samples/sec   Loss 21.7210   LearningRate 0.087999   Epoch: 2   Global Step: 72840   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:18:45,550-Speed 122.77 samples/sec   Loss 21.8461   LearningRate 0.087998   Epoch: 2   Global Step: 72850   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:19:06,386-Speed 122.87 samples/sec   Loss 22.3106   LearningRate 0.087996   Epoch: 2   Global Step: 72860   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:19:27,168-Speed 123.19 samples/sec   Loss 22.1321   LearningRate 0.087994   Epoch: 2   Global Step: 72870   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-04 00:19:47,983-Speed 123.00 samples/sec   Loss 22.1779   LearningRate 0.087993   Epoch: 2   Global Step: 72880   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:20:08,826-Speed 122.82 samples/sec   Loss 22.0976   LearningRate 0.087991   Epoch: 2   Global Step: 72890   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:20:29,658-Speed 122.89 samples/sec   Loss 22.0762   LearningRate 0.087989   Epoch: 2   Global Step: 72900   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:20:50,430-Speed 123.25 samples/sec   Loss 22.5135   LearningRate 0.087988   Epoch: 2   Global Step: 72910   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:21:11,185-Speed 123.35 samples/sec   Loss 22.4913   LearningRate 0.087986   Epoch: 2   Global Step: 72920   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:21:31,965-Speed 123.20 samples/sec   Loss 22.1459   LearningRate 0.087984   Epoch: 2   Global Step: 72930   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:21:52,754-Speed 123.15 samples/sec   Loss 22.1581   LearningRate 0.087983   Epoch: 2   Global Step: 72940   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:22:13,562-Speed 123.03 samples/sec   Loss 22.3406   LearningRate 0.087981   Epoch: 2   Global Step: 72950   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:22:34,384-Speed 122.95 samples/sec   Loss 22.3298   LearningRate 0.087979   Epoch: 2   Global Step: 72960   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:22:55,250-Speed 122.69 samples/sec   Loss 22.0995   LearningRate 0.087978   Epoch: 2   Global Step: 72970   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:23:16,145-Speed 122.52 samples/sec   Loss 22.3162   LearningRate 0.087976   Epoch: 2   Global Step: 72980   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-04 00:23:37,097-Speed 122.19 samples/sec   Loss 22.2072   LearningRate 0.087974   Epoch: 2   Global Step: 72990   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-04 00:23:58,027-Speed 122.32 samples/sec   Loss 22.1896   LearningRate 0.087973   Epoch: 2   Global Step: 73000   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-04 00:24:18,960-Speed 122.30 samples/sec   Loss 21.9991   LearningRate 0.087971   Epoch: 2   Global Step: 73010   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:24:39,881-Speed 122.37 samples/sec   Loss 22.3900   LearningRate 0.087970   Epoch: 2   Global Step: 73020   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:25:00,749-Speed 122.68 samples/sec   Loss 22.2596   LearningRate 0.087968   Epoch: 2   Global Step: 73030   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:25:21,547-Speed 123.09 samples/sec   Loss 22.2652   LearningRate 0.087966   Epoch: 2   Global Step: 73040   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:25:42,322-Speed 123.23 samples/sec   Loss 21.8229   LearningRate 0.087965   Epoch: 2   Global Step: 73050   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:26:03,126-Speed 123.06 samples/sec   Loss 21.9885   LearningRate 0.087963   Epoch: 2   Global Step: 73060   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:26:23,919-Speed 123.12 samples/sec   Loss 22.1381   LearningRate 0.087961   Epoch: 2   Global Step: 73070   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:26:44,678-Speed 123.32 samples/sec   Loss 21.9694   LearningRate 0.087960   Epoch: 2   Global Step: 73080   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:27:05,431-Speed 123.37 samples/sec   Loss 22.5089   LearningRate 0.087958   Epoch: 2   Global Step: 73090   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:27:26,156-Speed 123.52 samples/sec   Loss 22.2793   LearningRate 0.087956   Epoch: 2   Global Step: 73100   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:27:46,901-Speed 123.41 samples/sec   Loss 22.8701   LearningRate 0.087955   Epoch: 2   Global Step: 73110   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:28:07,669-Speed 123.27 samples/sec   Loss 21.8908   LearningRate 0.087953   Epoch: 2   Global Step: 73120   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:28:28,451-Speed 123.19 samples/sec   Loss 22.7660   LearningRate 0.087951   Epoch: 2   Global Step: 73130   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:28:49,155-Speed 123.66 samples/sec   Loss 22.0742   LearningRate 0.087950   Epoch: 2   Global Step: 73140   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:29:09,889-Speed 123.47 samples/sec   Loss 21.9150   LearningRate 0.087948   Epoch: 2   Global Step: 73150   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:29:30,637-Speed 123.39 samples/sec   Loss 22.2785   LearningRate 0.087946   Epoch: 2   Global Step: 73160   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:29:51,416-Speed 123.21 samples/sec   Loss 22.5152   LearningRate 0.087945   Epoch: 2   Global Step: 73170   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:30:12,169-Speed 123.37 samples/sec   Loss 22.3502   LearningRate 0.087943   Epoch: 2   Global Step: 73180   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:30:32,907-Speed 123.45 samples/sec   Loss 22.8368   LearningRate 0.087942   Epoch: 2   Global Step: 73190   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:30:53,612-Speed 123.65 samples/sec   Loss 22.1391   LearningRate 0.087940   Epoch: 2   Global Step: 73200   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:31:14,308-Speed 123.70 samples/sec   Loss 22.3542   LearningRate 0.087938   Epoch: 2   Global Step: 73210   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-04 00:31:35,081-Speed 123.25 samples/sec   Loss 22.3085   LearningRate 0.087937   Epoch: 2   Global Step: 73220   Fp16 Grad Scale: 262144   Required: 326 hours\nTraining: 2025-06-04 00:31:55,783-Speed 123.66 samples/sec   Loss 22.0423   LearningRate 0.087935   Epoch: 2   Global Step: 73230   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:32:16,466-Speed 123.78 samples/sec   Loss 22.1837   LearningRate 0.087933   Epoch: 2   Global Step: 73240   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:32:37,178-Speed 123.61 samples/sec   Loss 22.2525   LearningRate 0.087932   Epoch: 2   Global Step: 73250   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:32:57,871-Speed 123.72 samples/sec   Loss 22.0641   LearningRate 0.087930   Epoch: 2   Global Step: 73260   Fp16 Grad Scale: 131072   Required: 326 hours\nTraining: 2025-06-04 00:33:18,497-Speed 124.12 samples/sec   Loss 21.9427   LearningRate 0.087928   Epoch: 2   Global Step: 73270   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:33:39,140-Speed 124.02 samples/sec   Loss 21.9203   LearningRate 0.087927   Epoch: 2   Global Step: 73280   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:33:59,871-Speed 123.49 samples/sec   Loss 22.0325   LearningRate 0.087925   Epoch: 2   Global Step: 73290   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:34:20,522-Speed 123.97 samples/sec   Loss 22.0484   LearningRate 0.087923   Epoch: 2   Global Step: 73300   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:34:41,177-Speed 123.95 samples/sec   Loss 22.8688   LearningRate 0.087922   Epoch: 2   Global Step: 73310   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:35:01,832-Speed 123.94 samples/sec   Loss 22.4565   LearningRate 0.087920   Epoch: 2   Global Step: 73320   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:35:22,461-Speed 124.10 samples/sec   Loss 21.9313   LearningRate 0.087918   Epoch: 2   Global Step: 73330   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-04 00:35:43,199-Speed 123.45 samples/sec   Loss 22.0042   LearningRate 0.087917   Epoch: 2   Global Step: 73340   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-04 00:36:03,931-Speed 123.49 samples/sec   Loss 22.3821   LearningRate 0.087915   Epoch: 2   Global Step: 73350   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-04 00:36:24,571-Speed 124.03 samples/sec   Loss 22.5915   LearningRate 0.087914   Epoch: 2   Global Step: 73360   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-04 00:36:45,173-Speed 124.27 samples/sec   Loss 21.9535   LearningRate 0.087912   Epoch: 2   Global Step: 73370   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-04 00:37:05,827-Speed 123.95 samples/sec   Loss 22.3372   LearningRate 0.087910   Epoch: 2   Global Step: 73380   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:37:26,484-Speed 123.93 samples/sec   Loss 21.9552   LearningRate 0.087909   Epoch: 2   Global Step: 73390   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:37:47,259-Speed 123.23 samples/sec   Loss 22.3893   LearningRate 0.087907   Epoch: 2   Global Step: 73400   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:38:08,035-Speed 123.22 samples/sec   Loss 22.6133   LearningRate 0.087905   Epoch: 2   Global Step: 73410   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:38:28,879-Speed 122.82 samples/sec   Loss 22.4678   LearningRate 0.087904   Epoch: 2   Global Step: 73420   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:38:49,760-Speed 122.61 samples/sec   Loss 21.9350   LearningRate 0.087902   Epoch: 2   Global Step: 73430   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:39:10,742-Speed 122.01 samples/sec   Loss 22.3415   LearningRate 0.087900   Epoch: 2   Global Step: 73440   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:39:31,705-Speed 122.12 samples/sec   Loss 22.6650   LearningRate 0.087899   Epoch: 2   Global Step: 73450   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:39:52,698-Speed 121.95 samples/sec   Loss 22.3164   LearningRate 0.087897   Epoch: 2   Global Step: 73460   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:40:13,658-Speed 122.15 samples/sec   Loss 22.1596   LearningRate 0.087895   Epoch: 2   Global Step: 73470   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:40:34,640-Speed 122.01 samples/sec   Loss 21.7750   LearningRate 0.087894   Epoch: 2   Global Step: 73480   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-04 00:40:55,612-Speed 122.07 samples/sec   Loss 22.6764   LearningRate 0.087892   Epoch: 2   Global Step: 73490   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:41:16,597-Speed 122.00 samples/sec   Loss 21.8915   LearningRate 0.087890   Epoch: 2   Global Step: 73500   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:41:37,558-Speed 122.14 samples/sec   Loss 22.5495   LearningRate 0.087889   Epoch: 2   Global Step: 73510   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:41:58,421-Speed 122.71 samples/sec   Loss 22.3852   LearningRate 0.087887   Epoch: 2   Global Step: 73520   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:42:19,222-Speed 123.07 samples/sec   Loss 22.2947   LearningRate 0.087886   Epoch: 2   Global Step: 73530   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:42:39,999-Speed 123.22 samples/sec   Loss 22.4954   LearningRate 0.087884   Epoch: 2   Global Step: 73540   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:43:00,783-Speed 123.18 samples/sec   Loss 22.2565   LearningRate 0.087882   Epoch: 2   Global Step: 73550   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:43:21,551-Speed 123.27 samples/sec   Loss 22.2641   LearningRate 0.087881   Epoch: 2   Global Step: 73560   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:43:42,317-Speed 123.29 samples/sec   Loss 22.3868   LearningRate 0.087879   Epoch: 2   Global Step: 73570   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:44:03,076-Speed 123.32 samples/sec   Loss 22.1906   LearningRate 0.087877   Epoch: 2   Global Step: 73580   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:44:23,808-Speed 123.48 samples/sec   Loss 22.0849   LearningRate 0.087876   Epoch: 2   Global Step: 73590   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-04 00:44:44,483-Speed 123.83 samples/sec   Loss 21.9014   LearningRate 0.087874   Epoch: 2   Global Step: 73600   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:45:05,075-Speed 124.33 samples/sec   Loss 23.0036   LearningRate 0.087872   Epoch: 2   Global Step: 73610   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:45:25,657-Speed 124.38 samples/sec   Loss 21.7397   LearningRate 0.087871   Epoch: 2   Global Step: 73620   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:45:46,379-Speed 123.55 samples/sec   Loss 22.4236   LearningRate 0.087869   Epoch: 2   Global Step: 73630   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:46:07,182-Speed 123.07 samples/sec   Loss 22.5455   LearningRate 0.087867   Epoch: 2   Global Step: 73640   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:46:28,010-Speed 122.91 samples/sec   Loss 22.5081   LearningRate 0.087866   Epoch: 2   Global Step: 73650   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:46:48,903-Speed 122.54 samples/sec   Loss 22.3985   LearningRate 0.087864   Epoch: 2   Global Step: 73660   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:47:09,801-Speed 122.51 samples/sec   Loss 22.3059   LearningRate 0.087862   Epoch: 2   Global Step: 73670   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:47:30,763-Speed 122.13 samples/sec   Loss 22.2172   LearningRate 0.087861   Epoch: 2   Global Step: 73680   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:47:51,767-Speed 121.89 samples/sec   Loss 22.6260   LearningRate 0.087859   Epoch: 2   Global Step: 73690   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:48:12,746-Speed 122.03 samples/sec   Loss 22.2181   LearningRate 0.087858   Epoch: 2   Global Step: 73700   Fp16 Grad Scale: 262144   Required: 325 hours\nTraining: 2025-06-04 00:48:33,709-Speed 122.13 samples/sec   Loss 21.7407   LearningRate 0.087856   Epoch: 2   Global Step: 73710   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:48:54,708-Speed 121.92 samples/sec   Loss 22.3090   LearningRate 0.087854   Epoch: 2   Global Step: 73720   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:49:15,683-Speed 122.05 samples/sec   Loss 22.3312   LearningRate 0.087853   Epoch: 2   Global Step: 73730   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:49:36,658-Speed 122.05 samples/sec   Loss 22.2822   LearningRate 0.087851   Epoch: 2   Global Step: 73740   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:49:57,516-Speed 122.74 samples/sec   Loss 22.2726   LearningRate 0.087849   Epoch: 2   Global Step: 73750   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:50:18,291-Speed 123.24 samples/sec   Loss 22.3099   LearningRate 0.087848   Epoch: 2   Global Step: 73760   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:50:39,064-Speed 123.24 samples/sec   Loss 22.3197   LearningRate 0.087846   Epoch: 2   Global Step: 73770   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:50:59,849-Speed 123.17 samples/sec   Loss 22.3775   LearningRate 0.087844   Epoch: 2   Global Step: 73780   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:51:20,593-Speed 123.41 samples/sec   Loss 21.9793   LearningRate 0.087843   Epoch: 2   Global Step: 73790   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:51:41,327-Speed 123.48 samples/sec   Loss 21.8673   LearningRate 0.087841   Epoch: 2   Global Step: 73800   Fp16 Grad Scale: 131072   Required: 325 hours\nTraining: 2025-06-04 00:52:02,004-Speed 123.81 samples/sec   Loss 22.3884   LearningRate 0.087839   Epoch: 2   Global Step: 73810   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:52:22,684-Speed 123.79 samples/sec   Loss 21.6625   LearningRate 0.087838   Epoch: 2   Global Step: 73820   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:52:43,335-Speed 123.97 samples/sec   Loss 22.0106   LearningRate 0.087836   Epoch: 2   Global Step: 73830   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:53:04,077-Speed 123.43 samples/sec   Loss 21.7183   LearningRate 0.087834   Epoch: 2   Global Step: 73840   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:53:24,852-Speed 123.23 samples/sec   Loss 22.4687   LearningRate 0.087833   Epoch: 2   Global Step: 73850   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:53:45,658-Speed 123.04 samples/sec   Loss 21.8499   LearningRate 0.087831   Epoch: 2   Global Step: 73860   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:54:06,476-Speed 122.98 samples/sec   Loss 22.0498   LearningRate 0.087830   Epoch: 2   Global Step: 73870   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:54:27,337-Speed 122.72 samples/sec   Loss 22.0289   LearningRate 0.087828   Epoch: 2   Global Step: 73880   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:54:48,257-Speed 122.38 samples/sec   Loss 22.2591   LearningRate 0.087826   Epoch: 2   Global Step: 73890   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:55:09,192-Speed 122.29 samples/sec   Loss 22.2771   LearningRate 0.087825   Epoch: 2   Global Step: 73900   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:55:30,196-Speed 121.89 samples/sec   Loss 21.8759   LearningRate 0.087823   Epoch: 2   Global Step: 73910   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-04 00:55:51,158-Speed 122.13 samples/sec   Loss 22.0259   LearningRate 0.087821   Epoch: 2   Global Step: 73920   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:56:12,141-Speed 122.01 samples/sec   Loss 22.0994   LearningRate 0.087820   Epoch: 2   Global Step: 73930   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:56:33,136-Speed 121.94 samples/sec   Loss 22.3790   LearningRate 0.087818   Epoch: 2   Global Step: 73940   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:56:54,112-Speed 122.05 samples/sec   Loss 22.6196   LearningRate 0.087816   Epoch: 2   Global Step: 73950   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:57:15,090-Speed 122.04 samples/sec   Loss 22.4201   LearningRate 0.087815   Epoch: 2   Global Step: 73960   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:57:36,034-Speed 122.23 samples/sec   Loss 22.2976   LearningRate 0.087813   Epoch: 2   Global Step: 73970   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:57:56,841-Speed 123.04 samples/sec   Loss 22.4198   LearningRate 0.087811   Epoch: 2   Global Step: 73980   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:58:17,619-Speed 123.21 samples/sec   Loss 22.4180   LearningRate 0.087810   Epoch: 2   Global Step: 73990   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:58:38,403-Speed 123.17 samples/sec   Loss 22.1755   LearningRate 0.087808   Epoch: 2   Global Step: 74000   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:58:59,218-Speed 123.00 samples/sec   Loss 21.9136   LearningRate 0.087806   Epoch: 2   Global Step: 74010   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 00:59:19,994-Speed 123.22 samples/sec   Loss 22.3928   LearningRate 0.087805   Epoch: 2   Global Step: 74020   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-04 00:59:40,778-Speed 123.18 samples/sec   Loss 22.7969   LearningRate 0.087803   Epoch: 2   Global Step: 74030   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-04 01:00:01,483-Speed 123.65 samples/sec   Loss 21.9535   LearningRate 0.087802   Epoch: 2   Global Step: 74040   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:00:22,075-Speed 124.32 samples/sec   Loss 22.6119   LearningRate 0.087800   Epoch: 2   Global Step: 74050   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:00:42,834-Speed 123.32 samples/sec   Loss 22.1438   LearningRate 0.087798   Epoch: 2   Global Step: 74060   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:01:03,802-Speed 122.10 samples/sec   Loss 21.6744   LearningRate 0.087797   Epoch: 2   Global Step: 74070   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:01:24,779-Speed 122.04 samples/sec   Loss 22.8148   LearningRate 0.087795   Epoch: 2   Global Step: 74080   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:01:45,610-Speed 122.90 samples/sec   Loss 21.9769   LearningRate 0.087793   Epoch: 2   Global Step: 74090   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:02:06,397-Speed 123.16 samples/sec   Loss 22.1913   LearningRate 0.087792   Epoch: 2   Global Step: 74100   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:02:27,191-Speed 123.12 samples/sec   Loss 22.0375   LearningRate 0.087790   Epoch: 2   Global Step: 74110   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:02:47,962-Speed 123.25 samples/sec   Loss 22.2006   LearningRate 0.087788   Epoch: 2   Global Step: 74120   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:03:08,633-Speed 123.85 samples/sec   Loss 22.1267   LearningRate 0.087787   Epoch: 2   Global Step: 74130   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:03:29,392-Speed 123.33 samples/sec   Loss 22.3233   LearningRate 0.087785   Epoch: 2   Global Step: 74140   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-04 01:03:50,123-Speed 123.49 samples/sec   Loss 22.3304   LearningRate 0.087783   Epoch: 2   Global Step: 74150   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-04 01:04:10,860-Speed 123.46 samples/sec   Loss 22.0471   LearningRate 0.087782   Epoch: 2   Global Step: 74160   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:04:31,610-Speed 123.38 samples/sec   Loss 22.2438   LearningRate 0.087780   Epoch: 2   Global Step: 74170   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:04:52,244-Speed 124.07 samples/sec   Loss 22.1309   LearningRate 0.087778   Epoch: 2   Global Step: 74180   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:05:12,987-Speed 123.42 samples/sec   Loss 22.8562   LearningRate 0.087777   Epoch: 2   Global Step: 74190   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:05:33,777-Speed 123.14 samples/sec   Loss 22.5077   LearningRate 0.087775   Epoch: 2   Global Step: 74200   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:05:54,715-Speed 122.27 samples/sec   Loss 22.2022   LearningRate 0.087773   Epoch: 2   Global Step: 74210   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:06:15,695-Speed 122.03 samples/sec   Loss 22.1318   LearningRate 0.087772   Epoch: 2   Global Step: 74220   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:06:36,688-Speed 121.95 samples/sec   Loss 21.6525   LearningRate 0.087770   Epoch: 2   Global Step: 74230   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:06:57,682-Speed 121.95 samples/sec   Loss 22.1129   LearningRate 0.087769   Epoch: 2   Global Step: 74240   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:07:18,655-Speed 122.07 samples/sec   Loss 22.0643   LearningRate 0.087767   Epoch: 2   Global Step: 74250   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:07:39,579-Speed 122.35 samples/sec   Loss 21.8696   LearningRate 0.087765   Epoch: 2   Global Step: 74260   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-04 01:08:00,397-Speed 122.98 samples/sec   Loss 21.8858   LearningRate 0.087764   Epoch: 2   Global Step: 74270   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-04 01:08:21,212-Speed 122.99 samples/sec   Loss 21.7939   LearningRate 0.087762   Epoch: 2   Global Step: 74280   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-04 01:08:41,997-Speed 123.17 samples/sec   Loss 22.1497   LearningRate 0.087760   Epoch: 2   Global Step: 74290   Fp16 Grad Scale: 262144   Required: 324 hours\nTraining: 2025-06-04 01:09:02,768-Speed 123.26 samples/sec   Loss 21.9780   LearningRate 0.087759   Epoch: 2   Global Step: 74300   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:09:23,540-Speed 123.24 samples/sec   Loss 22.3148   LearningRate 0.087757   Epoch: 2   Global Step: 74310   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:09:44,312-Speed 123.25 samples/sec   Loss 22.4692   LearningRate 0.087755   Epoch: 2   Global Step: 74320   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:10:05,082-Speed 123.26 samples/sec   Loss 21.9920   LearningRate 0.087754   Epoch: 2   Global Step: 74330   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:10:25,863-Speed 123.19 samples/sec   Loss 22.3934   LearningRate 0.087752   Epoch: 2   Global Step: 74340   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:10:46,637-Speed 123.23 samples/sec   Loss 22.1990   LearningRate 0.087750   Epoch: 2   Global Step: 74350   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:11:07,402-Speed 123.29 samples/sec   Loss 22.2266   LearningRate 0.087749   Epoch: 2   Global Step: 74360   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:11:28,166-Speed 123.30 samples/sec   Loss 22.1486   LearningRate 0.087747   Epoch: 2   Global Step: 74370   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:11:48,923-Speed 123.34 samples/sec   Loss 22.1871   LearningRate 0.087745   Epoch: 2   Global Step: 74380   Fp16 Grad Scale: 131072   Required: 324 hours\nTraining: 2025-06-04 01:12:09,659-Speed 123.46 samples/sec   Loss 22.0140   LearningRate 0.087744   Epoch: 2   Global Step: 74390   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:12:30,414-Speed 123.35 samples/sec   Loss 21.9519   LearningRate 0.087742   Epoch: 2   Global Step: 74400   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:12:51,160-Speed 123.40 samples/sec   Loss 22.0169   LearningRate 0.087741   Epoch: 2   Global Step: 74410   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:13:11,933-Speed 123.24 samples/sec   Loss 22.1514   LearningRate 0.087739   Epoch: 2   Global Step: 74420   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:13:32,683-Speed 123.38 samples/sec   Loss 22.1581   LearningRate 0.087737   Epoch: 2   Global Step: 74430   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:13:53,461-Speed 123.21 samples/sec   Loss 22.0000   LearningRate 0.087736   Epoch: 2   Global Step: 74440   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:14:14,240-Speed 123.21 samples/sec   Loss 23.0256   LearningRate 0.087734   Epoch: 2   Global Step: 74450   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:14:35,028-Speed 123.15 samples/sec   Loss 22.4666   LearningRate 0.087732   Epoch: 2   Global Step: 74460   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:14:55,808-Speed 123.20 samples/sec   Loss 22.5874   LearningRate 0.087731   Epoch: 2   Global Step: 74470   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:15:16,586-Speed 123.21 samples/sec   Loss 22.2561   LearningRate 0.087729   Epoch: 2   Global Step: 74480   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:15:37,371-Speed 123.17 samples/sec   Loss 22.0215   LearningRate 0.087727   Epoch: 2   Global Step: 74490   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:15:58,157-Speed 123.16 samples/sec   Loss 22.4035   LearningRate 0.087726   Epoch: 2   Global Step: 74500   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-04 01:16:18,942-Speed 123.17 samples/sec   Loss 22.3723   LearningRate 0.087724   Epoch: 2   Global Step: 74510   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-04 01:16:39,722-Speed 123.20 samples/sec   Loss 22.0348   LearningRate 0.087722   Epoch: 2   Global Step: 74520   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-04 01:17:00,514-Speed 123.13 samples/sec   Loss 22.4211   LearningRate 0.087721   Epoch: 2   Global Step: 74530   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-04 01:17:21,305-Speed 123.14 samples/sec   Loss 22.2704   LearningRate 0.087719   Epoch: 2   Global Step: 74540   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-04 01:17:42,134-Speed 122.91 samples/sec   Loss 22.1255   LearningRate 0.087717   Epoch: 2   Global Step: 74550   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-04 01:18:02,942-Speed 123.03 samples/sec   Loss 22.0974   LearningRate 0.087716   Epoch: 2   Global Step: 74560   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-04 01:18:23,740-Speed 123.10 samples/sec   Loss 22.4559   LearningRate 0.087714   Epoch: 2   Global Step: 74570   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-04 01:18:44,472-Speed 123.48 samples/sec   Loss 22.4098   LearningRate 0.087713   Epoch: 2   Global Step: 74580   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-04 01:19:05,005-Speed 124.69 samples/sec   Loss 21.9698   LearningRate 0.087711   Epoch: 2   Global Step: 74590   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-04 01:19:25,907-Speed 122.48 samples/sec   Loss 22.3368   LearningRate 0.087709   Epoch: 2   Global Step: 74600   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:19:46,997-Speed 121.39 samples/sec   Loss 22.2233   LearningRate 0.087708   Epoch: 2   Global Step: 74610   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:20:07,738-Speed 123.43 samples/sec   Loss 22.2968   LearningRate 0.087706   Epoch: 2   Global Step: 74620   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:20:28,319-Speed 124.39 samples/sec   Loss 22.3341   LearningRate 0.087704   Epoch: 2   Global Step: 74630   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:20:49,145-Speed 122.93 samples/sec   Loss 22.2180   LearningRate 0.087703   Epoch: 2   Global Step: 74640   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:21:09,980-Speed 122.88 samples/sec   Loss 22.2106   LearningRate 0.087701   Epoch: 2   Global Step: 74650   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:21:30,821-Speed 122.84 samples/sec   Loss 21.8994   LearningRate 0.087699   Epoch: 2   Global Step: 74660   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:21:51,791-Speed 122.08 samples/sec   Loss 22.2356   LearningRate 0.087698   Epoch: 2   Global Step: 74670   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:22:12,615-Speed 122.94 samples/sec   Loss 21.9493   LearningRate 0.087696   Epoch: 2   Global Step: 74680   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:22:33,473-Speed 122.74 samples/sec   Loss 21.6123   LearningRate 0.087694   Epoch: 2   Global Step: 74690   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:22:54,424-Speed 122.19 samples/sec   Loss 22.2403   LearningRate 0.087693   Epoch: 2   Global Step: 74700   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-04 01:23:15,178-Speed 123.36 samples/sec   Loss 22.6408   LearningRate 0.087691   Epoch: 2   Global Step: 74710   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:23:36,004-Speed 122.93 samples/sec   Loss 21.7844   LearningRate 0.087689   Epoch: 2   Global Step: 74720   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:23:56,964-Speed 122.14 samples/sec   Loss 22.2266   LearningRate 0.087688   Epoch: 2   Global Step: 74730   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:24:17,736-Speed 123.25 samples/sec   Loss 22.3883   LearningRate 0.087686   Epoch: 2   Global Step: 74740   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:24:38,613-Speed 122.62 samples/sec   Loss 21.7693   LearningRate 0.087685   Epoch: 2   Global Step: 74750   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:24:59,470-Speed 122.74 samples/sec   Loss 22.2545   LearningRate 0.087683   Epoch: 2   Global Step: 74760   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:25:20,276-Speed 123.05 samples/sec   Loss 22.1600   LearningRate 0.087681   Epoch: 2   Global Step: 74770   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:25:41,203-Speed 122.34 samples/sec   Loss 22.0587   LearningRate 0.087680   Epoch: 2   Global Step: 74780   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:26:02,012-Speed 123.03 samples/sec   Loss 21.6951   LearningRate 0.087678   Epoch: 2   Global Step: 74790   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:26:22,810-Speed 123.10 samples/sec   Loss 22.3278   LearningRate 0.087676   Epoch: 2   Global Step: 74800   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:26:43,728-Speed 122.38 samples/sec   Loss 22.1681   LearningRate 0.087675   Epoch: 2   Global Step: 74810   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:27:04,514-Speed 123.17 samples/sec   Loss 21.9343   LearningRate 0.087673   Epoch: 2   Global Step: 74820   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:27:25,294-Speed 123.20 samples/sec   Loss 22.2249   LearningRate 0.087671   Epoch: 2   Global Step: 74830   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:27:46,148-Speed 122.76 samples/sec   Loss 22.2492   LearningRate 0.087670   Epoch: 2   Global Step: 74840   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:28:06,928-Speed 123.20 samples/sec   Loss 22.5494   LearningRate 0.087668   Epoch: 2   Global Step: 74850   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:28:27,725-Speed 123.10 samples/sec   Loss 22.0689   LearningRate 0.087666   Epoch: 2   Global Step: 74860   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:28:48,558-Speed 122.89 samples/sec   Loss 22.0451   LearningRate 0.087665   Epoch: 2   Global Step: 74870   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:29:09,285-Speed 123.51 samples/sec   Loss 22.3595   LearningRate 0.087663   Epoch: 2   Global Step: 74880   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:29:30,145-Speed 122.73 samples/sec   Loss 22.2283   LearningRate 0.087661   Epoch: 2   Global Step: 74890   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:29:51,052-Speed 122.45 samples/sec   Loss 22.5613   LearningRate 0.087660   Epoch: 2   Global Step: 74900   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:30:11,795-Speed 123.42 samples/sec   Loss 22.2605   LearningRate 0.087658   Epoch: 2   Global Step: 74910   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-04 01:30:32,595-Speed 123.08 samples/sec   Loss 21.9258   LearningRate 0.087657   Epoch: 2   Global Step: 74920   Fp16 Grad Scale: 262144   Required: 323 hours\nTraining: 2025-06-04 01:30:53,365-Speed 123.26 samples/sec   Loss 22.1547   LearningRate 0.087655   Epoch: 2   Global Step: 74930   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:31:14,156-Speed 123.14 samples/sec   Loss 22.1328   LearningRate 0.087653   Epoch: 2   Global Step: 74940   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:31:35,007-Speed 122.78 samples/sec   Loss 22.2342   LearningRate 0.087652   Epoch: 2   Global Step: 74950   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:31:55,799-Speed 123.13 samples/sec   Loss 21.7993   LearningRate 0.087650   Epoch: 2   Global Step: 74960   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:32:16,569-Speed 123.26 samples/sec   Loss 22.0838   LearningRate 0.087648   Epoch: 2   Global Step: 74970   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:32:37,432-Speed 122.71 samples/sec   Loss 21.6163   LearningRate 0.087647   Epoch: 2   Global Step: 74980   Fp16 Grad Scale: 131072   Required: 323 hours\nTraining: 2025-06-04 01:32:58,205-Speed 123.24 samples/sec   Loss 21.8890   LearningRate 0.087645   Epoch: 2   Global Step: 74990   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:33:19,010-Speed 123.05 samples/sec   Loss 22.8472   LearningRate 0.087643   Epoch: 2   Global Step: 75000   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:33:39,814-Speed 123.06 samples/sec   Loss 21.9971   LearningRate 0.087642   Epoch: 2   Global Step: 75010   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:34:00,579-Speed 123.29 samples/sec   Loss 22.0792   LearningRate 0.087640   Epoch: 2   Global Step: 75020   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:34:21,417-Speed 122.86 samples/sec   Loss 22.2971   LearningRate 0.087638   Epoch: 2   Global Step: 75030   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-04 01:34:42,237-Speed 122.96 samples/sec   Loss 22.1791   LearningRate 0.087637   Epoch: 2   Global Step: 75040   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:35:02,941-Speed 123.65 samples/sec   Loss 21.9883   LearningRate 0.087635   Epoch: 2   Global Step: 75050   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:35:23,802-Speed 122.72 samples/sec   Loss 22.0430   LearningRate 0.087633   Epoch: 2   Global Step: 75060   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:35:44,603-Speed 123.08 samples/sec   Loss 22.3017   LearningRate 0.087632   Epoch: 2   Global Step: 75070   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:36:05,365-Speed 123.30 samples/sec   Loss 22.3713   LearningRate 0.087630   Epoch: 2   Global Step: 75080   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:36:26,213-Speed 122.80 samples/sec   Loss 22.2613   LearningRate 0.087629   Epoch: 2   Global Step: 75090   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:36:47,000-Speed 123.16 samples/sec   Loss 22.1537   LearningRate 0.087627   Epoch: 2   Global Step: 75100   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:37:07,760-Speed 123.32 samples/sec   Loss 22.1932   LearningRate 0.087625   Epoch: 2   Global Step: 75110   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:37:28,576-Speed 122.99 samples/sec   Loss 22.5642   LearningRate 0.087624   Epoch: 2   Global Step: 75120   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:37:49,350-Speed 123.23 samples/sec   Loss 22.3775   LearningRate 0.087622   Epoch: 2   Global Step: 75130   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:38:10,164-Speed 123.00 samples/sec   Loss 22.3652   LearningRate 0.087620   Epoch: 2   Global Step: 75140   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-04 01:38:31,006-Speed 122.84 samples/sec   Loss 22.2794   LearningRate 0.087619   Epoch: 2   Global Step: 75150   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-04 01:38:51,777-Speed 123.26 samples/sec   Loss 22.0948   LearningRate 0.087617   Epoch: 2   Global Step: 75160   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-04 01:39:12,553-Speed 123.22 samples/sec   Loss 21.6238   LearningRate 0.087615   Epoch: 2   Global Step: 75170   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-04 01:39:33,413-Speed 122.73 samples/sec   Loss 22.0200   LearningRate 0.087614   Epoch: 2   Global Step: 75180   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-04 01:39:54,193-Speed 123.20 samples/sec   Loss 22.6094   LearningRate 0.087612   Epoch: 2   Global Step: 75190   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-04 01:40:15,002-Speed 123.03 samples/sec   Loss 22.3165   LearningRate 0.087610   Epoch: 2   Global Step: 75200   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:40:35,803-Speed 123.08 samples/sec   Loss 21.9929   LearningRate 0.087609   Epoch: 2   Global Step: 75210   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:40:56,522-Speed 123.56 samples/sec   Loss 22.1884   LearningRate 0.087607   Epoch: 2   Global Step: 75220   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:41:17,380-Speed 122.74 samples/sec   Loss 22.3524   LearningRate 0.087605   Epoch: 2   Global Step: 75230   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:41:38,180-Speed 123.08 samples/sec   Loss 22.0500   LearningRate 0.087604   Epoch: 2   Global Step: 75240   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:41:58,955-Speed 123.23 samples/sec   Loss 22.2770   LearningRate 0.087602   Epoch: 2   Global Step: 75250   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:42:19,832-Speed 122.63 samples/sec   Loss 21.8492   LearningRate 0.087601   Epoch: 2   Global Step: 75260   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:42:40,832-Speed 121.91 samples/sec   Loss 21.9658   LearningRate 0.087599   Epoch: 2   Global Step: 75270   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:43:01,810-Speed 122.04 samples/sec   Loss 22.2152   LearningRate 0.087597   Epoch: 2   Global Step: 75280   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:43:22,434-Speed 124.13 samples/sec   Loss 22.2422   LearningRate 0.087596   Epoch: 2   Global Step: 75290   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:43:43,199-Speed 123.29 samples/sec   Loss 22.5873   LearningRate 0.087594   Epoch: 2   Global Step: 75300   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-04 01:44:04,182-Speed 122.01 samples/sec   Loss 22.0381   LearningRate 0.087592   Epoch: 2   Global Step: 75310   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-04 01:44:24,845-Speed 123.90 samples/sec   Loss 21.9692   LearningRate 0.087591   Epoch: 2   Global Step: 75320   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:44:45,444-Speed 124.28 samples/sec   Loss 22.0060   LearningRate 0.087589   Epoch: 2   Global Step: 75330   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:45:06,087-Speed 124.02 samples/sec   Loss 22.0086   LearningRate 0.087587   Epoch: 2   Global Step: 75340   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:45:26,896-Speed 123.03 samples/sec   Loss 22.6090   LearningRate 0.087586   Epoch: 2   Global Step: 75350   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:45:47,855-Speed 122.15 samples/sec   Loss 21.7794   LearningRate 0.087584   Epoch: 2   Global Step: 75360   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:46:08,643-Speed 123.15 samples/sec   Loss 22.3811   LearningRate 0.087582   Epoch: 2   Global Step: 75370   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:46:29,413-Speed 123.26 samples/sec   Loss 22.2135   LearningRate 0.087581   Epoch: 2   Global Step: 75380   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:46:50,171-Speed 123.33 samples/sec   Loss 21.9398   LearningRate 0.087579   Epoch: 2   Global Step: 75390   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:47:10,757-Speed 124.36 samples/sec   Loss 22.1960   LearningRate 0.087577   Epoch: 2   Global Step: 75400   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:47:31,614-Speed 122.75 samples/sec   Loss 22.3437   LearningRate 0.087576   Epoch: 2   Global Step: 75410   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:47:52,548-Speed 122.29 samples/sec   Loss 22.0133   LearningRate 0.087574   Epoch: 2   Global Step: 75420   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-04 01:48:13,193-Speed 124.01 samples/sec   Loss 22.3022   LearningRate 0.087572   Epoch: 2   Global Step: 75430   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-04 01:48:33,871-Speed 123.81 samples/sec   Loss 22.2800   LearningRate 0.087571   Epoch: 2   Global Step: 75440   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-04 01:48:54,523-Speed 123.96 samples/sec   Loss 22.6788   LearningRate 0.087569   Epoch: 2   Global Step: 75450   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:49:15,166-Speed 124.02 samples/sec   Loss 21.8322   LearningRate 0.087568   Epoch: 2   Global Step: 75460   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:49:35,912-Speed 123.40 samples/sec   Loss 22.0299   LearningRate 0.087566   Epoch: 2   Global Step: 75470   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:49:56,688-Speed 123.23 samples/sec   Loss 21.8436   LearningRate 0.087564   Epoch: 2   Global Step: 75480   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:50:17,426-Speed 123.45 samples/sec   Loss 22.2132   LearningRate 0.087563   Epoch: 2   Global Step: 75490   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:50:38,266-Speed 122.84 samples/sec   Loss 21.8956   LearningRate 0.087561   Epoch: 2   Global Step: 75500   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:50:59,053-Speed 123.16 samples/sec   Loss 22.4425   LearningRate 0.087559   Epoch: 2   Global Step: 75510   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:51:19,834-Speed 123.19 samples/sec   Loss 22.4316   LearningRate 0.087558   Epoch: 2   Global Step: 75520   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:51:40,753-Speed 122.38 samples/sec   Loss 22.4235   LearningRate 0.087556   Epoch: 2   Global Step: 75530   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:52:01,507-Speed 123.36 samples/sec   Loss 22.2960   LearningRate 0.087554   Epoch: 2   Global Step: 75540   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:52:22,290-Speed 123.19 samples/sec   Loss 21.9700   LearningRate 0.087553   Epoch: 2   Global Step: 75550   Fp16 Grad Scale: 262144   Required: 322 hours\nTraining: 2025-06-04 01:52:43,250-Speed 122.14 samples/sec   Loss 22.2695   LearningRate 0.087551   Epoch: 2   Global Step: 75560   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:53:04,146-Speed 122.52 samples/sec   Loss 22.4789   LearningRate 0.087549   Epoch: 2   Global Step: 75570   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:53:24,901-Speed 123.35 samples/sec   Loss 21.7859   LearningRate 0.087548   Epoch: 2   Global Step: 75580   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:53:45,794-Speed 122.53 samples/sec   Loss 22.2868   LearningRate 0.087546   Epoch: 2   Global Step: 75590   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:54:06,581-Speed 123.16 samples/sec   Loss 22.3502   LearningRate 0.087544   Epoch: 2   Global Step: 75600   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:54:27,503-Speed 122.36 samples/sec   Loss 22.1854   LearningRate 0.087543   Epoch: 2   Global Step: 75610   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:54:48,364-Speed 122.72 samples/sec   Loss 22.2100   LearningRate 0.087541   Epoch: 2   Global Step: 75620   Fp16 Grad Scale: 131072   Required: 322 hours\nTraining: 2025-06-04 01:55:09,159-Speed 123.11 samples/sec   Loss 22.4021   LearningRate 0.087540   Epoch: 2   Global Step: 75630   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 01:55:30,054-Speed 122.52 samples/sec   Loss 21.9770   LearningRate 0.087538   Epoch: 2   Global Step: 75640   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 01:55:50,992-Speed 122.27 samples/sec   Loss 22.0996   LearningRate 0.087536   Epoch: 2   Global Step: 75650   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 01:56:11,784-Speed 123.13 samples/sec   Loss 22.2177   LearningRate 0.087535   Epoch: 2   Global Step: 75660   Fp16 Grad Scale: 262144   Required: 321 hours\nTraining: 2025-06-04 01:56:32,656-Speed 122.66 samples/sec   Loss 21.8367   LearningRate 0.087533   Epoch: 2   Global Step: 75670   Fp16 Grad Scale: 262144   Required: 321 hours\nTraining: 2025-06-04 01:56:53,605-Speed 122.21 samples/sec   Loss 21.9162   LearningRate 0.087531   Epoch: 2   Global Step: 75680   Fp16 Grad Scale: 262144   Required: 321 hours\nTraining: 2025-06-04 01:57:14,409-Speed 123.06 samples/sec   Loss 22.1735   LearningRate 0.087530   Epoch: 2   Global Step: 75690   Fp16 Grad Scale: 262144   Required: 321 hours\nTraining: 2025-06-04 01:57:35,312-Speed 122.47 samples/sec   Loss 22.5800   LearningRate 0.087528   Epoch: 2   Global Step: 75700   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 01:57:56,170-Speed 122.74 samples/sec   Loss 22.2070   LearningRate 0.087526   Epoch: 2   Global Step: 75710   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 01:58:17,038-Speed 122.68 samples/sec   Loss 22.2524   LearningRate 0.087525   Epoch: 2   Global Step: 75720   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 01:58:38,003-Speed 122.11 samples/sec   Loss 22.0630   LearningRate 0.087523   Epoch: 2   Global Step: 75730   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 01:58:58,800-Speed 123.10 samples/sec   Loss 22.1573   LearningRate 0.087521   Epoch: 2   Global Step: 75740   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 01:59:19,647-Speed 122.80 samples/sec   Loss 22.0942   LearningRate 0.087520   Epoch: 2   Global Step: 75750   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 01:59:40,604-Speed 122.16 samples/sec   Loss 22.7806   LearningRate 0.087518   Epoch: 2   Global Step: 75760   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:00:01,412-Speed 123.04 samples/sec   Loss 22.5901   LearningRate 0.087516   Epoch: 2   Global Step: 75770   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:00:22,212-Speed 123.08 samples/sec   Loss 21.9943   LearningRate 0.087515   Epoch: 2   Global Step: 75780   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:00:43,168-Speed 122.17 samples/sec   Loss 22.1542   LearningRate 0.087513   Epoch: 2   Global Step: 75790   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:01:03,925-Speed 123.34 samples/sec   Loss 22.0637   LearningRate 0.087512   Epoch: 2   Global Step: 75800   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:01:24,786-Speed 122.72 samples/sec   Loss 22.0623   LearningRate 0.087510   Epoch: 2   Global Step: 75810   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:01:45,641-Speed 122.76 samples/sec   Loss 21.9025   LearningRate 0.087508   Epoch: 2   Global Step: 75820   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:02:06,437-Speed 123.11 samples/sec   Loss 22.4081   LearningRate 0.087507   Epoch: 2   Global Step: 75830   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:02:27,342-Speed 122.46 samples/sec   Loss 22.0714   LearningRate 0.087505   Epoch: 2   Global Step: 75840   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:02:48,206-Speed 122.70 samples/sec   Loss 22.6073   LearningRate 0.087503   Epoch: 2   Global Step: 75850   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:03:09,125-Speed 122.38 samples/sec   Loss 22.1147   LearningRate 0.087502   Epoch: 2   Global Step: 75860   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:03:30,103-Speed 122.04 samples/sec   Loss 22.2872   LearningRate 0.087500   Epoch: 2   Global Step: 75870   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:03:51,005-Speed 122.48 samples/sec   Loss 22.3895   LearningRate 0.087498   Epoch: 2   Global Step: 75880   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:04:11,817-Speed 123.01 samples/sec   Loss 22.3942   LearningRate 0.087497   Epoch: 2   Global Step: 75890   Fp16 Grad Scale: 131072   Required: 321 hours\nTraining: 2025-06-04 02:04:32,594-Speed 123.22 samples/sec   Loss 22.1861   LearningRate 0.087495   Epoch: 2   Global Step: 75900   Fp16 Grad Scale: 262144   Required: 321 hours\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation with dataset","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/Arcface_torch/dataset_evaluation.py\nimport logging\nimport os\nimport argparse\nimport torch\n\nfrom backbones import get_model\nfrom eval import verification\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n\n\ndef evaluate_dataset(bin_path: str, model: torch.nn.Module, image_size=(112, 112)):\n    \"\"\"\n    Evaluate the dataset and log the results.\n    \"\"\"\n\n    dataset_name = os.path.basename(bin_path).split('.')[0]\n\n    # Load dataset\n    dataset = verification.load_bin(bin_path, image_size)\n    dataset = ([img.to(device) for img in dataset[0]], dataset[1])\n    \n    model.to(device)\n    model.eval()\n    # Perform evaluation\n    acc1, std1, acc2, std2, xnorm, embeddings_list = verification.test(dataset, model, 1, 10)\n\n    # Log results\n    # Log results\n    print(f\"[{dataset_name}] XNorm: {xnorm:.6f}\")\n    print(f\"[{dataset_name}] Accuracy (Flip): {acc2:.5f} ± {std2:.5f}\")\n    print(f\"[{dataset_name}] Highest Accuracy: {acc1:.5f}\")\n\n\n\n    return acc1, std1, acc2, std2, xnorm, embeddings_list\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Evaluate a face recognition dataset.\")\n    \n    # Command-line arguments\n    parser.add_argument(\"--image_size\", type=int, nargs=2, default=(64, 64), help=\"Image size (width height).\")\n    parser.add_argument(\"--model_name\", type=str, default = 'r100', help=\"backbone model name.\")\n    parser.add_argument(\"--model_path\", type=str, default = 'weights/glint360k_cosface_r100_fp16_0.1/backbone.pth', help=\"backbone model path.\")\n    args = parser.parse_args()\n\n    model = get_model(args.model_name, fp16=False, image_size = args.image_size[0])\n    model.load_state_dict(torch.load(args.model_path, map_location=device, weights_only = True))\n\n    bin_folder = '/kaggle/working/VN-celeb-mini'\n    for bin_name in os.listdir(bin_folder):\n        bin_path = os.path.join(bin_folder, bin_name)\n        if bin_path.endswith('bin'):\n            print(bin_name)\n            evaluate_dataset(bin_path, model, tuple(args.image_size))\n            print(\"__\"*20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/Arcface_torch\n!python dataset_evaluation.py\\\n--model_path /kaggle/working/Arcface_torch/VN-celeb-mini_112x112_r18_plus_workdirs_ver2_img_size_128/model.pt\\\n--model_name 'r18_plus'\\\n--image_size 128 128","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/Arcface_torch/calc_threshold.py\nimport pickle\nimport torch\nimport mxnet as mx\nfrom mxnet import ndarray as nd\nimport numpy as np\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nfrom models.Recognition.Arcface_torch.backbones import get_model\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n\n# Thiết bị\ndevice = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n\n# Tải mô hình\nnet = get_model(\"r100\", fp16=False)\nnet.load_state_dict(torch.load(\"models/Recognition/Arcface_torch/weights/glint360k_cosface_r100_fp16_0.1/backbone.pth\", map_location=device))\nnet.to(device)\nnet.eval()\n\n# Hàm tải dữ liệu từ file .bin với chuẩn hóa\ndef load_bin(path, image_size):\n    try:\n        with open(path, 'rb') as f:\n            bins, issame_list = pickle.load(f)  # py2\n    except UnicodeDecodeError as e:\n        with open(path, 'rb') as f:\n            bins, issame_list = pickle.load(f, encoding='bytes')  # py3\n    data_list = []\n    for flip in [0, 1]:\n        data = torch.empty((len(issame_list) * 2, 3, image_size[0], image_size[1]))\n        data_list.append(data)\n    for idx in range(len(issame_list) * 2):\n        _bin = bins[idx]\n        img = mx.image.imdecode(_bin)\n        if img.shape[1] != image_size[0]:\n            img = mx.image.resize_short(img, image_size[0])\n        img = nd.transpose(img, axes=(2, 0, 1))  # [C, H, W]\n        img = img.astype(np.float32) / 255.0  # Chuẩn hóa về [0, 1]\n        for flip in [0, 1]:\n            if flip == 1:\n                img = mx.ndarray.flip(data=img, axis=2)\n            data_list[flip][idx][:] = torch.from_numpy(img.asnumpy())\n        if idx % 1000 == 0:\n            print('loading bin', idx)\n    print(data_list[0].shape)\n    return data_list, issame_list  # Dùng toàn bộ dataset\n\n# Tải dữ liệu (toàn bộ lfw.bin: 12000 ảnh, 6000 cặp)\ndata_list, issame_list = load_bin('/kaggle/input/ms1m-retinaface-t1/ms1m-retinaface-t1/lfw.bin', (112, 112))\n\n# Hàm tính embedding\ndef get_embeddings(data_list, model, batch_size=512):\n    num_samples = data_list[0].shape[0]  # Số mẫu đầy đủ (12000)\n    embeddings_flip0 = []\n    \n    model.eval()\n    with torch.no_grad():\n        for i in tqdm(range(0, num_samples, batch_size), desc=\"Processing flip=0\"):\n            batch = data_list[0][i:i + batch_size].to(device)\n            emb = model(batch)\n            embeddings_flip0.append(emb)\n    \n    embeddings_flip0 = torch.cat(embeddings_flip0, dim=0)  # Toàn bộ embedding\n    return embeddings_flip0\n\n# Tính embedding\nembeddings = get_embeddings(data_list, net, batch_size=512)\nprint(\"Embeddings shape:\", embeddings.shape)  # Dự kiến: [12000, 512]\n\n# Hàm tính khoảng cách\ndef compute_distances(embeddings, issame_list):\n    l2_distances = []\n    cosine_distances = []\n    labels = []\n\n    for i in range(len(issame_list)):  # Lặp qua toàn bộ 6000 cặp\n        emb1 = embeddings[2 * i]\n        emb2 = embeddings[2 * i + 1]\n        l2_dist = torch.norm(emb1 - emb2, p=2).item()\n        l2_distances.append(l2_dist)\n        cosine_sim = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n        cosine_dist = 1 - cosine_sim\n        cosine_distances.append(cosine_dist)\n        labels.append(issame_list[i])\n\n    l2_distances = np.array(l2_distances)\n    cosine_distances = np.array(cosine_distances)\n    labels = np.array(labels)\n    return l2_distances, cosine_distances, labels\n\n# Tính khoảng cách\nl2_distances, cosine_distances, labels = compute_distances(embeddings, issame_list)\n\nprint(\"L2 distances shape:\", l2_distances.shape)  # Dự kiến: (6000,)\nprint(\"Cosine distances shape:\", cosine_distances.shape)  # Dự kiến: (6000,)\nprint(\"Labels shape:\", labels.shape)  # Dự kiến: (6000,)\n\n# Phân tích L2\nl2_same_class_distances = l2_distances[labels == True]\nl2_diff_class_distances = l2_distances[labels == False]\nprint(\"Mean distance L2 (same class):\", l2_same_class_distances.mean())\nprint(\"Mean distance L2 (diff class):\", l2_diff_class_distances.mean())\nl2_threshold = (l2_same_class_distances.mean() + l2_diff_class_distances.mean()) / 2\nprint(\"Simple L2 threshold:\", l2_threshold)\n\n# Phân tích Cosine\ncosine_same_class_distances = cosine_distances[labels == True]\ncosine_diff_class_distances = cosine_distances[labels == False]\nprint(\"Mean distance Cosine (same class):\", cosine_same_class_distances.mean())\nprint(\"Mean distance Cosine (diff class):\", cosine_diff_class_distances.mean())\ncosine_threshold = (cosine_same_class_distances.mean() + cosine_diff_class_distances.mean()) / 2\nprint(\"Simple Cosine threshold:\", cosine_threshold)\n\n# Dùng ROC Curve cho L2\nfpr, tpr, thresholds = roc_curve(labels, l2_distances, pos_label=0)\nroc_auc = auc(fpr, tpr)\noptimal_idx = np.argmax(tpr - fpr)\noptimal_threshold = thresholds[optimal_idx]\nprint(\"Optimal L2 threshold (ROC):\", optimal_threshold)\nprint(\"L2 AUC:\", roc_auc)\n\n# Vẽ ROC Curve cho L2\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for L2 Distance')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Dùng ROC Curve cho Cosine\nfpr, tpr, thresholds = roc_curve(labels, cosine_distances, pos_label=0)\nroc_auc = auc(fpr, tpr)\noptimal_idx = np.argmax(tpr - fpr)\noptimal_threshold = thresholds[optimal_idx]\nprint(\"Optimal Cosine threshold (ROC):\", optimal_threshold)\nprint(\"Cosine AUC:\", roc_auc)\n\n# Vẽ ROC Curve cho Cosine\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Cosine Distance')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python calc_threshold.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_point = torch.load('/kaggle/working/Arcface_torch/ms1m_retinaface_t1_workdirs/checkpoint_gpu_1.pt')\nprint(check_point['epoch'])\nprint(check_point['global_step'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cd /kaggle/working/Arcface_torch/ms1m_retinaface_t1_workdirs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/faces_webface_112x112_r50_se_workdirs.zip /kaggle/working/Arcface_torch/faces_webface_112x112_r50_se_workdirs","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}